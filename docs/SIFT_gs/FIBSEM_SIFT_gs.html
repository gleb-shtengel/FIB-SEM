<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>SIFT_gs.FIBSEM_SIFT_gs API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>SIFT_gs.FIBSEM_SIFT_gs</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
use_cp = False
if use_cp:
    import cupy as cp
import pandas as pd
import os
from pathlib import Path
import time
import glob
import re

import matplotlib
import matplotlib.image as mpimg
from matplotlib import pylab, mlab, pyplot
plt = pyplot
from IPython.core.pylabtools import figsize, getfigs
from pylab import *
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image as PILImage
from PIL.TiffTags import TAGS

from struct import *
#from tqdm import tqdm_notebook as tqdm
from tqdm.notebook import tqdm

import skimage
#print(skimage.__version__)
from skimage.measure import ransac
from skimage.transform import ProjectiveTransform, AffineTransform, EuclideanTransform, warp
try:
    import skimage.external.tifffile as tiff
except:
    import tifffile as tiff
from scipy.signal import savgol_filter
from scipy import ndimage
from scipy.signal import convolve2d
from scipy.ndimage import gaussian_filter

from sklearn.linear_model import (LinearRegression,
    TheilSenRegressor,
    RANSACRegressor,
    HuberRegressor)
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline


import dask
import dask.array as da
from dask.distributed import Client, progress, get_task_stream
from dask.diagnostics import ProgressBar

import cv2
print(&#39;Open CV version: &#39;, cv2. __version__)
import mrcfile
import h5py
import npy2bdv
import pickle
import webbrowser
from IPython.display import IFrame

EPS = np.finfo(float).eps

import warnings
warnings.filterwarnings(&#34;ignore&#34;, category=DeprecationWarning)



######################################################
#    General Help Functions
######################################################
def get_spread(data, window=501, porder=3):
    &#39;&#39;&#39;
    Calculates spread - standard deviation of the (signal - Sav-Gol smoothed signal).
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

    Parameters:
    data : 1D array
    window : int
        aperture (number of points) for Sav-Gol filter)
    porder : int
        polynomial order for Sav-Gol filter

    Returns:
        data_spread : float

    &#39;&#39;&#39;

    try:
        #sm_data = savgol_filter(data.astype(double), window, porder)
        sm_data = savgol_filter(data.astype(double), window, porder, mode=&#39;mirror&#39;)
        data_spread = np.std(data-sm_data)
    except :
        print(&#39;spread error&#39;)
        data_spread = 0.0
    return data_spread


def get_min_max_thresholds(image, **kwargs):
    &#39;&#39;&#39;
    Determines the data range (min and max) with given fractional thresholds for cumulative distribution.
    ©G.Shtengel 11/2022 gleb.shtengel@gmail.com

    Calculates the histogram of pixel intensities of the image with number of bins determined by parameter nbins (default = 256)
    and normalizes it to get the probability distribution function (PDF), from which a cumulative distribution function (CDF) is calculated.
    Then given the thr_min, thr_max parameters, the minimum and maximum values of the data range are found
    by determining the intensities at which CDF= thr_min and (1- thr_max), respectively

    Parameters:
    ----------
    image : 2D array
        Image to be analyzed

    kwargs:
     ----------
    thr_min : float
        lower CDF threshold for determining the minimum data value. Default is 1.0e-3
    thr_max : float
        upper CDF threshold for determining the maximum data value. Default is 1.0e-3
    nbins : int
        number of histogram bins for building the PDF and CDF
    log  : bolean
        If True, the histogram will have log scale. Default is false
    disp_res : bolean
        If True display the results. Default is True.
    save_res : boolean
        If True the image will be saved. Default is False.
    dpi : int
        Default is 300
    save_filename : string
        the name of the image to perform this operations (defaulut is &#39;min_max_thresholds.png&#39;).

    Returns
    (dmin, dmax) : float array
    &#39;&#39;&#39;
    thr_min = kwargs.get(&#39;thr_min&#39;, 1.0e-3)
    thr_max = kwargs.get(&#39;thr_max&#39;, 1.0e-3)
    nbins = kwargs.get(&#39;nbins&#39;, 256)
    disp_res = kwargs.get(&#39;disp_res&#39;, True)
    log = kwargs.get(&#39;log&#39;, False)
    save_res = kwargs.get(&#39;save_res&#39;, False)
    dpi = kwargs.get(&#39;dpi&#39;, 300)
    save_filename = kwargs.get(&#39;save_filename&#39;, &#39;min_max_thresholds.png&#39;)

    if disp_res:
        fsz=11
        fig, axs = subplots(2,1, figsize = (6,8))
        hist, bins, patches = axs[0].hist(image.ravel(), bins=nbins, log=log)
    else:
        hist, bins = np.histogram(image.ravel(), bins=nbins)
    pdf = hist / np.prod(image.shape)
    cdf = np.cumsum(pdf)
    data_max = bins[argmin(abs(cdf-(1.0-thr_max)))]
    data_min = bins[argmin(abs(cdf-thr_min))]

    if disp_res:
        xCDF = bins[0:-1]+(bins[1]-bins[0])/2.0
        xthr = [xCDF[0], xCDF[-1]]
        ythr_min = [thr_min, thr_min]
        ythr_max = [1-thr_max, 1-thr_max]
        axs[1].plot(xCDF, cdf, label=&#39;CDF&#39;)
        axs[1].plot(xthr, ythr_min, &#39;r&#39;, label=&#39;thr_min={:.5f}&#39;.format(thr_min))
        axs[1].plot(xthr, ythr_max, &#39;g&#39;, label=&#39;1.0 - thr_max = {:.5f}&#39;.format(1-thr_max))
        axs[1].set_xlabel(&#39;Intensity Level&#39;, fontsize = fsz)
        axs[0].set_ylabel(&#39;PDF&#39;, fontsize = fsz)
        axs[1].set_ylabel(&#39;CDF&#39;, fontsize = fsz)
        xi = data_min - (np.abs(data_max-data_min)/2)
        xa = data_max + (np.abs(data_max-data_min)/2)
        rys = [[0, np.max(hist)], [0, 1]]
        for ax, ry in zip(axs, rys):
            ax.plot([data_min, data_min], ry, &#39;r&#39;, linestyle = &#39;--&#39;, label = &#39;data_min={:.1f}&#39;.format(data_min))
            ax.plot([data_max, data_max], ry, &#39;g&#39;, linestyle = &#39;--&#39;, label = &#39;data_max={:.1f}&#39;.format(data_max))
            ax.set_xlim(xi, xa)
            ax.grid(True)
        axs[1].legend(loc=&#39;center&#39;, fontsize=fsz)
        axs[1].set_title(&#39;Data Min and max with thr_min={:.0e},  thr_max={:.0e}&#39;.format(thr_min, thr_max), fontsize = fsz)
        if save_res:
            fig.savefig(save_filename, dpi=dpi)
    return np.array((data_min, data_max))

def argmax2d(X):
    return np.unravel_index(X.argmax(), X.shape)


def find_BW(fr, FSC, SNRt):
    npts = np.shape(FSC)[0]*0.75
    j = 15
    while (j&lt;npts-1) and FSC[j]&gt;SNRt:
        j = j+1
    BW = fr[j-1] + (fr[j]-fr[j-1])*(SNRt-FSC[j-1])/(FSC[j]-FSC[j-1])
    return BW


def radial_profile(data, center):
    &#39;&#39;&#39;
    Calculates radially average profile of the 2D array (used for FRC and auto-correlation)
    ©G.Shtengel 08/2020 gleb.shtengel@gmail.com

    Parameters:
    data : 2D array

    center : list of two ints
        [xcenter, ycenter]

    Returns
        radialprofile : float array
            limited to x-size//2 of the input data
    &#39;&#39;&#39;
    ysz, xsz = data.shape
    y, x = np.indices((data.shape))
    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)
    r = r.astype(int)
    tbin = np.bincount(r.ravel(), data.ravel())
    nr = np.bincount(r.ravel())
    radialprofile = (np.array(tbin) / nr)[0:(xsz//2+1)]
    return radialprofile


def radial_profile_select_angles(data, center, astart = 89, astop = 91, symm=4):
    &#39;&#39;&#39;
    Calculates radially average profile of the 2D array (used for FRC) within a select range of angles.
    ©G.Shtengel 08/2020 gleb.shtengel@gmail.com

    Parameters:
    data : 2D array
    center : list of two ints
        [xcenter, ycenter]

    astart : float
        Start angle for radial averaging. Default is 0
    astop : float
        Stop angle for radial averaging. Default is 90
    symm : int
        Symmetry factor (how many times Start and stop angle intervalks are repeated within 360 deg). Default is 4.


    Returns
        radialprofile : float array
            limited to x-size//2 of the input data
    &#39;&#39;&#39;
    y, x = np.indices((data.shape))
    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)
    r_ang = (np.angle(x - center[0]+1j*(y - center[1]), deg=True)).ravel()
    r = r.astype(np.int)

    if symm&gt;0:
        ind = np.squeeze(np.array(np.where((r_ang &gt; astart) &amp; (r_ang &lt; astart))))
        for i in np.arange(symm):
            ai = astart +360.0/symm*i -180.0
            aa = astop +360.0/symm*i -180.0
            ind = np.concatenate((ind, np.squeeze((np.array(np.where((r_ang &gt; ai) &amp; (r_ang &lt; aa)))))), axis=0)
    else:
        ind = np.squeeze(np.where((r_ang &gt; astart) &amp; (r_ang &lt; astop)))

    rr = np.ravel(r)[ind]
    dd = np.ravel(data)[ind]

    tbin = np.bincount(rr, dd)
    nr = np.bincount(rr)
    radialprofile = tbin / nr
    return radialprofile


def smooth(x, window_len=11, window=&#39;hanning&#39;):
    &#34;&#34;&#34;smooth the data using a window with requested size.

    This method is based on the convolution of a scaled window with the signal.
    The signal is prepared by introducing reflected copies of the signal
    (with the window size) in both ends so that transient parts are minimized
    in the begining and end part of the output signal.

    input:
        x: the input signal
        window_len: the dimension of the smoothing window; should be an odd integer
        window: the type of window from &#39;flat&#39;, &#39;hanning&#39;, &#39;hamming&#39;, &#39;bartlett&#39;, &#39;blackman&#39;
            flat window will produce a moving average smoothing.

    output:
        the smoothed signal

    example:

    t=linspace(-2,2,0.1)
    x=sin(t)+randn(len(t))*0.1
    y=smooth(x)

    see also:

    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve
    scipy.signal.lfilter

    TODO: the window parameter could be the window itself if an array instead of a string
    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.
    &#34;&#34;&#34;

    if x.ndim != 1:
        raise ValueError(&#34;smooth only accepts 1 dimension arrays.&#34;)

    if x.size &lt; window_len:
        raise ValueError(&#34;Input vector needs to be bigger than window size.&#34;)


    if window_len&lt;3:
        return x


    if not window in [&#39;flat&#39;, &#39;hanning&#39;, &#39;hamming&#39;, &#39;bartlett&#39;, &#39;blackman&#39;]:
        raise ValueError(&#34;Window is on of &#39;flat&#39;, &#39;hanning&#39;, &#39;hamming&#39;, &#39;bartlett&#39;, &#39;blackman&#39;&#34;)

    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]

    if window == &#39;flat&#39;: #moving average
        w=np.ones(window_len,&#39;d&#39;)
    else:
        w=eval(&#39;np.&#39;+window+&#39;(window_len)&#39;)

    y=np.convolve(w/w.sum(),s,mode=&#39;valid&#39;)
    return y[(window_len//2-1):-(window_len//2)]


################################################
#      Single Frame Image Processing Functions
################################################
def _center_and_normalize_points_gs(points):
    &#34;&#34;&#34;Center and normalize image points.

    The points are transformed in a two-step procedure that is expressed
    as a transformation matrix. The matrix of the resulting points is usually
    better conditioned than the matrix of the original points.
    Center the image points, such that the new coordinate system has its
    origin at the centroid of the image points.
    Normalize the image points, such that the mean distance from the points
    to the origin of the coordinate system is sqrt(D).
    If the points are all identical, the returned values will contain nan.

    Parameters:
    ----------
    points : (N, D) array
        The coordinates of the image points.
    Returns:
    -------
    matrix : (D+1, D+1) array
        The transformation matrix to obtain the new points.
    new_points : (N, D) array
        The transformed image points.
    References
    ----------
    .. [1] Hartley, Richard I. &#34;In defense of the eight-point algorithm.&#34;
           Pattern Analysis and Machine Intelligence, IEEE Transactions on 19.6
           (1997): 580-593.
    &#34;&#34;&#34;
    n, d = points.shape
    centroid = np.mean(points, axis=0)

    centered = points - centroid
    rms = np.sqrt(np.sum(centered ** 2) / n)

    # if all the points are the same, the transformation matrix cannot be
    # created. We return an equivalent matrix with np.nans as sentinel values.
    # This obviates the need for try/except blocks in functions calling this
    # one, and those are only needed when actual 0 is reached, rather than some
    # small value; ie, we don&#39;t need to worry about numerical stability here,
    # only actual 0.
    if rms == 0:
        return np.full((d + 1, d + 1), np.nan), np.full_like(points, np.nan)

    norm_factor = np.sqrt(d) / rms

    part_matrix = norm_factor * np.concatenate(
            (np.eye(d), -centroid[:, np.newaxis]), axis=1
            )
    matrix = np.concatenate(
            (part_matrix, [[0,] * d + [1]]), axis=0
            )

    points_h = np.row_stack([points.T, np.ones(n)])

    new_points_h = (matrix @ points_h).T

    new_points = new_points_h[:, :d]
    new_points /= new_points_h[:, d:]

    return matrix, new_points


def Single_Image_SNR(img, **kwargs):
    &#39;&#39;&#39;
    Estimates SNR based on a single image.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com
    Calculates SNR of a single image base on auto-correlation analysis after [1].

    Parameters
    ---------
    img : 2D array

    kwargs:
    edge_fraction : float
        fraction of the full autocetrrelation range used to calculate the &#34;mean value&#34; (default is 0.10)
    extrapolate_signal : boolean
        extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
    disp_res : boolean
        display results (plots) (default is True)
    save_res_png : boolean
        save the analysis output into a PNG file (default is True)
    res_fname : string
        filename for the result image (&#39;SNR_result.png&#39;)
    img_label : string
        optional image label
    dpi : int
        dots-per-inch resolution for the output image

    Returns:
        xSNR, ySNR, rSNR : float, float, float
            SNR determined using the method in [1] along X- and Y- directions.
            If there is a direction with slow varying data - that direction provides more accurate SNR estimate
            Y-streaks in typical FIB-SEM data provide slow varying Y-component because streaks
            usually get increasingly worse with increasing Y.
            So for typical FIB-SEM data use ySNR

    [1] J. T. L. Thong et al, Single-image signal-to-noise ratio estimation. Scanning, 328–336 (2001).
    &#39;&#39;&#39;
    edge_fraction = kwargs.get(&#34;edge_fraction&#34;, 0.10)
    extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
    thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
    save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;SNR_results.png&#39;)
    img_label = kwargs.get(&#34;img_label&#34;, &#39;Orig. Image&#39;)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    #first make image size even
    ysz, xsz = img.shape
    img = img[0:ysz//2*2, 0:xsz//2*2]
    ysz, xsz = img.shape

    xy_ratio = xsz/ysz
    data_FT = fftshift(fftn(ifftshift(img-img.mean())))
    data_FC = (np.multiply(data_FT,np.conj(data_FT)))/xsz/ysz
    data_ACR = np.abs(fftshift(fftn(ifftshift(data_FC))))
    data_ACR_peak = data_ACR[ysz//2, xsz//2]
    data_ACR_log = np.log(data_ACR)
    data_ACR = data_ACR / data_ACR_peak
    radial_ACR = radial_profile(data_ACR, [xsz//2, ysz//2])
    r_ACR = np.concatenate((radial_ACR[::-1], radial_ACR[1:-1]))

    #rsz = xsz
    rsz = len(r_ACR)
    rcr = np.linspace(-rsz//2, rsz//2-1, rsz)
    xcr = np.linspace(-xsz//2, xsz//2-1, xsz)
    ycr = np.linspace(-ysz//2, ysz//2-1, ysz)

    xl = xcr[xsz//2-2:xsz//2]
    xacr_left = data_ACR[ysz//2, (xsz//2-2):(xsz//2)]
    xc = xcr[xsz//2]
    xr = xcr[xsz//2+1 : xsz//2+3]
    xacr_right = data_ACR[ysz//2, (xsz//2+1):(xsz//2+3)]
    if extrapolate_signal:
        xNFacl = xacr_left[0] + (xc-xl[0])/(xl[1]-xl[0])*(xacr_left[1]-xacr_left[0])
        xNFacr = xacr_right[0] + (xc-xr[0])/(xr[1]-xr[0])*(xacr_right[1]-xacr_right[0])
    else:
        xNFacl = xacr_left[1]
        xNFacr = xacr_right[0]
    x_left = xcr[xsz//2-2:xsz//2+1]
    xacr_left = np.concatenate((xacr_left, np.array([xNFacl])))
    x_right = xcr[xsz//2 : xsz//2+3]
    xacr_right = np.concatenate((np.array([xNFacr]), xacr_right))

    yl = ycr[ysz//2-2:ysz//2]
    yacr_left = data_ACR[(ysz//2-2):(ysz//2), xsz//2]
    yc = ycr[ysz//2]
    yr = ycr[ysz//2+1 : ysz//2+3]
    yacr_right = data_ACR[(ysz//2+1):(ysz//2+3), xsz//2]
    if extrapolate_signal:
        yNFacl = yacr_left[0] + (yc-yl[0])/(yl[1]-yl[0])*(yacr_left[1]-yacr_left[0])
        yNFacr = yacr_right[0] + (yc-yr[0])/(yr[1]-yr[0])*(yacr_right[1]-yacr_right[0])
    else:
        yNFacl = yacr_left[1]
        yNFacr = yacr_right[0]
    y_left = ycr[ysz//2-2:ysz//2+1]
    yacr_left = np.concatenate((yacr_left, np.array([yNFacl])))
    y_right = ycr[ysz//2 : ysz//2+3]
    yacr_right = np.concatenate((np.array([yNFacr]), yacr_right))

    rl = rcr[rsz//2-2:rsz//2]
    racr_left = r_ACR[(rsz//2-2):(rsz//2)]
    rc = rcr[rsz//2]
    rr = rcr[rsz//2+1 : rsz//2+3]
    racr_right = r_ACR[(rsz//2+1):(rsz//2+3)]
    if extrapolate_signal:
        rNFacl = racr_left[0] + (rc-rl[0])/(rl[1]-rl[0])*(racr_left[1]-racr_left[0])
        rNFacr = racr_right[0] + (rc-rr[0])/(rr[1]-rr[0])*(racr_right[1]-racr_right[0])
    else:
        rNFacl = racr_left[1]
        rNFacr = racr_right[0]
    r_left = rcr[rsz//2-2:rsz//2+1]
    racr_left = np.concatenate((racr_left, np.array([rNFacl])))
    r_right = rcr[rsz//2 : rsz//2+3]
    racr_right = np.concatenate((np.array([rNFacr]), racr_right))

    x_acr = data_ACR[ysz//2, xsz//2]
    x_noise_free_acr = xacr_right[0]
    xedge = int32(xsz*edge_fraction)
    x_mean_value = np.mean(data_ACR[ysz//2, 0:xedge])
    xx_mean_value = np.linspace(-xsz//2, (-xsz//2+xedge-1), xedge)
    yedge = int32(ysz*edge_fraction)
    y_acr = data_ACR[ysz//2, xsz//2]
    y_noise_free_acr = yacr_right[0]
    y_mean_value = np.mean(data_ACR[0:yedge, xsz//2])
    yy_mean_value = np.linspace(-ysz//2, (-ysz//2+yedge-1), yedge)
    redge = int32(rsz*edge_fraction)
    r_acr = data_ACR[ysz//2, xsz//2]
    r_noise_free_acr = racr_right[0]
    r_mean_value = np.mean(r_ACR[0:redge])
    rr_mean_value = np.linspace(-rsz//2, (-rsz//2+redge-1), redge)

    xSNR = (x_noise_free_acr-x_mean_value)/(x_acr - x_noise_free_acr)
    ySNR = (y_noise_free_acr-y_mean_value)/(y_acr - y_noise_free_acr)
    rSNR = (r_noise_free_acr-r_mean_value)/(r_acr - r_noise_free_acr)
    if disp_res:
        fs=12

        if xy_ratio &lt; 2.5:
            fig, axs = subplots(1,4, figsize = (20, 5))
        else:
            fig = plt.figure(figsize = (20, 5))
            ax0 = fig.add_subplot(2, 2, 1)
            ax1 = fig.add_subplot(2, 2, 3)
            ax2 = fig.add_subplot(1, 4, 3)
            ax3 = fig.add_subplot(1, 4, 4)
            axs = [ax0, ax1, ax2, ax3]
        fig.subplots_adjust(left=0.03, bottom=0.06, right=0.99, top=0.92, wspace=0.25, hspace=0.10)

        range_disp = get_min_max_thresholds(img, thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res=False)
        axs[0].imshow(img, cmap=&#39;Greys&#39;, vmin=range_disp[0], vmax=range_disp[1])
        axs[0].grid(True)
        axs[0].set_title(img_label)
        if save_res_png:
            axs[0].text(0, 1.1 + (xy_ratio-1.0)/20.0, res_fname, transform=axs[0].transAxes)
        axs[1].imshow(data_ACR_log, extent=[-xsz//2-1, xsz//2, -ysz//2-1, ysz//2])
        axs[1].grid(True)
        axs[1].set_title(&#39;Autocorrelation (log scale)&#39;)

        axs[2].plot(xcr, data_ACR[ysz//2, :], &#39;r&#39;, linewidth =0.5, label=&#39;X&#39;)
        axs[2].plot(ycr, data_ACR[:, xsz//2], &#39;b&#39;, linewidth =0.5, label=&#39;Y&#39;)
        axs[2].plot(rcr, r_ACR, &#39;g&#39;, linewidth =0.5, label=&#39;R&#39;)
        axs[2].plot(xx_mean_value, xx_mean_value*0 + x_mean_value, &#39;r--&#39;, linewidth =2.0, label=&#39;&lt;X&gt;={:.5f}&#39;.format(x_mean_value))
        axs[2].plot(yy_mean_value, yy_mean_value*0 + y_mean_value, &#39;b--&#39;, linewidth =2.0, label=&#39;&lt;Y&gt;={:.5f}&#39;.format(y_mean_value))
        axs[2].plot(rr_mean_value, rr_mean_value*0 + r_mean_value, &#39;g--&#39;, linewidth =2.0, label=&#39;&lt;R&gt;={:.5f}&#39;.format(r_mean_value))
        axs[2].grid(True)
        axs[2].legend()
        axs[2].set_title(&#39;Normalized autocorr. cross-sections&#39;)
        axs[3].plot(xcr, data_ACR[ysz//2, :], &#39;rx&#39;, label=&#39;X data&#39;)
        axs[3].plot(ycr, data_ACR[:, xsz//2], &#39;bd&#39;, label=&#39;Y data&#39;)
        axs[3].plot(rcr, r_ACR, &#39;g+&#39;, ms=10, label=&#39;R data&#39;)
        axs[3].plot(xcr[xsz//2], data_ACR[ysz//2, xsz//2], &#39;md&#39;, label=&#39;Peak: {:.4f}, {:.4f}&#39;.format(xcr[xsz//2], data_ACR[ysz//2, xsz//2]))
        axs[3].plot(x_left, xacr_left, &#39;r&#39;)
        axs[3].plot(x_right, xacr_right, &#39;r&#39;, label=&#39;X extrap.: {:.4f}, {:.4f}&#39;.format(x_right[0], xacr_right[0]))
        axs[3].plot(y_left, yacr_left, &#39;b&#39;)
        axs[3].plot(y_right, yacr_right, &#39;b&#39;, label=&#39;Y extrap.: {:.4f}, {:.4f}&#39;.format(y_right[0], yacr_right[0]))
        axs[3].plot(r_left, racr_left, &#39;g&#39;)
        axs[3].plot(r_right, racr_right, &#39;g&#39;, label=&#39;R extrap.: {:.4f}, {:.4f}&#39;.format(r_right[0], racr_right[0]))
        axs[3].text(0.03, 0.92, &#39;xSNR = {:.2f}&#39;.format(xSNR), color=&#39;r&#39;, transform=axs[3].transAxes, fontsize=fs)
        axs[3].text(0.03, 0.86, &#39;ySNR = {:.2f}&#39;.format(ySNR), color=&#39;b&#39;, transform=axs[3].transAxes, fontsize=fs)
        axs[3].text(0.03, 0.80, &#39;rSNR = {:.2f}&#39;.format(rSNR), color=&#39;g&#39;, transform=axs[3].transAxes, fontsize=fs)
        axs[3].grid(True)
        axs[3].legend()
        axs[3].set_xlim(-5,5)
        axs[3].set_title(&#39;Normalized autocorr. cross-sections&#39;)

        if save_res_png:
            #print(&#39;X:   ACR peak = {:.4f}, Noise-Free ACR Peak = {:.4f}, Squared Mean = {:.4f}&#39;.format(x_acr, x_noise_free_acr, x_mean_value ))
            #print(&#39;xSNR = {:.2f}&#39;.format(xSNR))
            #print(&#39;Y:   ACR peak = {:.4f}, Noise-Free ACR Peak = {:.4f}, Squared Mean = {:.4f}&#39;.format(y_acr, y_noise_free_acr, y_mean_value ))
            #print(&#39;ySNR = {:.4f}&#39;.format(ySNR))
            #print(&#39;R:   ACR peak = {:.4f}, Noise-Free ACR Peak = {:.4f}, Squared Mean = {:.4f}&#39;.format(r_acr, r_noise_free_acr, r_mean_value ))
            #print(&#39;rSNR = {:.4f}&#39;.format(rSNR))
            fig.savefig(res_fname, dpi=dpi)
            print(&#39;Saved the results into the file: &#39;, res_fname)

    return xSNR, ySNR, rSNR


def Single_Image_Noise_ROIs(img, Noise_ROIs, Hist_ROI, **kwargs):
    &#39;&#39;&#39;
    Analyses the noise statistics in the selected ROI&#39;s of the EM data
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Performs following:
    1.  Smooth the image by 2D convolution with a given kernel.
    2.  Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
    3.  Build a histogram of Smoothed Image.
    4.  For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
    5.  Plot the dependence of the noise variance vs. image intensity.
    6.  One of the parameters is a DarkCount. If it is not explicitly defined as input parameter, it will be set to 0.
    7.  The equation is determined for a line that passes through the points Intensity=DarkCount and Noise Variance = 0 and is a best fit for
        the [Mean Intensity, Noise Variance] points determined for each ROI (Step 1 above).
    8.  The data is plotted. Following values of SNR are defined from the slope of the line in Step 7:
        a.  PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity at the histogram peak determined in the Step 3.
        b.  MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
        c.  DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance), where Max and Min Intensity are determined by
            corresponding cumulative threshold parameters, and Noise Variance is taken at the intensity in the middle of the range
            (Min Intensity + Max Intensity)/2.0.

    Parameters
    ----------
    img : 2D array
        original image
    Noise_ROIs : list of lists: [[left, right, top, bottom]]
        list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the noise.
    Hist_ROI : list [left, right, top, bottom]
        coordinates (indices) of the boundaries of the image subset to evaluate the real data histogram.

    kwargs:
    DarkCount : float
        the value of the Intensity Data at 0.
    kernel : 2D float array
        a kernel to perform 2D smoothing convolution.
    nbins_disp : int
        (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
    thresholds_disp : list [thr_min_disp, thr_max_disp]
        (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
    nbins_analysis : int
        (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
    thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
        (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
    nbins_analysis : int
         (default 256) number of histogram bins for building the data histogram in Step 5.
    disp_res : boolean
        (default is False) - to plot/ display the results
    save_res_png : boolean
        save the analysis output into a PNG file (default is True)
    res_fname : string
        filename for the sesult image (&#39;SNR_result.png&#39;)
    img_label : string
        optional image label
    Notes : string
        optional additional notes
    dpi : int

    Returns:
    mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR
        mean_vals and var_vals are the Mean Intensity and Noise Variance values for the Noise_ROIs (Step 1)
        NF_slope is the slope of the linear fit curve (Step 4)
        PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 6)
    &#39;&#39;&#39;
    st = 1.0/np.sqrt(2.0)
    def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
    def_kernel = def_kernel/def_kernel.sum()
    kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
    DarkCount = kwargs.get(&#34;DarkCount&#34;, 0)
    nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
    thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
    nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
    thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;Noise_Analysis_ROIs.png&#39;)
    img_label = kwargs.get(&#34;img_label&#34;, &#39;&#39;)
    Notes = kwargs.get(&#34;Notes&#34;, &#39;&#39;)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    fs=11
    img_filtered = convolve2d(img, kernel, mode=&#39;same&#39;)
    range_disp = get_min_max_thresholds(img_filtered, thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res=False)

    xi, xa, yi, ya = Hist_ROI
    img_hist = img[yi:ya, xi:xa]
    img_hist_filtered = img_filtered[yi:ya, xi:xa]

    range_analysis = get_min_max_thresholds(img_hist_filtered, thr_min = thresholds_analysis[0], thr_max = thresholds_analysis[1], nbins = nbins_analysis, disp_res=False)
    if disp_res:
        print(&#39;The EM data range for noise analysis: {:.1f} - {:.1f},  DarkCount={:.1f}&#39;.format(range_analysis[0], range_analysis[1], DarkCount))
    bins_analysis = np.linspace(range_analysis[0], range_analysis[1], nbins_analysis)

    xy_ratio = img.shape[1]/img.shape[0]
    xsz = 15
    ysz = xsz*3.5/xy_ratio

    n_ROIs = len(Noise_ROIs)+1
    mean_vals = np.zeros(n_ROIs)
    var_vals = np.zeros(n_ROIs)
    mean_vals[0] = DarkCount

    if disp_res:
        fig = plt.figure(figsize=(xsz,ysz))
        axs0 = fig.add_subplot(3,1,1)
        axs1 = fig.add_subplot(3,1,2)
        axs2 = fig.add_subplot(3,3,7)
        axs3 = fig.add_subplot(3,3,8)
        axs4 = fig.add_subplot(3,3,9)
        fig.subplots_adjust(left=0.01, bottom=0.06, right=0.99, top=0.95, wspace=0.25, hspace=0.10)

        axs0.text(0.01, 1.13, res_fname + &#39;,   &#39; +  Notes, transform=axs0.transAxes, fontsize=fs-3)
        axs0.imshow(img, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs0.axis(False)
        axs0.set_title(&#39;Original Image: &#39; + img_label, color=&#39;r&#39;, fontsize=fs+1)
        Hist_patch = patches.Rectangle((xi,yi),abs(xa-xi)-2,abs(ya-yi)-2, linewidth=1.0, edgecolor=&#39;white&#39;,facecolor=&#39;none&#39;)
        axs1.add_patch(Hist_patch)

        axs2.imshow(img_hist_filtered, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs2.axis(False)
        axs2.set_title(&#39;Smoothed ROI&#39;, fontsize=fs+1)

    if disp_res:
        hist, bins, hist_patches = axs3.hist(img_hist_filtered.ravel(), range=range_disp, bins = nbins_disp)
    else:
        hist, bins = np.histogram(img_hist_filtered.ravel(), range=range_disp, bins = nbins_disp)

    bin_centers = np.array(bins[1:] - (bins[1]-bins[0])/2.0)
    hist_center_ind = np.argwhere((bin_centers&gt;range_analysis[0]) &amp; (bin_centers&lt;range_analysis[1]))
    hist_smooth = savgol_filter(np.array(hist), (nbins_disp//10)*2+1, 7)
    I_peak = bin_centers[hist_smooth.argmax()]
    I_mean = np.mean(img)
    C_peak = hist_smooth.max()

    if disp_res:
        axs3.plot(bin_centers[hist_center_ind], hist_smooth[hist_center_ind], color=&#39;grey&#39;, linestyle=&#39;dashed&#39;, linewidth=2)
        Ipeak_lbl = &#39;$I_{peak}$&#39; +&#39;={:.1f}&#39;.format(I_peak)
        axs3.plot(I_peak, C_peak, &#39;rd&#39;, label = Ipeak_lbl)
        axs3.set_title(&#39;Histogram of the Smoothed ROI&#39;, fontsize=fs+1)
        axs3.grid(True)
        axs3.set_xlabel(&#39;Smoothed ROI Image Intensity&#39;, fontsize=fs+1)
        for hist_patch in np.array(hist_patches)[bin_centers&lt;range_analysis[0]]:
            hist_patch.set_facecolor(&#39;lime&#39;)
        for hist_patch in np.array(hist_patches)[bin_centers&gt;range_analysis[1]]:
            hist_patch.set_facecolor(&#39;red&#39;)
        ylim3=array(axs3.get_ylim())
        I_min, I_max = range_analysis
        axs3.plot([I_min, I_min],[ylim3[0]-1000, ylim3[1]], color=&#39;lime&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{min}$&#39; +&#39;={:.1f}&#39;.format(I_min))
        axs3.plot([I_max, I_max],[ylim3[0]-1000, ylim3[1]], color=&#39;red&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{max}$&#39; +&#39;={:.1f}&#39;.format(I_max))
        axs3.set_ylim(ylim3)
        axs3.legend(loc=&#39;upper right&#39;, fontsize=fs+1)
        axs1.imshow(img_filtered, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs1.axis(False)
        axs1.set_title(&#39;Smoothed Image&#39;)
        axs4.plot(DarkCount, 0, &#39;d&#39;, color = &#39;black&#39;, label=&#39;Dark Count&#39;)

    for j, ROI in enumerate(tqdm(Noise_ROIs, desc = &#39;analyzing ROIs&#39;)):
        xi, xa, yi, ya = ROI
        img_ROI = img[yi:ya, xi:xa]
        img_ROI_filtered = img_filtered[yi:ya, xi:xa]

        imdiff = (img_ROI - img_ROI_filtered)
        x = np.mean(img_ROI)
        y = np.var(imdiff)
        mean_vals[j+1] = x
        var_vals[j+1] = y

        if disp_res:
            patch_col = get_cmap(&#34;gist_rainbow_r&#34;)((j)/(n_ROIs))
            rect_patch = patches.Rectangle((xi,yi),abs(xa-xi)-2,abs(ya-yi)-2, linewidth=0.5, edgecolor=patch_col,facecolor=&#39;none&#39;)
            axs0.add_patch(rect_patch)
            axs4.plot(x, y, &#39;d&#39;, color = patch_col) #, label=&#39;patch {:d}&#39;.format(j))

    NF_slope = np.mean(var_vals[1:]/(mean_vals[1:]-DarkCount))
    mean_val_fit = np.array([mean_vals.min(), mean_vals.max()])
    var_val_fit = (mean_val_fit-DarkCount)*NF_slope
    VAR = (I_peak - DarkCount) * NF_slope
    VAR_at_mean = ((I_max + I_min) /2.0 - DarkCount) * NF_slope
    PSNR = (I_peak - DarkCount)/np.sqrt(VAR)
    MSNR = (I_mean - DarkCount)/np.sqrt((I_mean - DarkCount)*NF_slope)
    DSNR = (I_max - I_min)/np.sqrt(VAR_at_mean)

    if disp_res:
        axs4.grid(True)
        axs4.set_title(&#39;Noise Distribution&#39;, fontsize=fs+1)
        axs4.set_xlabel(&#39;ROI Image Intensity Mean&#39;, fontsize=fs+1)
        axs4.set_ylabel(&#39;ROI Image Intensity Variance&#39;, fontsize=fs+1)
        axs4.plot(mean_val_fit, var_val_fit, color=&#39;orange&#39;, label=&#39;Fit:  y = (x {:.1f}) * {:.2f}&#39;.format(DarkCount, NF_slope))
        axs4.legend(loc = &#39;upper left&#39;, fontsize=fs+2)
        ylim4=array(axs4.get_ylim())
        V_min = (I_min-DarkCount)*NF_slope
        V_max = (I_max-DarkCount)*NF_slope
        V_peak = (I_peak-DarkCount)*NF_slope
        axs4.plot([I_min, I_min],[ylim4[0], V_min], color=&#39;lime&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{min}$&#39; +&#39;={:.1f}&#39;.format(I_min))
        axs4.plot([I_max, I_max],[ylim4[0], V_max], color=&#39;red&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{max}$&#39; +&#39;={:.1f}&#39;.format(I_max))
        axs4.plot([I_peak, I_peak],[ylim4[0], V_peak], color=&#39;black&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{peak}$&#39; +&#39;={:.1f}&#39;.format(I_peak))
        axs4.set_ylim(ylim4)
        txt1 = &#39;Peak Intensity:  {:.1f}&#39;.format(I_peak)
        axs4.text(0.05, 0.65, txt1, transform=axs4.transAxes, fontsize=fs+1)
        txt2 = &#39;Variance={:.1f}, STD={:.1f}&#39;.format(VAR, np.sqrt(VAR))
        axs4.text(0.05, 0.55, txt2, transform=axs4.transAxes, fontsize=fs+1)
        txt3 = &#39;PSNR = {:.2f}&#39;.format(PSNR)
        axs4.text(0.05, 0.45, txt3, transform=axs4.transAxes, fontsize=fs+1)
        txt3 = &#39;DSNR = {:.2f}&#39;.format(DSNR)
        axs4.text(0.05, 0.35, txt3, transform=axs4.transAxes, fontsize=fs+1)
        if save_res_png:
            fig.savefig(res_fname, dpi=dpi)
            print(&#39;results saved into the file: &#39;+res_fname)

    return mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR


def Single_Image_Noise_Statistics(img, **kwargs):
    &#39;&#39;&#39;
    Analyses the noise statistics of the EM data image.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Performs following:
    1. Smooth the image by 2D convolution with a given kernel.
    2. Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
    3. Build a histogram of Smoothed Image.
    4. For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
    5. Plot the dependence of the noise variance vs. image intensity.
    6. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
        it will be set to 0
    7. The equation is determined for a line that passes through the point:
            Intensity=DarkCount and Noise Variance = 0
            and is a best fit for the [Mean Intensity, Noise Variance] points
            determined for each ROI (Step 1 above).
    8. The data is plotted. Two values of SNR are defined from the slope of the line in Step 4:
        PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity
            at the histogram peak determined in the Step 5.
        MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
        DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
            where Max and Min Intensity are determined by corresponding cummulative
            threshold parameters, and Noise Variance is taken at the intensity
            in the middle of the range (Min Intensity + Max Intensity)/2.0

    Parameters
    ----------
        img : 2d array

        kwargs:
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        DarkCount : float
            the value of the Intensity Data at 0.
        kernel : 2D float array
            a kernel to perfrom 2D smoothing convolution.
        nbins_disp : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
        thresholds_disp : list [thr_min_disp, thr_max_disp]
            (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
        nbins_analysis : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
        thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
            (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
        nbins_analysis : int
             (default 256) number of histogram bins for building the data histogram in Step 5.
        disp_res : boolean
            (default is False) - to plot/ display the results
        save_res_png : boolean
            save the analysis output into a PNG file (default is True)
        res_fname : string
            filename for the result image (&#39;Noise_Analysis.png&#39;)
        img_label : string
            optional image label
        Notes : string
            optional additional notes
        dpi : int

    Returns:
    mean_vals, var_vals, I0, PSNR, DSNR, popt, result
        mean_vals and var_vals are the Mean Intensity and Noise Variance values for Step5, I0 is zero intercept (should be close to DarkCount)
        PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 8)
    &#39;&#39;&#39;
    st = 1.0/np.sqrt(2.0)
    def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    def_kernel = def_kernel/def_kernel.sum()
    kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
    DarkCount = kwargs.get(&#34;DarkCount&#34;, 0)
    nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
    thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
    nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
    thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;Noise_Analysis.png&#39;)
    image_name = kwargs.get(&#34;image_name&#34;, &#39;&#39;)
    Notes = kwargs.get(&#34;Notes&#34;, &#39;&#39;)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    xi = 0
    yi = 0
    ysz, xsz = img.shape
    xa = xi + xsz
    ya = yi + ysz

    xi_eval = xi + evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = xa
    yi_eval = yi + evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ya

    img = img[yi_eval:ya_eval, xi_eval:xa_eval]
    img_filtered = convolve2d(img, kernel, mode=&#39;same&#39;)[1:-1, 1:-1]
    img = img[1:-1, 1:-1]

    range_disp = get_min_max_thresholds(img_filtered, thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res = disp_res)
    if disp_res:
        print(&#39;The EM data range for display:            {:.1f} - {:.1f}&#39;.format(range_disp[0], range_disp[1]))
    range_analysis = get_min_max_thresholds(img_filtered, thr_min = thresholds_analysis[0], thr_max = thresholds_analysis[1], nbins = nbins_analysis, disp_res = disp_res)
    if disp_res:
        print(&#39;The EM data range for noise analysis:     {:.1f} - {:.1f}&#39;.format(range_analysis[0], range_analysis[1]))
    bins_analysis = np.linspace(range_analysis[0], range_analysis[1], nbins_analysis)

    imdiff = (img-img_filtered)
    range_imdiff = get_min_max_thresholds(imdiff, thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res = disp_res)

    xy_ratio = img.shape[1]/img.shape[0]
    xsz = 15
    ysz = xsz/1.5*xy_ratio
    if disp_res:
        fs=11
        fig, axss = subplots(2,3, figsize=(xsz,ysz),  gridspec_kw={&#34;height_ratios&#34; : [1,1]})
        fig.subplots_adjust(left=0.07, bottom=0.06, right=0.99, top=0.92, wspace=0.15, hspace=0.10)
        axs = axss.ravel()
        axs[0].text(-0.1, 1.1, res_fname + &#39;,       &#39; +  Notes, transform=axs[0].transAxes, fontsize=fs)

        axs[0].imshow(img, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs[0].axis(False)
        axs[0].set_title(&#39;Original Image: &#39; + image_name, color=&#39;r&#39;, fontsize=fs+1)

        axs[1].imshow(img_filtered, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs[1].axis(False)
        axs[1].set_title(&#39;Smoothed Image&#39;)
        Low_mask = img*0.0+255.0
        High_mask = Low_mask.copy()
        Low_mask[img_filtered &gt; range_analysis[0]] = np.nan
        axs[1].imshow(Low_mask, cmap=&#34;brg_r&#34;)
        High_mask[img_filtered &lt; range_analysis[1]] = np.nan
        axs[1].imshow(High_mask, cmap=&#34;gist_rainbow&#34;)


        axs[2].imshow(imdiff, cmap=&#34;Greys&#34;, vmin = range_imdiff[0], vmax = range_imdiff[1])
        axs[2].axis(False)
        axs[2].set_title(&#39;Image Difference&#39;, fontsize=fs+1)

    if disp_res:
        hist, bins, patches = axs[4].hist(img_filtered.ravel(), range=range_disp, bins = nbins_disp)
    else:
        hist, bins = np.histogram(img_hist_filtered.ravel(), range=range_disp, bins = nbins_disp)
    bin_centers = np.array(bins[1:] - (bins[1]-bins[0])/2.0)
    hist_center_ind = np.argwhere((bin_centers&gt;range_analysis[0]) &amp; (bin_centers&lt;range_analysis[1]))
    hist_smooth = savgol_filter(np.array(hist), (nbins_disp//10)*2+1, 7)
    I_peak = bin_centers[hist_smooth.argmax()]
    C_peak = hist_smooth.max()
    Ipeak_lbl = &#39;$I_{peak}$&#39; +&#39;={:.1f}&#39;.format(I_peak)

    if disp_res:
        axs[4].plot(bin_centers[hist_center_ind], hist_smooth[hist_center_ind], color=&#39;grey&#39;, linestyle=&#39;dashed&#39;, linewidth=2)
        axs[4].plot(I_peak, C_peak, &#39;rd&#39;, label = Ipeak_lbl)
        axs[4].legend(loc=&#39;upper left&#39;, fontsize=fs+1)
        axs[4].set_title(&#39;Histogram of the Smoothed Image&#39;, fontsize=fs+1)
        axs[4].grid(True)
        axs[4].set_xlabel(&#39;Image Intensity&#39;, fontsize=fs+1)
        for patch in np.array(patches)[bin_centers&lt;range_analysis[0]]:
            patch.set_facecolor(&#39;lime&#39;)
        for patch in np.array(patches)[bin_centers&gt;range_analysis[1]]:
            patch.set_facecolor(&#39;red&#39;)
        ylim4=array(axs[4].get_ylim())
        axs[4].plot([range_analysis[0], range_analysis[0]],[ylim4[0]-1000, ylim4[1]], color=&#39;lime&#39;, linestyle=&#39;dashed&#39;, label=&#39;Ilow&#39;)
        axs[4].plot([range_analysis[1], range_analysis[1]],[ylim4[0]-1000, ylim4[1]], color=&#39;red&#39;, linestyle=&#39;dashed&#39;, label=&#39;Ihigh&#39;)
        axs[4].set_ylim(ylim4)
        txt1 = &#39;Smoothing Kernel&#39;
        axs[4].text(0.69, 0.955, txt1, transform=axs[4].transAxes, backgroundcolor=&#39;white&#39;, fontsize=fs-1)
        txt2 = &#39;{:.3f}  {:.3f}  {:.3f}&#39;.format(kernel[0,0], kernel[0,1], kernel[0,2])
        axs[4].text(0.69, 0.910, txt2, transform=axs[4].transAxes, backgroundcolor=&#39;white&#39;, fontsize=fs-2)
        txt3 = &#39;{:.3f}  {:.3f}  {:.3f}&#39;.format(kernel[1,0], kernel[1,1], kernel[1,2])
        axs[4].text(0.69, 0.865, txt3, transform=axs[4].transAxes, backgroundcolor=&#39;white&#39;, fontsize=fs-2)
        txt3 = &#39;{:.3f}  {:.3f}  {:.3f}&#39;.format(kernel[2,0], kernel[2,1], kernel[2,2])
        axs[4].text(0.69, 0.820, txt3, transform=axs[4].transAxes, backgroundcolor=&#39;white&#39;, fontsize=fs-2)

    if disp_res:
        hist, bins, patches = axs[5].hist(imdiff.ravel(), bins = nbins_disp)
        axs[5].grid(True)
        axs[5].set_title(&#39;Histogram of the Difference Map&#39;, fontsize=fs+1)
        axs[5].set_xlabel(&#39;Image Difference&#39;, fontsize=fs+1)
    else:
        hist, bins = np.histogram(imdiff.ravel(), bins = nbins_disp)

    ind_new = np.digitize(img_filtered, bins_analysis)
    result = np.array([(np.mean(img_filtered[ind_new == j]), np.var(imdiff[ind_new == j]))  for j in range(1, nbins_analysis)])
    non_nan_ind = np.argwhere(np.invert(np.isnan(result[:, 0])))
    mean_vals = np.squeeze(result[non_nan_ind, 0])
    var_vals = np.squeeze(result[non_nan_ind, 1])
    try:
        popt = np.polyfit(mean_vals, var_vals, 1)
        if disp_res:
            print(&#39;popt: &#39;, popt)

        I_array = np.array((range_analysis[0], range_analysis[1], I_peak))
        if disp_res:
            print(&#39;I_array: &#39;, I_array)
        Var_array = np.polyval(popt, I_array)
        Var_peak = Var_array[2]
    except:
        if disp_res:
            print(&#34;np.polyfit could not converge&#34;)
        popt = np.array([np.var(imdiff)/np.mean(img_filtered-DarkCount), 0])
        I_array = np.array((range_analysis[0], range_analysis[1], I_peak))
        Var_peak = np.var(imdiff)
    var_fit = np.polyval(popt, mean_vals)
    I0 = -popt[1]/popt[0]
    Slope_header = np.mean(var_vals/(mean_vals-DarkCount))
    var_fit_header = (mean_vals-DarkCount) * Slope_header
    if disp_res:
        axs[3].plot(mean_vals, var_vals, &#39;r.&#39;, label=&#39;data&#39;)
        axs[3].plot(mean_vals, var_fit, &#39;b&#39;, label=&#39;linear fit: {:.1f}*x + {:.1f}&#39;.format(popt[0], popt[1]))
        axs[3].plot(mean_vals, var_fit_header, &#39;magenta&#39;, label=&#39;linear fit with header offset&#39;)
        axs[3].grid(True)
        axs[3].set_title(&#39;Noise Distribution&#39;, fontsize=fs+1)
        axs[3].set_xlabel(&#39;Image Intensity Mean&#39;, fontsize=fs+1)
        axs[3].set_ylabel(&#39;Image Intensity Variance&#39;, fontsize=fs+1)
        ylim3=array(axs[3].get_ylim())
        lbl_low = &#39;$I_{low}$&#39;+&#39;, thr={:.1e}&#39;.format(thresholds_analysis[0])
        lbl_high = &#39;$I_{high}$&#39;+&#39;, thr={:.1e}&#39;.format(thresholds_analysis[1])
        axs[3].plot([range_analysis[0], range_analysis[0]],[ylim3[0]-1000, ylim3[1]], color=&#39;lime&#39;, linestyle=&#39;dashed&#39;, label=lbl_low)
        axs[3].plot([range_analysis[1], range_analysis[1]],[ylim3[0]-1000, ylim3[1]], color=&#39;red&#39;, linestyle=&#39;dashed&#39;, label=lbl_high)
        axs[3].legend(loc=&#39;upper center&#39;, fontsize=fs+1)
        axs[3].set_ylim(ylim3)

    PSNR = (I_peak-I0)/np.sqrt(Var_peak)
    PSNR_header = (I_peak-DarkCount)/np.sqrt(Var_peak)
    DSNR = (range_analysis[1]-range_analysis[0])/np.sqrt(Var_peak)
    if disp_res:
        print(&#39;Var at peak: {:.1f}&#39;.format(Var_peak))
        print(&#39;PSNR={:.2f}, PSNR_header={:.2f}, DSNR={:.2f}&#39;.format(PSNR, PSNR_header, DSNR))

        txt1 = &#39;Histogram Peak:  &#39; + Ipeak_lbl
        axs[3].text(0.25, 0.27, txt1, transform=axs[3].transAxes, fontsize=fs+1)
        txt2 = &#39;DSNR = &#39; +&#39;$(I_{high}$&#39; +&#39;$ - I_{low})$&#39; +&#39;/&#39;+&#39;$σ_{peak}$&#39; + &#39; = {:.2f}&#39;.format(DSNR)
        axs[3].text(0.25, 0.22, txt2, transform=axs[3].transAxes, fontsize=fs+1)

        txt3 = &#39;Zero Intercept:    &#39; +&#39;$I_{0}$&#39; +&#39;={:.1f}&#39;.format(I0)
        axs[3].text(0.25, 0.17, txt3, transform=axs[3].transAxes, color=&#39;blue&#39;, fontsize=fs+1)
        txt4 = &#39;PSNR = &#39; +&#39;$(I_{peak}$&#39; +&#39;$ - I_{0})$&#39; +&#39;/&#39;+&#39;$σ_{peak}$&#39; + &#39; = {:.2f}&#39;.format(PSNR)
        axs[3].text(0.25, 0.12, txt4, transform=axs[3].transAxes, color=&#39;blue&#39;, fontsize=fs+1)

        txt5 = &#39;Header Zero Int.:    &#39; +&#39;$I_{0}$&#39; +&#39;={:.1f}&#39;.format(DarkCount)
        axs[3].text(0.25, 0.07, txt5, transform=axs[3].transAxes, color=&#39;magenta&#39;, fontsize=fs+1)
        txt6 = &#39;PSNR = &#39; +&#39;$(I_{peak}$&#39; +&#39;$ - I_{0})$&#39; +&#39;/&#39;+&#39;$σ_{peak}$&#39; + &#39; = {:.2f}&#39;.format(PSNR_header)
        axs[3].text(0.25, 0.02, txt6, transform=axs[3].transAxes, color=&#39;magenta&#39;, fontsize=fs+1)

        if save_res_png:
            fig.savefig(res_fname, dpi=300)
            print(&#39;results saved into the file: &#39;+res_fname)
    return mean_vals, var_vals, I0, PSNR, DSNR, popt, result


def Perform_2D_fit(img, estimator, **kwargs):
    &#39;&#39;&#39;
    Bin the image and then perform 2D polynomial (currently only 2D parabolic) fit on the binned image.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Parameters
    ----------
    img : 2D array
        original image
    estimator : RANSACRegressor(),
                LinearRegression(),
                TheilSenRegressor(),
                HuberRegressor()
    kwargs:
    image_name : str
        Image name (for display purposes)
    bins : int
        binsize for image binning. If not provided, bins=10
    Analysis_ROIs : list of lists: [[left, right, top, bottom]]
        list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the parabolic fit.
    calc_corr : boolean
        If True - the full image correction is calculated
    ignore_Y  : boolean
        If True - the parabolic fit to only X is perfromed
    disp_res : boolean
        (default is False) - to plot/ display the results
    save_res_png : boolean
        save the analysis output into a PNG file (default is False)
    res_fname : string
        filename for the result image (&#39;Image_Flattening.png&#39;)
    Xsect : int
        X - coordinate for Y-crossection
    Ysect : int
        Y - coordinate for X-crossection
    dpi : int

    Returns:
    intercept, coefs, mse, img_correction_array
    &#39;&#39;&#39;
    image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
    ysz, xsz = img.shape
    calc_corr = kwargs.get(&#34;calc_corr&#34;, False)
    ignore_Y = kwargs.get(&#34;ignore_Y&#34;, False)
    Xsect = kwargs.get(&#34;Xsect&#34;, xsz//2)
    Ysect = kwargs.get(&#34;Ysect&#34;, ysz//2)
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    bins = kwargs.get(&#34;bins&#34;, 10) #bins = 10
    Analysis_ROIs = kwargs.get(&#34;Analysis_ROIs&#34;, [])
    save_res_png = kwargs.get(&#34;save_res_png&#34;, False)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;Image_Flattening.png&#39;)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    img_binned = img[0:ysz//bins*bins, 0:xsz//bins*bins].astype(float).reshape(ysz//bins, bins, xsz//bins, bins).sum(3).sum(1)/bins/bins
    if len(Analysis_ROIs)&gt;0:
            Analysis_ROIs_binned = [[ind//bins for ind in Analysis_ROI] for Analysis_ROI in Analysis_ROIs]
    else:
        Analysis_ROIs_binned = []
    vmin, vmax = get_min_max_thresholds(img_binned, disp_res=False)
    yszb, xszb = img_binned.shape
    yb, xb = np.indices(img_binned.shape)
    if len(Analysis_ROIs_binned)&gt;0:
        img_1D_list = []
        xb_1d_list = []
        yb_1d_list = []
        for Analysis_ROI_binned in Analysis_ROIs_binned:
            #Analysis_ROI : list of [left, right, top, bottom]
            img_1D_list = img_1D_list + img_binned[Analysis_ROI_binned[2]:Analysis_ROI_binned[3], Analysis_ROI_binned[0]:Analysis_ROI_binned[1]].ravel().tolist()
            xb_1d_list  = xb_1d_list + xb[Analysis_ROI_binned[2]:Analysis_ROI_binned[3], Analysis_ROI_binned[0]:Analysis_ROI_binned[1]].ravel().tolist()
            yb_1d_list  = yb_1d_list + yb[Analysis_ROI_binned[2]:Analysis_ROI_binned[3], Analysis_ROI_binned[0]:Analysis_ROI_binned[1]].ravel().tolist()
        img_1D = np.array(img_1D_list)
        xb_1d = np.array(xb_1d_list)
        yb_1d = np.array(yb_1d_list)
    else:
        img_1D = img_binned.ravel()
        xb_1d = xb.ravel()
        yb_1d = yb.ravel()

    img_binned_1D = img_binned.ravel()
    X_binned = np.vstack((xb.ravel(), yb.ravel())).T
    X = np.vstack((xb_1d, yb_1d)).T

    ysz, xsz = img.shape
    yf, xf = np.indices(img.shape)
    xf_1d = xf.ravel()/bins
    yf_1d = yf.ravel()/bins
    Xf = np.vstack((xf_1d, yf_1d)).T

    model = make_pipeline(PolynomialFeatures(2), estimator)

    if ignore_Y:
        ymean = np.mean(yb_1d)
        yb_1d_flat = yb_1d*0.0+ymean
        X_yflat = np.vstack((xb_1d, yb_1d_flat)).T
        model.fit(X_yflat, img_1D)

    else:
        model.fit(X, img_1D)

    model = make_pipeline(PolynomialFeatures(2), estimator)
    model.fit(X, img_1D)
    if hasattr(model[-1], &#39;estimator_&#39;):
        if ignore_Y:
            model[-1].estimator_.coef_[0] = model[-1].estimator_.coef_[0] + model[-1].estimator_.coef_[2]*ymean + model[-1].estimator_.coef_[5]*ymean*ymean
            model[-1].estimator_.coef_[1] = model[-1].estimator_.coef_[1] + model[-1].estimator_.coef_[4]*ymean
            model[-1].estimator_.coef_[2] = 0.0
            model[-1].estimator_.coef_[4] = 0.0
            model[-1].estimator_.coef_[5] = 0.0
        coefs = model[-1].estimator_.coef_
        intercept = model[-1].estimator_.intercept_
    else:
        if ignore_Y:
            model[-1].coef_[0] = model[-1].coef_[0] + model[-1].coef_[2]*ymean + model[-1].coef_[5]*ymean*ymean
            model[-1].coef_[1] = model[-1].coef_[1] + model[-1].coef_[4]*ymean
            model[-1].coef_[2] = 0.0
            model[-1].coef_[4] = 0.0
            model[-1].coef_[5] = 0.0
        coefs = model[-1].coef_
        intercept = model[-1].intercept_
    img_fit_1d = model.predict(X)
    scr = model.score(X, img_1D)
    mse = mean_squared_error(img_fit_1d, img_1D)
    img_fit = model.predict(X_binned).reshape(yszb, xszb)
    if calc_corr:
        img_correction_array = np.mean(img_fit_1d) / model.predict(Xf).reshape(ysz, xsz)
    else:
        img_correction_array = img * 0.0

    if disp_res:
        print(&#39;Estimator coefficients ( 1  x  y  x^2  x*y  y^2): &#39;, coefs)
        print(&#39;Estimator intercept: &#39;, intercept)

        fig, axs = subplots(2,2, figsize = (12, 8))
        axs[0, 0].imshow(img_binned, cmap=&#39;Greys&#39;, vmin=vmin, vmax=vmax)
        axs[0, 0].grid(True)
        axs[0, 0].plot([Xsect//bins, Xsect//bins], [0, yszb], &#39;lime&#39;, linewidth = 0.5)
        axs[0, 0].plot([0, xszb], [Ysect//bins, Ysect//bins], &#39;cyan&#39;, linewidth = 0.5)
        if len(Analysis_ROIs_binned)&gt;0:
            col_ROIs = &#39;yellow&#39;
            axs[0, 0].text(0.3, 0.9, &#39;with Analysis ROIs&#39;, color=col_ROIs, transform=axs[0, 0].transAxes)
            for Analysis_ROI_binned in Analysis_ROIs_binned:
            #Analysis_ROI : list of [left, right, top, bottom]
                xi, xa, yi, ya = Analysis_ROI_binned
                ROI_patch = patches.Rectangle((xi,yi),abs(xa-xi)-2,abs(ya-yi)-2, linewidth=0.75, edgecolor=col_ROIs,facecolor=&#39;none&#39;)
                axs[0, 0].add_patch(ROI_patch)

        axs[0, 0].set_xlim((0, xszb))
        axs[0, 0].set_ylim((yszb, 0))
        axs[0, 0].set_title(&#39;{:d}-x Binned Raw:&#39;.format(bins) + image_name)

        axs[0, 1].imshow(img_fit, cmap=&#39;Greys&#39;, vmin=vmin, vmax=vmax)
        axs[0, 1].grid(True)
        axs[0, 1].plot([Xsect//bins, Xsect//bins], [0, yszb], &#39;lime&#39;, linewidth = 0.5)
        axs[0, 1].plot([0, xszb], [Ysect//bins, Ysect//bins], &#39;cyan&#39;, linewidth = 0.5)
        axs[0, 1].set_xlim((0, xszb))
        axs[0, 1].set_ylim((yszb,0))
        axs[0, 1].set_title(&#39;{:d}-x Binned Fit: &#39;.format(bins) + image_name)

        axs[1, 0].plot(img[Ysect, :],&#39;b&#39;, label = image_name, linewidth =0.5)
        axs[1, 0].plot(xb[0,:]*bins, img_binned[Ysect//bins, :],&#39;cyan&#39;, label = &#39;Binned &#39;+ image_name)
        axs[1, 0].plot(xb[0,:]*bins, img_fit[Ysect//bins, :], &#39;yellow&#39;, linewidth=4, linestyle=&#39;--&#39;, label = &#39;Fit: &#39;+ image_name)
        axs[1, 0].legend()
        axs[1, 0].grid(True)
        axs[1, 0].set_xlabel(&#39;X-coordinate&#39;)

        axs[1, 1].plot(img[:, Xsect],&#39;g&#39;, label = image_name, linewidth =0.5)
        axs[1, 1].plot(yb[:, 0]*bins, img_binned[:, Xsect//bins],&#39;lime&#39;, label = &#39;Binned &#39;+image_name)
        axs[1, 1].plot(yb[:, 0]*bins, img_fit[:, Xsect//bins], &#39;yellow&#39;, linewidth=4, linestyle=&#39;--&#39;, label = &#39;Fit: &#39;+ image_name)
        axs[1, 1].legend()
        axs[1, 1].grid(True)
        axs[1, 1].set_xlabel(&#39;Y-coordinate&#39;)
        if save_res_png:
            fig.savefig(res_fname, dpi=dpi)
            print(&#39;results saved into the file: &#39;+res_fname)
    return intercept, coefs, mse, img_correction_array



##############################################
#      Two-Frame Image processing Functions
##############################################
def mutual_information_2d(x, y, sigma=1, bin=256, normalized=False):
    &#34;&#34;&#34;
    Computes (normalized) mutual information between two 1D variate from a
    joint histogram.
    Parameters
    ----------
    x : 1D array
        first variable
    y : 1D array
        second variable
    sigma: float
        sigma for Gaussian smoothing of the joint histogram
    Returns
    -------
    mi: float
        the computed similarity measure
    &#34;&#34;&#34;
    bins = (bin, bin)

    jh = np.histogram2d(x, y, bins=bins)[0]

    # smooth the jh with a gaussian filter of given sigma
    #ndimage.gaussian_filter(jh, sigma=sigma, mode=&#39;constant&#39;,
    #                             output=jh)
    # compute marginal histograms
    jh = jh + EPS
    sh = np.sum(jh)
    jh = jh / sh
    s1 = np.sum(jh, axis=0).reshape((-1, jh.shape[0]))
    s2 = np.sum(jh, axis=1).reshape((jh.shape[1], -1))

    # Normalised Mutual Information of:
    # Studholme,  jhill &amp; jhawkes (1998).
    # &#34;A normalized entropy measure of 3-D medical image alignment&#34;.
    # in Proc. Medical Imaging 1998, vol. 3338, San Diego, CA, pp. 132-143.
    if normalized:
        mi = ((np.sum(s1 * np.log(s1)) + np.sum(s2 * np.log(s2)))
                / np.sum(jh * np.log(jh))) - 1
    else:
        mi = ( np.sum(jh * np.log(jh)) - np.sum(s1 * np.log(s1))
               - np.sum(s2 * np.log(s2)))
    return mi


def mutual_information_2d_cp(x, y, sigma=1, bin=256, normalized=False):
    &#34;&#34;&#34;
    Computes (normalized) mutual information between two 1D variate from a
    joint histogram using CUPY package.
    Parameters
    ----------
    x : 1D array
        first variable
    y : 1D array
        second variable
    sigma: float
        sigma for Gaussian smoothing of the joint histogram
    Returns
    -------
    mi: float
        the computed similarity measure
    &#34;&#34;&#34;
    bins = (bin, bin)

    jhf = cp.histogram2d(x, y, bins=bins)

    # smooth the jh with a gaussian filter of given sigma
    #ndimage.gaussian_filter(jh, sigma=sigma, mode=&#39;constant&#39;,
    #                             output=jh)
    # compute marginal histograms
    jh = jhf[0] + EPS
    sh = cp.sum(jh)
    jh = jh / sh
    s1 = cp.sum(jh, axis=0).reshape((-1, jh.shape[0]))
    s2 = cp.sum(jh, axis=1).reshape((jh.shape[1], -1))

    # Normalised Mutual Information of:
    # Studholme,  jhill &amp; jhawkes (1998).
    # &#34;A normalized entropy measure of 3-D medical image alignment&#34;.
    # in Proc. Medical Imaging 1998, vol. 3338, San Diego, CA, pp. 132-143.
    if normalized:
        mi = ((cp.sum(s1 * cp.log(s1)) + cp.sum(s2 * cp.log(s2)))
                / cp.sum(jh * cp.log(jh))) - 1
    else:
        mi = ( cp.sum(jh * cp.log(jh)) - np.sum(s1 * cp.log(s1))
               - cp.sum(s2 * cp.log(s2)))
    return mi


def Two_Image_NCC_SNR(img1, img2, **kwargs):
    &#39;&#39;&#39;
    Estimates normalized cross-correlation and SNR of two images.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Calculates SNR from cross-correlation of two images after [1, 2, 3].

    Parameters
    ---------
    img1 : 2D array
    img2 : 2D array

    kwargs:
    zero_mean: boolean
        if True, cross-correlation is zero-mean

    Returns:
        NCC, SNR : float, float
            NCC - normalized cross-correlation coefficient
            SNR - Signal-to-Noise ratio based on NCC

   [1] J. Frank, L. AI-Ali, Signal-to-noise ratio of electron micrographs obtained by cross correlation. Nature 256, 4 (1975).
   [2] J. Frank, in: Computer Processing of Electron Microscopic Images. Ed. P.W. Hawkes (Springer, Berlin, 1980).
   [3] M. Radermacher, T. Ruiz, On cross-correlations, averages and noise in electron microscopy. Acta Crystallogr. Sect. F Struct. Biol. Commun. 75, 12–18 (2019).

    &#39;&#39;&#39;
    zero_mean = kwargs.get(&#34;zero_mean&#34;, True)

    if img1.shape==img2.shape:
        ysz, xsz = img1.shape
        if zero_mean:
            img1 = img1- img1.mean()
            img2 = img2- img2.mean()
        xy = np.sum((img1.ravel()*img2.ravel()))/(xsz*ysz)
        xx = np.sum((img1.ravel()*img1.ravel()))/(xsz*ysz)
        yy = np.sum((img2.ravel()*img2.ravel()))/(xsz*ysz)
        NCC = xy / np.sqrt(xx*yy)
        SNR = NCC / (1-NCC)

    else:
        print(&#34;img1 and img2 shapes must be equal&#34;)
        NCC = 0.0
        SNR = 0.0

    return NCC, SNR

def Two_Image_FSC(img1, img2, **kwargs):
    &#39;&#39;&#39;
    Perform Fourier Shell Correlation to determine the image resolution, after [1]. ©G.Shtengel, 10/2019. gleb.shtengel@gmail.com
    FSC is determined from radially averaged foirier cross-correlation (with optional selection of range of angles for radial averaging).

    Parameters
    ---------
    img1 : 2D array
    img2 : 2D array

    kwargs:
    SNRt : float
        SNR threshold for determining the resolution bandwidth
    astart : float
        Start angle for radial averaging. Default is 0
    astop : float
        Stop angle for radial averaging. Default is 90
    symm : int
        Symmetry factor (how many times Start and stop angle intervalks are repeated within 360 deg). Default is 4.
    disp_res : boolean
        display results (plots) (default is False)
    ax : axis object (matplotlip)
        to export the plot
    save_res_png : boolean
        save results into PNG file (default is False)
    res_fname : string
        filename for the result image (&#39;SNR_result.png&#39;)
    img_labels : [string, string]
        optional image labels
    dpi : int
        dots-per-inch resolution for the output image
    pixel : float
        optional pixel size in nm. If not provided, will be ignored.
        if provided, second axis will be added on top with inverse pixels
    xrange : [float, float]
        range of x axis in FSC plot in inverse pixels
        if not provided [0, 0.5] range will be used

    Returns FSC_sp_frequencies, FSC_data, x2, T, FSC_bw
        FSC_sp_frequencies : float array
            Spatial Frequency (/Nyquist) - for FSC plot
        FSC_data: float array
        x2 : float array
            Spatial Frequency (/Nyquist) - for threshold line plot
        T : float array
            threshold line plot
        FSC_bw : float
            the value of FSC determined as an intersection of smoothed data threshold

    [1]. M. van Heela, and M. Schatzb, &#34;Fourier shell correlation threshold criteria,&#34; Journal of Structural Biology 151, 250-262 (2005)
    &#39;&#39;&#39;
    SNRt = kwargs.get(&#34;SNRt&#34;, 0.1)
    astart = kwargs.get(&#34;astart&#34;, 0.0)
    astop = kwargs.get(&#34;astop&#34;, 90.0)
    symm = kwargs.get(&#34;symm&#34;, 4)
    disp_res = kwargs.get(&#34;disp_res&#34;, False)
    ax = kwargs.get(&#34;ax&#34;, &#39;&#39;)
    save_res_png = kwargs.get(&#34;save_res_png&#34;, False)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;FSC_results.png&#39;)
    img_labels = kwargs.get(&#34;img_labels&#34;, [&#39;Image 1&#39;, &#39;Image 2&#39;])
    dpi = kwargs.get(&#34;dpi&#34;, 300)
    pixel = kwargs.get(&#34;pixel&#34;, 0.0)
    xrange = kwargs.get(&#34;xrange&#34;, [0, 0.5])

    #Check whether the inputs dimensions match and the images are square
    if disp_res:
        if ( np.shape(img1) != np.shape(img2) ) :
            print(&#39;input images must have the same dimensions&#39;)
        if ( np.shape(img1)[0] != np.shape(img1)[1]) :
            print(&#39;input images must be squares&#39;)
    I1 = fftshift(fftn(ifftshift(img1)))  # I1 and I2 store the FFT of the images to be used in the calcuation for the FSC
    I2 = fftshift(fftn(ifftshift(img2)))

    C_imre = np.multiply(I1,np.conj(I2))
    C12_ar = abs(np.multiply((I1+I2),np.conj(I1+I2)))
    y0,x0 = argmax2d(C12_ar)
    C1 = radial_profile_select_angles(abs(np.multiply(I1,np.conj(I1))), [x0,y0], astart, astop, symm)
    C2 = radial_profile_select_angles(abs(np.multiply(I2,np.conj(I2))), [x0,y0], astart, astop, symm)
    C  = radial_profile_select_angles(np.real(C_imre), [x0,y0], astart, astop, symm) + 1j * radial_profile_select_angles(np.imag(C_imre), [x0,y0], astart, astop, symm)

    FSC_data = abs(C)/np.sqrt(abs(np.multiply(C1,C2)))
    &#39;&#39;&#39;
    T is the SNR threshold calculated accoring to the input SNRt, if nothing is given
    a default value of 0.1 is used.

    x2 contains the normalized spatial frequencies
    &#39;&#39;&#39;
    r = np.arange(1+np.shape(img1)[0])
    n = 2*np.pi*r
    n[0] = 1
    eps = np.finfo(float).eps
    t1 = np.divide(np.ones(np.shape(n)),n+eps)
    t2 = SNRt + 2*np.sqrt(SNRt)*t1 + np.divide(np.ones(np.shape(n)),np.sqrt(n))
    t3 = SNRt + 2*np.sqrt(SNRt)*t1 + 1
    T = np.divide(t2,t3)
    #FSC_sp_frequencies = np.arange(np.shape(C)[0])/(np.shape(img1)[0]/sqrt(2.0))
    #x2 = r/(np.shape(img1)[0]/sqrt(2.0))
    FSC_sp_frequencies = np.arange(np.shape(C)[0])/(np.shape(img1)[0])
    x2 = r/(np.shape(img1)[0])
    FSC_data_smooth = smooth(FSC_data, 20)
    FSC_bw = find_BW(FSC_sp_frequencies, FSC_data_smooth, SNRt)
    &#39;&#39;&#39;
    If the disp_res input is set to True, an output plot is generated.
    &#39;&#39;&#39;
    if disp_res:
        if ax==&#39;&#39;:
            fig = plt.figure(figsize=(8,12))
            axs0 = fig.add_subplot(3,2,1)
            axs1 = fig.add_subplot(3,2,2)
            axs2 = fig.add_subplot(3,2,3)
            axs3 = fig.add_subplot(3,2,4)
            ax = fig.add_subplot(3,1,3)
            fig.subplots_adjust(left=0.01, bottom=0.06, right=0.99, top=0.975, wspace=0.25, hspace=0.10)
            vmin1, vmax1 = get_min_max_thresholds(img1, disp_res=False)
            vmin2, vmax2 = get_min_max_thresholds(img2, disp_res=False)
            axs0.imshow(img1, cmap=&#39;Greys&#39;, vmin=vmin1, vmax=vmax1)
            axs1.imshow(img2, cmap=&#39;Greys&#39;, vmin=vmin2, vmax=vmax2)
            x = np.linspace(0, 1.41, 500)
            axs2.set_xlim(-1,1)
            axs2.set_ylim(-1,1)
            axs2.imshow(np.log(abs(I1)), extent=[-1, 1, -1, 1], cmap = &#39;Greys_r&#39;)
            axs3.set_xlim(-1,1)
            axs3.set_ylim(-1,1)
            axs3.imshow(np.log(abs(I2)), extent=[-1, 1, -1, 1], cmap = &#39;Greys_r&#39;)
            for i in np.arange(symm):
                ai = np.radians(astart + 360.0/symm*i)
                aa = np.radians(astop + 360.0/symm*i)
                axs2.plot(x * np.cos(ai), x * np.sin(ai), color=&#39;orange&#39;, linewidth = 0.5)
                axs3.plot(x * np.cos(ai), x * np.sin(ai), color=&#39;orange&#39;, linewidth = 0.5)
                axs2.plot(x * np.cos(aa), x * np.sin(aa), color=&#39;orange&#39;, linewidth = 0.5)
                axs3.plot(x * np.cos(aa), x * np.sin(aa), color=&#39;orange&#39;, linewidth = 0.5)
            ttls = img_labels.copy()
            ttls.append(&#39;FFT of &#39;+img_labels[0])
            ttls.append(&#39;FFT of &#39;+img_labels[1])
            for axi, ttl in zip([axs0, axs1, axs2, axs3], ttls):
                axi.grid(False)
                axi.axis(False)
                axi.set_title(ttl)

    if disp_res or ax != &#39;&#39;:
        ax.plot(FSC_sp_frequencies, FSC_data, label = &#39;FSC data&#39;, color=&#39;r&#39;)
        ax.plot(FSC_sp_frequencies, FSC_data_smooth, label = &#39;FSC data smoothed&#39;, color=&#39;b&#39;)
        ax.plot(x2, x2*0.0+SNRt, &#39;--&#39;, label = &#39;Threshold SNR = {:.3f}&#39;.format(SNRt), color=&#39;m&#39;)
        if pixel&gt;1e-6:
            label = &#39;FSC BW = {:.3f} inv.pix., or {:.2f} nm&#39;.format(FSC_bw, pixel/FSC_bw)
        else:
            label = &#39;FSC BW = {:.3f}&#39;.format(FSC_bw)
        ax.plot(np.array((FSC_bw,FSC_bw)), np.array((0.0,1.0)), &#39;--&#39;, label = label, color = &#39;g&#39;)
        ax.set_xlim(xrange)
        ax.legend()
        ax.set_xlabel(&#39;Spatial Frequency (inverse pixels)&#39;)
        ax.set_ylabel(&#39;FSC Magnitude&#39;)
        ax.grid(True)
        if pixel&gt;1e-6:

            def forward(x):
                return x/pixel
            def inverse(x):
                return x*pixel
            secax = ax.secondary_xaxis(&#39;top&#39;, functions=(forward, inverse))
            secax.set_xlabel(&#39;Spatial Frequency ($nm^{-1}$)&#39;)

    if disp_res:
        ax.set_position([0.1, 0.05, 0.85, 0.28])
        print(&#39;FSC BW = {:.5f}&#39;.format(FSC_bw))
        if save_res_png:
            fig.savefig(res_fname, dpi=dpi)
            print(&#39;Saved the results into the file: &#39;, res_fname)
    return (FSC_sp_frequencies, FSC_data, x2, T, FSC_bw)


def Two_Image_Analysis(params):
    &#39;&#39;&#39;
    Analyzes the registration  quality between two frames (for DASK registration analysis)

    Parameterss:
    params : list of params
        params = [frame1_filename, frame2_filename, eval_bounds, eval_metrics]
        eval_bounds = [xi,  xa, yi, ya]
        eval_metrics = [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;]

    Returns
        results : list of results
    &#39;&#39;&#39;
    frame1_filename, frame2_filename, eval_bounds, eval_metrics = params
    xi_eval,  xa_eval, yi_eval, ya_eval = eval_bounds


    I1 = tiff.imread(frame1_filename)
    I1c = I1[yi_eval:ya_eval, xi_eval:xa_eval]
    I2 = tiff.imread(frame2_filename)
    I2c = I2[yi_eval:ya_eval, xi_eval:xa_eval]
    fr_mean = abs(I1c/2.0 + I2c/2.0)
    dy, dx = shape(I2c)

    results = []
    for metric in eval_metrics:
        if metric == &#39;NSAD&#39;:
            results.append(mean(abs(I1c-I2c))/(np.mean(fr_mean)-np.amin(fr_mean)))
        if metric == &#39;NCC&#39;:
            results.append(Two_Image_NCC_SNR(I1c, I2c)[0])
        if metric == &#39;NMI&#39;:
            results.append(mutual_information_2d(I1c.ravel(), I2c.ravel(), sigma=1.0, bin=2048, normalized=True))
        if metric == &#39;FSC&#39;:
            #SNRt is SNR threshold for determining the resolution bandwidth
            # force square images for FSC
            if dx != dy:
                d=min((dx//2, dy//2))
                results.append(Two_Image_FSC(I1c[dy//2-d:dy//2+d, dx//2-d:dx//2+d], I2c[dy//2-d:dy//2+d, dx//2-d:dx//2+d], SNRt=0.143, disp_res=False)[4])
            else:
                results.append(Two_Image_FSC(I1c, I2c, SNRt=0.143, disp_res=False)[4])

    return results



##########################################
#         MRC stack analysis functions
##########################################

def evaluate_registration_two_frames(params_mrc):
    &#39;&#39;&#39;
    Helper function used by DASK routine. Analyzes registration between two frames.
    ©G.Shtengel, 10/2020. gleb.shtengel@gmail.com

    Parameters:
    params_mrc : list of mrc_filename, fr, evals, save_frame_png, filename_frame_png
    mrc_filename  : string
        full path to mrc filename
    fr : int
        Index of the SECOND frame
    evals :  list of image bounds to be used for evaluation exi_eval, xa_eval, yi_eval, ya_eval


    Returns:
    image_nsad, image_ncc, image_mi   : float, float, float

    &#39;&#39;&#39;
    mrc_filename, fr, invert_data, evals, save_frame_png, filename_frame_png = params_mrc
    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;)
    header = mrc_obj.header
    &#39;&#39;&#39;
    mode 0 -&gt; uint8
    mode 1 -&gt; int16
    mode 2 -&gt; float32
    mode 4 -&gt; complex64
    mode 6 -&gt; uint16
    &#39;&#39;&#39;
    mrc_mode = mrc_obj.header.mode
    if mrc_mode==0:
        dt_mrc=uint8
    if mrc_mode==1:
        dt_mrc=int16
    if mrc_mode==2:
        dt_mrc=float32
    if mrc_mode==4:
        dt_mrc=complex64
    if mrc_mode==6:
        dt_mrc=uint16

    xi_eval, xa_eval, yi_eval, ya_eval = evals
    if invert_data:
        prev_frame = -1.0 * (((mrc_obj.data[fr-1, yi_eval:ya_eval, xi_eval:xa_eval]).astype(dt_mrc)).astype(float))
        curr_frame = -1.0 * (((mrc_obj.data[fr, yi_eval:ya_eval, xi_eval:xa_eval]).astype(dt_mrc)).astype(float))
    else:
        prev_frame = (mrc_obj.data[fr-1, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float)
        curr_frame = (mrc_obj.data[fr, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float)
    fr_mean = np.abs(curr_frame/2.0 + prev_frame/2.0)
    image_nsad =  np.mean(np.abs(curr_frame-prev_frame))/(np.mean(fr_mean)-np.amin(fr_mean))
    #image_nsad =  np.mean(np.abs(curr_frame-prev_frame))/(np.mean(fr_mean))
    image_ncc = Two_Image_NCC_SNR(curr_frame, prev_frame)[0]
    image_mi = mutual_information_2d(prev_frame.ravel(), curr_frame.ravel(), sigma=1.0, bin=2048, normalized=True)

    if save_frame_png:
        fr_img = (mrc_obj.data[fr, :, :].astype(dt_mrc)).astype(float)
        yshape, xshape = fr_img.shape
        fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
        fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
        dmin, dmax = get_min_max_thresholds(fr_img[yi_eval:ya_eval, xi_eval:xa_eval])
        if invert_data:
            ax.imshow(fr_img, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
        else:
            ax.imshow(fr_img, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.text(0.06, 0.95, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(fr, image_nsad, image_ncc, image_mi), color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
        rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        ax.axis(&#39;off&#39;)
        fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
        plt.close(fig)

    mrc_obj.close()
    return image_nsad, image_ncc, image_mi


def analyze_mrc_stack_registration(mrc_filename, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Read MRC stack and analyze registration - calculate NSAD, NCC, and MI.
    ©G.Shtengel, 04/2021. gleb.shtengel@gmail.com

    Parameters
    ---------
    mrc_filename : str
        File name (full path) of the mrc stack to be analyzed
    DASK client (needs to be initialized and running by this time)

    kwargs:
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    frame_inds : array
        Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
    invert_data : boolean
        If True, the data will be inverted
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    save_filename : str
        Path to the filename to save the results. If empty, mrc_filename+&#39;_RegistrationQuality.csv&#39; will be used
    save_sample_frames_png : bolean
        If True, sample frames with superimposed eval box and registration analysis data will be saved into png files. Default is True

    Returns reg_summary : PD data frame, registration_summary_xlsx : path to summary XLSX workbook
    &#39;&#39;&#39;
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    save_filename = kwargs.get(&#34;save_filename&#34;, mrc_filename )
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    registration_summary_xlsx = save_filename.replace(&#39;.mrc&#39;, &#39;_RegistrationQuality.xlsx&#39;)
    save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, True)

    if sliding_evaluation_box:
        print(&#39;Will use sliding (linearly) evaluation box&#39;)
        print(&#39;   Starting with box:  &#39;, start_evaluation_box)
        print(&#39;   Finishing with box: &#39;, stop_evaluation_box)
    else:
        print(&#39;Will use fixed evaluation box: &#39;, evaluation_box)

    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;)
    header = mrc_obj.header
    mrc_mode = header.mode
    nx, ny, nz = int32(header[&#39;nx&#39;]), int32(header[&#39;ny&#39;]), int32(header[&#39;nz&#39;])
    header_dict = {}
    for record in header.dtype.names: # create dictionary from the header data
        if (&#39;extra&#39; not in record) and (&#39;label&#39; not in record):
            header_dict[record] = header[record]
    &#39;&#39;&#39;
    mode 0 -&gt; uint8
    mode 1 -&gt; int16
    mode 2 -&gt; float32
    mode 4 -&gt; complex64
    mode 6 -&gt; uint16
    &#39;&#39;&#39;
    if mrc_mode==0:
        dt_mrc=uint8
    if mrc_mode==1:
        dt_mrc=int16
    if mrc_mode==2:
        dt_mrc=float32
    if mrc_mode==4:
        dt_mrc=complex64
    if mrc_mode==6:
        dt_mrc=uint16
    print(&#39;mrc_mode={:d} &#39;.format(mrc_mode), &#39;, dt_mrc=&#39;, dt_mrc)

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = nx
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ny
    evals = [xi_eval, xa_eval, yi_eval, ya_eval]

    frame_inds_default = np.arange(nz-1)+1
    frame_inds = np.array(kwargs.get(&#34;frame_inds&#34;, frame_inds_default))
    nf = frame_inds[-1]-frame_inds[0]+1
    if frame_inds[0]==0:
        frame_inds = frame_inds+1
    sample_frame_inds = [frame_inds[nf//10], frame_inds[nf//2], frame_inds[nf//10*9]]
    print(&#39;Will analyze regstrations in {:d} frames&#39;.format(len(frame_inds)))
    print(&#39;Will save the data into &#39; + registration_summary_xlsx)
    if sliding_evaluation_box:
        dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
        dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
    else:
        dx_eval = 0
        dy_eval = 0

    params_mrc_mult = []
    xi_evals = np.zeros(nf, dtype=int16)
    xa_evals = np.zeros(nf, dtype=int16)
    yi_evals = np.zeros(nf, dtype=int16)
    ya_evals = np.zeros(nf, dtype=int16)
    for j, fr in enumerate(frame_inds):
        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*(fr-frame_inds[0])//nf
            yi_eval = start_evaluation_box[0] + dy_eval*(fr-frame_inds[0])//nf
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny
            evals = [xi_eval, xa_eval, yi_eval, ya_eval]
        xi_evals[j] = xi_eval
        xa_evals[j] = xa_eval
        yi_evals[j] = yi_eval
        ya_evals[j] = ya_eval
        if fr in sample_frame_inds:
            save_frame_png = save_sample_frames_png
            filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame{:d}.png&#39;.format(fr)
        else:
            save_frame_png = False
            filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame.png&#39;
        params_mrc_mult.append([mrc_filename, fr, invert_data, evals, save_frame_png, filename_frame_png])

    if use_DASK:
        mrc_obj.close()
        print(&#39;Using DASK distributed&#39;)
        futures = DASK_client.map(evaluate_registration_two_frames, params_mrc_mult, retries = DASK_client_retries)
        dask_results = DASK_client.gather(futures)
        image_nsad = np.array([res[0] for res in dask_results])
        image_ncc = np.array([res[1] for res in dask_results])
        image_mi = np.array([res[2] for res in dask_results])
    else:
        print(&#39;Using Local Computation&#39;)
        image_nsad = np.zeros(nf, dtype=float)
        image_ncc = np.zeros(nf, dtype=float)
        image_mi = np.zeros(nf, dtype=float)
        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*frame_inds[0]//nf
            yi_eval = start_evaluation_box[0] + dy_eval*frame_inds[0]//nf
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny
        if invert_data:
            prev_frame = -1.0 * ((mrc_obj.data[frame_inds[0]-1, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float))
        else:
            prev_frame =(mrc_obj.data[frame_inds[0]-1, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float)
        for j, frame_ind in enumerate(tqdm(frame_inds, desc=&#39;Evaluating frame registration: &#39;)):
            if sliding_evaluation_box:
                xi_eval = start_evaluation_box[2] + dx_eval*j//nf
                yi_eval = start_evaluation_box[0] + dy_eval*j//nf
                if start_evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + start_evaluation_box[3]
                else:
                    xa_eval = nx
                if start_evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + start_evaluation_box[1]
                else:
                    ya_eval = ny

            if invert_data:
                curr_frame = -1.0 * ((mrc_obj.data[frame_ind, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float))
            else:
                curr_frame = (mrc_obj.data[frame_ind, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float)
            if use_cp:
                curr_frame_cp = cp.array(curr_frame)
                prev_frame_cp = cp.array(prev_frame)
                fr_mean = cp.abs(curr_frame_cp/2.0 + prev_frame_cp/2.0)
            else:
                fr_mean = abs(curr_frame/2.0 + prev_frame/2.0)

            image_ncc[j-1] = Two_Image_NCC_SNR(curr_frame, prev_frame)[0]
            if use_cp:
                image_nsad[j-1] =  cp.asnumpy(cp.mean(cp.abs(curr_frame_cp-prev_frame_cp))/(cp.mean(fr_mean)-cp.amin(fr_mean)))
                image_mi[j-1] = cp.asnumpy(mutual_information_2d_cp(prev_frame_cp.ravel(), curr_frame_cp.ravel(), sigma=1.0, bin=2048, normalized=True))
            else:
                image_nsad[j-1] =  mean(abs(curr_frame-prev_frame))/(np.mean(fr_mean)-np.amin(fr_mean))
                image_mi[j-1] = mutual_information_2d(prev_frame.ravel(), curr_frame.ravel(), sigma=1.0, bin=2048, normalized=True)
            prev_frame = curr_frame.copy()
            del curr_frame_cp, prev_frame_cp
            if (frame_ind in sample_frame_inds) and save_sample_frames_png:
                filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame{:d}.png&#39;.format(j)
                fr_img = (mrc_obj.data[frame_ind, :, :].astype(dt_mrc)).astype(float)
                yshape, xshape = fr_img.shape
                fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
                fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
                dmin, dmax = get_min_max_thresholds(fr_img[yi_eval:ya_eval, xi_eval:xa_eval])
                if invert_data:
                    ax.imshow(fr_img, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
                else:
                    ax.imshow(fr_img, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
                ax.text(0.06, 0.95, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(frame_ind, image_nsad[j-1], image_ncc[j-1], image_mi[j-1]), color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
                rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
                ax.add_patch(rect_patch)
                ax.axis(&#39;off&#39;)
                fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
                plt.close(fig)

        mrc_obj.close()

    nsads = [np.mean(image_nsad), np.median(image_nsad), np.std(image_nsad)]
    #image_ncc = image_ncc[1:-1]
    nccs = [np.mean(image_ncc), np.median(image_ncc), np.std(image_ncc)]
    nmis = [np.mean(image_mi), np.median(image_mi), np.std(image_mi)]

    print(&#39;Saving the Registration Quality Statistics into the file: &#39;, registration_summary_xlsx)
    xlsx_writer = pd.ExcelWriter(registration_summary_xlsx, engine=&#39;xlsxwriter&#39;)
    columns=[&#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;, &#39;Image NSAD&#39;, &#39;Image NCC&#39;, &#39;Image MI&#39;]
    reg_summary = pd.DataFrame(np.vstack((frame_inds, xi_evals, xa_evals, yi_evals, ya_evals, image_nsad, image_ncc, image_mi)).T, columns = columns, index = None)
    reg_summary.to_excel(xlsx_writer, index=None, sheet_name=&#39;Registration Quality Statistics&#39;)
    Stack_info = pd.DataFrame([{&#39;Stack Filename&#39; : mrc_filename, &#39;Sample_ID&#39; : Sample_ID, &#39;invert_data&#39; : invert_data}]).T # prepare to be save in transposed format
    header_info = pd.DataFrame([header_dict]).T
    Stack_info = Stack_info.append(header_info)
    Stack_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;Stack Info&#39;)
    xlsx_writer.save()

    return reg_summary, registration_summary_xlsx


def show_eval_box_mrc_stack(mrc_filename, **kwargs):
    &#39;&#39;&#39;
    Read MRC stack and display the eval box for each frame from the list.
    ©G.Shtengel, 04/2021. gleb.shtengel@gmail.com

    Parameters
    ---------
    mrc_filename : str
        File name (full path) of the mrc stack to be analyzed

    kwargs:
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    ax : matplotlib ax artist
        if provided, the data is exported to external ax object.
    frame_inds : array
        List of frame indices to display the evaluation box. If not provided, three frames will be used:
        [nz//10,  nz//2, nz//10*9] where nz is number of frames in mrc stack
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    invert_data : Boolean
    &#39;&#39;&#39;
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    save_filename = kwargs.get(&#34;save_filename&#34;, mrc_filename )
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    ax = kwargs.get(&#34;ax&#34;, &#39;&#39;)
    plot_internal = (ax == &#39;&#39;)

    mrc = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;)
    header = mrc.header
    nx, ny, nz = int32(header[&#39;nx&#39;]), int32(header[&#39;ny&#39;]), int32(header[&#39;nz&#39;])
    &#39;&#39;&#39;
        mode 0 -&gt; uint8
        mode 1 -&gt; int16
        mode 2 -&gt; float32
        mode 4 -&gt; complex64
        mode 6 -&gt; uint16
    &#39;&#39;&#39;
    mrc_mode = header.mode
    if mrc_mode==0:
        dt_mrc=uint8
    if mrc_mode==1:
        dt_mrc=int16
    if mrc_mode==2:
        dt_mrc=float32
    if mrc_mode==4:
        dt_mrc=complex64
    if mrc_mode==6:
        dt_mrc=uint16

    frame_inds = kwargs.get(&#34;frame_inds&#34;, [nz//10,  nz//2, nz//10*9] )

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = nx
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ny

    if sliding_evaluation_box:
        dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
        dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
    else:
        dx_eval = 0
        dy_eval = 0

    for fr_ind in frame_inds:
        eval_frame = (mrc.data[fr_ind, :, :].astype(dt_mrc)).astype(float)

        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*fr_ind//nz
            yi_eval = start_evaluation_box[0] + dy_eval*fr_ind//nz
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny

        if plot_internal:
            fig, ax = subplots(1,1, figsize = (10.0, 11.0*ny/nx))
        dmin, dmax = get_min_max_thresholds(eval_frame[yi_eval:ya_eval, xi_eval:xa_eval], disp_res=False)
        if invert_data:
            ax.imshow(eval_frame, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
        else:
            ax.imshow(eval_frame, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.grid(True, color = &#34;cyan&#34;)
        ax.set_title(Sample_ID + &#39; &#39;+mrc_filename +&#39;,  frame={:d}&#39;.format(fr_ind))
        rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        if save_res_png  and plot_internal:
            fname = os.path.splitext(save_filename)[0] + &#39;_frame_{:d}_evaluation_box.png&#39;.format(fr_ind)
            fig.savefig(fname, dpi=300)

    mrc.close()


def bin_crop_mrc_stack(mrc_filename, **kwargs):
    &#39;&#39;&#39;
    Bins and crops a 3D mrc stack along X-, Y-, or Z-directions and saves it into MRC or HDF5 format. ©G.Shtengel 08/2022 gleb.shtengel@gmail.com

    Parameters:
        mrc_filename : str
            name (full path) of the mrc file to be binned
    **kwargs:
        fnm_types : list of strings.
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is [&#39;mrc&#39;]. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        zbin_factor : int
            binning factor in z-direction
        xbin_factor : int
            binning factor in x-direction
        ybin_factor : int
            binning factor in y-direction
        mode  : str
            Binning mode. Default is &#39;mean&#39;, other option is &#39;sum&#39;
        frmax : int
            Maximum frame to bin. If not present, the entire file is binned
        binned_copped_filename : str
            name (full path) of the mrc file to save the results into. If not present, the new file name is constructed from the original by adding &#34;_zbinXX&#34; at the end.
        xi : int
            left edge of the crop
        xa : int
            right edge of the crop
        yi : int
            top edge of the crop
        ya : int
            bottom edge of the crop
        fri : int
            start frame
        fra : int
            stop frame
    Returns:
        fnms_saved : list of str
            Names of the new (binned and cropped) data files.
    &#39;&#39;&#39;
    fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
    xbin_factor = kwargs.get(&#34;xbin_factor&#34;, 1)      # binning factor in in x-direction
    ybin_factor = kwargs.get(&#34;ybin_factor&#34;, 1)      # binning factor in in y-direction
    zbin_factor = kwargs.get(&#34;zbin_factor&#34;, 1)      # binning factor in in z-direction

    mode = kwargs.get(&#39;mode&#39;, &#39;mean&#39;)                   # binning mode. Default is &#39;mean&#39;, other option is &#39;sum&#39;
    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;, permissive=True)
    header = mrc_obj.header
    &#39;&#39;&#39;
        mode 0 -&gt; uint8
        mode 1 -&gt; int16
        mode 2 -&gt; float32
        mode 4 -&gt; complex64
        mode 6 -&gt; uint16
    &#39;&#39;&#39;
    mrc_mode = mrc_obj.header.mode
    voxel_size_angstr = mrc_obj.voxel_size
    voxel_size_angstr_new = voxel_size_angstr.copy()
    voxel_size_angstr_new.x = voxel_size_angstr.x * xbin_factor
    voxel_size_angstr_new.y = voxel_size_angstr.y * ybin_factor
    voxel_size_angstr_new.z = voxel_size_angstr.z * zbin_factor
    voxel_size_new = voxel_size_angstr.copy()
    voxel_size_new.x = voxel_size_angstr_new.x / 10.0
    voxel_size_new.y = voxel_size_angstr_new.y / 10.0
    voxel_size_new.z = voxel_size_angstr_new.z / 10.0
    nx, ny, nz = int32(header[&#39;nx&#39;]), int32(header[&#39;ny&#39;]), int32(header[&#39;nz&#39;])
    frmax = kwargs.get(&#39;frmax&#39;, nz)
    xi = kwargs.get(&#39;xi&#39;, 0)
    xa = kwargs.get(&#39;xa&#39;, nx)
    yi = kwargs.get(&#39;yi&#39;, 0)
    ya = kwargs.get(&#39;ya&#39;, ny)
    fri = kwargs.get(&#39;fri&#39;, 0)
    fra = kwargs.get(&#39;fra&#39;, nz)
    nx_binned = (xa-xi)//xbin_factor
    ny_binned = (ya-yi)//ybin_factor
    xa = xi + nx_binned * xbin_factor
    ya = yi + ny_binned * ybin_factor
    binned_copped_filename_default = os.path.splitext(mrc_filename)[0] + &#39;_binned_croped.mrc&#39;
    binned_copped_filename = kwargs.get(&#39;binned_copped_filename&#39;, binned_copped_filename_default)
    binned_mrc_filename = os.path.splitext(binned_copped_filename)[0] + &#39;.mrc&#39;
    dt = type(mrc_obj.data[0,0,0])
    print(&#39;Source mrc_mode: {:d}, source data type:&#39;.format(mrc_mode), dt)
    print(&#39;Source Voxel Size (Angstroms): {:2f} x {:2f} x {:2f}&#39;.format(voxel_size_angstr.x, voxel_size_angstr.y, voxel_size_angstr.z))
    if mode == &#39;sum&#39;:
        mrc_mode = 1
        dt = int16
    print(&#39;Result mrc_mode: {:d}, source data type:&#39;.format(mrc_mode), dt)
    st_frames = np.arange(fri, fra, zbin_factor)
    print(&#39;New Data Set Shape:  {:d} x {:d} x {:d}&#39;.format(nx_binned, ny_binned, len(st_frames)))

    fnms_saved = []
    if &#39;mrc&#39; in fnm_types:
        fnms_saved.append(binned_mrc_filename)
        mrc_new = mrcfile.new_mmap(binned_mrc_filename, shape=(len(st_frames), ny_binned, nx_binned), mrc_mode=mrc_mode, overwrite=True)
        mrc_new.voxel_size = voxel_size_angstr_new
        #mrc_new.header.cella = voxel_size_angstr_new
        print(&#39;Result Voxel Size (Angstroms): {:2f} x {:2f} x {:2f}&#39;.format(voxel_size_angstr_new.x, voxel_size_angstr_new.y, voxel_size_angstr_new.z))
        desc = &#39;Saving the data stack into MRC file&#39;

    if &#39;h5&#39; in fnm_types:
        binned_h5_filename = os.path.splitext(binned_mrc_filename)[0] + &#39;.h5&#39;
        try:
            os.remove(binned_h5_filename)
        except:
            pass
        fnms_saved.append(binned_h5_filename)
        bdv_writer = npy2bdv.BdvWriter(binned_h5_filename, nchannels=1, blockdim=((1, 256, 256),))
        bdv_writer.append_view(stack=None, virtual_stack_dim=(len(st_frames),ny_binned,nx_binned),
                    time=0, channel=0,
                    voxel_size_xyz=(voxel_size_new.x, voxel_size_new.y, voxel_size_new.z), voxel_units=&#39;nm&#39;)
        if &#39;mrc&#39; in fnm_types:
            desc = &#39;Saving the data stack into MRC and H5 files&#39;
        else:
            desc = &#39;Saving the data stack into H5 file&#39;

    for j, st_frame in enumerate(tqdm(st_frames, desc=desc)):
        # need to fix this
        if mode == &#39;mean&#39;:
            zbinnd_fr = np.mean(mrc_obj.data[st_frame:min(st_frame+zbin_factor, nz-1), yi:ya, xi:xa], axis=0)
        else:
            zbinnd_fr = np.sum(mrc_obj.data[st_frame:min(st_frame+zbin_factor, nz-1), yi:ya, xi:xa], axis=0)
        if (xbin_factor &gt; 1) or (ybin_factor &gt; 1):
            if mode == &#39;mean&#39;:
                zbinnd_fr = np.mean(np.mean(zbinnd_fr.reshape(ny_binned, ybin_factor, nx_binned, xbin_factor), axis=3), axis=1)
            else:
                zbinnd_fr = np.sum(np.sum(zbinnd_fr.reshape(ny_binned, ybin_factor, nx_binned, xbin_factor), axis=3), axis=1)
        if &#39;mrc&#39; in fnm_types:
            mrc_new.data[j,:,:] = zbinnd_fr.astype(dt)
        if &#39;h5&#39; in fnm_types:
            bdv_writer.append_plane(plane=zbinnd_fr, z=j, time=0, channel=0)

    if &#39;mrc&#39; in fnm_types:
        mrc_new.close()

    if &#39;h5&#39; in fnm_types:
        bdv_writer.write_xml()
        bdv_writer.close()

    mrc_obj.close()

    return fnms_saved


def bin_crop_frames(bin_crop_parameters):
    &#39;&#39;&#39;
    Help function used by bin_crop_mrc_stack_DASK
    &#39;&#39;&#39;
    import logging
    logger = logging.getLogger(&#34;distributed.utils_perf&#34;)
    logger.setLevel(logging.ERROR)
    mrc_filename, save_filename, dtp, start_frame, stop_frame, xbin_factor, ybin_factor, zbin_factor, mode, xi, xa, yi, ya = bin_crop_parameters
    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;, permissive=True)
    if mode == &#39;mean&#39;:
        zbinnd_fr = np.mean(mrc_obj.data[start_frame:stop_frame, yi:ya, xi:xa], axis=0)
    else:
        zbinnd_fr = np.sum(mrc_obj.data[start_frame:stop_frame, yi:ya, xi:xa], axis=0)
    if (xbin_factor &gt; 1) or (ybin_factor &gt; 1):
        if mode == &#39;mean&#39;:
            zbinnd_fr = np.mean(np.mean(zbinnd_fr.reshape(ny_binned, ybin_factor, nx_binned, xbin_factor), axis=3), axis=1)
        else:
            zbinnd_fr = np.sum(np.sum(zbinnd_fr.reshape(ny_binned, ybin_factor, nx_binned, xbin_factor), axis=3), axis=1)
    tiff.imsave(save_filename, zbinnd_fr.astype(dtp))
    mrc_obj.close()
    return save_filename


def bin_crop_mrc_stack_DASK(DASK_client, mrc_filename, **kwargs):
    &#39;&#39;&#39;
    Bins a 3D mrc stack along Z-direction (optional binning in X-Y plane as well) and crops it along X- and Y- directions. ©G.Shtengel 08/2022 gleb.shtengel@gmail.com

    Parameters:
        DASK_client
        mrc_filename : str
            name (full path) of the mrc file to be binned
    **kwargs:
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        fnm_types : list of strings.
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is [&#39;mrc&#39;]. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        zbin_factor : int
            binning factor in z-direction
        xbin_factor : int
            binning factor in x-direction
        ybin_factor : int
            binning factor in y-direction
        mode  : str
            Binning mode. Default is &#39;mean&#39;, other option is &#39;sum&#39;
        frmax : int
            Maximum frame to bin. If not present, the entire file is binned
        binned_copped_filename : str
            name (full path) of the mrc file to save the results into. If not present, the new file name is constructed from the original by adding &#34;_zbinXX&#34; at the end.
        xi : int
            left edge of the crop
        xa : int
            right edge of the crop
        yi : int
            top edge of the crop
        ya : int
            bottom edge of the crop
        fri : int
            start frame
        fra : int
            stop frame
        disp_res : bolean
            Display messages and intermediate results
    Returns:
        fnms_saved : list of str
            Names of the new (binned and cropped) data files.
    &#39;&#39;&#39;
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 3)
    fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
    xbin_factor = kwargs.get(&#34;xbin_factor&#34;, 1)      # binning factor in in x-direction
    ybin_factor = kwargs.get(&#34;ybin_factor&#34;, 1)      # binning factor in in y-direction
    zbin_factor = kwargs.get(&#34;zbin_factor&#34;, 1)      # binning factor in in z-direction
    disp_res  = kwargs.get(&#34;disp_res&#34;, True )

    mode = kwargs.get(&#39;mode&#39;, &#39;mean&#39;)                   # binning mode. Default is &#39;mean&#39;, other option is &#39;sum&#39;
    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;, permissive=True)
    header = mrc_obj.header
    &#39;&#39;&#39;
        mode 0 -&gt; uint8
        mode 1 -&gt; int16
        mode 2 -&gt; float32
        mode 4 -&gt; complex64
        mode 6 -&gt; uint16
    &#39;&#39;&#39;
    mrc_mode = mrc_obj.header.mode
    #voxel_size_angstr = mrc_obj.header.cella
    voxel_size_angstr = mrc_obj.voxel_size
    voxel_size_angstr_new = voxel_size_angstr.copy()
    voxel_size_angstr_new.x = voxel_size_angstr.x * xbin_factor
    voxel_size_angstr_new.y = voxel_size_angstr.y * ybin_factor
    voxel_size_angstr_new.z = voxel_size_angstr.z * zbin_factor
    voxel_size_new = voxel_size_angstr.copy()
    voxel_size_new.x = voxel_size_angstr_new.x / 10.0
    voxel_size_new.y = voxel_size_angstr_new.y / 10.0
    voxel_size_new.z = voxel_size_angstr_new.z / 10.0
    nx, ny, nz = int32(header[&#39;nx&#39;]), int32(header[&#39;ny&#39;]), int32(header[&#39;nz&#39;])
    frmax = kwargs.get(&#39;frmax&#39;, nz)
    xi = kwargs.get(&#39;xi&#39;, 0)
    xa = kwargs.get(&#39;xa&#39;, nx)
    yi = kwargs.get(&#39;yi&#39;, 0)
    ya = kwargs.get(&#39;ya&#39;, ny)
    fri = kwargs.get(&#39;fri&#39;, 0)
    fra = kwargs.get(&#39;fra&#39;, nz)
    nx_binned = (xa-xi)//xbin_factor
    ny_binned = (ya-yi)//ybin_factor
    xa = xi + nx_binned * xbin_factor
    ya = yi + ny_binned * ybin_factor
    binned_copped_filename_default = os.path.splitext(mrc_filename)[0] + &#39;_binned_croped.mrc&#39;
    binned_copped_filename = kwargs.get(&#39;binned_copped_filename&#39;, binned_copped_filename_default)
    binned_mrc_filename = os.path.splitext(binned_copped_filename)[0] + &#39;.mrc&#39;
    dtp = type(mrc_obj.data[0,0,0])
    print(&#39;Source mrc_mode: {:d}, source data type:&#39;.format(mrc_mode), dtp)
    print(&#39;Source Voxel Size (Angstroms): {:2f} x {:2f} x {:2f}&#39;.format(voxel_size_angstr.x, voxel_size_angstr.y, voxel_size_angstr.z))
    if mode == &#39;sum&#39;:
        mrc_mode = 1
        dtp = int16
    print(&#39;Result mrc_mode: {:d}, source data type:&#39;.format(mrc_mode), dtp)

    st_frames = np.arange(fri, fra, zbin_factor)
    print(&#39;New Data Set Shape:  {:d} x {:d} x {:d}&#39;.format(nx_binned, ny_binned, len(st_frames)))

    bin_crop_parameters_dataset = []
    for j, st_frame in enumerate(tqdm(st_frames, desc=&#39;Setting up DASK parameter sets&#39;, display=disp_res)):
        save_filename = os.path.join(os.path.split(mrc_filename)[0],&#39;Binned_Cropped_Frame_{:d}.tif&#39;.format(j))
        start_frame = st_frame
        stop_frame = min(st_frame+zbin_factor, nz-1)
        bin_crop_parameters_dataset.append([mrc_filename, save_filename, dtp, start_frame, stop_frame, xbin_factor, ybin_factor, zbin_factor, mode, xi, xa, yi, ya])

    print(&#39;Binning / Cropping and Saving Intermediate Frames&#39;)
    if use_DASK:
        if disp_res:
            print(&#39;Starting DASK jobs&#39;)
        futures_bin_crop = DASK_client.map(bin_crop_frames, bin_crop_parameters_dataset, retries = DASK_client_retries)
        binned_cropped_filenames = np.array(DASK_client.gather(futures_bin_crop))
        if disp_res:
            print(&#39;Finished DASK jobs&#39;)
    else:   # if DASK is not used - perform local computations
        if disp_res:
            print(&#39;Will perform local computations&#39;)
        binned_cropped_filenames = []
        for bin_crop_parameters in tqdm(bin_crop_parameters_dataset, desc = &#39;Transforming and saving frame chunks&#39;, display = disp_res):
            binned_cropped_filenames.append(bin_crop_frames(bin_crop_parameters))

    print(&#34;Creating Dask Array Stack&#34;)
    # now build dask array of the transformed dataset
    # read the first file to get the shape and dtype (ASSUMING THAT ALL FILES SHARE THE SAME SHAPE/TYPE)
    frame0 = tiff.imread(binned_cropped_filenames[0])
    lazy_imread = dask.delayed(tiff.imread)  # lazy reader
    lazy_arrays = [lazy_imread(fn) for fn in binned_cropped_filenames]
    dask_arrays = [ da.from_delayed(delayed_reader, shape=frame0.shape, dtype=frame0.dtype)   for delayed_reader in lazy_arrays]
    # Stack infividual frames into one large dask.array
    FIBSEMstack = da.stack(dask_arrays, axis=0)
    nz, ny, nx = FIBSEMstack.shape

    print(&#39;Saving Intermediate Frames into Final Stacks&#39;)
    save_kwargs = {&#39;fnm_types&#39; : fnm_types,
                &#39;fnm_reg&#39; : binned_mrc_filename,
                &#39;voxel_size&#39; : voxel_size_new,
                &#39;dtp&#39; : dtp,
                &#39;disp_res&#39; : True}
    fnms_saved = save_data_stack(FIBSEMstack, **save_kwargs)

    for fnm in tqdm(binned_cropped_filenames, desc=&#39;Removing Intermediate Files: &#39;, display = disp_res):
        try:
            os.remove(os.path.join(fnm))
        except:
            pass

    return fnms_saved


##########################################
#         TIF stack analysis functions
##########################################

def show_eval_box_tif_stack(tif_filename, **kwargs):
    &#39;&#39;&#39;
    Read tif stack and display the eval box for each frame from the list.
    ©G.Shtengel, 08/2022. gleb.shtengel@gmail.com

    Parameters
    ---------
    tif_filename : str
        File name (full path) of the tif stack to be analyzed

    kwargs:
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    ax : matplotlib ax artist
        if provided, the data is exported to external ax object.
    frame_inds : array
        List of frame indices to display the evaluation box. If not provided, three frames will be used:
        [nz//10,  nz//2, nz//10*9] where nz is number of frames in tif stack
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    invert_data : Boolean
    &#39;&#39;&#39;
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    save_filename = kwargs.get(&#34;save_filename&#34;, tif_filename )
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    ax = kwargs.get(&#34;ax&#34;, &#39;&#39;)
    plot_internal = (ax == &#39;&#39;)

    with tiff.TiffFile(tif_filename) as tif:
        tif_tags = {}
        for tag in tif.pages[0].tags.values():
            name, value = tag.name, tag.value
            tif_tags[name] = value
    try:
        shape = eval(tif_tags[&#39;ImageDescription&#39;])
        nz, ny, nx = shape[&#39;shape&#39;]
    except:
        try:
            shape = eval(tif_tags[&#39;image_description&#39;])
            nz, ny, nx = shape[&#39;shape&#39;]
        except:
            fr0 = tiff.imread(tif_filename, key=0)
            ny, nx = np.shape(fr0)
            nz = eval(tif_tags[&#39;nimages&#39;])

    frame_inds = kwargs.get(&#34;frame_inds&#34;, [nz//10,  nz//2, nz//10*9] )

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = nx
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ny

    if sliding_evaluation_box:
        dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
        dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
    else:
        dx_eval = 0
        dy_eval = 0

    for fr_ind in frame_inds:
        #eval_frame = (tif.data[fr_ind, :, :].astype(dt)).astype(float)
        eval_frame = tiff.imread(tif_filename, key=fr_ind).astype(float)

        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*fr_ind//nz
            yi_eval = start_evaluation_box[0] + dy_eval*fr_ind//nz
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny

        if plot_internal:
            fig, ax = subplots(1,1, figsize = (10.0, 11.0*ny/nx))
        dmin, dmax = get_min_max_thresholds(eval_frame[yi_eval:ya_eval, xi_eval:xa_eval], disp_res=False)
        if invert_data:
            ax.imshow(eval_frame, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
        else:
            ax.imshow(eval_frame, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.grid(True, color = &#34;cyan&#34;)
        ax.set_title(Sample_ID + &#39; &#39;+tif_filename +&#39;,  frame={:d}&#39;.format(fr_ind))
        rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        if save_res_png  and plot_internal:
            fname = os.path.splitext(save_filename)[0] + &#39;_frame_{:d}_evaluation_box.png&#39;.format(fr_ind)
            fig.savefig(fname, dpi=300)


def evaluate_registration_two_frames_tif(params_tif):
    &#39;&#39;&#39;
    Helper function used by DASK routine. Analyzes registration between two frames.
    ©G.Shtengel, 08/2022. gleb.shtengel@gmail.com

    Parameters:
    params_tif : list of tif_filename, fr, evals
    tif_filename  : string
        full path to tif filename
    fr : int
        Index of the SECOND frame
    evals :  list of image bounds to be used for evaluation exi_eval, xa_eval, yi_eval, ya_eval, save_frame_png, filename_frame_png


    Returns:
    image_nsad, image_ncc, image_mi   : float, float, float

    &#39;&#39;&#39;
    tif_filename, fr, invert_data, evals, save_frame_png, filename_frame_png = params_tif
    xi_eval, xa_eval, yi_eval, ya_eval = evals

    frame0 = tiff.imread(tif_filename, key=int(fr-1)).astype(float)
    frame1 = tiff.imread(tif_filename, key=int(fr)).astype(float)

    if invert_data:
        prev_frame = -1.0 * frame0[yi_eval:ya_eval, xi_eval:xa_eval]
        curr_frame = -1.0 * frame1[yi_eval:ya_eval, xi_eval:xa_eval]
    else:
        prev_frame = frame0[yi_eval:ya_eval, xi_eval:xa_eval]
        curr_frame = frame1[yi_eval:ya_eval, xi_eval:xa_eval]
    fr_mean = np.abs(curr_frame/2.0 + prev_frame/2.0)
    #image_nsad =  np.mean(np.abs(curr_frame-prev_frame))/(np.mean(fr_mean)-np.amin(fr_mean))
    image_nsad =  np.mean(np.abs(curr_frame-prev_frame))/(np.mean(fr_mean)-np.amin(fr_mean))
    image_ncc = Two_Image_NCC_SNR(curr_frame, prev_frame)[0]
    image_mi = mutual_information_2d(prev_frame.ravel(), curr_frame.ravel(), sigma=1.0, bin=2048, normalized=True)
    if save_frame_png:
        yshape, xshape = frame0.shape
        fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
        fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
        dmin, dmax = get_min_max_thresholds(frame0[yi_eval:ya_eval, xi_eval:xa_eval])
        if invert_data:
            ax.imshow(frame0, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
        else:
            ax.imshow(frame0, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.text(0.06, 0.95, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(fr, image_nsad, image_ncc, image_mi), color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
        rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        ax.axis(&#39;off&#39;)
        fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
        plt.close(fig)

    return image_nsad, image_ncc, image_mi


def analyze_tif_stack_registration(tif_filename, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Read MRC stack and analyze registration - calculate NSAD, NCC, and MI.
    ©G.Shtengel, 08/2022. gleb.shtengel@gmail.com

    Parameters
    ---------
    tif_filename : str
        File name (full path) of the mrc stack to be analyzed
    DASK client (needs to be initialized and running by this time)

    kwargs:
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    frame_inds : array
        Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
    invert_data : boolean
        If True, the data will be inverted
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    save_filename : str
        Path to the filename to save the results. If empty, tif_filename+&#39;_RegistrationQuality.csv&#39; will be used

    Returns reg_summary : PD data frame, registration_summary_xlsx : path to summary XLSX workbook
    &#39;&#39;&#39;
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    save_filename = kwargs.get(&#34;save_filename&#34;, tif_filename )
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    registration_summary_xlsx = save_filename.replace(&#39;.mrc&#39;, &#39;_RegistrationQuality.xlsx&#39;)

    if sliding_evaluation_box:
        print(&#39;Will use sliding (linearly) evaluation box&#39;)
        print(&#39;   Starting with box:  &#39;, start_evaluation_box)
        print(&#39;   Finishing with box: &#39;, stop_evaluation_box)
    else:
        print(&#39;Will use fixed evaluation box: &#39;, evaluation_box)

    with tiff.TiffFile(tif_filename) as tif:
        tif_tags = {}
        for tag in tif.pages[0].tags.values():
            name, value = tag.name, tag.value
            tif_tags[name] = value
    #print(tif_tags)
    try:
        shape = eval(tif_tags[&#39;ImageDescription&#39;])
        nz, ny, nx = shape[&#39;shape&#39;]
    except:
        try:
            shape = eval(tif_tags[&#39;image_description&#39;])
            nz, ny, nx = shape[&#39;shape&#39;]
        except:
            fr0 = tiff.imread(tif_filename, key=0)
            ny, nx = np.shape(fr0)
            nz = eval(tif_tags[&#39;nimages&#39;])
    header_dict = {&#39;nx&#39; : nx, &#39;ny&#39; : ny, &#39;nz&#39; : nz }

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = nx
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ny
    evals = [xi_eval, xa_eval, yi_eval, ya_eval]

    frame_inds_default = np.arange(nz-1)+1
    frame_inds = np.array(kwargs.get(&#34;frame_inds&#34;, frame_inds_default))
    nf = frame_inds[-1]-frame_inds[0]+1
    if frame_inds[0]==0:
        frame_inds = frame_inds+1
    sample_frame_inds = [frame_inds[nf//10], frame_inds[nf//2], frame_inds[nf//10*9]]
    print(&#39;Will analyze regstrations in {:d} frames&#39;.format(len(frame_inds)))
    print(&#39;Will save the data into &#39; + registration_summary_xlsx)
    if sliding_evaluation_box:
        dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
        dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
    else:
        dx_eval = 0
        dy_eval = 0

    params_tif_mult = []
    xi_evals = np.zeros(nf, dtype=int16)
    xa_evals = np.zeros(nf, dtype=int16)
    yi_evals = np.zeros(nf, dtype=int16)
    ya_evals = np.zeros(nf, dtype=int16)
    for j, fr in enumerate(frame_inds):
        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*(fr-frame_inds[0])//nf
            yi_eval = start_evaluation_box[0] + dy_eval*(fr-frame_inds[0])//nf
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny
            evals = [xi_eval, xa_eval, yi_eval, ya_eval]
        xi_evals[j] = xi_eval
        xa_evals[j] = xa_eval
        yi_evals[j] = yi_eval
        ya_evals[j] = ya_eval
        if fr in sample_frame_inds:
            save_frame_png = save_sample_frames_png
            filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame{:d}.png&#39;.format(fr)
        else:
            save_frame_png = False
            filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame.png&#39;
        params_tif_mult.append([tif_filename, fr, invert_data, evals, save_frame_png, filename_frame_png])

    if use_DASK:
        print(&#39;Using DASK distributed&#39;)
        futures = DASK_client.map(evaluate_registration_two_frames_tif, params_tif_mult, retries = DASK_client_retries)
        dask_results = DASK_client.gather(futures)
        image_nsad = np.array([res[0] for res in dask_results])
        image_ncc = np.array([res[1] for res in dask_results])
        image_mi = np.array([res[2] for res in dask_results])
    else:
        print(&#39;Using Local Computation&#39;)
        image_nsad = np.zeros((nf), dtype=float)
        image_ncc = np.zeros((nf), dtype=float)
        image_mi = np.zeros((nf), dtype=float)
        results = []
        for params_tif_mult_pair in tqdm(params_tif_mult, desc=&#39;Evaluating frame registration: &#39;):
            print(params_tif_mult_pair)
            [tif_filename, fr, invert_data, evals] = params_tif_mult_pair
            print(fr)
            results.append(evaluate_registration_two_frames_tif(params_tif_mult_pair))
        image_nsad = np.array([res[0] for res in results])
        image_ncc = np.array([res[1] for res in results])
        image_mi = np.array([res[2] for res in results])

    nsads = [np.mean(image_nsad), np.median(image_nsad), np.std(image_nsad)]
    #image_ncc = image_ncc[1:-1]
    nccs = [np.mean(image_ncc), np.median(image_ncc), np.std(image_ncc)]
    nmis = [np.mean(image_mi), np.median(image_mi), np.std(image_mi)]

    print(&#39;Saving the Registration Quality Statistics into the file: &#39;, registration_summary_xlsx)
    xlsx_writer = pd.ExcelWriter(registration_summary_xlsx, engine=&#39;xlsxwriter&#39;)
    columns=[&#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;, &#39;Image NSAD&#39;, &#39;Image NCC&#39;, &#39;Image MI&#39;]
    reg_summary = pd.DataFrame(np.vstack((frame_inds, xi_evals, xa_evals, yi_evals, ya_evals, image_nsad, image_ncc, image_mi)).T, columns = columns, index = None)
    reg_summary.to_excel(xlsx_writer, index=None, sheet_name=&#39;Registration Quality Statistics&#39;)
    Stack_info = pd.DataFrame([{&#39;Stack Filename&#39; : tif_filename, &#39;Sample_ID&#39; : Sample_ID, &#39;invert_data&#39; : invert_data}]).T # prepare to be save in transposed format
    header_info = pd.DataFrame([header_dict]).T
    Stack_info = Stack_info.append(header_info)
    Stack_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;Stack Info&#39;)
    xlsx_writer.save()

    return reg_summary, registration_summary_xlsx


##########################################
#         helper functions for analysis of FiJi registration
##########################################

def read_transformation_matrix_from_xf_file(xf_filename):
    &#39;&#39;&#39;
    Reads transformation matrix created by FiJi-based workflow from *.xf file. ©G.Shtengel 10/2022 gleb.shtengel@gmail.com

    Parameters:
    xf_filename : str
        Full path to *.xf file containing the transformation matrix data

    Returns:
    transformation_matrix : array
    &#39;&#39;&#39;
    npdt_recalled = pd.read_csv(xf_filename, sep = &#39;  &#39;, header = None)
    tr = npdt_recalled.to_numpy()
    transformation_matrix = np.zeros((len(tr), 3, 3))
    transformation_matrix[:, 0, 0:2] = tr[:,0:2]
    transformation_matrix[:, 0, 2] = tr[:,6]
    transformation_matrix[:, 1, 0:2] = tr[:,2:4]
    transformation_matrix[:, 1, 2] = tr[:,9]
    transformation_matrix[:, 2, 2] = np.ones((len(tr)))
    return transformation_matrix

def analyze_transformation_matrix(transformation_matrix, xf_filename):
    &#39;&#39;&#39;
    Analyzes the transformation matrix created by FiJi-based workflow. ©G.Shtengel 10/2022 gleb.shtengel@gmail.com

    Parameters:
    transformation_matrix : array
        Transformation matrix (read by read_transformation_matrix_from_xf_file above).
    xf_filename : str
        Full path to *.xf file containing the transformation matrix data

    Returns:
    tr_matr_cum : array
    Cumulative transformation matrix
    &#39;&#39;&#39;
    Xshift_orig = transformation_matrix[:, 0, 2]
    Yshift_orig = transformation_matrix[:, 1, 2]
    Xscale_orig = transformation_matrix[:, 0, 0]
    Yscale_orig = transformation_matrix[:, 1, 1]
    tr_matr_cum = transformation_matrix.copy()

    prev_mt = np.eye(3,3)
    for j, cur_mt in enumerate(tqdm(transformation_matrix, desc=&#39;Calculating Cummilative Transformation Matrix&#39;)):
        if any(np.isnan(cur_mt)):
            print(&#39;Frame: {:d} has ill-defined transformation matrix, will use identity transformation instead:&#39;.format(j))
            print(cur_mt)
        else:
            prev_mt = np.matmul(cur_mt, prev_mt)
        tr_matr_cum[j] = prev_mt
    # Now insert identity matrix for the zero frame which does not need to be trasformed
    tr_matr_cum_orig = tr_matr_cum.copy()

    s00_cum_orig = tr_matr_cum[:, 0, 0].copy()
    s11_cum_orig = tr_matr_cum[:, 1, 1].copy()
    s01_cum_orig = tr_matr_cum[:, 0, 1].copy()
    s10_cum_orig = tr_matr_cum[:, 1, 0].copy()

    Xshift_cum_orig = tr_matr_cum_orig[:, 0, 2]
    Yshift_cum_orig = tr_matr_cum_orig[:, 1, 2]


    #print(&#39;Recalculating Shifts&#39;)
    s00_cum_orig = tr_matr_cum[:, 0, 0]
    s11_cum_orig = tr_matr_cum[:, 1, 1]
    fr = np.arange(0, len(s00_cum_orig), dtype=float)
    s00_slp = -1.0 * (np.sum(fr)-np.dot(s00_cum_orig,fr))/np.dot(fr,fr) # find the slope of a linear fit with fiorced first scale=1
    s00_fit = 1.0 + s00_slp * fr
    s00_cum_new = s00_cum_orig + 1.0 - s00_fit
    s11_slp = -1.0 * (np.sum(fr)-np.dot(s11_cum_orig,fr))/np.dot(fr,fr) # find the slope of a linear fit with fiorced first scale=1
    s11_fit = 1.0 + s11_slp * fr
    s11_cum_new = s11_cum_orig + 1.0 - s11_fit

    s01_slp = np.dot(s01_cum_orig,fr)/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
    s01_fit = s01_slp * fr
    s01_cum_new = s01_cum_orig - s01_fit
    s10_slp = np.dot(s10_cum_orig,fr)/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
    s10_fit = s10_slp * fr
    s10_cum_new = s10_cum_orig - s10_fit

    Xshift_cum = tr_matr_cum[:, 0, 2]
    Yshift_cum = tr_matr_cum[:, 1, 2]

    subtract_linear_fit=True

    # Subtract linear trend from offsets
    if subtract_linear_fit:
        fr = np.arange(0, len(Xshift_cum) )
        pX = np.polyfit(fr, Xshift_cum, 1)
        Xfit = np.polyval(pX, fr)
        pY = np.polyfit(fr, Yshift_cum, 1)
        Yfit = np.polyval(pY, fr)
        Xshift_residual = Xshift_cum - Xfit
        Yshift_residual = Yshift_cum - Yfit
    else:
        Xshift_residual = Xshift_cum.copy()
        Yshift_residual = Yshift_cum.copy()

    # define new cum. transformation matrix where the offests may have linear slopes subtracted
    tr_matr_cum_residual = tr_matr_cum.copy()
    tr_matr_cum_residual[:, 0, 2] = Xshift_residual
    tr_matr_cum_residual[:, 1, 2] = Yshift_residual
    tr_matr_cum_residual[:, 0, 0] = s00_cum_new
    tr_matr_cum_residual[:, 1, 1] = s11_cum_new
    tr_matr_cum_residual[:, 0, 1] = s01_cum_new
    tr_matr_cum_residual[:, 1, 0] = s10_cum_new

    fs = 12
    fig5, axs5 = subplots(3,3, figsize=(18, 12), sharex=True)
    fig5.subplots_adjust(left=0.15, bottom=0.08, right=0.99, top=0.94)

    # plot scales
    axs5[0, 0].plot(Xscale_orig, &#39;r&#39;, label = &#39;Sxx fr.-to-fr.&#39;)
    axs5[0, 0].plot(Yscale_orig, &#39;b&#39;, label = &#39;Syy fr.-to-fr.&#39;)
    axs5[0, 0].set_title(&#39;Frame-to-Frame Scale Change&#39;, fontsize = fs + 1)
    axs5[1, 0].plot(tr_matr_cum_orig[:, 0, 0], &#39;r&#39;, linestyle=&#39;dotted&#39;, label = &#39;Sxx cum.&#39;)
    axs5[1, 0].plot(tr_matr_cum_orig[:, 1, 1], &#39;b&#39;, linestyle=&#39;dotted&#39;, label = &#39;Syy cum.&#39;)
    axs5[1, 0].plot(s00_fit, &#39;r&#39;, label = &#39;Sxx cum. - lin. fit&#39;)
    axs5[1, 0].plot(s11_fit, &#39;b&#39;, label = &#39;Syy cum. - lin. fit&#39;)
    axs5[1, 0].set_title(&#39;Cumulative Scale&#39;, fontsize = fs + 1)
    axs5[2, 0].plot(tr_matr_cum_residual[:, 0, 0], &#39;r&#39;, label = &#39;Sxx cum. - residual&#39;)
    axs5[2, 0].plot(tr_matr_cum_residual[:, 1, 1], &#39;b&#39;, label = &#39;Syy cum. - residual&#39;)
    axs5[2, 0].set_title(&#39;Residual Cumulative Scale&#39;, fontsize = fs + 1)
    axs5[2, 0].set_xlabel(&#39;Frame&#39;, fontsize = fs + 1)

    # plot shears
    axs5[0, 1].plot(transformation_matrix[:, 0, 1], &#39;r&#39;, label = &#39;Sxy fr.-to-fr.&#39;)
    axs5[0, 1].plot(transformation_matrix[:, 1, 0], &#39;b&#39;, label = &#39;Syx fr.-to-fr.&#39;)
    axs5[0, 1].set_title(&#39;Frame-to-Frame Shear&#39;, fontsize = fs + 1)
    axs5[1, 1].plot(tr_matr_cum_orig[:, 0, 1], &#39;r&#39;, linestyle=&#39;dotted&#39;, label = &#39;Sxy cum.&#39;)
    axs5[1, 1].plot(tr_matr_cum_orig[:, 1, 0], &#39;b&#39;, linestyle=&#39;dotted&#39;, label = &#39;Syx cum.&#39;)
    axs5[1, 1].plot(s01_fit, &#39;r&#39;, label = &#39;Sxy cum. - lin. fit&#39;)
    axs5[1, 1].plot(s10_fit, &#39;b&#39;, label = &#39;Syx cum. - lin. fit&#39;)
    axs5[1, 1].set_title(&#39;Cumulative Shear&#39;, fontsize = fs + 1)
    axs5[2, 1].plot(tr_matr_cum_residual[:, 0, 1], &#39;r&#39;, label = &#39;Sxy cum. - residual&#39;)
    axs5[2, 1].plot(tr_matr_cum_residual[:, 1, 0], &#39;b&#39;, label = &#39;Syx cum. - residual&#39;)
    axs5[2, 1].set_title(&#39;Residual Cumulative Shear&#39;, fontsize = fs + 1)
    axs5[2, 1].set_xlabel(&#39;Frame&#39;, fontsize = fs + 1)

    # plot shifts
    axs5[0, 2].plot(Xshift_orig, &#39;r&#39;, label = &#39;Tx fr.-to-fr.&#39;)
    axs5[0, 2].plot(Yshift_orig, &#39;b&#39;, label = &#39;Ty fr.-to-fr.&#39;)
    axs5[0, 2].set_title(&#39;Frame-to-Frame Shift&#39;, fontsize = fs + 1)
    axs5[1, 2].plot(Xshift_cum, &#39;r&#39;, linestyle=&#39;dotted&#39;, label = &#39;Tx cum.&#39;)
    axs5[1, 2].plot(Xfit, &#39;r&#39;, label = &#39;Tx cum. - lin. fit&#39;)
    axs5[1, 2].plot(Yshift_cum, &#39;b&#39;, linestyle=&#39;dotted&#39;, label = &#39;Ty cum.&#39;)
    axs5[1, 2].plot(Yfit, &#39;b&#39;, label = &#39;Ty cum. - lin. fit&#39;)
    axs5[1, 2].set_title(&#39;Cumulative Shift&#39;, fontsize = fs + 1)
    axs5[2, 2].plot(tr_matr_cum_residual[:, 0, 2], &#39;r&#39;, label = &#39;Tx cum. - residual&#39;)
    axs5[2, 2].plot(tr_matr_cum_residual[:, 1, 2], &#39;b&#39;, label = &#39;Ty cum. - residual&#39;)
    axs5[2, 2].set_title(&#39;Residual Cumulative Shift&#39;, fontsize = fs + 1)
    axs5[2, 2].set_xlabel(&#39;Frame&#39;, fontsize = fs + 1)

    for ax in axs5.ravel():
        ax.grid(True)
        ax.legend()
    fig5.suptitle(xf_filename, fontsize = fs + 2)
    fig5.savefig(xf_filename +&#39;_Transform_Summary.png&#39;, dpi=300)
    return tr_matr_cum


##########################################
#         helper functions for results presentation
##########################################


def read_kwargs_xlsx(file_xlsx, kwargs_sheet_name, **kwargs):
    &#39;&#39;&#39;
    Reads (SIFT processing) kwargs from XLSX file and returns them as dictionary. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters:
    file_xlsx : str
        Full path to XLSX file containing a worksheet with SIFt parameters saves as two columns: (name, value)
    kwargs_sheet_name : str
        Name of the worksheet containing SIFT parameters
    &#39;&#39;&#39;
    disp_res_local = kwargs.get(&#39;disp_res&#39;, False)

    kwargs_dict_initial = {}
    try:
        stack_info = pd.read_excel(file_xlsx, header=None, sheet_name=kwargs_sheet_name).T                #read from transposed
        if len(stack_info.keys())&gt;0:
            if len(stack_info.keys())&gt;0:
                for key in stack_info.keys():
                    kwargs_dict_initial[stack_info[key][0]] = stack_info[key][1]
            else:
                kwargs_dict_initial[&#39;Stack Filename&#39;] = stack_info.index[1]
    except:
        if disp_res_local:
            print(&#39;No stack info record present, using defaults&#39;)
    kwargs_dict = {}
    for key in kwargs_dict_initial:
        if &#39;TransformType&#39; in key:
            exec(&#39;kwargs_dict[&#34;TransformType&#34;] = &#39; + kwargs_dict_initial[key].split(&#39;.&#39;)[-1].split(&#34;&#39;&#34;)[0])
        elif &#39;targ_vector&#39; in key:
            exec(&#39;kwargs_dict[&#34;targ_vector&#34;] = np.array(&#39; + kwargs_dict_initial[key].replace(&#39; &#39;, &#39;,&#39;)+ &#39;)&#39;)
        elif &#39;l2_matrix&#39; in key:
            exec(&#39;kwargs_dict[&#34;l2_matrix&#34;] = np.array(&#39; + kwargs_dict_initial[key].replace(&#39; &#39;, &#39;,&#39;) + &#39;)&#39;)
        elif &#39;fit_params&#39; in key:
            exec(&#39;kwargs_dict[&#34;fit_params&#34;] = &#39; + kwargs_dict_initial[key])
        elif &#39;subtract_linear_fit&#39; in key:
            exec(&#39;kwargs_dict[&#34;subtract_linear_fit&#34;] = np.array(&#39; + kwargs_dict_initial[key]+&#39;)&#39;)
        elif &#39;Stack Filename&#39; in key:
            exec(&#39;kwargs_dict[&#34;Stack Filename&#34;] = str(kwargs_dict_initial[key])&#39;)
        else:
            try:
                exec(&#39;kwargs_dict[&#34;&#39;+str(key)+&#39;&#34;] = &#39;+ str(kwargs_dict_initial[key]))
            except:
                exec(&#39;kwargs_dict[&#34;&#39;+str(key)+&#39;&#34;] = &#34;&#39; + kwargs_dict_initial[key].replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\n&#39;, &#39;,&#39;) + &#39;&#34;&#39;)
    if &#39;dump_filename&#39; in kwargs.keys():
        kwargs_dict[&#39;dump_filename&#39;] = kwargs[&#39;dump_filename&#39;]
    #correct for pandas mixed read failures
    try:
        if kwargs_dict[&#39;mrc_mode&#39;]:
            kwargs_dict[&#39;mrc_mode&#39;]=1
    except:
        pass
    try:
        if kwargs_dict[&#39;int_order&#39;]:
            kwargs_dict[&#39;int_order&#39;]=1
    except:
        pass
    try:
        if kwargs_dict[&#39;flipY&#39;] == 1:
            kwargs_dict[&#39;flipY&#39;] = True
        else:
            kwargs_dict[&#39;flipY&#39;] = False
    except:
        pass
    try:
        if kwargs_dict[&#39;BFMatcher&#39;] == 1:
            kwargs_dict[&#39;BFMatcher&#39;] = True
        else:
            kwargs_dict[&#39;BFMatcher&#39;] = False
    except:
        pass
    try:
        if kwargs_dict[&#39;invert_data&#39;] == 1:
            kwargs_dict[&#39;invert_data&#39;] = True
        else:
            kwargs_dict[&#39;invert_data&#39;] = False
    except:
        pass
    try:
        if kwargs_dict[&#39;sliding_evaluation_box&#39;] == 1:
            kwargs_dict[&#39;sliding_evaluation_box&#39;] = True
        else:
            kwargs_dict[&#39;sliding_evaluation_box&#39;] = False
    except:
        pass

    return kwargs_dict


def generate_report_mill_rate_xlsx(Mill_Rate_Data_xlsx, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for mill rate evaluation from XLSX spreadsheet file. ©G.Shtengel 12/2022 gleb.shtengel@gmail.com

    Parameters:
    Mill_Rate_Data_xlsx : str
        Path to the xlsx workbook containing the Working Distance (WD), Milling Y Voltage (MV), and FOV center shifts data.

    kwargs:
    Mill_Volt_Rate_um_per_V : float
        Milling Voltage to Z conversion (µm/V). Default is 31.235258870176065.

    &#39;&#39;&#39;
    disp_res = kwargs.get(&#39;disp_res&#39;, False)
    if disp_res:
        print(&#39;Loading kwarg Data&#39;)
    saved_kwargs = read_kwargs_xlsx(Mill_Rate_Data_xlsx, &#39;kwargs Info&#39;, **kwargs)
    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    Saved_Mill_Volt_Rate_um_per_V = saved_kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, 31.235258870176065)
    Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, Saved_Mill_Volt_Rate_um_per_V)

    if disp_res:
        print(&#39;Loading Working Distance and Milling Y Voltage Data&#39;)
    try:
        int_results = pd.read_excel(Mill_Rate_Data_xlsx, sheet_name=&#39;FIBSEM Data&#39;)
    except:
        int_results = pd.read_excel(Mill_Rate_Data_xlsx, sheet_name=&#39;Milling Rate Data&#39;)
    fr = int_results[&#39;Frame&#39;]
    WD = int_results[&#39;Working Distance (mm)&#39;]
    MillingYVoltage = int_results[&#39;Milling Y Voltage (V)&#39;]

    if disp_res:
        print(&#39;Generating Plot&#39;)
    fs = 12
    Mill_Volt_Rate_um_per_V = 31.235258870176065

    fig, axs = subplots(2,1, figsize = (6,7), sharex=True)
    fig.subplots_adjust(left=0.12, bottom=0.06, right=0.99, top=0.96, wspace=0.05, hspace=0.05)
    axs[0].plot(fr, WD, label=&#39;WD, Exp. Data&#39;, color=&#39;blue&#39;)
    axs[0].grid(True)
    axs[0].set_ylabel(&#39;Working Distance (mm)&#39;)
    #axs[0].set_xlim(xi, xa)
    WD_fit_coef = np.polyfit(fr, WD, 1)
    WD_fit=np.polyval(WD_fit_coef, fr)
    axs[0].plot(fr, WD_fit, label=&#39;Fit, slope = {:.2f} nm/line&#39;.format(WD_fit_coef[0]*1.0e6), color=&#39;red&#39;)
    axs[0].legend(fontsize=12)

    axs[1].plot(fr, MillingYVoltage, label=&#39;Mill. Y Volt. Exp. Data&#39;, color=&#39;green&#39;)
    axs[1].grid(True)
    axs[1].set_ylabel(&#39;Milling Y Voltage (V)&#39;)
    MV_fit_coef = np.polyfit(fr, MillingYVoltage, 1)
    MV_fit=np.polyval(MV_fit_coef, fr)
    axs[1].plot(fr, MV_fit, label=&#39;Fit, slope = {:.3f} nm/line&#39;.format(MV_fit_coef[0]*Mill_Volt_Rate_um_per_V*-1.0e3), color=&#39;orange&#39;)
    axs[1].legend(fontsize=12)
    axs[1].text(0.02, 0.05, &#39;Milling Voltage to Z conversion: {:.4f} µm/V&#39;.format(Mill_Volt_Rate_um_per_V), transform=axs[1].transAxes, fontsize=12)
    axs[1].set_xlabel(&#39;Frame&#39;)
    ldm = 70
    data_dir_short = data_dir if len(data_dir)&lt;ldm else &#39;... &#39;+ data_dir[-ldm:]
    try:
        axs[0].text(-0.15, 1.05, Sample_ID + &#39;    &#39; +  data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    except:
        axs[0].text(-0.15, 1.05, data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    fig.savefig(os.path.join(data_dir, Mill_Rate_Data_xlsx.replace(&#39;.xlsx&#39;,&#39;_Mill_Rate.png&#39;)), dpi=300)


def generate_report_FOV_center_shift_xlsx(Mill_Rate_Data_xlsx, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for FOV center shift from XLSX spreadsheet file. ©G.Shtengel 12/2022 gleb.shtengel@gmail.com

    Parameters:
    Mill_Rate_Data_xlsx : str
        Path to the xlsx workbook containing the Working Distance (WD), Milling Y Voltage (MV), and FOV center shifts data.

    kwargs:
    Mill_Volt_Rate_um_per_V : float
        Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.

    Returns: trend_x, trend_y
        Smoothed FOV shifts
    &#39;&#39;&#39;
    disp_res = kwargs.get(&#39;disp_res&#39;, False)
    if disp_res:
        print(&#39;Loading kwarg Data&#39;)
    saved_kwargs = read_kwargs_xlsx(Mill_Rate_Data_xlsx, &#39;kwargs Info&#39;, **kwargs)
    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)

    if disp_res:
        print(&#39;Loading FOV Center Location Data&#39;)
    try:
        int_results = pd.read_excel(Mill_Rate_Data_xlsx, sheet_name=&#39;FIBSEM Data&#39;)
    except:
        int_results = pd.read_excel(Mill_Rate_Data_xlsx, sheet_name=&#39;Milling Rate Data&#39;)
    fr = int_results[&#39;Frame&#39;]
    center_x = int_results[&#39;FOV X Center (Pix)&#39;]
    center_y = int_results[&#39;FOV Y Center (Pix)&#39;]
    apert = np.min((51, len(fr)-1))
    trend_x = savgol_filter(center_x*1.0, apert, 1) - center_x[0]
    trend_y = savgol_filter(center_y*1.0, apert, 1) - center_y[0]

    if disp_res:
        print(&#39;Generating Plot&#39;)
    fs = 12

    fig, axs = subplots(2,1, figsize = (6,7), sharex=True)
    fig.subplots_adjust(left=0.12, bottom=0.06, right=0.99, top=0.96, wspace=0.05, hspace=0.05)
    axs[0].plot(fr, center_x, label=&#39;FOV X center, Data&#39;, color=&#39;red&#39;)
    axs[0].plot(fr, center_y, label=&#39;FOV Y center, Data&#39;, color=&#39;blue&#39;)
    axs[0].grid(True)
    axs[0].set_ylabel(&#39;FOV Center (Pix)&#39;)
    #axs[0].set_xlim(xi, xa)
    axs[0].legend(fontsize=12)

    axs[1].plot(fr, trend_x, label=&#39;FOV X center shift, smoothed&#39;, color=&#39;red&#39;)
    axs[1].plot(fr, trend_y, label=&#39;FOV Y center shift, smoothed&#39;, color=&#39;blue&#39;)
    axs[1].grid(True)
    axs[1].set_ylabel(&#39;FOV Center Shift (Pix)&#39;)
    axs[1].legend(fontsize=12)
    axs[1].set_xlabel(&#39;Frame&#39;)
    ldm = 70
    data_dir_short = data_dir if len(data_dir)&lt;ldm else &#39;... &#39;+ data_dir[-ldm:]
    try:
        axs[0].text(-0.15, 1.05, Sample_ID + &#39;    &#39; +  data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    except:
        axs[0].text(-0.15, 1.05, data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    fig.savefig(os.path.join(data_dir, Mill_Rate_Data_xlsx.replace(&#39;.xlsx&#39;,&#39;_FOV_XYcenter.png&#39;)), dpi=300)
    return


def generate_report_data_minmax_xlsx(minmax_xlsx_file, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for data Min-Max from XLSX spreadsheet file. ©G.Shtengel 10/2022 gleb.shtengel@gmail.com

    Parameters:
    minmax_xlsx_file : str
        Path to the xlsx workbook containing Min-Max data
    &#39;&#39;&#39;
    disp_res = kwargs.get(&#39;disp_res&#39;, False)
    if disp_res:
        print(&#39;Loading kwarg Data&#39;)
    saved_kwargs = read_kwargs_xlsx(minmax_xlsx_file, &#39;kwargs Info&#39;, **kwargs)
    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = saved_kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    threshold_min = saved_kwargs.get(&#34;threshold_min&#34;, 0.0)
    threshold_max = saved_kwargs.get(&#34;threshold_min&#34;, 0.0)
    fit_params_saved = saved_kwargs.get(&#34;fit_params&#34;, [&#39;SG&#39;, 101, 3])
    fit_params = kwargs.get(&#34;fit_params&#34;, fit_params_saved)
    preserve_scales =  saved_kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below

    if disp_res:
        print(&#39;Loading MinMax Data&#39;)
    try:
        int_results = pd.read_excel(minmax_xlsx_file, sheet_name=&#39;FIBSEM Data&#39;)
    except:
        int_results = pd.read_excel(minmax_xlsx_file, sheet_name=&#39;MinMax Data&#39;)
    frames = int_results[&#39;Frame&#39;]
    frame_min = int_results[&#39;Min&#39;]
    frame_max = int_results[&#39;Max&#39;]
    data_min_glob  = np.min(frame_min)
    data_max_glob  = np.max(frame_max)
    &#39;&#39;&#39;
    sliding_min = int_results[&#39;Sliding Min&#39;]
    sliding_max = int_results[&#39;Sliding Max&#39;]
    &#39;&#39;&#39;
    sliding_min = savgol_filter(frame_min.astype(double), min([fit_params[1], fit_params[1]]), fit_params[2])
    sliding_max = savgol_filter(frame_max.astype(double), min([fit_params[1], fit_params[1]]), fit_params[2])

    if disp_res:
        print(&#39;Generating Plot&#39;)
    fs = 12
    fig0, ax0 = subplots(1,1,figsize=(6,4))
    fig0.subplots_adjust(left=0.14, bottom=0.11, right=0.99, top=0.94)
    ax0.plot(frame_min, &#39;b&#39;, linewidth=1, label=&#39;Frame Minima&#39;)
    ax0.plot(sliding_min, &#39;b&#39;, linewidth=2, linestyle = &#39;dotted&#39;, label=&#39;Sliding Minima&#39;)
    ax0.plot(frame_max, &#39;r&#39;, linewidth=1, label=&#39;Frame Maxima&#39;)
    ax0.plot(sliding_max, &#39;r&#39;, linewidth=2, linestyle = &#39;dotted&#39;, label=&#39;Sliding Maxima&#39;)
    ax0.legend()
    ax0.grid(True)
    ax0.set_xlabel(&#39;Frame&#39;)
    ax0.set_ylabel(&#39;Minima and Maxima Values&#39;)
    dxn = (data_max_glob - data_min_glob)*0.1
    ax0.set_ylim((data_min_glob - dxn, data_max_glob+dxn))
    # if needed, display the data in a narrower range
    #ax0.set_ylim((-4500, -1500))
    xminmax = [0, len(frame_min)]
    y_min = [data_min_glob, data_min_glob]
    y_max = [data_max_glob, data_max_glob]
    ax0.plot(xminmax, y_min, &#39;b&#39;, linestyle = &#39;--&#39;)
    ax0.plot(xminmax, y_max, &#39;r&#39;, linestyle = &#39;--&#39;)
    ax0.text(len(frame_min)/20.0, data_min_glob-dxn/1.75, &#39;data_min_glob={:.1f}&#39;.format(data_min_glob), fontsize = fs-2, c=&#39;b&#39;)
    ax0.text(len(frame_min)/20.0, data_max_glob+dxn/2.25, &#39;data_max_glob={:.1f}&#39;.format(data_max_glob), fontsize = fs-2, c=&#39;r&#39;)
    ax0.text(len(frame_min)/20.0, data_min_glob+dxn*4.5, &#39;threshold_min={:.1e}&#39;.format(threshold_min), fontsize = fs-2, c=&#39;b&#39;)
    ax0.text(len(frame_min)/20.0, data_min_glob+dxn*5.5, &#39;threshold_max={:.1e}&#39;.format(threshold_max), fontsize = fs-2, c=&#39;r&#39;)
    ldm = 70
    data_dir_short = data_dir if len(data_dir)&lt;ldm else &#39;... &#39;+ data_dir[-ldm:]

    try:
        ax0.text(-0.15, 1.05, Sample_ID + &#39;    &#39; +  data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    except:
        ax0.text(-0.15, 1.05, data_dir_short, fontsize = fs-2, transform=ax0.transAxes)
    &#39;&#39;&#39;
    try:
        fig0.suptitle(Sample_ID + &#39;    &#39; +  data_dir_short, fontsize = fs-2)
    except:
        fig0.suptitle(data_dir_short, fontsize = fs-2)
    &#39;&#39;&#39;
    fig0.savefig(os.path.join(data_dir, minmax_xlsx_file.replace(&#39;.xlsx&#39;,&#39;_Min_Max.png&#39;)), dpi=300)


def generate_report_transf_matrix_from_xlsx(transf_matrix_xlsx_file, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for Transformation Matrix from XLSX spreadsheet file. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters:
    transf_matrix_xlsx_file : str
        Path to the xlsx workbook containing Transformation Matrix data

    &#39;&#39;&#39;
    disp_res = kwargs.get(&#39;disp_res&#39;, False)
    if disp_res:
        print(&#39;Loading kwarg Data&#39;)
    saved_kwargs = read_kwargs_xlsx(transf_matrix_xlsx_file, &#39;kwargs Info&#39;, **kwargs)
    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = saved_kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    TransformType = saved_kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    SIFT_nfeatures = saved_kwargs.get(&#34;SIFT_nfeatures&#34;, 0)
    SIFT_nOctaveLayers = saved_kwargs.get(&#34;SIFT_nOctaveLayers&#34;, 3)
    SIFT_contrastThreshold = saved_kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.025)
    SIFT_edgeThreshold = saved_kwargs.get(&#34;SIFT_edgeThreshold&#34;, 10)
    SIFT_sigma = saved_kwargs.get(&#34;SIFT_sigma&#34;, 1.6)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = saved_kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = saved_kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = saved_kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = saved_kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = saved_kwargs.get(&#34;max_iter&#34;, 1000)
    BFMatcher = saved_kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = saved_kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    kp_max_num = saved_kwargs.get(&#34;kp_max_num&#34;, -1)
    save_res_png  = saved_kwargs.get(&#34;save_res_png&#34;, True)

    preserve_scales = saved_kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
    fit_params = saved_kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    subtract_linear_fit = saved_kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # The linear slopes along X- and Y- directions (respectively) will be subtracted from the cumulative shifts.
    subtract_FOVtrend_from_fit = saved_kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])
    pad_edges =  saved_kwargs.get(&#34;pad_edges&#34;, True)

    if disp_res:
        print(&#39;Loading Original Transformation Data&#39;)
    orig_transf_matrix = pd.read_excel(transf_matrix_xlsx_file, sheet_name=&#39;Orig. Transformation Matrix&#39;)
    transformation_matrix = np.vstack((orig_transf_matrix[&#39;T00 (Sxx)&#39;],
                         orig_transf_matrix[&#39;T01 (Sxy)&#39;],
                         orig_transf_matrix[&#39;T02 (Tx)&#39;],
                         orig_transf_matrix[&#39;T10 (Syx)&#39;],
                         orig_transf_matrix[&#39;T11 (Syy)&#39;],
                         orig_transf_matrix[&#39;T12 (Ty)&#39;],
                         orig_transf_matrix[&#39;T20 (0.0)&#39;],
                         orig_transf_matrix[&#39;T21 (0.0)&#39;],
                         orig_transf_matrix[&#39;T22 (1.0)&#39;])).T.reshape((len(orig_transf_matrix[&#39;T00 (Sxx)&#39;]), 3, 3))

    if disp_res:
        print(&#39;Loading Cumulative Transformation Data&#39;)
    cum_transf_matrix = pd.read_excel(transf_matrix_xlsx_file, sheet_name=&#39;Cum. Transformation Matrix&#39;)
    tr_matr_cum = np.vstack((cum_transf_matrix[&#39;T00 (Sxx)&#39;],
                         cum_transf_matrix[&#39;T01 (Sxy)&#39;],
                         cum_transf_matrix[&#39;T02 (Tx)&#39;],
                         cum_transf_matrix[&#39;T10 (Syx)&#39;],
                         cum_transf_matrix[&#39;T11 (Syy)&#39;],
                         cum_transf_matrix[&#39;T12 (Ty)&#39;],
                         cum_transf_matrix[&#39;T20 (0.0)&#39;],
                         cum_transf_matrix[&#39;T21 (0.0)&#39;],
                         cum_transf_matrix[&#39;T22 (1.0)&#39;])).T.reshape((len(cum_transf_matrix[&#39;T00 (Sxx)&#39;]), 3, 3))

    if disp_res:
        print(&#39;Loading Intermediate Data&#39;)
    int_results = pd.read_excel(transf_matrix_xlsx_file, sheet_name=&#39;Intermediate Results&#39;)
    s00_cum_orig = int_results[&#39;s00_cum_orig&#39;]
    s11_cum_orig = int_results[&#39;s11_cum_orig&#39;]
    s00_fit = int_results[&#39;s00_fit&#39;]
    s11_fit = int_results[&#39;s11_fit&#39;]
    s01_cum_orig = int_results[&#39;s01_cum_orig&#39;]
    s10_cum_orig = int_results[&#39;s10_cum_orig&#39;]
    s01_fit = int_results[&#39;s01_fit&#39;]
    s10_fit = int_results[&#39;s10_fit&#39;]
    Xshift_cum_orig = int_results[&#39;Xshift_cum_orig&#39;]
    Yshift_cum_orig = int_results[&#39;Yshift_cum_orig&#39;]
    Xshift_cum = int_results[&#39;Xshift_cum&#39;]
    Yshift_cum = int_results[&#39;Yshift_cum&#39;]
    Xfit = int_results[&#39;Xfit&#39;]
    Yfit = int_results[&#39;Yfit&#39;]

    if disp_res:
        print(&#39;Loading Statistics&#39;)
    stat_results = pd.read_excel(transf_matrix_xlsx_file, sheet_name=&#39;Reg. Stat. Info&#39;)
    npts = stat_results[&#39;Npts&#39;]
    error_abs_mean = stat_results[&#39;Mean Abs Error&#39;]

    fs = 14
    lwf = 2
    lwl = 1
    fig5, axs5 = subplots(4,3, figsize=(18, 16), sharex=True)
    fig5.subplots_adjust(left=0.07, bottom=0.03, right=0.99, top=0.95)
    # display the info
    axs5[0,0].axis(False)
    axs5[0,0].text(-0.1, 0.9, Sample_ID, fontsize = fs + 4)
    #axs5[0,0].text(-0.1, 0.73, &#39;Global Data Range:  Min={:.2f}, Max={:.2f}&#39;.format(data_min_glob, data_max_glob), transform=axs5[0,0].transAxes, fontsize = fs)

    if TransformType == RegularizedAffineTransform:
        tstr = [&#39;{:d}&#39;.format(x) for x in targ_vector]
        otext = &#39;Reg.Aff.Transf., λ= {:.1e}, t=[&#39;.format(l2_matrix[0,0]) + &#39; &#39;.join(tstr) + &#39;], w/&#39; + solver
    else:
        otext = TransformType.__name__ + &#39; with &#39; + solver + &#39; solver&#39;
    axs5[0,0].text(-0.1, 0.80, otext, transform=axs5[0,0].transAxes, fontsize = fs)

    SIFT1text = &#39;SIFT: nFeatures = {:d}, nOctaveLayers = {:d}, &#39;.format(SIFT_nfeatures, SIFT_nOctaveLayers)
    axs5[0,0].text(-0.1, 0.65, SIFT1text, transform=axs5[0,0].transAxes, fontsize = fs)

    SIFT2text = &#39;SIFT: contrThr = {:.3f}, edgeThr = {:.2f}, σ= {:.2f}&#39;.format(SIFT_contrastThreshold, SIFT_edgeThreshold, SIFT_sigma)
    axs5[0,0].text(-0.1, 0.50, SIFT2text, transform=axs5[0,0].transAxes, fontsize = fs)

    sbtrfit = (&#39;ON, &#39; if  subtract_linear_fit[0] else &#39;OFF, &#39;) + (&#39;ON&#39; if  subtract_linear_fit[1] else &#39;OFF&#39;) + (&#39;(ON, &#39; if  subtract_FOVtrend_from_fit[0] else &#39;(OFF, &#39;) + (&#39;ON)&#39; if  subtract_FOVtrend_from_fit[1] else &#39;OFF)&#39;)
    axs5[0,0].text(-0.1, 0.35, &#39;drmax={:.1f}, Max # of KeyPts={:d}, Max # of Iter.={:d}&#39;.format(drmax, kp_max_num, max_iter), transform=axs5[0,0].transAxes, fontsize = fs)
    padedges = &#39;ON&#39; if pad_edges else &#39;OFF&#39;
    if preserve_scales:
        fit_method = fit_params[0]
        if fit_method == &#39;LF&#39;:
            fit_str = &#39;, Meth: Linear Fit&#39;
            fm_string = &#39;linear&#39;
        else:
            if fit_method == &#39;SG&#39;:
                fit_str = &#39;, Meth: Sav.-Gol., &#39; + str(fit_params[1:])
                fm_string = &#39;Sav.-Gol.&#39;
            else:
                fit_str = &#39;, Meth: Pol.Fit, ord.={:d}&#39;.format(fit_params[1])
                fm_string = &#39;polyn.&#39;
        preserve_scales_string = &#39;Pres. Scls: ON&#39; + fit_str
    else:
        preserve_scales_string = &#39;Preserve Scales: OFF&#39;
    axs5[0,0].text(-0.1, 0.20, preserve_scales_string, transform=axs5[0,0].transAxes, fontsize = fs)
    axs5[0,0].text(-0.1, 0.05, &#39;Subtract Shift Fit: &#39; + sbtrfit + &#39;, Pad Edges: &#39; + padedges, transform=axs5[0,0].transAxes, fontsize = fs)
    # plot number of keypoints
    axs5[0, 1].plot(npts, &#39;g&#39;, linewidth = lwl, label = &#39;# of key-points per frame&#39;)
    axs5[0, 1].set_title(&#39;# of key-points per frame&#39;)
    axs5[0, 1].text(0.03, 0.2, &#39;Mean # of kpts= {:.0f}   Median # of kpts= {:.0f}&#39;.format(np.mean(npts), np.median(npts)), transform=axs5[0, 1].transAxes, fontsize = fs-1)
    # plot Standard deviations
    axs5[0, 2].plot(error_abs_mean, &#39;magenta&#39;, linewidth = lwl, label = &#39;Mean Abs Error over keyponts per frame&#39;)
    axs5[0, 2].set_title(&#39;Mean Abs Error keyponts per frame&#39;)
    axs5[0, 2].text(0.03, 0.2, &#39;Mean Abs Error= {:.3f}   Median Abs Error= {:.3f}&#39;.format(np.mean(error_abs_mean), np.median(error_abs_mean)), transform=axs5[0, 2].transAxes, fontsize = fs-1)

    # plot scales terms
    axs5[1, 0].plot(transformation_matrix[:, 0, 0], &#39;r&#39;, linewidth = lwl, label = &#39;Sxx frame-to-frame&#39;)
    axs5[1, 0].plot(transformation_matrix[:, 1, 1], &#39;b&#39;, linewidth = lwl, label = &#39;Syy frame-to-frame&#39;)
    axs5[1, 0].set_title(&#39;Frame-to-Frame Scale Change&#39;, fontsize = fs)
    axs5[2, 0].plot(s00_cum_orig, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Sxx cum.&#39;)
    axs5[2, 0].plot(s11_cum_orig, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Syy cum.&#39;)
    if preserve_scales:
        axs5[2, 0].plot(s00_fit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Sxx cum. - &#39;+fm_string+&#39; fit&#39;)
        axs5[2, 0].plot(s11_fit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Syy cum. - &#39;+fm_string+&#39; fit&#39;)
    axs5[2, 0].set_title(&#39;Cumulative Scale&#39;, fontsize = fs)
    yi10,ya10 = axs5[1, 0].get_ylim()
    dy0 = (ya10-yi10)/2.0
    yi20,ya20 = axs5[2, 0].get_ylim()
    if (ya20-yi20)&lt;0.01*dy0:
        axs5[2, 0].set_ylim((yi20-dy0, ya20+dy0))
    axs5[3, 0].plot(tr_matr_cum[:, 0, 0], &#39;r&#39;, linewidth = lwl, label = &#39;Sxx cum. - residual&#39;)
    axs5[3, 0].plot(tr_matr_cum[:, 1, 1], &#39;b&#39;, linewidth = lwl, label = &#39;Syy cum. - residual&#39;)
    axs5[3, 0].set_title(&#39;Residual Cumulative Scale&#39;, fontsize = fs)
    axs5[3, 0].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)
    yi30,ya30 = axs5[3, 0].get_ylim()
    if (ya30-yi30)&lt;0.01*dy0:
        axs5[3, 0].set_ylim((yi30-dy0, ya30+dy0))

    # plot shear terms
    axs5[1, 1].plot(transformation_matrix[:, 0, 1], &#39;r&#39;, linewidth = lwl, label = &#39;Sxy frame-to-frame&#39;)
    axs5[1, 1].plot(transformation_matrix[:, 1, 0], &#39;b&#39;, linewidth = lwl, label = &#39;Syx frame-to-frame&#39;)
    axs5[1, 1].set_title(&#39;Frame-to-Frame Shear Change&#39;, fontsize = fs)
    axs5[2, 1].plot(s01_cum_orig, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Sxy cum.&#39;)
    axs5[2, 1].plot(s10_cum_orig, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Syx cum.&#39;)
    if preserve_scales:
        axs5[2, 1].plot(s01_fit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Sxy cum. - &#39;+fm_string+&#39; fit&#39;)
        axs5[2, 1].plot(s10_fit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Syx cum. - &#39;+fm_string+&#39; fit&#39;)
    axs5[2, 1].set_title(&#39;Cumulative Shear&#39;, fontsize = fs)
    yi11,ya11 = axs5[1, 1].get_ylim()
    dy1 = (ya11-yi11)/2.0
    yi21,ya21 = axs5[2, 1].get_ylim()
    if (ya21-yi21)&lt;0.01*dy1:
        axs5[2, 1].set_ylim((yi21-dy1, ya21+dy1))
    axs5[3, 1].plot(tr_matr_cum[:, 0, 1], &#39;r&#39;, linewidth = lwl, label = &#39;Sxy cum. - residual&#39;)
    axs5[3, 1].plot(tr_matr_cum[:, 1, 0], &#39;b&#39;, linewidth = lwl, label = &#39;Syx cum. - residual&#39;)
    axs5[3, 1].set_title(&#39;Residual Cumulative Shear&#39;, fontsize = fs)
    axs5[3, 1].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)
    yi31,ya31 = axs5[3, 1].get_ylim()
    if (ya31-yi21)&lt;0.01*dy1:
        axs5[3, 1].set_ylim((yi31-dy1, ya31+dy1))

    # plot shifts
    axs5[1, 2].plot(transformation_matrix[:, 0, 2], &#39;r&#39;, linewidth = lwl, label = &#39;Tx fr.-to-fr.&#39;)
    axs5[1, 2].plot(transformation_matrix[:, 1, 2], &#39;b&#39;, linewidth = lwl, label = &#39;Ty fr.-to-fr.&#39;)
    axs5[1, 2].set_title(&#39;Frame-to-Frame Shift&#39;, fontsize = fs)
    if preserve_scales:
        axs5[2, 2].plot(Xshift_cum_orig, &#39;r&#39;, linewidth = lwl, label = &#39;Tx cum. - orig.&#39;)
        axs5[2, 2].plot(Yshift_cum_orig, &#39;b&#39;, linewidth = lwl, label = &#39;Ty cum. - orig.&#39;)
        axs5[2, 2].plot(Xshift_cum, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Tx cum. - pres. scales&#39;)
        axs5[2, 2].plot(Yshift_cum, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Ty cum. - pres. scales&#39;)
    else:
        axs5[2, 2].plot(Xshift_cum, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Tx cum.&#39;)
        axs5[2, 2].plot(Yshift_cum, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Ty cum.&#39;)
    if subtract_linear_fit[0]:
        axs5[2, 2].plot(Xfit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Tx cum. - lin. fit&#39;)
    if subtract_linear_fit[1]:
        axs5[2, 2].plot(Yfit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Ty cum. - lin. fit&#39;)
    axs5[2, 2].set_title(&#39;Cumulative Shift&#39;, fontsize = fs)
    axs5[3, 2].plot(tr_matr_cum[:, 0, 2], &#39;r&#39;, linewidth = lwl, label = &#39;Tx cum. - residual&#39;)
    axs5[3, 2].plot(tr_matr_cum[:, 1, 2], &#39;b&#39;, linewidth = lwl, label = &#39;Ty cum. - residual&#39;)
    axs5[3, 2].set_title(&#39;Residual Cumulative Shift&#39;, fontsize = fs)
    axs5[3, 2].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)

    for ax in axs5.ravel()[1:]:
        ax.grid(True)
        ax.legend(fontsize = fs-1)
    fig5.suptitle(transf_matrix_xlsx_file, fontsize = fs)
    if save_res_png :
        fig5.savefig(transf_matrix_xlsx_file.replace(&#39;.xlsx&#39;, &#39;.png&#39;), dpi=300)


def generate_report_transf_matrix_details(transf_matrix_bin_file, *kwarrgs):
    &#39;&#39;&#39;
    Generate Report Plot for Transformation Matrix from binary dump file. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com
    The binary dump file should contain list with these parameters (in this order):
        [saved_kwargs, npts, error_abs_mean, transformation_matrix,
        s00_cum_orig, s11_cum_orig, s00_fit, s11_fit, tr_matr_cum, s01_cum_orig, s10_cum_orig, s01_fit, s10_fit,
        Xshift_cum_orig, Yshift_cum_orig, Xshift_cum, Yshift_cum, Xfit, Yfit]

    Parameters:
    transf_matrix_bin_file : str
        Path to the binary dump file

    &#39;&#39;&#39;
    with open(transf_matrix_bin_file, &#34;rb&#34;) as f:
        [saved_kwargs, npts, error_abs_mean,
         transformation_matrix, s00_cum_orig, s11_cum_orig, s00_fit, s11_fit,
         tr_matr_cum, s01_cum_orig, s10_cum_orig, s01_fit, s10_fit,
         Xshift_cum_orig, Yshift_cum_orig, Xshift_cum, Yshift_cum, Xfit, Yfit] = pickle.load(f)

    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = saved_kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    TransformType = saved_kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    SIFT_nfeatures = saved_kwargs.get(&#34;SIFT_nfeatures&#34;, 0)
    SIFT_nOctaveLayers = saved_kwargs.get(&#34;SIFT_nOctaveLayers&#34;, 3)
    SIFT_contrastThreshold = saved_kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.025)
    SIFT_edgeThreshold = saved_kwargs.get(&#34;SIFT_edgeThreshold&#34;, 10)
    SIFT_sigma = saved_kwargs.get(&#34;SIFT_sigma&#34;, 1.6)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = saved_kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = saved_kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = saved_kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = saved_kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = saved_kwargs.get(&#34;max_iter&#34;, 1000)
    BFMatcher = saved_kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = saved_kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    kp_max_num = saved_kwargs.get(&#34;kp_max_num&#34;, -1)
    save_res_png  = saved_kwargs.get(&#34;save_res_png&#34;, True)

    preserve_scales =  saved_kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
    fit_params =  saved_kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    subtract_linear_fit =  saved_kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # The linear slopes along X- and Y- directions (respectively) will be subtracted from the cumulative shifts.
    subtract_FOVtrend_from_fit = saved_kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])
    #print(&#34;subtract_linear_fit:&#34;, subtract_linear_fit)
    pad_edges =  saved_kwargs.get(&#34;pad_edges&#34;, True)

    fs = 14
    lwf = 2
    lwl = 1
    fig5, axs5 = subplots(4,3, figsize=(18, 16), sharex=True)
    fig5.subplots_adjust(left=0.07, bottom=0.03, right=0.99, top=0.95)
    # display the info
    axs5[0,0].axis(False)
    axs5[0,0].text(-0.1, 0.9, Sample_ID, fontsize = fs + 4)
    #axs5[0,0].text(-0.1, 0.73, &#39;Global Data Range:  Min={:.2f}, Max={:.2f}&#39;.format(data_min_glob, data_max_glob), transform=axs5[0,0].transAxes, fontsize = fs)

    if TransformType == RegularizedAffineTransform:
        tstr = [&#39;{:d}&#39;.format(x) for x in targ_vector]
        otext = &#39;Reg.Aff.Transf., λ= {:.1e}, t=[&#39;.format(l2_matrix[0,0]) + &#39; &#39;.join(tstr) + &#39;], w/&#39; + solver
    else:
        otext = TransformType.__name__ + &#39; with &#39; + solver + &#39; solver&#39;
    axs5[0,0].text(-0.1, 0.80, otext, transform=axs5[0,0].transAxes, fontsize = fs)

    SIFT1text = &#39;SIFT: nFeatures = {:d}, nOctaveLayers = {:d}, &#39;.format(SIFT_nfeatures, SIFT_nOctaveLayers)
    axs5[0,0].text(-0.1, 0.65, SIFT1text, transform=axs5[0,0].transAxes, fontsize = fs)

    SIFT2text = &#39;SIFT: contrThr = {:.3f}, edgeThr = {:.2f}, σ= {:.2f}&#39;.format(SIFT_contrastThreshold, SIFT_edgeThreshold, SIFT_sigma)
    axs5[0,0].text(-0.1, 0.50, SIFT2text, transform=axs5[0,0].transAxes, fontsize = fs)

    sbtrfit = (&#39;ON, &#39; if  subtract_linear_fit[0] else &#39;OFF, &#39;) + (&#39;ON&#39; if  subtract_linear_fit[1] else &#39;OFF&#39;)
    axs5[0,0].text(-0.1, 0.35, &#39;drmax={:.1f}, Max # of KeyPts={:d}, Max # of Iter.={:d}&#39;.format(drmax, kp_max_num, max_iter), transform=axs5[0,0].transAxes, fontsize = fs)
    padedges = &#39;ON&#39; if pad_edges else &#39;OFF&#39;
    if preserve_scales:
        fit_method = fit_params[0]
        if fit_method == &#39;LF&#39;:
            fit_str = &#39;, Meth: Linear Fit&#39;
            fm_string = &#39;linear&#39;
        else:
            if fit_method == &#39;SG&#39;:
                fit_str = &#39;, Meth: Sav.-Gol., &#39; + str(fit_params[1:])
                fm_string = &#39;Sav.-Gol.&#39;
            else:
                fit_str = &#39;, Meth: Pol.Fit, ord.={:d}&#39;.format(fit_params[1])
                fm_string = &#39;polyn.&#39;
        preserve_scales_string = &#39;Pres. Scls: ON&#39; + fit_str
    else:
        preserve_scales_string = &#39;Preserve Scales: OFF&#39;
    axs5[0,0].text(-0.1, 0.20, preserve_scales_string, transform=axs5[0,0].transAxes, fontsize = fs)
    axs5[0,0].text(-0.1, 0.05, &#39;Subtract Shift Fit: &#39; + sbtrfit + &#39;, Pad Edges: &#39; + padedges, transform=axs5[0,0].transAxes, fontsize = fs)
    # plot number of keypoints
    axs5[0, 1].plot(npts, &#39;g&#39;, linewidth = lwl, label = &#39;# of key-points per frame&#39;)
    axs5[0, 1].set_title(&#39;# of key-points per frame&#39;)
    axs5[0, 1].text(0.03, 0.2, &#39;Mean # of kpts= {:.0f}   Median # of kpts= {:.0f}&#39;.format(np.mean(npts), np.median(npts)), transform=axs5[0, 1].transAxes, fontsize = fs-1)
    # plot Standard deviations
    axs5[0, 2].plot(error_abs_mean, &#39;magenta&#39;, linewidth = lwl, label = &#39;Mean Abs Error over keyponts per frame&#39;)
    axs5[0, 2].set_title(&#39;Mean Abs Error keyponts per frame&#39;)
    axs5[0, 2].text(0.03, 0.2, &#39;Mean Abs Error= {:.3f}   Median Abs Error= {:.3f}&#39;.format(np.mean(error_abs_mean), np.median(error_abs_mean)), transform=axs5[0, 2].transAxes, fontsize = fs-1)

    # plot scales terms
    axs5[1, 0].plot(transformation_matrix[:, 0, 0], &#39;r&#39;, linewidth = lwl, label = &#39;Sxx frame-to-frame&#39;)
    axs5[1, 0].plot(transformation_matrix[:, 1, 1], &#39;b&#39;, linewidth = lwl, label = &#39;Syy frame-to-frame&#39;)
    axs5[1, 0].set_title(&#39;Frame-to-Frame Scale Change&#39;, fontsize = fs)
    axs5[2, 0].plot(s00_cum_orig, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Sxx cum.&#39;)
    axs5[2, 0].plot(s11_cum_orig, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Syy cum.&#39;)
    if preserve_scales:
        axs5[2, 0].plot(s00_fit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Sxx cum. - &#39;+fm_string+&#39; fit&#39;)
        axs5[2, 0].plot(s11_fit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Syy cum. - &#39;+fm_string+&#39; fit&#39;)
    axs5[2, 0].set_title(&#39;Cumulative Scale&#39;, fontsize = fs)
    yi10,ya10 = axs5[1, 0].get_ylim()
    dy0 = (ya10-yi10)/2.0
    yi20,ya20 = axs5[2, 0].get_ylim()
    if (ya20-yi20)&lt;0.01*dy0:
        axs5[2, 0].set_ylim((yi20-dy0, ya20+dy0))
    axs5[3, 0].plot(tr_matr_cum[:, 0, 0], &#39;r&#39;, linewidth = lwl, label = &#39;Sxx cum. - residual&#39;)
    axs5[3, 0].plot(tr_matr_cum[:, 1, 1], &#39;b&#39;, linewidth = lwl, label = &#39;Syy cum. - residual&#39;)
    axs5[3, 0].set_title(&#39;Residual Cumulative Scale&#39;, fontsize = fs)
    axs5[3, 0].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)
    yi30,ya30 = axs5[3, 0].get_ylim()
    if (ya30-yi30)&lt;0.01*dy0:
        axs5[3, 0].set_ylim((yi30-dy0, ya30+dy0))

    # plot shear terms
    axs5[1, 1].plot(transformation_matrix[:, 0, 1], &#39;r&#39;, linewidth = lwl, label = &#39;Sxy frame-to-frame&#39;)
    axs5[1, 1].plot(transformation_matrix[:, 1, 0], &#39;b&#39;, linewidth = lwl, label = &#39;Syx frame-to-frame&#39;)
    axs5[1, 1].set_title(&#39;Frame-to-Frame Shear Change&#39;, fontsize = fs)
    axs5[2, 1].plot(s01_cum_orig, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Sxy cum.&#39;)
    axs5[2, 1].plot(s10_cum_orig, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Syx cum.&#39;)
    if preserve_scales:
        axs5[2, 1].plot(s01_fit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Sxy cum. - &#39;+fm_string+&#39; fit&#39;)
        axs5[2, 1].plot(s10_fit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Syx cum. - &#39;+fm_string+&#39; fit&#39;)
    axs5[2, 1].set_title(&#39;Cumulative Shear&#39;, fontsize = fs)
    yi11,ya11 = axs5[1, 1].get_ylim()
    dy1 = (ya11-yi11)/2.0
    yi21,ya21 = axs5[2, 1].get_ylim()
    if (ya21-yi21)&lt;0.01*dy1:
        axs5[2, 1].set_ylim((yi21-dy1, ya21+dy1))
    axs5[3, 1].plot(tr_matr_cum[:, 0, 1], &#39;r&#39;, linewidth = lwl, label = &#39;Sxy cum. - residual&#39;)
    axs5[3, 1].plot(tr_matr_cum[:, 1, 0], &#39;b&#39;, linewidth = lwl, label = &#39;Syx cum. - residual&#39;)
    axs5[3, 1].set_title(&#39;Residual Cumulative Shear&#39;, fontsize = fs)
    axs5[3, 1].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)
    yi31,ya31 = axs5[3, 1].get_ylim()
    if (ya31-yi21)&lt;0.01*dy1:
        axs5[3, 1].set_ylim((yi31-dy1, ya31+dy1))

    # plot shifts
    axs5[1, 2].plot(transformation_matrix[:, 0, 2], &#39;r&#39;, linewidth = lwl, label = &#39;Tx fr.-to-fr.&#39;)
    axs5[1, 2].plot(transformation_matrix[:, 1, 2], &#39;b&#39;, linewidth = lwl, label = &#39;Ty fr.-to-fr.&#39;)
    axs5[1, 2].set_title(&#39;Frame-to-Frame Shift&#39;, fontsize = fs)
    if preserve_scales:
        axs5[2, 2].plot(Xshift_cum_orig, &#39;r&#39;, linewidth = lwl, label = &#39;Tx cum. - orig.&#39;)
        axs5[2, 2].plot(Yshift_cum_orig, &#39;b&#39;, linewidth = lwl, label = &#39;Ty cum. - orig.&#39;)
        axs5[2, 2].plot(Xshift_cum, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Tx cum. - pres. scales&#39;)
        axs5[2, 2].plot(Yshift_cum, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Ty cum. - pres. scales&#39;)
    else:
        axs5[2, 2].plot(Xshift_cum, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Tx cum.&#39;)
        axs5[2, 2].plot(Yshift_cum, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Ty cum.&#39;)
    if subtract_linear_fit[0]:
        axs5[2, 2].plot(Xfit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Tx cum. - lin. fit&#39;)
    if subtract_linear_fit[1]:
        axs5[2, 2].plot(Yfit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Ty cum. - lin. fit&#39;)
    axs5[2, 2].set_title(&#39;Cumulative Shift&#39;, fontsize = fs)
    axs5[3, 2].plot(tr_matr_cum[:, 0, 2], &#39;r&#39;, linewidth = lwl, label = &#39;Tx cum. - residual&#39;)
    axs5[3, 2].plot(tr_matr_cum[:, 1, 2], &#39;b&#39;, linewidth = lwl, label = &#39;Ty cum. - residual&#39;)
    axs5[3, 2].set_title(&#39;Residual Cumulative Shift&#39;, fontsize = fs)
    axs5[3, 2].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)

    for ax in axs5.ravel()[1:]:
        ax.grid(True)
        ax.legend(fontsize = fs-1)
    fn = os.path.join(data_dir, fnm_reg)
    fig5.suptitle(fn, fontsize = fs)
    if save_res_png :
        fig5.savefig(fn.replace(&#39;.mrc&#39;, &#39;_Transform_Summary.png&#39;), dpi=300)


def generate_report_from_xls_registration_summary(file_xlsx, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for FIB-SEM data set registration from xlxs workbook file. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com
    XLS file should have pages (sheets):
        - &#39;Registration Quality Statistics&#39; - containing columns with the the evaluation box data and registration quality metrics data:
            &#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;, &#39;Npts&#39;, &#39;Mean Abs Error&#39;, &#39;Image NSAD&#39;, &#39;Image NCC&#39;, &#39;Image MI&#39;
        - &#39;Stack Info&#39; - containing the fields:
            &#39;Stack Filename&#39; and &#39;data_dir&#39;
        - &#39;SIFT kwargs&#39; (optional) - containg the kwargs with SIFT registration parameters.

    Parameters:
    xlsx_fname : str
        full path to the XLSX workbook file

    kwargs
    ---------
    sample_frame_files : list
        List of paths to sample frame images
    png_file : str
        filename to save the results. Default is file_xlsx with extension &#39;.xlsx&#39; replaced with &#39;.png&#39;
    invert_data : bolean
        If True, the representative data frames will use inverse LUT.
    dump_filename : str
        Filename of a binary dump of the FIBSEM_dataset object.

    &#39;&#39;&#39;
    xlsx_name = os.path.basename(os.path.abspath(file_xlsx))
    base_dir = os.path.dirname(os.path.abspath(file_xlsx))
    sample_frame_mask = xlsx_name.replace(&#39;_RegistrationQuality.xlsx&#39;, &#39;_sample_image_frame*.*&#39;)
    unsorted_sample_frame_files = glob.glob(os.path.join(base_dir, sample_frame_mask))
    try:
        unsorter_frames = [int(x.split(&#39;frame&#39;)[1].split(&#39;.png&#39;)[0]) for x in unsorted_sample_frame_files]
        sorted_inds = argsort(unsorter_frames)
        existing_sample_frame_files = [unsorted_sample_frame_files[i] for i in sorted_inds]
    except:
        existing_sample_frame_files = unsorted_sample_frame_files
    sample_frame_files = kwargs.get(&#39;sample_frame_files&#39;, existing_sample_frame_files)
    png_file_default = file_xlsx.replace(&#39;.xlsx&#39;,&#39;.png&#39;)
    png_file = kwargs.get(&#34;png_file&#34;, png_file_default)
    dump_filename = kwargs.get(&#34;dump_filename&#34;, &#39;&#39;)

    Regisration_data = pd.read_excel(file_xlsx, sheet_name=&#39;Registration Quality Statistics&#39;)
    # columns=[&#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;, &#39;Npts&#39;, &#39;Mean Abs Error&#39;, &#39;Image NSAD&#39;, &#39;Image NCC&#39;, &#39;Image MI&#39;]
    frames = Regisration_data[&#39;Frame&#39;]
    xi_evals = Regisration_data[&#39;xi_eval&#39;]
    xa_evals = Regisration_data[&#39;xa_eval&#39;]
    yi_evals = Regisration_data[&#39;yi_eval&#39;]
    ya_evals = Regisration_data[&#39;ya_eval&#39;]

    &#39;&#39;&#39;
    image_nsad = Regisration_data[&#39;NSAD&#39;]
    image_ncc = Regisration_data[&#39;NCC&#39;]
    image_nmi = Regisration_data[&#39;NMI&#39;]
    nsads = [np.mean(image_nsad), np.median(image_nsad), np.std(image_nsad)]
    nccs = [np.mean(image_ncc), np.median(image_ncc), np.std(image_ncc)]
    nmis = [np.mean(image_nmi), np.median(image_nmi), np.std(image_nmi)]
    &#39;&#39;&#39;
    eval_metrics = Regisration_data.columns[5:]
    num_metrics = len(eval_metrics)

    num_frames = len(frames)

    stack_info_dict = read_kwargs_xlsx(file_xlsx, &#39;Stack Info&#39;, **kwargs)
    if &#39;dump_filename&#39; in stack_info_dict.keys():
        dump_filename = kwargs.get(&#34;dump_filename&#34;, stack_info_dict[&#39;dump_filename&#39;])
    else:
        dump_filename = kwargs.get(&#34;dump_filename&#34;, &#39;&#39;)
    try:
        if np.isnan(dump_filename):
            dump_filename = &#39;&#39;
    except:
        pass
    stack_info_dict[&#39;dump_filename&#39;] = dump_filename

    try:
        invert_data =  kwargs.get(&#34;invert_data&#34;, stack_info_dict[&#39;invert_data&#39;])
    except:
        invert_data =  kwargs.get(&#34;invert_data&#34;, False)

    default_stack_name = file_xlsx.replace(&#39;_RegistrationQuality.xlsx&#39;,&#39;.mrc&#39;)
    stack_filename = os.path.normpath(stack_info_dict.get(&#39;Stack Filename&#39;, default_stack_name))
    data_dir = stack_info_dict.get(&#39;data_dir&#39;, &#39;&#39;)
    ftype = stack_info_dict.get(&#34;ftype&#34;, 0)


    heights = [0.8]*3 + [1.5]*num_metrics
    gs_kw = dict(height_ratios=heights)
    fig, axs = subplots((num_metrics+3), 1, figsize=(6, 2*(num_metrics+2)), gridspec_kw=gs_kw)
    fig.subplots_adjust(left=0.14, bottom=0.04, right=0.99, top=0.98, wspace=0.18, hspace=0.04)
    for ax in axs[0:3]:
        ax.axis(&#39;off&#39;)

    fs=12
    lwl=1

    if len(sample_frame_files)&gt;0:
        sample_frame_images_available = True
        for jf, ax in enumerate(axs[0:3]):
            try:
                ax.imshow(mpimg.imread(sample_frame_files[jf]))
                ax.axis(False)
            except:
                pass
    else:
        sample_frame_images_available = False
        sample_data_available = True
        if stack_exists:
            print(&#39;Will use sample images from the registered stack&#39;)
            use_raw_data = False
            if Path(stack_filename).suffix == &#39;.mrc&#39;:
                mrc_obj = mrcfile.mmap(stack_filename, mode=&#39;r&#39;)
                header = mrc_obj.header
                mrc_mode = header.mode
                &#39;&#39;&#39;
                mode 0 -&gt; uint8
                mode 1 -&gt; int16
                mode 2 -&gt; float32
                mode 4 -&gt; complex64
                mode 6 -&gt; uint16
                &#39;&#39;&#39;
                if mrc_mode==0:
                    dt_mrc=uint8
                if mrc_mode==1:
                    dt_mrc=int16
                if mrc_mode==2:
                    dt_mrc=float32
                if mrc_mode==4:
                    dt_mrc=complex64
                if mrc_mode==6:
                    dt_mrc=uint16
        else:
            print(&#39;Will use sample images from the raw data&#39;)
            if os.path.exists(dump_filename):
                print(&#39;Trying to recall the data from &#39;, dump_filename)
            try:
                print(&#39;Looking for the raw data in the directory&#39;, data_dir)
                if ftype == 0:
                    fls = sorted(glob.glob(os.path.join(data_dir,&#39;*.dat&#39;)))
                    if len(fls) &lt; 1:
                        fls = sorted(glob.glob(os.path.join(data_dir,&#39;*/*.dat&#39;)))
                if ftype == 1:
                    fls = sorted(glob.glob(os.path.join(data_dir,&#39;*.tif&#39;)))
                    if len(fls) &lt; 1:
                        fls = sorted(glob.glob(os.path.join(data_dir,&#39;*/*.tif&#39;)))
                num_frames = len(fls)
                stack_info_dict[&#39;disp_res&#39;]=False
                raw_dataset = FIBSEM_dataset(fls, recall_parameters=os.path.exists(dump_filename), **stack_info_dict)
                XResolution = raw_dataset.XResolution
                YResolution = raw_dataset.YResolution
                if pad_edges and perfrom_transformation:
                    #shape = [test_frame.YResolution, test_frame.XResolution]
                    shape = [YResolution, XResolution]
                    xmn, xmx, ymn, ymx = determine_pad_offsets(shape, raw_dataset.tr_matr_cum_residual)
                    padx = int(xmx - xmn)
                    pady = int(ymx - ymn)
                    xi = int(np.max([xmx, 0]))
                    yi = int(np.max([ymx, 0]))
                    # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
                    # so that the transformed images are not clipped.
                    # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
                    # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
                    # those are calculated below base on the amount of padding calculated above
                    shift_matrix = np.array([[1.0, 0.0, xi],
                                             [0.0, 1.0, yi],
                                             [0.0, 0.0, 1.0]])
                    inv_shift_matrix = np.linalg.inv(shift_matrix)
                else:
                    padx = 0
                    pady = 0
                    xi = 0
                    yi = 0
                    shift_matrix = np.eye(3,3)
                    inv_shift_matrix = np.eye(3,3)
                xsz = XResolution + padx
                xa = xi + XResolution
                ysz = YResolution + pady
                ya = yi + YResolution
                use_raw_data = True
            except:
                sample_data_available = False
                use_raw_data = False
        if sample_data_available:
            print(&#39;Sample data is available&#39;)
        else:
            print(&#39;Sample data is NOT available&#39;)

        if num_frames//10*9 &gt; 0:
            ev_ind2 = num_frames//10*9
        else:
            ev_ind2 = num_frames-1
        eval_inds = [num_frames//10,  num_frames//2, ev_ind2]
        #print(eval_inds)

        for j, eval_ind in enumerate(eval_inds):
            ax = axs[j]
            if sample_data_available:
                if stack_exists:
                    if Path(stack_filename).suffix == &#39;.mrc&#39;:
                        frame_img = (mrc_obj.data[frames[eval_ind], :, :].astype(dt_mrc)).astype(float)
                    if Path(stack_filename).suffix == &#39;.tif&#39;:
                        frame_img = tiff.imread(stack_filename, key=eval_ind)
                else:
                    dtp=float
                    chunk_frames = np.arange(eval_ind, min(eval_ind+zbin_factor, len(fls)-2))
                    frame_filenames = np.array(raw_dataset.fls)[chunk_frames]
                    tr_matrices = np.array(raw_dataset.tr_matr_cum_residual)[chunk_frames]
                    frame_img = transform_chunk_of_frames(frame_filenames, xsz, ysz, ftype,
                            flatten_image, image_correction_file,
                            perfrom_transformation, tr_matrices, shift_matrix, inv_shift_matrix,
                            xi, xa, yi, ya,
                            ImgB_fraction=0.0,
                            invert_data=False,
                            int_order=1,
                            flipY = raw_dataset.flipY)
                #print(eval_ind, np.shape(frame_img), yi_evals[eval_ind], ya_evals[eval_ind], xi_evals[eval_ind], xa_evals[eval_ind])
                if use_raw_data:
                    eval_ind = eval_ind//zbin_factor
                dmin, dmax = get_min_max_thresholds(frame_img[yi_evals[eval_ind]:ya_evals[eval_ind], xi_evals[eval_ind]:xa_evals[eval_ind]])
                if invert_data:
                    ax.imshow(frame_img, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
                else:
                    ax.imshow(frame_img, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)

                ax.text(0.03, 1.01, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(frames[eval_ind], image_nsad[eval_ind], image_ncc[eval_ind], image_nmi[eval_ind]), color=&#39;red&#39;, transform=ax.transAxes)
                rect_patch = patches.Rectangle((xi_evals[eval_ind], yi_evals[eval_ind]),abs(xa_evals[eval_ind]-xi_evals[eval_ind])-2,abs(ya_evals[eval_ind]-yi_evals[eval_ind])-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
                ax.add_patch(rect_patch)
            ax.axis(&#39;off&#39;)

        if stack_exists:
            if Path(stack_filename).suffix == &#39;.mrc&#39;:
                mrc_obj.close()

    axes_names = {&#39;NSAD&#39; : &#39;Norm. Sum of Abs. Diff&#39;,
                 &#39;NCC&#39; : &#39;Norm. Cross-Corr.&#39;,
                  &#39;NMI&#39; : &#39;Norm. Mutual Inf.&#39;,
                 &#39;FSC&#39; : &#39;FSC BW (inv pix)&#39;}
    colors = [&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;magenta&#39;, &#39;lime&#39;]

    for j, metric in enumerate(eval_metrics):
        metric_data = Regisration_data[metric]
        nmis = []
        axs[j+3].plot(frames, Regisration_data[metric], linewidth=lwl, color = colors[j])
        try:
            axs[j+3].set_ylabel(axes_names[metric], fontsize=fs-2)
        except:
            axs[j+3].set_ylabel(metric, fontsize=fs-2)
        axs[j+3].text(0.02, 0.04, (metric+&#39; mean = {:.3f}   &#39; + metric + &#39; median = {:.3f}  &#39; + metric + &#39; STD = {:.3f}&#39;).format(np.mean(metric_data), np.median(metric_data), np.std(metric_data)), transform=axs[j+3].transAxes, fontsize = fs-4)

    axs[-1].set_xlabel(&#39;Binned Frame #&#39;)
    for ax in axs[2:]:
        ax.grid(True)

    axs[0].text(-0.15, 2.7,stack_filename, transform=axs[3].transAxes)
    fig.savefig(png_file, dpi=300)


def plot_registrtion_quality_xlsx(data_files, labels, **kwargs):
    &#39;&#39;&#39;
    Read and plot together multiple registration quality summaries.
    ©G.Shtengel, 04/2021. gleb.shtengel@gmail.com

    Parameters:
    data_files : array of str
        Filenames (full paths) of the registration summaries (*.xlsx files)
    labels : array of str
        Labels (for each registration)

    kwargs:
    frame_inds : array or list of int
        Array or list oif frame indecis to use to azalyze the data.
    save_res_png : boolean
        If True, the PNG&#39;s of summary plots as well as summary Excel notebook are saved
    save_filename : str
        Filename (full path) to save the results (default is data_dir +&#39;Regstration_Summary.png&#39;)
    nsad_bounds : list of floats
        Bounds for NSAD plot (default is determined by get_min_max_thresholds with thresholds of 1e-4)
    ncc_bounds : list of floats
        Bounds for NCC plot (default is determined by get_min_max_thresholds with thresholds of 1e-4)
    nmi_bounds : list of floats
        Bounds for NMI plot (default is determined by get_min_max_thresholds with thresholds of 1e-4)
    colors : array or list of colors
        Optional colors for each plot/file. If not provided, will be auto-generated.
    linewidths : array of float
        linewidths for individual files. If not provided, all linewidts are set to 0.5

    Returns
    xlsx_fname : str
        Filename of the summary Excel notebook
    &#39;&#39;&#39;
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    linewidths = kwargs.get(&#34;linewidths&#34;, np.ones(len(data_files))*0.5)
    data_dir = os.path.split(data_files[0])[0]
    default_save_filename = os.path.join(data_dir, &#39;Regstration_Summary.png&#39;)
    save_filename = kwargs.get(&#34;save_filename&#34;, default_save_filename)
    nsad_bounds = kwargs.get(&#34;nsad_bounds&#34;, [0.0, 0.0])
    ncc_bounds = kwargs.get(&#34;ncc_bounds&#34;, [0.0, 0.0])
    nmi_bounds = kwargs.get(&#34;nmi_bounds&#34;, [0.0, 0.0])
    frame_inds = kwargs.get(&#34;frame_inds&#34;, [])

    nfls = len(data_files)
    reg_datas = []
    for data_file in data_files:
        # fl = os.path.join(data_dir, df)
        # data = pd.read_csv(fl)
        data = pd.read_excel(data_file, sheet_name=&#39;Registration Quality Statistics&#39;)
        reg_datas.append(data)

    lw0 = 0.5
    lw1 = 1

    fs=12
    fs2=10
    fig1, axs1 = subplots(3,1, figsize=(7, 11), sharex=True)
    fig1.subplots_adjust(left=0.1, bottom=0.05, right=0.99, top=0.96, wspace=0.2, hspace=0.1)

    ax_nsad = axs1[0]
    ax_ncc = axs1[1]
    ax_nmi = axs1[2]
    ax_nsad.set_ylabel(&#39;Normalized Sum of Abs. Differences&#39;, fontsize=fs)
    ax_ncc.set_ylabel(&#39;Normalized Cross-Correlation&#39;, fontsize=fs)
    ax_nmi.set_ylabel(&#39;Normalized Mutual Information&#39;, fontsize=fs)
    ax_nmi.set_xlabel(&#39;Frame&#39;, fontsize=fs)

    spreads=[]
    my_cols = [get_cmap(&#34;gist_rainbow_r&#34;)((nfls-j)/(nfls)) for j in np.arange(nfls)]
    my_cols[0] = &#39;grey&#39;
    my_cols[-1] = &#39;red&#39;
    my_cols = kwargs.get(&#34;colors&#34;, my_cols)

    means = []
    image_nsads= []
    image_nccs= []
    image_snrs= []
    image_nmis = []
    frame_inds_glob = []
    for j, reg_data in enumerate(tqdm(reg_datas, desc=&#39;generating the registration quality summary plots&#39;)):
        #my_col = get_cmap(&#34;gist_rainbow_r&#34;)((nfls-j)/(nfls))
        #my_cols.append(my_col)
        my_col = my_cols[j]
        pf = labels[j]
        lw0 = linewidths[j]
        if len(frame_inds)&gt;0:
            try:
                image_nsad = np.array(reg_data[&#39;Image NSAD&#39;])[frame_inds]
                image_ncc = np.array(reg_data[&#39;Image NCC&#39;])[frame_inds]
                image_nmi = np.array(reg_data[&#39;Image MI&#39;])[frame_inds]
            except:
                image_nsad = np.array(reg_data[&#39;NSAD&#39;])[frame_inds]
                image_ncc = np.array(reg_data[&#39;NCC&#39;])[frame_inds]
                image_nmi = np.array(reg_data[&#39;NMI&#39;])[frame_inds]
            frame_inds_loc = frame_inds.copy()
        else:
            try:
                image_nsad = np.array(reg_data[&#39;Image NSAD&#39;])
                image_ncc = np.array(reg_data[&#39;Image NCC&#39;])
                image_nmi = np.array(reg_data[&#39;Image MI&#39;])
            except:
                image_nsad = np.array(reg_data[&#39;NSAD&#39;])
                image_ncc = np.array(reg_data[&#39;NCC&#39;])
                image_nmi = np.array(reg_data[&#39;NMI&#39;])
            frame_inds_loc = np.arange(len(image_ncc))
        fr_i = min(frame_inds_loc) - (max(frame_inds_loc) - min(frame_inds_loc))*0.05
        fr_a = max(frame_inds_loc) + (max(frame_inds_loc) - min(frame_inds_loc))*0.05
        image_nsads.append(image_nsad)
        image_nccs.append(image_ncc)
        image_snr = image_ncc/(1.0-image_ncc)
        image_snrs.append(image_snr)
        image_nmis.append(image_nmi)
        frame_inds_glob.append(frame_inds_loc)

        eval_metrics = [image_nsad, image_ncc, image_snr, image_nmi]
        spreads.append([get_spread(metr) for metr in eval_metrics])
        means.append([np.mean(metr) for metr in eval_metrics])

        ax_nsad.plot(frame_inds_loc, image_nsad, c=my_col, linewidth=lw0)
        ax_nsad.plot(image_nsad[0], c=my_col, linewidth=lw1, label=pf)
        ax_ncc.plot(frame_inds_loc, image_ncc, c=my_col, linewidth=lw0)
        ax_ncc.plot(image_ncc[0], c=my_col, linewidth=lw1, label=pf)
        ax_nmi.plot(frame_inds_loc, image_nmi, c=my_col, linewidth=lw0)
        ax_nmi.plot(image_nmi[0], c=my_col, linewidth=lw1, label=pf)

    for ax in axs1.ravel():
        ax.grid(True)
        ax.legend(fontsize=fs2)

    if nsad_bounds[0]==nsad_bounds[1]:
        nsad_min, nsad_max = get_min_max_thresholds(np.concatenate(image_nsads),
                                                    thr_min=1e-4, thr_max=1e-4,
                                                    nbins=256, disp_res=False)
    else:
        nsad_min, nsad_max = nsad_bounds
    ax_nsad.set_ylim(nsad_min, nsad_max)

    if ncc_bounds[0]==ncc_bounds[1]:
        ncc_min, ncc_max = get_min_max_thresholds(np.concatenate(image_nccs),
                                                    thr_min=1e-4, thr_max=1e-4,
                                                    nbins=256, disp_res=False)
    else:
        ncc_min, ncc_max = ncc_bounds
    ax_ncc.set_ylim(ncc_min, ncc_max)

    if nmi_bounds[0]==nmi_bounds[1]:
        nmi_min, nmi_max = get_min_max_thresholds(np.concatenate(image_nmis),
                                                    thr_min=1e-4, thr_max=1e-4,
                                                    nbins=256, disp_res=False)
    else:
        nmi_min, nmi_max = nmi_bounds
    ax_nmi.set_ylim(nmi_min, nmi_max)
    ax_nmi.set_xlim(fr_i, fr_a)


    ax_nsad.text(-0.05, 1.05, data_dir, transform=ax_nsad.transAxes, fontsize=10)
    if save_res_png:
        fig1.savefig(save_filename, dpi=300)

    # Generate the Cell Text
    cell_text = []
    fig2_data = []
    limits = []
    rows = labels
    fst=9

    for j, (mean, spread) in enumerate(zip(means, spreads)):
        cell_text.append([&#39;{:.4f}&#39;.format(mean[0]), &#39;{:.4f}&#39;.format(spread[0]),
                          &#39;{:.4f}&#39;.format(mean[1]), &#39;{:.4f}&#39;.format(spread[1]), &#39;{:.4f}&#39;.format(mean[2]),
                          &#39;{:.4f}&#39;.format(mean[3]), &#39;{:.4f}&#39;.format(spread[3])])
        fig2_data.append([mean[0], spread[0], mean[1], spread[1], mean[2], mean[3], spread[3]])

    # Generate the table
    fig2, ax = subplots(1, 1, figsize=(9.5,1.3))
    fig2.subplots_adjust(left=0.32, bottom=0.01, right=0.98, top=0.86, wspace=0.05, hspace=0.05)
    ax.axis(False)
    ax.text(-0.30, 1.07, &#39;SIFT Registration Comparisons:  &#39; + data_dir, fontsize=fst)
    llw1=0.3
    clw = [llw1, llw1]

    columns = [&#39;NSAD Mean&#39;, &#39;NSAD Spread&#39;, &#39;NCC Mean&#39;, &#39;NCC Spread&#39;, &#39;Mean SNR&#39;, &#39;NMI Mean&#39;, &#39;NMI Spread&#39;]

    n_cols = len(columns)
    n_rows = len(rows)

    tbl = ax.table(cellText = cell_text,
                   rowLabels = rows,
                   colLabels = columns,
                   cellLoc = &#39;center&#39;,
                   colLoc = &#39;center&#39;,
                   bbox = [0.01, 0, 0.995, 1.0],
                  fontsize=16)
    tbl.auto_set_column_width(col=3)

    table_props = tbl.properties()
    try:
        table_cells = table_props[&#39;child_artists&#39;]
    except:
        table_cells = table_props[&#39;children&#39;]

    tbl.auto_set_font_size(False)
    for j, cell in enumerate(table_cells[0:n_cols*n_rows]):
        cell.get_text().set_color(my_cols[j//n_cols])
        cell.get_text().set_fontsize(fst)
    for j, cell in enumerate(table_cells[n_cols*(n_rows+1):]):
        cell.get_text().set_color(my_cols[j])
    for cell in table_cells[n_cols*n_rows:]:
    #    cell.get_text().set_fontweight(&#39;bold&#39;)
        cell.get_text().set_fontsize(fst)
    save_filename2 = save_filename.replace(&#39;.png&#39;, &#39;_table.png&#39;)
    if save_res_png:
        fig2.savefig(save_filename2, dpi=300)

    ysize_fig = 4
    ysize_tbl = 0.25 * nfls
    fst3 = 8
    fig3, axs3 = subplots(2, 1, figsize=(7, ysize_fig+ysize_tbl),  gridspec_kw={&#34;height_ratios&#34; : [ysize_tbl, ysize_fig]})
    fig3.subplots_adjust(left=0.10, bottom=0.10, right=0.98, top=0.96, wspace=0.05, hspace=0.05)

    for j, reg_data in enumerate(reg_datas):
        my_col = my_cols[j]
        pf = labels[j]
        lw0 = linewidths[j]
        if len(frame_inds)&gt;0:
            try:
                image_ncc = np.array(reg_data[&#39;Image NCC&#39;])[frame_inds]
            except:
                image_ncc = np.array(reg_data[&#39;NCC&#39;])[frame_inds]
            frame_inds_loc = frame_inds.copy()
        else:
            try:
                image_ncc = np.array(reg_data[&#39;Image NCC&#39;])
            except:
                image_ncc = np.array(reg_data[&#39;NCC&#39;])
            frame_inds_loc = np.arange(len(image_ncc))

        axs3[1].plot(frame_inds_loc, image_ncc, c=my_col, linewidth=lw0)
        axs3[1].plot(image_ncc[0], c=my_col, linewidth=lw1, label=pf)
    axs3[1].grid(True)
    axs3[1].legend(fontsize=fs2)
    axs3[1].set_ylabel(&#39;Normalized Cross-Correlation&#39;, fontsize=fs)
    axs3[1].set_xlabel(&#39;Frame&#39;, fontsize=fs)
    axs3[1].set_ylim(ncc_min, ncc_max)
    axs3[1].set_xlim(fr_i, fr_a)
    axs3[0].axis(False)
    axs3[0].text(-0.1, 1.07, &#39;SIFT Registration Comparisons:  &#39; + data_dir, fontsize=fst3)
    llw1=0.3
    clw = [llw1, llw1]

    columns2 = [&#39;NCC Mean&#39;, &#39;NCC Spread&#39;, &#39;SNR Mean&#39;, &#39;SNR Spread&#39;]
    cell2_text = []
    fig3_data = []
    limits = []
    rows = labels

    for j, (mean, spread) in enumerate(zip(means, spreads)):
        cell2_text.append([&#39;{:.4f}&#39;.format(mean[1]), &#39;{:.4f}&#39;.format(spread[1]),
                          &#39;{:.4f}&#39;.format(mean[2]), &#39;{:.4f}&#39;.format(spread[2])])
    n_cols = len(columns2)
    n_rows = len(rows)

    tbl2 = axs3[0].table(cellText = cell2_text,
                   rowLabels = rows,
                   colLabels = columns2,
                   cellLoc = &#39;center&#39;,
                   colLoc = &#39;center&#39;,
                   bbox = [0.38, 0, 0.55, 1.0],  # (left, bottom, width, height)
                  fontsize=16)
    rl = max([len(pf) for pf in labels])
    #tbl2.auto_set_column_width(col=[rl+5, len(columns2[0]), len(columns2[1]), len(columns2[2]), len(columns2[3])])
    tbl2.auto_set_column_width(col=list(range(len(columns2)+1)))
    tbl2.auto_set_font_size(False)

    table2_props = tbl2.properties()
    try:
        table2_cells = table2_props[&#39;child_artists&#39;]
    except:
        table2_cells = table2_props[&#39;children&#39;]

    tbl.auto_set_font_size(False)
    for j, cell in enumerate(table2_cells[0:n_cols*n_rows]):
        cell.get_text().set_color(my_cols[j//n_cols])
        cell.get_text().set_fontsize(fst3)
    for j, cell in enumerate(table2_cells[n_cols*(n_rows+1):]):
        cell.get_text().set_color(my_cols[j])
    for cell in table2_cells[n_cols*n_rows:]:
    #    cell.get_text().set_fontweight(&#39;bold&#39;)
        cell.get_text().set_fontsize(fst3)
    save_filename3 = save_filename.replace(&#39;.png&#39;, &#39;_fig_and_table.png&#39;)
    if save_res_png:
        fig3.savefig(save_filename3, dpi=300)

    if save_res_png:
        # Generate a single multi-page CSV file
        xlsx_fname = save_filename.replace(&#39;.png&#39;, &#39;.xlsx&#39;)
        # Create a Pandas Excel writer using XlsxWriter as the engine.
        writer = pd.ExcelWriter(xlsx_fname, engine=&#39;xlsxwriter&#39;)
        fig2_df = pd.DataFrame(fig2_data, columns=columns)
        fig2_df.insert(0, &#39;&#39;, labels)
        fig2_df.insert(1, &#39;Path&#39;, data_files)
        fig2_df.to_excel(writer, index=None, sheet_name=&#39;Summary&#39;)
        for reg_data, label in zip(tqdm(reg_datas, desc=&#39;saving the data into xlsx file&#39;), labels):
            data_fn = label[0:31]
            reg_data.to_excel(writer, sheet_name=data_fn)
        writer.save()
    return xlsx_fname


#######################################
#    class FIBSEM_frame
#######################################

class FIBSEM_frame:
    &#34;&#34;&#34;
    A class representing single FIB-SEM data frame.
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com
    Contains the info/settings on a single FIB-SEM data frame and the procedures that can be performed on it.

    Attributes (only some more important are listed here)
    ----------
    fname : str
        filename of the individual data frame
    header : str
        1024 bytes - header
    FileVersion : int
        file version number
    ChanNum : int
        Number of channels
    EightBit : int
        8-bit data switch: 0 for 16-bit data, 1 for 8-bit data
    Scaling : 2D array of floats
        scaling parameters allowing to convert I16 data into actual electron counts
    Sample_ID : str
        Sample_ID
    Notes : str
        Experiment notes
    DetA : str
        Detector A name
    DetB : str
        Detector B name (&#39;None&#39; if there is no Detector B)
    XResolution : int
        number of pixels - frame size in horizontal direction
    YResolution : int
        number of pixels - frame size in vertical direction
    PixelSize : float
        pixel size in nm. Default is 8.0

    Methods
    -------
    print_header()
        Prints a formatted content of the file header

    display_images()
        Display auto-scaled detector images without saving the figure into the file.

    save_images_jpeg(**kwargs)
        Display auto-scaled detector images and save the figure into JPEG file (s).

    save_images_tif(images_to_save = &#39;Both&#39;)
        Save the detector images into TIF file (s).

    get_image_min_max(image_name = &#39;ImageA&#39;, thr_min = 1.0e-4, thr_max = 1.0e-3, nbins=256, disp_res = False)
        Calculates the data range of the EM data.

    RawImageA_8bit_thresholds(thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
        Convert the Image A into 8-bit array

    RawImageB_8bit_thresholds(thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
            Convert the Image B into 8-bit array

    save_snapshot(display = True, dpi=300, thr_min = 1.0e-3, thr_max = 1.0e-3, nbins=256, **kwargs):
        Builds an image that contains both the Detector A and Detector B (if present) images as well as a table with important FIB-SEM parameters.

    analyze_noise_ROIs(**kwargs):
        Analyses the noise statistics in the selected ROI&#39;s of the EM data. (Calls Single_Image_Noise_ROIs(img, Noise_ROIs, Hist_ROI, **kwargs):)

    analyze_noise_statistics(**kwargs):
        Analyses the noise statistics of the EM data image. (Calls Single_Image_Noise_Statistics(img, **kwargs):)

    analyze_SNR_autocorr(**kwargs):
        Estimates SNR using auto-correlation analysis of a single image. (Calls Single_Image_SNR(img, **kwargs):)

    show_eval_box(**kwargs):
        Show the box used for evaluating the noise

    determine_field_fattening_parameters(**kwargs):
        Perfrom 2D parabolic fit (calls Perform_2D_fit(Img, estimator, **kwargs)) and determine the field-flattening parameters

    flatten_image(**kwargs):
        Flatten the image
    &#34;&#34;&#34;

    def __init__(self, fname, **kwargs):
        self.fname = fname
        self.ftype = kwargs.get(&#34;ftype&#34;, 0) # ftype=0 - Shan Xu&#39;s binary format  ftype=1 - tif files
        self.use_dask_arrays = kwargs.get(&#34;use_dask_arrays&#34;, False)
        if self.ftype == 1:
            self.RawImageA = tiff.imread(fname)

    # for tif files
        if self.ftype == 1:
            self.FileVersion = -1
            self.DetA = &#39;Detector A&#39;     # Name of detector A
            self.DetB = &#39;None&#39;     # Name of detector B
            try:
                with PILImage.open(self.fname) as img:
                    tif_header = {TAGS[key] : img.tag[key] for key in img.tag_v2}
                    self.header = tif_header
                try:
                    if tif_header[&#39;BitsPerSample&#39;][0]==8:
                        self.EightBit = 1
                    else:
                        self.EightBit = 0
                except:
                    self.EightBit = int(type(self.RawImageA[0,0])==np.uint8)
            except:
                self.header = &#39;&#39;
                self.EightBit = int(type(self.RawImageA[0,0])==np.uint8)

            self.PixelSize = kwargs.get(&#34;PixelSize&#34;, 8.0)
            self.Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
            self.YResolution, self.XResolution = self.RawImageA.shape
            self.Scaling = np.array([[1.0, 0.0, 1.0, 1.0], [1.0, 0.0, 1.0, 1.0]]).T

    # for Shan Xu&#39;s data files
        if self.ftype == 0:
            fid = open(self.fname, &#34;rb&#34;)
            fid.seek(0, 0)
            self.header = fid.read(1024) # Read in self.header
            self.FileMagicNum = unpack(&#39;&gt;L&#39;,self.header[0:4])[0]                       # Read in magic number, should be 3555587570
            self.FileVersion = unpack(&#39;&gt;h&#39;,self.header[4:6])[0]                        # Read in file version number
            self.FileType = unpack(&#39;&gt;h&#39;,self.header[6:8])[0]                           # Read in file type, 1 is Zeiss Neon detectors
            self.SWdate = (unpack(&#39;10s&#39;,self.header[8:18])[0]).decode(&#34;utf-8&#34;)         # Read in SW date
            self.TimeStep = unpack(&#39;&gt;d&#39;,self.header[24:32])[0]                         # Read in AI sampling time (including oversampling) in seconds
            self.ChanNum = unpack(&#39;b&#39;,self.header[32:33])[0]                           # Read in number of channels
            self.EightBit = unpack(&#39;b&#39;,self.header[33:34])[0]                          # Read in 8-bit data switch

            if self.FileVersion == 1:
                # Read in self.AI channel self.Scaling factors, (col#: self.AI#), (row#: offset, gain, 2nd order, 3rd order)
                self.ScalingS = unpack(&#39;&gt;&#39;+str(4*self.ChanNum)+&#39;d&#39;,self.header[36:(36+self.ChanNum*32)])
            elif self.FileVersion == 2 or self.FileVersion == 3 or self.FileVersion == 4 or self.FileVersion == 5 or self.FileVersion == 6:
                self.ScalingS = unpack(&#39;&gt;&#39;+str(4*self.ChanNum)+&#39;f&#39;,self.header[36:(36+self.ChanNum*16)])
            else:
                self.ScalingS = unpack(&#39;&gt;8f&#39;,self.header[36:68])
                self.Scaling = transpose(np.asarray(self.ScalingS).reshape(2,4))

            if self.FileVersion &gt; 8 :
                self.RestartFlag = unpack(&#39;b&#39;,self.header[68:69])[0]              # Read in restart flag
                self.StageMove = unpack(&#39;b&#39;,self.header[69:70])[0]                # Read in stage move flag
                self.FirstPixelX = unpack(&#39;&gt;l&#39;,self.header[70:74])[0]              # Read in first pixel X coordinate (center = 0)
                self.FirstPixelY = unpack(&#39;&gt;l&#39;,self.header[74:78])[0]              # Read in first pixel Y coordinate (center = 0)

            self.XResolution = unpack(&#39;&gt;L&#39;,self.header[100:104])[0]                # Read X resolution
            self.YResolution = unpack(&#39;&gt;L&#39;,self.header[104:108])[0]                # Read Y resolution

            if self.FileVersion == 1 or self.FileVersion == 2 or self.FileVersion == 3:
                self.Oversampling = unpack(&#39;&gt;B&#39;,self.header[108:109])[0]               # self.AI oversampling
                self.AIDelay = unpack(&#39;&gt;h&#39;,self.header[109:111])[0]                    # self.AI delay (# of samples)
            else:
                self.Oversampling = unpack(&#39;&gt;H&#39;,self.header[108:110])[0]

            self.ZeissScanSpeed = unpack(&#39;b&#39;,self.header[111:112])[0] # Scan speed (Zeiss #)

            if self.FileVersion == 1 or self.FileVersion == 2 or self.FileVersion == 3:
                self.ScanRate = unpack(&#39;&gt;d&#39;,self.header[112:120])[0]                   # Actual AO (scanning) rate
                self.FramelineRampdownRatio = unpack(&#39;&gt;d&#39;,self.header[120:128])[0]     # Frameline rampdown ratio
                self.Xmin = unpack(&#39;&gt;d&#39;,self.header[128:136])[0]                       # X coil minimum voltage
                self.Xmax = unpack(&#39;&gt;d&#39;,self.header[136:144])[0]                       # X coil maximum voltage
                self.Detmin = -10                                                      # Detector minimum voltage
                self.Detmax = 10                                                       # Detector maximum voltage
            else:
                self.ScanRate = unpack(&#39;&gt;f&#39;,self.header[112:116])[0]                   # Actual AO (scanning) rate
                self.FramelineRampdownRatio = unpack(&#39;&gt;f&#39;,self.header[116:120])[0]     # Frameline rampdown ratio
                self.Xmin = unpack(&#39;&gt;f&#39;,self.header[120:124])[0]                       # X coil minimum voltage
                self.Xmax = unpack(&#39;&gt;f&#39;,self.header[124:128])[0]                       # X coil maximum voltage
                self.Detmin = unpack(&#39;&gt;f&#39;,self.header[128:132])[0]                     # Detector minimum voltage
                self.Detmax = unpack(&#39;&gt;f&#39;,self.header[132:136])[0]                     # Detector maximum voltage
                self.DecimatingFactor = unpack(&#39;&gt;H&#39;,self.header[136:138])[0]           # Decimating factor

            self.AI1 = unpack(&#39;b&#39;,self.header[151:152])[0]                              # self.AI Ch1
            self.AI2 = unpack(&#39;b&#39;,self.header[152:153])[0]                              # self.AI Ch2
            self.AI3 = unpack(&#39;b&#39;,self.header[153:154])[0]                              # self.AI Ch3
            self.AI4 = unpack(&#39;b&#39;,self.header[154:155])[0]                              # self.AI Ch4

            self.Notes = (unpack(&#39;200s&#39;,self.header[180:380])[0]).decode(&#34;utf-8&#34;)       # Read in notes

            if self.FileVersion &gt; 8 :
                self.Sample_ID = (unpack(&#39;25s&#39;,self.header[155:180])[0]).decode(&#34;utf-8&#34;).strip(&#39;\x00&#39;) # Read in Sample ID
            else:
                self.Sample_ID = self.Notes.split(&#39;,&#39;)[0].strip(&#39;\x00&#39;)

            if self.FileVersion == 1 or self.FileVersion == 2:
                self.DetA = (unpack(&#39;10s&#39;,self.header[380:390])[0]).decode(&#34;utf-8&#34;)     # Name of detector A
                self.DetB = (unpack(&#39;18s&#39;,self.header[390:408])[0]).decode(&#34;utf-8&#34;)     # Name of detector B
                self.DetC = (unpack(&#39;20s&#39;,self.header[700:720])[0]).decode(&#34;utf-8&#34;)     # Name of detector C
                self.DetD = (unpack(&#39;20s&#39;,self.header[720:740])[0]).decode(&#34;utf-8&#34;)     # Name of detector D
                self.Mag = unpack(&#39;&gt;d&#39;,self.header[408:416])[0]                         # Magnification
                self.PixelSize = unpack(&#39;&gt;d&#39;,self.header[416:424])[0]                   # Pixel size in nm
                self.WD = unpack(&#39;&gt;d&#39;,self.header[424:432])[0]                          # Working distance in mm
                self.EHT = unpack(&#39;&gt;d&#39;,self.header[432:440])[0]                         # EHT in kV
                self.SEMApr = unpack(&#39;b&#39;,self.header[440:441])[0]                       # SEM aperture number
                self.HighCurrent = unpack(&#39;b&#39;,self.header[441:442])[0]                  # high current mode (1=on, 0=off)
                self.SEMCurr = unpack(&#39;&gt;d&#39;,self.header[448:456])[0]                     # SEM probe current in A
                self.SEMRot = unpack(&#39;&gt;d&#39;,self.header[456:464])[0]                      # SEM scan roation in degree
                self.ChamVac = unpack(&#39;&gt;d&#39;,self.header[464:472])[0]                     # Chamber vacuum
                self.GunVac = unpack(&#39;&gt;d&#39;,self.header[472:480])[0]                      # E-gun vacuum
                self.SEMStiX = unpack(&#39;&gt;d&#39;,self.header[480:488])[0]                     # SEM stigmation X
                self.SEMStiY = unpack(&#39;&gt;d&#39;,self.header[488:496])[0]                     # SEM stigmation Y
                self.SEMAlnX = unpack(&#39;&gt;d&#39;,self.header[496:504])[0]                     # SEM aperture alignment X
                self.SEMAlnY = unpack(&#39;&gt;d&#39;,self.header[504:512])[0]                     # SEM aperture alignment Y
                self.StageX = unpack(&#39;&gt;d&#39;,self.header[512:520])[0]                      # Stage position X in mm
                self.StageY = unpack(&#39;&gt;d&#39;,self.header[520:528])[0]                      # Stage position Y in mm
                self.StageZ = unpack(&#39;&gt;d&#39;,self.header[528:536])[0]                      # Stage position Z in mm
                self.StageT = unpack(&#39;&gt;d&#39;,self.header[536:544])[0]                      # Stage position T in degree
                self.StageR = unpack(&#39;&gt;d&#39;,self.header[544:552])[0]                      # Stage position R in degree
                self.StageM = unpack(&#39;&gt;d&#39;,self.header[552:560])[0]                      # Stage position M in mm
                self.BrightnessA = unpack(&#39;&gt;d&#39;,self.header[560:568])[0]                 # Detector A brightness (%)
                self.ContrastA = unpack(&#39;&gt;d&#39;,self.header[568:576])[0]                   # Detector A contrast (%)
                self.BrightnessB = unpack(&#39;&gt;d&#39;,self.header[576:584])[0]                 # Detector B brightness (%)
                self.ContrastB = unpack(&#39;&gt;d&#39;,self.header[584:592])[0]                   # Detector B contrast (%)
                self.Mode = unpack(&#39;b&#39;,self.header[600:601])[0]                         # FIB mode: 0=SEM, 1=FIB, 2=Milling, 3=SEM+FIB, 4=Mill+SEM, 5=SEM Drift Correction, 6=FIB Drift Correction, 7=No Beam, 8=External, 9=External+SEM
                self.FIBFocus = unpack(&#39;&gt;d&#39;,self.header[608:616])[0]                    # FIB focus in kV
                self.FIBProb = unpack(&#39;b&#39;,self.header[616:617])[0]                      # FIB probe number
                self.FIBCurr = unpack(&#39;&gt;d&#39;,self.header[624:632])[0]                     # FIB emission current
                self.FIBRot = unpack(&#39;&gt;d&#39;,self.header[632:640])[0]                      # FIB scan rotation
                self.FIBAlnX = unpack(&#39;&gt;d&#39;,self.header[640:648])[0]                     # FIB aperture alignment X
                self.FIBAlnY = unpack(&#39;&gt;d&#39;,self.header[648:656])[0]                     # FIB aperture alignment Y
                self.FIBStiX = unpack(&#39;&gt;d&#39;,self.header[656:664])[0]                     # FIB stigmation X
                self.FIBStiY = unpack(&#39;&gt;d&#39;,self.header[664:672])[0]                     # FIB stigmation Y
                self.FIBShiftX = unpack(&#39;&gt;d&#39;,self.header[672:680])[0]                   # FIB beam shift X in micron
                self.FIBShiftY = unpack(&#39;&gt;d&#39;,self.header[680:688])[0]                   # FIB beam shift Y in micron
            else:
                self.DetA = (unpack(&#39;10s&#39;,self.header[380:390])[0]).decode(&#34;utf-8&#34;)     # Name of detector A
                self.DetB = (unpack(&#39;18s&#39;,self.header[390:408])[0]).decode(&#34;utf-8&#34;)     # Name of detector B
                self.DetC = (unpack(&#39;20s&#39;,self.header[410:430])[0]).decode(&#34;utf-8&#34;)     # Name of detector C
                self.DetD = (unpack(&#39;20s&#39;,self.header[430:450])[0]).decode(&#34;utf-8&#34;)     # Name of detector D
                self.Mag = unpack(&#39;&gt;f&#39;,self.header[460:464])[0]                         # Magnification
                self.PixelSize = unpack(&#39;&gt;f&#39;,self.header[464:468])[0]                   # Pixel size in nm
                self.WD = unpack(&#39;&gt;f&#39;,self.header[468:472])[0]                          # Working distance in mm
                self.EHT = unpack(&#39;&gt;f&#39;,self.header[472:476])[0]                         # EHT in kV
                self.SEMApr = unpack(&#39;b&#39;,self.header[480:481])[0]                       # SEM aperture number
                self.HighCurrent = unpack(&#39;b&#39;,self.header[481:482])[0]                  # high current mode (1=on, 0=off)
                self.SEMCurr = unpack(&#39;&gt;f&#39;,self.header[490:494])[0]                     # SEM probe current in A
                self.SEMRot = unpack(&#39;&gt;f&#39;,self.header[494:498])[0]                      # SEM scan roation in degree
                self.ChamVac = unpack(&#39;&gt;f&#39;,self.header[498:502])[0]                     # Chamber vacuum
                self.GunVac = unpack(&#39;&gt;f&#39;,self.header[502:506])[0]                      # E-gun vacuum
                self.SEMShiftX = unpack(&#39;&gt;f&#39;,self.header[510:514])[0]                   # SEM beam shift X
                self.SEMShiftY = unpack(&#39;&gt;f&#39;,self.header[514:518])[0]                   # SEM beam shift Y
                self.SEMStiX = unpack(&#39;&gt;f&#39;,self.header[518:522])[0]                     # SEM stigmation X
                self.SEMStiY = unpack(&#39;&gt;f&#39;,self.header[522:526])[0]                     # SEM stigmation Y
                self.SEMAlnX = unpack(&#39;&gt;f&#39;,self.header[526:530])[0]                     # SEM aperture alignment X
                self.SEMAlnY = unpack(&#39;&gt;f&#39;,self.header[530:534])[0]                     # SEM aperture alignment Y
                self.StageX = unpack(&#39;&gt;f&#39;,self.header[534:538])[0]                      # Stage position X in mm
                self.StageY = unpack(&#39;&gt;f&#39;,self.header[538:542])[0]                      # Stage position Y in mm
                self.StageZ = unpack(&#39;&gt;f&#39;,self.header[542:546])[0]                      # Stage position Z in mm
                self.StageT = unpack(&#39;&gt;f&#39;,self.header[546:550])[0]                      # Stage position T in degree
                self.StageR = unpack(&#39;&gt;f&#39;,self.header[550:554])[0]                      # Stage position R in degree
                self.StageM = unpack(&#39;&gt;f&#39;,self.header[554:558])[0]                      # Stage position M in mm
                self.BrightnessA = unpack(&#39;&gt;f&#39;,self.header[560:564])[0]                 # Detector A brightness (%)
                self.ContrastA = unpack(&#39;&gt;f&#39;,self.header[564:568])[0]                   # Detector A contrast (%)
                self.BrightnessB = unpack(&#39;&gt;f&#39;,self.header[568:572])[0]                 # Detector B brightness (%)
                self.ContrastB = unpack(&#39;&gt;f&#39;,self.header[572:576])[0]                   # Detector B contrast (%)
                self.Mode = unpack(&#39;b&#39;,self.header[600:601])[0]                         # FIB mode: 0=SEM, 1=FIB, 2=Milling, 3=SEM+FIB, 4=Mill+SEM, 5=SEM Drift Correction, 6=FIB Drift Correction, 7=No Beam, 8=External, 9=External+SEM
                self.FIBFocus = unpack(&#39;&gt;f&#39;,self.header[604:608])[0]                    # FIB focus in kV
                self.FIBProb = unpack(&#39;b&#39;,self.header[608:609])[0]                      # FIB probe number
                self.FIBCurr = unpack(&#39;&gt;f&#39;,self.header[620:624])[0]                     # FIB emission current
                self.FIBRot = unpack(&#39;&gt;f&#39;,self.header[624:628])[0]                      # FIB scan rotation
                self.FIBAlnX = unpack(&#39;&gt;f&#39;,self.header[628:632])[0]                     # FIB aperture alignment X
                self.FIBAlnY = unpack(&#39;&gt;f&#39;,self.header[632:636])[0]                     # FIB aperture alignment Y
                self.FIBStiX = unpack(&#39;&gt;f&#39;,self.header[636:640])[0]                     # FIB stigmation X
                self.FIBStiY = unpack(&#39;&gt;f&#39;,self.header[640:644])[0]                     # FIB stigmation Y
                self.FIBShiftX = unpack(&#39;&gt;f&#39;,self.header[644:648])[0]                   # FIB beam shift X in micron
                self.FIBShiftY = unpack(&#39;&gt;f&#39;,self.header[648:652])[0]                   # FIB beam shift Y in micron

            if self.FileVersion &gt; 4:
                self.MillingXResolution = unpack(&#39;&gt;L&#39;,self.header[652:656])[0]                       # FIB milling X resolution
                self.MillingYResolution = unpack(&#39;&gt;L&#39;,self.header[656:660])[0]                       # FIB milling Y resolution
                self.MillingXSize = unpack(&#39;&gt;f&#39;,self.header[660:664])[0]                             # FIB milling X size (um)
                self.MillingYSize = unpack(&#39;&gt;f&#39;,self.header[664:668])[0]                             # FIB milling Y size (um)
                self.MillingULAng = unpack(&#39;&gt;f&#39;,self.header[668:672])[0]                             # FIB milling upper left inner angle (deg)
                self.MillingURAng = unpack(&#39;&gt;f&#39;,self.header[672:676])[0]                             # FIB milling upper right inner angle (deg)
                self.MillingLineTime = unpack(&#39;&gt;f&#39;,self.header[676:680])[0]                          # FIB line milling time (s)
                self.FIBFOV = unpack(&#39;&gt;f&#39;,self.header[680:684])[0]                                   # FIB FOV (um)
                self.MillingLinesPerImage = unpack(&#39;&gt;H&#39;,self.header[684:686])[0]                     # FIB milling lines per image
                self.MillingPIDOn = unpack(&#39;&gt;b&#39;,self.header[686:687])[0]                             # FIB milling PID on
                self.MillingPIDMeasured = unpack(&#39;&gt;b&#39;,self.header[689:690])[0]                       # FIB milling PID measured (0:specimen, 1:beamdump)
                self.MillingPIDTarget = unpack(&#39;&gt;f&#39;,self.header[690:694])[0]                         # FIB milling PID target
                self.MillingPIDTargetSlope = unpack(&#39;&gt;f&#39;,self.header[694:698])[0]                    # FIB milling PID target slope
                self.MillingPIDP = unpack(&#39;&gt;f&#39;,self.header[698:702])[0]                              # FIB milling PID P
                self.MillingPIDI = unpack(&#39;&gt;f&#39;,self.header[702:706])[0]                              # FIB milling PID I
                self.MillingPIDD = unpack(&#39;&gt;f&#39;,self.header[706:710])[0]                              # FIB milling PID D
                self.MachineID = (unpack(&#39;30s&#39;,self.header[800:830])[0]).decode(&#34;utf-8&#34;)             # Machine ID
                self.SEMSpecimenI = unpack(&#39;&gt;f&#39;,self.header[672:676])[0]                             # SEM specimen current (nA)

            if self.FileVersion &gt; 5 :
                self.Temperature = unpack(&#39;&gt;f&#39;,self.header[850:854])[0]                              # Temperature (F)
                self.FaradayCupI = unpack(&#39;&gt;f&#39;,self.header[854:858])[0]                              # Faraday cup current (nA)
                self.FIBSpecimenI = unpack(&#39;&gt;f&#39;,self.header[858:862])[0]                             # FIB specimen current (nA)
                self.BeamDump1I = unpack(&#39;&gt;f&#39;,self.header[862:866])[0]                               # Beam dump 1 current (nA)
                self.SEMSpecimenI = unpack(&#39;&gt;f&#39;,self.header[866:870])[0]                             # SEM specimen current (nA)
                self.MillingYVoltage = unpack(&#39;&gt;f&#39;,self.header[870:874])[0]                          # Milling Y voltage (V)
                self.FocusIndex = unpack(&#39;&gt;f&#39;,self.header[874:878])[0]                               # Focus index
                self.FIBSliceNum = unpack(&#39;&gt;L&#39;,self.header[878:882])[0]                              # FIB slice #

            if self.FileVersion &gt; 7:
                self.BeamDump2I = unpack(&#39;&gt;f&#39;,self.header[882:886])[0]                              # Beam dump 2 current (nA)
                self.MillingI = unpack(&#39;&gt;f&#39;,self.header[886:890])[0]                                # Milling current (nA)

            self.FileLength = unpack(&#39;&gt;q&#39;,self.header[1000:1008])[0]                                # Read in file length in bytes

#                Finish self.header read
#
#                Read raw data
#                fid.seek(1024, 0)
#                n_elements = self.ChanNum * self.XResolution * self.YResolution
#                print(n_elements, self.ChanNum, self.XResolution, self.YResolution)
#                if self.EightBit==1:
#                    raw_data = fid.read(n_elements) # Read in data
#                    Raw = unpack(&#39;&gt;&#39;+str(n_elements)+&#39;B&#39;,raw_data)
#                else:
#                    #raw_data = fid.read(2*n_elements) # Read in data
#                    #Raw = unpack(&#39;&gt;&#39;+str(n_elements)+&#39;h&#39;,raw_data)
#                fid.close
#                finish reading raw data

            n_elements = self.ChanNum * self.XResolution * self.YResolution
            fid.seek(1024, 0)
            if self.EightBit==1:
                dt = np.dtype(np.uint8)
                dt = dt.newbyteorder(&#39;&gt;&#39;)
                if self.use_dask_arrays:
                    Raw = da.from_array(np.frombuffer(fid.read(n_elements), dtype=dt))
                else:
                    Raw = np.frombuffer(fid.read(n_elements), dtype=dt)
            else:
                dt = np.dtype(np.int16)
                dt = dt.newbyteorder(&#39;&gt;&#39;)
                if self.use_dask_arrays:
                    Raw = da.from_array(np.frombuffer(fid.read(2*n_elements),dtype=dt))
                else:
                    Raw = np.frombuffer(fid.read(2*n_elements),dtype=dt)
            fid.close
            # finish reading raw data

            Raw = np.array(Raw).reshape(self.YResolution, self.XResolution, self.ChanNum)
            #print(shape(Raw), type(Raw), type(Raw[0,0]))

            #data = np.asarray(datab).reshape(self.YResolution,self.XResolution,ChanNum)
            if self.EightBit == 1:
                if self.AI1 == 1:
                    self.RawImageA = Raw[:,:,0]
                    self.ImageA = (Raw[:,:,0].astype(float32)*self.ScanRate/self.Scaling[0,0]/self.Scaling[2,0]/self.Scaling[3,0]+self.Scaling[1,0]).astype(int32)
                    if self.AI2 == 1:
                        self.RawImageB = Raw[:,:,1]
                        self.ImageB = (Raw[:,:,1].astype(float32)*self.ScanRate/self.Scaling[0,1]/self.Scaling[2,1]/self.Scaling[3,1]+self.Scaling[1,1]).astype(int32)
                elif self.AI2 == 1:
                    self.RawImageB = Raw[:,:,0]
                    self.ImageB = (Raw[:,:,0].astype(float32)*self.ScanRate/self.Scaling[0,0]/self.Scaling[2,0]/self.Scaling[3,0]+self.Scaling[1,0]).astype(int32)
            else:
                if self.FileVersion == 1 or self.FileVersion == 2 or self.FileVersion == 3 or self.FileVersion == 4 or self.FileVersion == 5 or self.FileVersion == 6:
                    if self.AI1 == 1:
                        self.RawImageA = Raw[:,:,0]
                        self.ImageA = self.Scaling[0,0] + self.RawImageA * self.Scaling[1,0]  # Converts raw I16 data to voltage based on self.Scaling factors
                        if self.AI2 == 1:
                            self.RawImageB = Raw[:,:,1]
                            self.ImageB = self.Scaling[0,1] + self.RawImageB * self.Scaling[1,1]
                            if self.AI3 == 1:
                                self.RawImageC = (Raw[:,:,2]).reshape(self.YResolution,self.XResolution)
                                self.ImageC = self.Scaling[0,2] + self.RawImageC * self.Scaling[1,2]
                                if self.AI4 == 1:
                                    self.RawImageD = (Raw[:,:,3]).reshape(self.YResolution,self.XResolution)
                                    self.ImageD = self.Scaling[0,3] + self.RawImageD * self.Scaling[1,3]
                            elif self.AI4 == 1:
                                self.RawImageD = (Raw[:,:,2]).reshape(self.YResolution,self.XResolution)
                                self.ImageD = self.Scaling[0,2] + self.RawImageD * self.Scaling[1,2]
                        elif self.AI3 == 1:
                            self.RawImageC = Raw[:,:,1]
                            self.ImageC = self.Scaling[0,1] + self.RawImageC * self.Scaling[1,1]
                            if self.AI4 == 1:
                                self.RawImageD = (Raw[:,:,2]).reshape(self.YResolution,self.XResolution)
                                self.ImageD = self.Scaling[0,2] + self.RawImageD * self.Scaling[1,2]
                        elif self.AI4 == 1:
                            self.RawImageD = Raw[:,:,1]
                            self.ImageD = self.Scaling[0,1] + self.RawImageD * self.Scaling[1,1]
                    elif self.AI2 == 1:
                        self.RawImageB = Raw[:,:,0]
                        self.ImageB = self.Scaling[0,0] + self.RawImageB * self.Scaling[1,0]
                        if self.AI3 == 1:
                            self.RawImageC = Raw[:,:,1]
                            self.ImageC = self.Scaling[0,1] + self.RawImageC * self.Scaling[1,1]
                            if self.AI4 == 1:
                                self.RawImageD = (Raw[:,:,2]).reshape(self.YResolution,self.XResolution)
                                self.ImageD = self.Scaling[0,2] + self.RawImageD * self.Scaling[1,2]
                        elif self.AI4 == 1:
                            self.RawImageD = Raw[:,:,1]
                            self.ImageD = self.Scaling[0,1] + self.RawImageD * self.Scaling[1,1]
                    elif self.AI3 == 1:
                        self.RawImageC = Raw[:,:,0]
                        self.ImageC = self.Scaling[0,0] + self.RawImageC * self.Scaling[1,0]
                        if self.AI4 == 1:
                            self.RawImageD = Raw[:,:,1]
                            self.ImageD = self.Scaling[0,1] + self.RawImageD * self.Scaling[1,1]
                    elif self.AI4 == 1:
                        self.RawImageD = Raw[:,:,0]
                        self.ImageD = self.Scaling[0,0] + self.RawImageD * self.Scaling[1,0]

                elif self.FileVersion == 7:
                    if self.AI1 == 1:
                        self.RawImageA = Raw[:,:,0]
                        self.ImageA = (self.RawImageA - self.Scaling[1,0])*self.Scaling[2,0]
                        if self.AI2 == 1:
                            self.RawImageB = Raw[:,:,1]
                            self.ImageB = (self.RawImageB - self.Scaling[1,1])*self.Scaling[2,1]
                    elif self.AI2 == 1:
                        self.RawImageB = Raw[:,:,0]
                        self.ImageB = (self.RawImageB - self.Scaling[1,1])*self.Scaling[2,1]

                elif  self.FileVersion == 8 or self.FileVersion == 9:
                    self.ElectronFactor1 = 0.1;             # 16-bit intensity is 10x electron counts
                    self.Scaling[3,0] = self.ElectronFactor1
                    self.ElectronFactor2 = 0.1;             # 16-bit intensity is 10x electron counts
                    self.Scaling[3,1] = self.ElectronFactor2
                    if self.AI1 == 1:
                        self.RawImageA = Raw[:,:,0]
                        self.ImageA = (self.RawImageA - self.Scaling[1,0]) * self.Scaling[2,0] / self.ScanRate * self.Scaling[0,0] / self.ElectronFactor1
                        # Converts raw I16 data to voltage based on self.Scaling factors
                        if self.AI2 == 1:
                            self.RawImageB = Raw[:,:,1]
                            self.ImageB = (self.RawImageB - self.Scaling[1,1]) * self.Scaling[2,1] / self.ScanRate * self.Scaling[0,1] / self.ElectronFactor2
                    elif self.AI2 == 1:
                        self.RawImageB = Raw[:,:,0]
                        self.ImageB = (self.RawImageB - self.Scaling[1,1]) * self.Scaling[2,1] / self.ScanRate * self.Scaling[0,1] / self.ElectronFactor2

    def print_header(self):
        &#39;&#39;&#39;
        Prints a formatted content of the file header

        &#39;&#39;&#39;
        if self.FileVersion == -1 :
            print(&#39;Sample_ID=&#39;, self.Sample_ID)
            print(&#39;DetA=&#39;, self.DetA)
            print(&#39;DetB=&#39;, self.DetB)
            print(&#39;EightBit=&#39;, self.EightBit)
            print(&#39;XResolution=&#39;, self.XResolution)
            print(&#39;YResolution=&#39;, self.YResolution)
            print(&#39;PixelSize=&#39;, self.PixelSize)
        else:
            print(&#39;FileMagicNum=&#39;, self.FileMagicNum)
            print(&#39;FileVersion=&#39;, self.FileVersion)
            print(&#39;FileType=&#39;, self.FileType)
            print(&#39;SWdate=&#39;, self.SWdate)
            print(&#39;TimeStep=&#39;, self.TimeStep)
            print(&#39;ChanNum=&#39;, self.ChanNum)
            print(&#39;EightBit=&#39;, self.EightBit)
            print(&#39;Scaling=&#39;, self.Scaling)
            if self.FileVersion &gt; 8 :
                print(&#39;RestartFlag=&#39;, self.RestartFlag)
                print(&#39;StageMove=&#39;, self.StageMove)
                print(&#39;FirstPixelX=&#39;, self.FirstPixelX)
                print(&#39;FirstPixelY=&#39;, self.FirstPixelY)
            print(&#39;XResolution=&#39;, self.XResolution)
            print(&#39;YResolution=&#39;, self.YResolution)
            if self.FileVersion == 1 or self.FileVersion == 2 or self.FileVersion == 3:
                print(&#39;AIDelay=&#39;, self.AIDelay)
            print(&#39;Oversampling=&#39;, self.Oversampling)
            print(&#39;ZeissScanSpeed=&#39;, self.ZeissScanSpeed)
            print(&#39;DecimatingFactor=&#39;, self.DecimatingFactor)
            print(&#39;ScanRate=&#39;, self.ScanRate)
            print(&#39;FramelineRampdownRatio=&#39;, self.FramelineRampdownRatio)
            print(&#39;Xmin=&#39;, self.Xmin)
            print(&#39;Xmax=&#39;, self.Xmax)
            print(&#39;Detmin=&#39;, self.Detmin)
            print(&#39;Detmax=&#39;, self.Detmax)
            print(&#39;AI1=&#39;, self.AI1)
            print(&#39;AI2=&#39;, self.AI2)
            print(&#39;AI3=&#39;, self.AI3)
            print(&#39;AI4=&#39;, self.AI4)
            if self.FileVersion &gt; 8 :
                 print(&#39;Sample_ID=&#39;, self.Sample_ID)
            print(&#39;Notes=&#39;, self.Notes)
            print(&#39;SEMShiftX=&#39;, self.SEMShiftX)
            print(&#39;SEMShiftY=&#39;, self.SEMShiftY)
            print(&#39;DetA=&#39;, self.DetA)
            print(&#39;DetB=&#39;, self.DetB)
            print(&#39;DetC=&#39;, self.DetC)
            print(&#39;DetD=&#39;, self.DetD)
            print(&#39;Mag=&#39;, self.Mag)
            print(&#39;PixelSize=&#39;, self.PixelSize)
            print(&#39;WD=&#39;, self.WD)
            print(&#39;EHT=&#39;, self.EHT)
            print(&#39;SEMApr=&#39;, self.SEMApr)
            print(&#39;HighCurrent=&#39;, self.HighCurrent)
            print(&#39;SEMCurr=&#39;, self.SEMCurr)
            print(&#39;SEMRot=&#39;, self.SEMRot)
            print(&#39;ChamVac=&#39;, self.ChamVac)
            print(&#39;GunVac=&#39;, self.GunVac)
            print(&#39;SEMStiX=&#39;, self.SEMStiX)
            print(&#39;SEMStiY=&#39;, self.SEMStiY)
            print(&#39;SEMAlnX=&#39;, self.SEMAlnX)
            print(&#39;SEMAlnY=&#39;, self.SEMAlnY)
            print(&#39;StageX=&#39;, self.StageX)
            print(&#39;StageY=&#39;, self.StageY)
            print(&#39;StageZ=&#39;, self.StageZ)
            print(&#39;StageT=&#39;, self.StageT)
            print(&#39;StageR=&#39;, self.StageR)
            print(&#39;StageM=&#39;, self.StageM)
            print(&#39;BrightnessA=&#39;, self.BrightnessA)
            print(&#39;ContrastA=&#39;, self.ContrastA)
            print(&#39;BrightnessB=&#39;, self.BrightnessB)
            print(&#39;ContrastB=&#39;, self.ContrastB)
            print(&#39;Mode=&#39;, self.Mode)
            print(&#39;FIBFocus=&#39;, self.FIBFocus)
            print(&#39;FIBProb=&#39;, self.FIBProb)
            print(&#39;FIBCurr=&#39;, self.FIBCurr)
            print(&#39;FIBRot=&#39;, self.FIBRot)
            print(&#39;FIBAlnX=&#39;, self.FIBAlnX)
            print(&#39;FIBAlnY=&#39;, self.FIBAlnY)
            print(&#39;FIBStiX=&#39;, self.FIBStiX)
            print(&#39;FIBStiY=&#39;, self.FIBStiY)
            print(&#39;FIBShiftX=&#39;, self.FIBShiftX)
            print(&#39;FIBShiftY=&#39;, self.FIBShiftY)
            if self.FileVersion &gt; 4:
                print(&#39;MillingXResolution=&#39;, self.MillingXResolution)
                print(&#39;MillingYResolution=&#39;, self.MillingYResolution)
                print(&#39;MillingXSize=&#39;, self.MillingXSize)
                print(&#39;MillingYSize=&#39;, self.MillingYSize)
                print(&#39;MillingULAng=&#39;, self.MillingULAng)
                print(&#39;MillingURAng=&#39;, self.MillingURAng)
                print(&#39;MillingLineTime=&#39;, self.MillingLineTime)
                print(&#39;FIBFOV (um)=&#39;, self.FIBFOV)
                print(&#39;MillingPIDOn=&#39;, self.MillingPIDOn)
                print(&#39;MillingPIDMeasured=&#39;, self.MillingPIDMeasured)
                print(&#39;MillingPIDTarget=&#39;, self.MillingPIDTarget)
                print(&#39;MillingPIDTargetSlope=&#39;, self.MillingPIDTargetSlope)
                print(&#39;MillingPIDP=&#39;, self.MillingPIDP)
                print(&#39;MillingPIDI=&#39;, self.MillingPIDI)
                print(&#39;MillingPIDD=&#39;, self.MillingPIDD)
                print(&#39;MachineID=&#39;, self.MachineID)
                print(&#39;SEMSpecimenI=&#39;, self.SEMSpecimenI)
            if self.FileVersion &gt; 5:
                print(&#39;Temperature=&#39;, self.Temperature)
                print(&#39;FaradayCupI=&#39;, self.FaradayCupI)
                print(&#39;FIBSpecimenI=&#39;, self.FIBSpecimenI)
                print(&#39;BeamDump1I=&#39;, self.BeamDump1I)
                print(&#39;MillingYVoltage=&#39;, self.MillingYVoltage)
                print(&#39;FocusIndex=&#39;, self.FocusIndex)
                print(&#39;FIBSliceNum=&#39;, self.FIBSliceNum)
            if self.FileVersion &gt; 7:
                print(&#39;BeamDump2I=&#39;, self.BeamDump2I)
                print(&#39;MillingI=&#39;, self.MillingI)
            print(&#39;SEMSpecimenI=&#39;, self.SEMSpecimenI)
            print(&#39;FileLength=&#39;, self.FileLength)

    def display_images(self):
        &#39;&#39;&#39;
        Display auto-scaled detector images without saving the figure into the file.

        &#39;&#39;&#39;
        fig, axs = subplots(2, 1, figsize=(10,5))
        axs[0].imshow(self.RawImageA, cmap=&#39;Greys&#39;)
        axs[1].imshow(self.RawImageB, cmap=&#39;Greys&#39;)
        ttls = [&#39;Detector A: &#39;+self.DetA.strip(&#39;\x00&#39;), &#39;Detector B: &#39;+self.DetB.strip(&#39;\x00&#39;)]
        for ax, ttl in zip(axs, ttls):
            ax.axis(False)
            ax.set_title(ttl, fontsize=10)
        fig.suptitle(self.fname)

    def save_images_jpeg(self, **kwargs):
        &#39;&#39;&#39;
        Display auto-scaled detector images and save the figure into JPEG file (s).

        Parameters
        ----------
        kwargs:
        images_to_save : str
            Images to save. options are: &#39;A&#39;, &#39;B&#39;, or &#39;Both&#39; (default).
        invert : boolean
            If True, the image will be inverted.

        &#39;&#39;&#39;
        images_to_save = kwargs.get(&#34;images_to_save&#34;, &#39;Both&#39;)
        invert = kwargs.get(&#34;invert&#34;, False)

        if images_to_save == &#39;Both&#39; or images_to_save == &#39;A&#39;:
            if self.ftype == 0:
                fname_jpg = os.path.splitext(self.fname)[0] + &#39;_&#39; + self.DetA.strip(&#39;\x00&#39;) + &#39;.jpg&#39;
            else:
                fname_jpg = os.path.splitext(self.fname)[0] + &#39;DetA.jpg&#39;
            Img = self.RawImageA_8bit_thresholds()[0]
            if invert:
                Img =  uint8(255) - Img
            PILImage.fromarray(Img).save(fname_jpg)

        try:
            if images_to_save == &#39;Both&#39; or images_to_save == &#39;B&#39;:
                if self.ftype == 0:
                    fname_jpg = os.path.splitext(self.fname)[0] +  &#39;_&#39; + self.DetB.strip(&#39;\x00&#39;) + &#39;.jpg&#39;
                else:
                    fname_jpg = os.path.splitext(self.fname)[0] + &#39;DetB.jpg&#39;
                Img = self.RawImageB_8bit_thresholds()[0]
                if invert:
                    Img =  uint8(255) - Img
                PILImage.fromarray(Img).save(fname_jpg)
        except:
            print(&#39;No Detector B image to save&#39;)

    def save_images_tif(self, images_to_save = &#39;Both&#39;):
        &#39;&#39;&#39;
        Save the detector images into TIF file (s).

        Parameters
        ----------
        images_to_save : str
            Images to save. options are: &#39;A&#39;, &#39;B&#39;, or &#39;Both&#39; (default).

        &#39;&#39;&#39;
        if self.ftype == 0:
            if images_to_save == &#39;Both&#39; or images_to_save == &#39;A&#39;:
                fnameA = os.path.splitext(self.fname)[0] + &#39;_&#39; + self.DetA.strip(&#39;\x00&#39;) + &#39;.tif&#39;
                tiff.imsave(fnameA, self.RawImageA)
            if self.DetB != &#39;None&#39;:
                if images_to_save == &#39;Both&#39; or images_to_save == &#39;B&#39;:
                    fnameB = os.path.splitext(self.fname)[0] + &#39;_&#39; + self.DetB.strip(&#39;\x00&#39;) + &#39;.tif&#39;
                    tiff.imsave(fnameB, self.RawImageB)
        else:
            print(&#39;original File is already in TIF format&#39;)

    def get_image_min_max(self, image_name = &#39;ImageA&#39;, thr_min = 1.0e-4, thr_max = 1.0e-3, nbins=256, disp_res = False):
        &#39;&#39;&#39;
        Calculates the data range of the EM data. ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

        Calculates histogram of pixel intensities of of the loaded image
        with number of bins determined by parameter nbins (default = 256)
        and normalizes it to get the probability distribution function (PDF),
        from which a cumulative distribution function (CDF) is calculated.
        Then given the threshold_min, threshold_max parameters,
        the minimum and maximum values for the image are found by finding
        the intensities at which CDF= threshold_min and (1- threshold_max), respectively.

        Parameters
        ----------
        image_name : string
            the name of the image to perform this operations (defaulut is &#39;RawImageA&#39;)
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        disp_res : boolean
            (default is False) - to plot/ display the results

        Returns:
            dmin, dmax: (float) minimum and maximum values of the data range.
        &#39;&#39;&#39;
        if image_name == &#39;ImageA&#39;:
            im = self.ImageA
        if image_name == &#39;ImageB&#39;:
            im = self.ImageB
        if image_name == &#39;RawImageA&#39;:
            im = self.RawImageA
        if image_name == &#39;RawImageB&#39;:
            im = self.RawImageB
        return get_min_max_thresholds(im, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=disp_res)

    def RawImageA_8bit_thresholds(self, thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
        &#39;&#39;&#39;
        Convert the Image A into 8-bit array

        Parameters
        ----------
        thr_min : float
            lower CDF threshold for determining the minimum data value
        thr_max : float
            upper CDF threshold for determining the maximum data value
        data_min : float
            If different from data_max, this value will be used as low bound for I8 data conversion
        data_max : float
            If different from data_min, this value will be used as high bound for I8 data conversion
        nbins : int
            number of histogram bins for building the PDF and CDF

        Returns
        dt, data_min, data_max
            dt : 2D uint8 array
                Converted data
            data_min : float
                value used as low bound for I8 data conversion
            data_max : float
                value used as high bound for I8 data conversion
        &#39;&#39;&#39;
        if self.EightBit==1:
            #print(&#39;8-bit image already - no need to convert&#39;)
            dt = self.RawImageA
        else:
            if data_min == data_max:
                data_min, data_max = self.get_image_min_max(image_name =&#39;RawImageA&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
            dt = ((np.clip(self.RawImageA, data_min, data_max) - data_min)/(data_max-data_min)*255.0).astype(np.uint8)
        return dt, data_min, data_max

    def RawImageB_8bit_thresholds(self, thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
        &#39;&#39;&#39;
        Convert the Image B into 8-bit array

        Parameters
        ----------
        thr_min : float
            lower CDF threshold for determining the minimum data value
        thr_max : float
            upper CDF threshold for determining the maximum data value
        data_min : float
            If different from data_max, this value will be used as low bound for I8 data conversion
        data_max : float
            If different from data_min, this value will be used as high bound for I8 data conversion
        nbins : int
            number of histogram bins for building the PDF and CDF

        Returns
        dt, data_min, data_max
            dt : 2D uint8 array
                Converted data
            data_min : float
                value used as low bound for I8 data conversion
            data_max : float
                value used as high bound for I8 data conversion
        &#39;&#39;&#39;
        if self.EightBit==1:
            #print(&#39;8-bit image already - no need to convert&#39;)
            dt = self.RawImageB
        else:
            if data_min == data_max:
                data_min, data_max = self.get_image_min_max(image_name =&#39;RawImageB&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
            dt = ((np.clip(self.RawImageB, data_min, data_max) - data_min)/(data_max-data_min)*255.0).astype(np.uint8)
        return dt, data_min, data_max

    def save_snapshot(self, **kwargs):
        &#39;&#39;&#39;
        Builds an image that contains both the Detector A and Detector B (if present) images as well as a table with important FIB-SEM parameters.

        kwargs:
         ----------
        thr_min : float
            lower CDF threshold for determining the minimum data value. Default is 1.0e-3
        thr_max : float
            upper CDF threshold for determining the maximum data value. Default is 1.0e-3
        data_min : float
            If different from data_max, this value will be used as low bound for I8 data conversion
        data_max : float
            If different from data_min, this value will be used as high bound for I8 data conversion
        nbins : int
            number of histogram bins for building the PDF and CDF
        disp_res : True
            If True display the results
        dpi : int
            Default is 300
        snapshot_name : string
            the name of the image to perform this operations (defaulut is frame_name + &#39;_snapshot.png&#39;).



        Returns
        dt, data_min, data_max
            dt : 2D uint8 array
                Converted data
            data_min : float
                value used as low bound for I8 data conversion
            data_max : float
                value used as high bound for I8 data conversion
        &#39;&#39;&#39;
        thr_min = kwargs.get(&#39;thr_min&#39;, 1.0e-3)
        thr_max = kwargs.get(&#39;thr_max&#39;, 1.0e-3)
        nbins = kwargs.get(&#39;nbins&#39;, 256)
        disp_res = kwargs.get(&#39;disp_res&#39;, True)
        dpi = kwargs.get(&#39;dpi&#39;, 300)
        snapshot_name = kwargs.get(&#39;snapshot_name&#39;, os.path.splitext(self.fname)[0] + &#39;_snapshot.png&#39;)

        ifDetB = (self.DetB != &#39;None&#39;)
        if ifDetB:
            try:
                dminB, dmaxB = self.get_image_min_max(image_name =&#39;RawImageB&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
                fig, axs = subplots(3, 1, figsize=(11,8))
            except:
                ifDetB = False
                pass
        if not ifDetB:
            fig, axs = subplots(2, 1, figsize=(7,8))
        fig.subplots_adjust(left=0.01, bottom=0.01, right=0.99, top=0.90, wspace=0.15, hspace=0.1)
        dminA, dmaxA = self.get_image_min_max(image_name =&#39;RawImageA&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
        axs[1].imshow(self.RawImageA, cmap=&#39;Greys&#39;, vmin=dminA, vmax=dmaxA)
        if ifDetB:
            axs[2].imshow(self.RawImageB, cmap=&#39;Greys&#39;, vmin=dminB, vmax=dmaxB)
        try:
            ttls = [self.Notes.strip(&#39;\x00&#39;),
                &#39;Detector A:  &#39;+ self.DetA.strip(&#39;\x00&#39;) + &#39;,  Data Range:  {:.1f} ÷ {:.1f} with thr_min={:.1e}, thr_max={:.1e}&#39;.format(dminA, dmaxA, thr_min, thr_max) + &#39;    (Brightness: {:.1f}, Contrast: {:.1f})&#39;.format(self.BrightnessA, self.ContrastA),
                &#39;Detector B:  &#39;+ self.DetB.strip(&#39;\x00&#39;) + &#39;,  Data Range:  {:.1f} ÷ {:.1f} with thr_min={:.1e}, thr_max={:.1e}&#39;.format(dminB, dmaxB, thr_min, thr_max) + &#39;    (Brightness: {:.1f}, Contrast: {:.1f})&#39;.format(self.BrightnessB, self.ContrastB)]
        except:
            ttls = [&#39;&#39;, &#39;Detector A&#39;, &#39;&#39;]
        for j, ax in enumerate(axs):
            ax.axis(False)
            ax.set_title(ttls[j], fontsize=10)
        fig.suptitle(self.fname)

        if self.FileVersion &gt; 8:
            cell_text = [[&#39;Sample ID&#39;, &#39;{:s}&#39;.format(self.Sample_ID.strip(&#39;\x00&#39;)), &#39;&#39;,
                          &#39;Frame Size&#39;, &#39;{:d} x {:d}&#39;.format(self.XResolution, self.YResolution), &#39;&#39;,
                          &#39;Scan Rate&#39;, &#39;{:.3f} MHz&#39;.format(self.ScanRate/1.0e6)],
                        [&#39;Machine ID&#39;, &#39;{:s}&#39;.format(self.MachineID.strip(&#39;\x00&#39;)), &#39;&#39;,
                          &#39;Pixel Size&#39;, &#39;{:.1f} nm&#39;.format(self.PixelSize), &#39;&#39;,
                          &#39;Oversampling&#39;, &#39;{:d}&#39;.format(self.Oversampling)],
                         [&#39;FileVersion&#39;, &#39;{:d}&#39;.format(self.FileVersion), &#39;&#39;,
                          &#39;Working Dist.&#39;, &#39;{:.3f} mm&#39;.format(self.WD), &#39;&#39;,
                          &#39;FIB Focus&#39;, &#39;{:.1f}  V&#39;.format(self.FIBFocus)],
                         [&#39;Bit Depth&#39;, &#39;{:d}&#39;.format(8 *(2 - self.EightBit)), &#39;&#39;,
                         &#39;EHT Voltage\n\nSEM Current&#39;, &#39;{:.3f} kV \n\n{:.3f} nA&#39;.format(self.EHT, self.SEMCurr*1.0e9), &#39;&#39;,
                         &#39;FIB Probe&#39;, &#39;{:d}&#39;.format(self.FIBProb)]]
        else:
            if self.FileVersion &gt; 0:
                cell_text = [[&#39;&#39;, &#39;&#39;, &#39;&#39;,
                              &#39;Frame Size&#39;, &#39;{:d} x {:d}&#39;.format(self.XResolution, self.YResolution), &#39;&#39;,
                              &#39;Scan Rate&#39;, &#39;{:.3f} MHz&#39;.format(self.ScanRate/1.0e6)],
                            [&#39;Machine ID&#39;, &#39;{:s}&#39;.format(self.MachineID.strip(&#39;\x00&#39;)), &#39;&#39;,
                              &#39;Pixel Size&#39;, &#39;{:.1f} nm&#39;.format(self.PixelSize), &#39;&#39;,
                              &#39;Oversampling&#39;, &#39;{:d}&#39;.format(self.Oversampling)],
                             [&#39;FileVersion&#39;, &#39;{:d}&#39;.format(self.FileVersion), &#39;&#39;,
                              &#39;Working Dist.&#39;, &#39;{:.3f} mm&#39;.format(self.WD), &#39;&#39;,
                              &#39;FIB Focus&#39;, &#39;{:.1f}  V&#39;.format(self.FIBFocus)],
                             [&#39;Bit Depth&#39;, &#39;{:d}&#39;.format(8 *(2 - self.EightBit)), &#39;&#39;,
                             &#39;EHT Voltage&#39;, &#39;{:.3f} kV&#39;.format(self.EHT), &#39;&#39;,
                             &#39;FIB Probe&#39;, &#39;{:d}&#39;.format(self.FIBProb)]]
            else:
                cell_text = [[&#39;&#39;, &#39;&#39;, &#39;&#39;,
                              &#39;Frame Size&#39;, &#39;{:d} x {:d}&#39;.format(self.XResolution, self.YResolution), &#39;&#39;,
                              &#39;Scan Rate&#39;, &#39;&#39;],
                            [&#39;Machine ID&#39;, &#39;&#39;, &#39;&#39;,
                              &#39;Pixel Size&#39;, &#39;{:.1f} nm&#39;.format(self.PixelSize), &#39;&#39;,
                              &#39;Oversampling&#39;, &#39;&#39;],
                             [&#39;FileVersion&#39;, &#39;{:d}&#39;.format(self.FileVersion), &#39;&#39;,
                              &#39;Working Dist.&#39;, &#39; &#39;, &#39;&#39;,
                              &#39;FIB Focus&#39;, &#39;&#39;],
                             [&#39;Bit Depth&#39;, &#39;{:d}&#39;.format(8 *(2 - self.EightBit)), &#39;&#39;,
                             &#39;EHT Voltage&#39;, &#39;&#39;, &#39;&#39;,
                             &#39;FIB Probe&#39;, &#39;&#39;]]
        llw0=0.3
        llw1=0.18
        llw2=0.02
        clw = [llw1, llw0, llw2, llw1, llw1, llw2, llw1, llw1]
        tbl = axs[0].table(cellText=cell_text,
                           colWidths=clw,
                           cellLoc=&#39;center&#39;,
                           colLoc=&#39;center&#39;,
                           bbox = [0.02, 0, 0.96, 1.0],
                           #bbox = [0.45, 1.02, 2.8, 0.55],
                           zorder=10)

        fig.savefig(snapshot_name, dpi=dpi)
        if disp_res == False:
            plt.close(fig)

    def analyze_noise_ROIs(self, Noise_ROIs, Hist_ROI, **kwargs):
        &#39;&#39;&#39;
        Analyses the noise statistics in the selected ROI&#39;s of the EM data.
        ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

        Calls Single_Image_Noise_ROIs(img, Noise_ROIs, Hist_ROI, **kwargs)
        Performs following:
        1. For each of the selected ROI&#39;s, this method will perfrom the following:
            1a. Smooth the data by 2D convolution with a given kernel.
            1b. Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
            1c. Calculate the mean intensity value of the data and variance of the above &#34;Noise&#34;
        2. Plot the dependence of the noise variance vs. image intensity.
        3. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
            it will be determined from the header data:
                for RawImageA it is self.Scaling[1,0]
                for RawImageB it is self.Scaling[1,1]
        4. The equation is determined for a line that passes through the point:
                Intensity=DarkCount and Noise Variance = 0
                and is a best fit for the [Mean Intensity, Noise Variance] points
                determined for each ROI (Step 1 above).
        5. Another ROI (defined by Hist_ROI parameter) is used to built an
            intensity histogram of the actual data. Peak of that histogram is determined.
        6. The data is plotted. Two values of SNR are defined from the slope of the line in Step 4:
            PSNR (Peak SNR) = Mean Intensity/sqrt(Noise Variance) at the intensity
                at the histogram peak determined in the Step 5.
            DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
                where Max and Min Intensity are determined by corresponding cummulative
                threshold parameters, and Noise Variance is taken at the intensity
                in the middle of the range (Min Intensity + Max Intensity)/2.0

        Parameters
        ----------
        Noise_ROIs : list of lists: [[left, right, top, bottom]]
            list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the noise.
        Hist_ROI : list [left, right, top, bottom]
            coordinates (indices) of the boundaries of the image subset to evaluate the real data histogram.

        kwargs:
        image_name : string
            the name of the image to perform this operations (defaulut is &#39;RawImageA&#39;).
        DarkCount : float
            the value of the Intensity Data at 0.
        kernel : 2D float array
            a kernel to perfrom 2D smoothing convolution.
        filename : str
            filename - used for plotting the data. If not explicitly defined will use the instance attribute self.fname
        nbins_disp : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
        thresholds_disp : list [thr_min_disp, thr_max_disp]
            (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
        nbins_analysis : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
        thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
            (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
        nbins_analysis : int
             (default 256) number of histogram bins for building the data histogram in Step 5.
        disp_res : boolean
            (default is False) - to plot/ display the results

        Returns:
        mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR
            mean_vals and var_vals are the Mean Intensity and Noise Variance values for the Noise_ROIs (Step 1)
            NF_slope is the slope of the linear fit curve (Step 4)
            PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 6)
        &#39;&#39;&#39;
        image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)

        if image_name == &#39;RawImageA&#39;:
            ImgEM = self.RawImageA.astype(float)
            DarkCount = self.Scaling[1,0]
        if image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;:
            ImgEM = self.RawImageB.astype(float)
            DarkCount = self.Scaling[1,1]

        if (image_name == &#39;RawImageA&#39;) or (image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;):
            st = 1.0/np.sqrt(2.0)
            def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
            def_kernel = def_kernel/def_kernel.sum()
            kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
            DarkCount = kwargs.get(&#34;DarkCount&#34;, DarkCount)
            nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
            thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
            nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
            thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
            Notes = kwargs.get(&#34;Notes&#34;, self.Notes.strip(&#39;\x00&#39;))
            kwargs[&#39;kernel&#39;] = kernel
            kwargs[&#39;DarkCount&#39;] = DarkCount
            kwargs[&#39;img_label&#39;] = image_name
            kwargs[&#39;res_fname&#39;] = os.path.splitext(self.fname)[0] + &#39;_&#39; + image_name + &#39;_Noise_Analysis_ROIs.png&#39;
            kwargs[&#39;Notes&#39;] = Notes
            mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR = Single_Image_Noise_ROIs(ImgEM, Noise_ROIs, Hist_ROI, **kwargs)

        else:
            print(&#39;No valid image name selected&#39;)
            mean_vals = 0.0
            var_vals = 0.0
            NF_slope = 0.0
            PSNR = 0.0
            MSNR = 0.0
            DSNR = 0.0

        return mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR

    def analyze_noise_statistics(self, **kwargs):
        &#39;&#39;&#39;
        Analyses the noise statistics of the EM data image.
        ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

        Calls Single_Image_Noise_Statistics(img, **kwargs)
        Performs following:
        1. Smooth the image by 2D convolution with a given kernel.
        2. Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
        3. Build a histogram of Smoothed Image.
        4. For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
        5. Plot the dependence of the noise variance vs. image intensity.
        6. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
            it will be set to 0
        7. The equation is determined for a line that passes through the point:
                Intensity=DarkCount and Noise Variance = 0
                and is a best fit for the [Mean Intensity, Noise Variance] points
                determined for each ROI (Step 1 above).
        8. The data is plotted. Two values of SNR are defined from the slope of the line in Step 7:
            PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity
                at the histogram peak determined in the Step 3.
            MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
            DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
                where Max and Min Intensity are determined by corresponding cummulative
                threshold parameters, and Noise Variance is taken at the intensity
                in the middle of the range (Min Intensity + Max Intensity)/2.0

        Parameters
        ----------
            kwargs:
            image_name : str
                Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
            evaluation_box : list of 4 int
                evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
                if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
            DarkCount : float
                the value of the Intensity Data at 0.
            kernel : 2D float array
                a kernel to perfrom 2D smoothing convolution.
            nbins_disp : int
                (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
            thresholds_disp : list [thr_min_disp, thr_max_disp]
                (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
            nbins_analysis : int
                (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
            thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
                (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
            nbins_analysis : int
                 (default 256) number of histogram bins for building the data histogram in Step 5.
            disp_res : boolean
                (default is False) - to plot/ display the results
            save_res_png : boolean
                save the analysis output into a PNG file (default is True)
            res_fname : string
                filename for the result image (&#39;Noise_Analysis.png&#39;)
            img_label : string
                optional image label
            Notes : string
                optional additional notes
            dpi : int

        Returns:
        mean_vals, var_vals, I0, PSNR, DSNR, popt, result
            mean_vals and var_vals are the Mean Intensity and Noise Variance values for Step 5
            I0 is zero intercept (should be close to DarkCount)
            PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 8)
        &#39;&#39;&#39;
        image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)

        if image_name == &#39;RawImageA&#39;:
            ImgEM = self.RawImageA.astype(float)
            DarkCount = self.Scaling[1,0]
        if image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;:
            ImgEM = self.RawImageB.astype(float)
            DarkCount = self.Scaling[1,1]

        if (image_name == &#39;RawImageA&#39;) or (image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;):
            st = 1.0/np.sqrt(2.0)
            evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
            def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
            def_kernel = def_kernel/def_kernel.sum()
            kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
            DarkCount = kwargs.get(&#34;DarkCount&#34;, DarkCount)
            nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
            thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
            nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
            thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
            disp_res = kwargs.get(&#34;disp_res&#34;, True)
            save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
            default_res_name = os.path.splitext(self.fname)[0] + &#39;_Noise_Analysis_&#39; + image_name + &#39;.png&#39;
            res_fname = kwargs.get(&#34;res_fname&#34;, default_res_name)
            img_label = kwargs.get(&#34;img_label&#34;, self.Sample_ID)
            Notes = kwargs.get(&#34;Notes&#34;, self.Notes.strip(&#39;\x00&#39;))
            dpi = kwargs.get(&#34;dpi&#34;, 300)

            noise_kwargs = {&#39;image_name&#39; : image_name,
                            &#39;evaluation_box&#39; : evaluation_box,
                            &#39;kernel&#39; : kernel,
                            &#39;DarkCount&#39; : DarkCount,
                            &#39;nbins_disp&#39; : nbins_disp,
                            &#39;thresholds_disp&#39; : thresholds_disp,
                            &#39;nbins_analysis&#39; : nbins_analysis,
                            &#39;thresholds_analysis&#39; : thresholds_analysis,
                            &#39;disp_res&#39; : disp_res,
                            &#39;save_res_png&#39; : save_res_png,
                            &#39;res_fname&#39; : res_fname,
                            &#39;Notes&#39; : Notes,
                            &#39;dpi&#39; : dpi}

            mean_vals, var_vals, I0, PSNR, DSNR, popt, result =  Single_Image_Noise_Statistics(ImgEM, **noise_kwargs)
        else:
            mean_vals, var_vals, I0, PSNR, DSNR, popt, result = [], [], 0.0, 0.0, np.array((0.0, 0.0)), []
        return mean_vals, var_vals, I0, PSNR, DSNR, popt, result


    def analyze_SNR_autocorr(self, **kwargs):
        &#39;&#39;&#39;
        Estimates SNR using auto-correlation analysis of a single image.
        ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

        Calculates SNR of a single image base on auto-correlation analysis of a single image, after [1].
        Calls function Single_Image_SNR(img, **kwargs)

        Parameters
        ---------
        kwargs:
        image_name : str
            Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
        edge_fraction : float
            fraction of the full autocetrrelation range used to calculate the &#34;mean value&#34; (default is 0.10)
        extrapolate_signal : boolean
            extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
        disp_res : boolean
            display results (plots) (default is True)
        save_res_png : boolean
            save the analysis output into a PNG file (default is True)
        res_fname : string
            filename for the sesult image (&#39;SNR_result.png&#39;)
        img_label : string
            optional image label
        dpi : int
            dots-per-inch resolution for the output image

        Returns:
            xSNR, ySNR : float, float
                SNR determind using the method in [1] along X- and Y- directions.
                If there is a direction with slow varying data - that direction provides more accurate SNR estimate
                Y-streaks in typical FIB-SEM data provide slow varying Y-component becuase streaks
                usually get increasingly worse with increasing Y.
                So for typical FIB-SEM data use ySNR

        [1] J. T. L. Thong et al, Single-image signal-tonoise ratio estimation. Scanning, 328–336 (2001).
        &#39;&#39;&#39;
        image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        edge_fraction = kwargs.get(&#34;edge_fraction&#34;, 0.10)
        extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)
        disp_res = kwargs.get(&#34;disp_res&#34;, True)
        save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
        default_res_name = os.path.splitext(self.fname)[0] + &#39;_AutoCorr_Noise_Analysis_&#39; + image_name + &#39;.png&#39;
        res_fname = kwargs.get(&#34;res_fname&#34;, default_res_name)
        dpi = kwargs.get(&#34;dpi&#34;, 300)

        SNR_kwargs = {&#39;edge_fraction&#39; : edge_fraction,
                        &#39;extrapolate_signal&#39; : extrapolate_signal,
                        &#39;disp_res&#39; : disp_res,
                        &#39;save_res_png&#39; : save_res_png,
                        &#39;res_fname&#39; : res_fname,
                        &#39;img_label&#39; : image_name,
                        &#39;dpi&#39; : dpi}

        if image_name == &#39;RawImageA&#39;:
            img = self.RawImageA
        if image_name == &#39;RawImageB&#39;:
            img = self.RawImageB
        if image_name == &#39;ImageA&#39;:
            img = self.ImageA
        if image_name == &#39;ImageB&#39;:
            img = self.ImageB

        xi = 0
        yi = 0
        ysz, xsz = img.shape
        xa = xi + xsz
        ya = yi + ysz
        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        xSNR, ySNR, rSNR= Single_Image_SNR(img[yi_eval:ya_eval, xi_eval:xa_eval], **SNR_kwargs)

        return xSNR, ySNR, rSNR


    def show_eval_box(self, **kwargs):
        &#39;&#39;&#39;
        Show the box used for noise analysis.
        ©G.Shtengel, 04/2021. gleb.shtengel@gmail.com

        kwargs
        ---------
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        image_name : str
            Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
        data_dir : str
            data directory (path)
        Sample_ID : str
            Sample ID
        invert_data : boolean
            If True - the data is inverted
        save_res_png  : boolean
            Save PNG image of the frame overlaid with with evaluation box
        &#39;&#39;&#39;
        image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        data_dir = kwargs.get(&#34;data_dir&#34;, os.path.dirname(self.fname))
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
        thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
        invert_data =  kwargs.get(&#34;invert_data&#34;, False)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, False )

        if image_name == &#39;RawImageA&#39;:
            img = self.RawImageA
        if image_name == &#39;RawImageB&#39;:
            img = self.RawImageB
        if image_name == &#39;ImageA&#39;:
            img = self.ImageA
        if image_name == &#39;ImageB&#39;:
            img = self.ImageB

        xi = 0
        yi = 0
        ysz, xsz = img.shape
        xa = xi + xsz
        ya = yi + ysz
        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        range_disp = get_min_max_thresholds(img[yi_eval:ya_eval, xi_eval:xa_eval], thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res=False)

        fig, ax = subplots(1,1, figsize = (10.0, 11.0*ysz/xsz))
        ax.imshow(img, cmap=&#39;Greys&#39;, vmin = range_disp[0], vmax = range_disp[1])
        ax.grid(True, color = &#34;cyan&#34;)
        ax.set_title(self.fname)
        rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=2.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        if save_res_png :
            fig.savefig(os.path.splitext(self.fname+&#39;_evaluation_box.png&#39;, dpi=300))


    def determine_field_fattening_parameters(self, **kwargs):
        &#39;&#39;&#39;
        Perfrom 2D parabolic fit (calls Perform_2D_fit(Img, estimator, **kwargs)) and determine the field-flattening parameters

        Parameters
        ----------
        kwargs:
        image_names : list of str
            Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
        estimator : RANSACRegressor(),
                    LinearRegression(),
                    TheilSenRegressor(),
                    HuberRegressor()
        bins : int
            binsize for image binning. If not provided, bins=10
        Analysis_ROIs : list of lists: [[left, right, top, bottom]]
            list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the parabolic fit.
        calc_corr : boolean
            If True - the full image correction is calculated
        ignore_Y  : boolean
            If True - the parabolic fit to only X is perfromed
        Xsect : int
            X - coordinate for Y-crossection
        Ysect : int
            Y - coordinate for X-crossection
        disp_res : boolean
            (default is False) - to plot/ display the results
        save_res_png : boolean
            save the analysis output into a PNG file (default is False)
        save_correction_binary = boolean
            save the mage)name and img_correction_array data into a binary file
        res_fname : string
            filename for the result image (&#39;**_Image_Flattening.png&#39;). The binary image is derived from the same root, e.g. &#39;**_Image_Flattening.bin&#39;
        label : string
            optional image label
        dpi : int

        Returns:
        img_correction_coeffs, img_correction_arrays
        &#39;&#39;&#39;
        image_names = kwargs.get(&#34;image_names&#34;, [&#39;RawImageA&#39;])
        estimator = kwargs.get(&#34;estimator&#34;, LinearRegression())
        if &#34;estimator&#34; in kwargs:
            del kwargs[&#34;estimator&#34;]
        calc_corr = kwargs.get(&#34;calc_corr&#34;, False)
        ignore_Y = kwargs.get(&#34;ignore_Y&#34;, False)
        lbl = kwargs.get(&#34;label&#34;, &#39;&#39;)
        disp_res = kwargs.get(&#34;disp_res&#34;, True)
        bins = kwargs.get(&#34;bins&#34;, 10) #bins = 10
        Analysis_ROIs = kwargs.get(&#34;Analysis_ROIs&#34;, [])
        save_res_png = kwargs.get(&#34;save_res_png&#34;, False)
        res_fname = kwargs.get(&#34;res_fname&#34;, os.path.splitext(self.fname)[0]+&#39;_Image_Flattening.png&#39;)
        save_correction_binary = kwargs.get(&#34;save_correction_binary&#34;, False)
        dpi = kwargs.get(&#34;dpi&#34;, 300)

        img_correction_arrays = []
        img_correction_coeffs = []
        for image_name in image_names:
            if image_name == &#39;RawImageA&#39;:
                img = self.RawImageA - self.Scaling[1,0]
            if image_name == &#39;RawImageB&#39;:
                img = self.RawImageB - self.Scaling[1,1]
            if image_name == &#39;ImageA&#39;:
                img = self.ImageA
            if image_name == &#39;ImageB&#39;:
                img = self.ImageB

            ysz, xsz = img.shape
            Xsect = kwargs.get(&#34;Xsect&#34;, xsz//2)
            Ysect = kwargs.get(&#34;Ysect&#34;, ysz//2)

            intercept, coefs, mse, img_correction_array = Perform_2D_fit(img, estimator, image_name=image_name, **kwargs)
            img_correction_arrays.append(img_correction_array)
            img_correction_coeffs.append(coefs)

        if calc_corr:
            self.image_correction_sources = image_names
            self.img_correction_arrays = img_correction_arrays
            if save_correction_binary:
                bin_fname = res_fname.replace(&#39;png&#39;, &#39;bin&#39;)
                pickle.dump([image_names, img_correction_arrays], open(bin_fname, &#39;wb&#39;)) # saves source name and correction array into the binary file
                self.image_correction_file = res_fname.replace(&#39;png&#39;, &#39;bin&#39;)
                print(&#39;Image Flattening Info saved into the binary file: &#39;, self.image_correction_file)
        #self.intercept = intercept
        self.img_correction_coeffs = img_correction_coeffs
        return intercept, img_correction_coeffs, img_correction_arrays


    def flatten_image(self, **kwargs):
        &#39;&#39;&#39;
        Flatten the image(s). Image flattening parameters must be determined (determine_field_fattening_parameters)

        Parameters
        ----------
        kwargs:
        image_correction_file : str
            full path to a binary filename that contains source names (image_correction_sources) and correction arrays (img_correction_arrays)
            if image_correction_file exists, the data is loaded from it.
        image_correction_sources : list of str
            Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
        img_correction_arrays : list of 2D arrays
            arrays containing field flatteting info

        Returns:
        flattened_images : list of 2D arrays
        &#39;&#39;&#39;

        if hasattr(self, &#39;image_correction_file&#39;):
            image_correction_file = kwargs.get(&#34;image_correction_file&#34;, self.image_correction_file)
        else:
            image_correction_file = kwargs.get(&#34;image_correction_file&#34;, &#39;&#39;)

        try:
            # try loading the image correction data from the binary file
            with open(image_correction_file, &#34;rb&#34;) as f:
                [image_correction_sources,  img_correction_arrays] = pickle.load(f)
        except:
            #  if that did not work, see if the correction data was provided directly
            if hasattr(self, &#39;image_correction_source&#39;):
                image_correction_sources = kwargs.get(&#34;image_correction_sources&#34;, self.image_correction_sources)
            else:
                image_correction_sources = kwargs.get(&#34;image_correction_sources&#34;, [False])

            if hasattr(self, &#39;img_correction_arrays&#39;):
                img_correction_arrays = kwargs.get(&#34;img_correction_arrays&#34;, self.img_correction_arrays)
            else:
                img_correction_arrays = kwargs.get(&#34;img_correction_arrays&#34;, [False])

        flattened_images = []
        for image_correction_source, img_correction_array in zip(image_correction_sources, img_correction_arrays):
            if (image_correction_source is not False) and (img_correction_array is not False):
                if image_correction_source == &#39;RawImageA&#39;:
                    flattened_image = (self.RawImageA - self.Scaling[1,0])*img_correction_array + self.Scaling[1,0]
                if image_correction_source == &#39;RawImageB&#39;:
                    flattened_image = (self.RawImageB - self.Scaling[1,1])*img_correction_array + self.Scaling[1,1]
                if image_correction_source == &#39;ImageA&#39;:
                    flattened_image = self.ImageA*img_correction_array
                if image_correction_source == &#39;ImageB&#39;:
                    flattened_image = self.ImageB*img_correction_array
            else:
                if image_correction_source == &#39;RawImageA&#39;:
                    flattened_image = self.RawImageA
                if image_correction_source == &#39;RawImageB&#39;:
                    flattened_image = self.RawImageB
                if image_correction_source == &#39;ImageA&#39;:
                    flattened_image = self.ImageA
                if image_correction_source == &#39;ImageB&#39;:
                    flattened_image = self.ImageB
            flattened_images.append(flattened_image)

        return flattened_images


###################################################
#   Helper functions for FIBSEM_dataset class
###################################################
def determine_regularized_affine_transform(src_pts, dst_pts, l2_matrix = None, targ_vector = None):
    &#34;&#34;&#34;
    Estimate N-D affine transformation with regularization from a set of corresponding points.
    ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

        We can determine the over-, well- and under-determined parameters
        with the total least-squares method.
        Number of source and destination coordinates must match.
        The transformation is defined as:
            X = (a0*x + a1*y + a2)
            Y = (b0*x + b1*y + b2)
        This is regularized Affine estimation - it is regularized so that the penalty is for deviation from a target (default target is rigid shift) transformation
        a0 =1, a1=0, b0=1, b1=1 are parameters for target (shift) transform. Deviation from these is penalized.

        The coefficients appear linearly so we can write
        A x = B, where:
            A   = [[x y 1 0 0 0]
                   [0 0 0 x y 1]]
            Htarget.T = [a0 a1 a2 b0 b1 b2]
            B.T = [X Y]

        In case of ordinary least-squares (OLS) the solution of this system
        of equations is:
        H = np.linalg.inv(A.T @ A) @ A @ B

        In case of least-squares with Tikhonov-like regularization:
        H = np.linalg.inv(A.T @ A + Γ.T @ Γ) @ (A @ B + Γ.T @ Γ @ Htarget)
        where Γ.T @ Γ (for simplicity will call it L2 vector) is regularization term and Htarget
        is a target solution, deviation from which is minimized in L2 sense
     &#34;&#34;&#34;

    src_matrix, src = _center_and_normalize_points_gs(src_pts)
    dst_matrix, dst = _center_and_normalize_points_gs(dst_pts)

    n, d = src.shape
    n2 = n*n   # normalization factor, so that shrinkage parameter does not depend on the number of points

    A = np.zeros((n * d, d * (d + 1)))
    # fill the A matrix with the appropriate block matrices; see docstring
    # for 2D example — this can be generalised to more blocks in the 3D and
    # higher-dimensional cases.
    for ddim in range(d):
        A[ddim*n : (ddim + 1) * n, ddim * (d + 1) : ddim * (d + 1) + d] = src
        A[ddim*n : (ddim + 1) * n, ddim * (d + 1) + d] = 1

    AtA = A.T @ A / n2

    if l2_matrix is None:
        l2 = 1.0e-5   # default shrinkage parameter
        l2_matrix = np.eye(2 * (d + 1)) * l2
        for ddim in range(d):
            ii = (d + 1) * (ddim + 1) - 1
            l2_matrix[ii,ii] = 0

    if targ_vector is None:
        targ_vector = np.zeros(2 * (d + 1))
        targ_vector[0] = 1
        targ_vector[4] = 1


    Hp = np.linalg.inv(AtA + l2_matrix) @ (A.T @ dst.T.ravel() / n2 + l2_matrix @ targ_vector)
    Hm = np.eye(d + 1)
    Hm[0:d, 0:d+1] = Hp.reshape(d, d + 1)
    H = np.linalg.inv(dst_matrix) @ Hm @ src_matrix
    return H

def _umeyama(src, dst, estimate_scale):
    &#34;&#34;&#34;
    Estimate N-D similarity transformation with or without scaling.

    Parameters
    ----------
    src : (M, N) array
        Source coordinates.
    dst : (M, N) array
        Destination coordinates.
    estimate_scale : bool
        Whether to estimate scaling factor.
    Returns
    -------
    T : (N + 1, N + 1)
        The homogeneous similarity transformation matrix. The matrix contains
        NaN values only if the problem is not well-conditioned.
    References
    ----------
    .. [1] &#34;Least-squares estimation of transformation parameters between two
            point patterns&#34;, Shinji Umeyama, PAMI 1991, :DOI:`10.1109/34.88573`
    &#34;&#34;&#34;

    num = src.shape[0]
    dim = src.shape[1]

    # Compute mean of src and dst.
    src_mean = src.mean(axis=0)
    dst_mean = dst.mean(axis=0)

    # Subtract mean from src and dst.
    src_demean = src - src_mean
    dst_demean = dst - dst_mean

    # Eq. (38).
    A = dst_demean.T @ src_demean / num

    # Eq. (39).
    d = np.ones((dim,), dtype=np.double)
    if np.linalg.det(A) &lt; 0:
        d[dim - 1] = -1

    T = np.eye(dim + 1, dtype=np.double)

    U, S, V = np.linalg.svd(A)

    # Eq. (40) and (43).
    rank = np.linalg.matrix_rank(A)
    if rank == 0:
        return np.nan * T
    elif rank == dim - 1:
        if np.linalg.det(U) * np.linalg.det(V) &gt; 0:
            T[:dim, :dim] = U @ V
        else:
            s = d[dim - 1]
            d[dim - 1] = -1
            T[:dim, :dim] = U @ np.diag(d) @ V
            d[dim - 1] = s
    else:
        T[:dim, :dim] = U @ np.diag(d) @ V

    if estimate_scale:
        # Eq. (41) and (42).
        scale = 1.0 / src_demean.var(axis=0).sum() * (S @ d)
    else:
        scale = 1.0

    T[:dim, dim] = dst_mean - scale * (T[:dim, :dim] @ src_mean.T)
    T[:dim, :dim] *= scale

    return T


class ShiftTransform(ProjectiveTransform):
    &#34;&#34;&#34;
    ScaleShift transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

    Has the following form:
        X = x +  a2
        Y = y + b2
    and the homogeneous transformation matrix is::
        [[1  0   a2]
         [0   1  b2]
         [0   0    1]]
    In 2D, the transformation parameters can be given as the homogeneous
    transformation matrix, above, or as the implicit parameters, scale,
    rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
    only the matrix form is allowed.
    In narrower transforms, such as the Euclidean (only rotation and
    translation) or Similarity (rotation, translation, and a global scale
    factor) transforms, it is possible to specify 3D transforms using implicit
    parameters also.

    Parameters
    ----------
    matrix : (D+1, D+1) array, optional
        Homogeneous transformation matrix. If this matrix is provided, it is an
        error to provide any of scale, rotation, shear, or translation.
    scale : {s as float or (sx, sy) as array, list or tuple}, optional
        Scale factor(s). If a single value, it will be assigned to both
        sx and sy. Only available for 2D.
        .. versionadded:: 0.17
           Added support for supplying a single scalar value.
    translation : (tx, ty) as array, list or tuple, optional
        Translation parameters. Only available for 2D.
    dimensionality : int, optional
        The dimensionality of the transform. This is not used if any other
        parameters are provided.
    Attributes
    ----------
    params : (D+1, D+1) array
        Homogeneous transformation matrix.
    Raises
    ------
    ValueError
        If both ``matrix`` and any of the other parameters are provided.
    &#34;&#34;&#34;

    def __init__(self, matrix=None, translation=None, *, dimensionality=2):

        # these parameters get overwritten if a higher-D matrix is given
        self._coeffs = range(dimensionality * (dimensionality + 1))

        if translation is not None and matrix is not None:
            raise ValueError(&#34;You cannot specify the transformation matrix and&#34;
                             &#34; the implicit parameters at the same time.&#34;)
        if translation is not None and dimensionality &gt; 2:
            raise ValueError(&#39;Parameter input is only supported in 2D.&#39;)
        elif matrix is not None:
            if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:
                raise ValueError(&#34;Invalid shape of transformation matrix.&#34;)
            else:
                dimensionality = matrix.shape[0] - 1
                nparam = dimensionality * (dimensionality + 1)
            self._coeffs = range(nparam)
            self.params = matrix
            self.params[0,1] = 0
            self.params[1,0] = 0
        elif translation is not None:  # note: 2D only
            self.params = np.array([[1.0, 0.0,  0.0],
                                    [0.0,  1.0, 0.0],
                                    [0.0,  0.0, 1.0]])
            self.params[0:2, 2] = translation
        else:
            # default to an identity transform
            self.params = np.eye(dimensionality + 1)
    def estimate(self, src, dst):
        &#39;&#39;&#39;
                Parameters
        ----------
        src : (N, 2) array
            Source coordinates.
        dst : (N, 2) array
            Destination coordinates.
        Returns
        -------
        success : bool
            True, if model estimation succeeds.
        &#39;&#39;&#39;
        translation = np.mean(np.array(dst.astype(float)-src.astype(float)), axis=0)
        self.params = np.array([[1.0, 0.0,  0.0],
                                [0.0,  1.0, 0.0],
                                [0.0,  0.0, 1.0]])
        self.params[0:2, 2] = translation
        return True

    @property
    def translation(self):
        return self.params[0:self.dimensionality, self.dimensionality]


class XScaleShiftTransform(ProjectiveTransform):
    &#39;&#39;&#39;
    XScaleShift transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

    Has the following form::
        X = a0*x +  a2 = sx*x + a2
        Y = y + b2 = y + b2
    where ``sx`` and ``sy`` are scale factors in the x and y directions,
    and the homogeneous transformation matrix is::
        [[a0  0   a2]
         [0   1   b2]
         [0   0    1]]
    In 2D, the transformation parameters can be given as the homogeneous
    transformation matrix, above, or as the implicit parameters, scale,
    rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
    only the matrix form is allowed.
    In narrower transforms, such as the Euclidean (only rotation and
    translation) or Similarity (rotation, translation, and a global scale
    factor) transforms, it is possible to specify 3D transforms using implicit
    parameters also.

    Parameters
    ----------
    matrix : (D+1, D+1) array, optional
        Homogeneous transformation matrix. If this matrix is provided, it is an
        error to provide any of scale, rotation, shear, or translation.
    scale : {s as float or (sx, sy) as array, list or tuple}, optional
        Scale factor(s). If a single value, it will be assigned to both
        sx and sy. Only available for 2D.
        .. versionadded:: 0.17
           Added support for supplying a single scalar value.
    translation : (tx, ty) as array, list or tuple, optional
        Translation parameters. Only available for 2D.
    dimensionality : int, optional
        The dimensionality of the transform. This is not used if any other
        parameters are provided.
    Attributes
    ----------
    params : (D+1, D+1) array
        Homogeneous transformation matrix.
    Raises
    ------
    ValueError
        If both ``matrix`` and any of the other parameters are provided.
    &#39;&#39;&#39;

    def __init__(self, matrix=None, scale=None, translation=None, *, dimensionality=2):
        params = (scale is not None) or (translation is not None)
        # these parameters get overwritten if a higher-D matrix is given
        self._coeffs = range(dimensionality * (dimensionality + 1))

        if params and matrix is not None:
            raise ValueError(&#34;You cannot specify the transformation matrix and&#34;
                             &#34; the implicit parameters at the same time.&#34;)
        if params and dimensionality &gt; 2:
            raise ValueError(&#39;Parameter input is only supported in 2D.&#39;)
        elif matrix is not None:
            if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:
                raise ValueError(&#34;Invalid shape of transformation matrix.&#34;)
            else:
                dimensionality = matrix.shape[0] - 1
                nparam = dimensionality * (dimensionality + 1)
            self._coeffs = range(nparam)
            self.params = matrix
            self.params[0,1] = 0
            self.params[1,0] = 0
        elif params:  # note: 2D only
            if scale is None:
                scale = (1, 1)

            if translation is None:
                translation = (0, 0)

            if np.isscalar(scale):
                sx = scale
            else:
                sx = scale

            self.params = np.array([[sx, 0,  0],
                                    [0,  1, 0],
                                    [0,  0,  1]])
            self.params[0:2, 2] = translation
        else:
            # default to an identity transform
            self.params = np.eye(dimensionality + 1)
    def estimate(self, src, dst):
        &#34;&#34;&#34;
                Parameters
        ----------
        src : (N, 2) array
            Source coordinates.
        dst : (N, 2) array
            Destination coordinates.
        Returns
        -------
        success : bool
            True, if model estimation succeeds.
        &#34;&#34;&#34;

        n, d = src.shape
        xsrc = np.array(src)[:,0].astype(float)
        ysrc = np.array(src)[:,1].astype(float)
        xdst = np.array(dst)[:,0].astype(float)
        ydst = np.array(dst)[:,1].astype(float)
        s00 = np.sum(xsrc)
        s01 = np.sum(xdst)
        sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
        #s10 = np.sum(ysrc)
        #s11 = np.sum(ydst)
        #sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)
        sy = 1.0

        tx = np.mean(xdst) - sx * np.mean(xsrc)
        ty = np.mean(ydst) - sy * np.mean(ysrc)

        self.params = np.array([[sx, 0,  tx],
                                [0,  sy, ty],
                                [0,  0,  1]])
        return True

    def print_res(self):
        print(&#39;Printing from iside the class XScaleShiftTransform&#39;)

    @property
    def scale(self):
        return np.sqrt(np.sum(self.params ** 2, axis=0))[:self.dimensionality]

    @property
    def translation(self):
        return self.params[0:self.dimensionality, self.dimensionality]


class ScaleShiftTransform(ProjectiveTransform):
    &#39;&#39;&#39;
    ScaleShift transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

    Has the following form::
        X = a0*x +  a2 = sx*x + a2
        Y = b1*y + b2 = sy*y + b2
    where ``sx`` and ``sy`` are scale factors in the x and y directions,
    and the homogeneous transformation matrix is::
        [[a0  0   a2]
         [0   b1  b2]
         [0   0    1]]
    In 2D, the transformation parameters can be given as the homogeneous
    transformation matrix, above, or as the implicit parameters, scale,
    rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
    only the matrix form is allowed.
    In narrower transforms, such as the Euclidean (only rotation and
    translation) or Similarity (rotation, translation, and a global scale
    factor) transforms, it is possible to specify 3D transforms using implicit
    parameters also.

    Parameters
    ----------
    matrix : (D+1, D+1) array, optional
        Homogeneous transformation matrix. If this matrix is provided, it is an
        error to provide any of scale, rotation, shear, or translation.
    scale : {s as float or (sx, sy) as array, list or tuple}, optional
        Scale factor(s). If a single value, it will be assigned to both
        sx and sy. Only available for 2D.
        .. versionadded:: 0.17
           Added support for supplying a single scalar value.
    translation : (tx, ty) as array, list or tuple, optional
        Translation parameters. Only available for 2D.
    dimensionality : int, optional
        The dimensionality of the transform. This is not used if any other
        parameters are provided.
    Attributes
    ----------
    params : (D+1, D+1) array
        Homogeneous transformation matrix.
    Raises
    ------
    ValueError
        If both ``matrix`` and any of the other parameters are provided.
    &#39;&#39;&#39;

    def __init__(self, matrix=None, scale=None, translation=None, *, dimensionality=2):
        params = (scale is not None) or (translation is not None)
        # these parameters get overwritten if a higher-D matrix is given
        self._coeffs = range(dimensionality * (dimensionality + 1))

        if params and matrix is not None:
            raise ValueError(&#34;You cannot specify the transformation matrix and&#34;
                             &#34; the implicit parameters at the same time.&#34;)
        if params and dimensionality &gt; 2:
            raise ValueError(&#39;Parameter input is only supported in 2D.&#39;)
        elif matrix is not None:
            if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:
                raise ValueError(&#34;Invalid shape of transformation matrix.&#34;)
            else:
                dimensionality = matrix.shape[0] - 1
                nparam = dimensionality * (dimensionality + 1)
            self._coeffs = range(nparam)
            self.params = matrix
            self.params[0,1] = 0
            self.params[1,0] = 0
        elif params:  # note: 2D only
            if scale is None:
                scale = (1, 1)

            if translation is None:
                translation = (0, 0)

            if np.isscalar(scale):
                sx = sy = scale
            else:
                sx, sy = scale

            self.params = np.array([[sx, 0,  0],
                                    [0,  sy, 0],
                                    [0,  0,  1]])
            self.params[0:2, 2] = translation
        else:
            # default to an identity transform
            self.params = np.eye(dimensionality + 1)
    def estimate(self, src, dst):
        &#34;&#34;&#34;
                Parameters
        ----------
        src : (N, 2) array
            Source coordinates.
        dst : (N, 2) array
            Destination coordinates.
        Returns
        -------
        success : bool
            True, if model estimation succeeds.
        &#34;&#34;&#34;

        n, d = src.shape
        xsrc = np.array(src)[:,0].astype(float)
        ysrc = np.array(src)[:,1].astype(float)
        xdst = np.array(dst)[:,0].astype(float)
        ydst = np.array(dst)[:,1].astype(float)
        s00 = np.sum(xsrc)
        s01 = np.sum(xdst)
        sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
        s10 = np.sum(ysrc)
        s11 = np.sum(ydst)
        sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)

        tx = np.mean(xdst) - sx * np.mean(xsrc)
        ty = np.mean(ydst) - sy * np.mean(ysrc)

        self.params = np.array([[sx, 0,  tx],
                                [0,  sy, ty],
                                [0,  0,  1]])
        return True

    @property
    def scale(self):
        return np.sqrt(np.sum(self.params ** 2, axis=0))[:self.dimensionality]

    @property
    def translation(self):
        return self.params[0:self.dimensionality, self.dimensionality]


class RegularizedAffineTransform(ProjectiveTransform):
    &#34;&#34;&#34;
    Regularized Affine transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

    Has the following form::
        X = a0*x + a1*y + a2 =
          = sx*x*cos(rotation) - sy*y*sin(rotation + shear) + a2
        Y = b0*x + b1*y + b2 =
          = sx*x*sin(rotation) + sy*y*cos(rotation + shear) + b2
    where ``sx`` and ``sy`` are scale factors in the x and y directions,
    and the homogeneous transformation matrix is::
        [[a0  a1  a2]
         [b0  b1  b2]
         [0   0    1]]
    In 2D, the transformation parameters can be given as the homogeneous
    transformation matrix, above, or as the implicit parameters, scale,
    rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
    only the matrix form is allowed.
    In narrower transforms, such as the Euclidean (only rotation and
    translation) or Similarity (rotation, translation, and a global scale
    factor) transforms, it is possible to specify 3D transforms using implicit
    parameters also.

    Parameters
    ----------
    matrix : (D+1, D+1) array, optional
        Homogeneous transformation matrix. If this matrix is provided, it is an
        error to provide any of scale, rotation, shear, or translation.
    scale : {s as float or (sx, sy) as array, list or tuple}, optional
        Scale factor(s). If a single value, it will be assigned to both
        sx and sy. Only available for 2D.
        .. versionadded:: 0.17
           Added support for supplying a single scalar value.
    rotation : float, optional
        Rotation angle in counter-clockwise direction as radians. Only
        available for 2D.
    shear : float, optional
        Shear angle in counter-clockwise direction as radians. Only available
        for 2D.
    translation : (tx, ty) as array, list or tuple, optional
        Translation parameters. Only available for 2D.
    dimensionality : int, optional
        The dimensionality of the transform. This is not used if any other
        parameters are provided.
    Attributes
    ----------
    params : (D+1, D+1) array
        Homogeneous transformation matrix.
    Raises
    ------
    ValueError
        If both ``matrix`` and any of the other parameters are provided.
    &#34;&#34;&#34;

    def __init__(self, matrix=None, scale=None, rotation=None, shear=None,
                 translation=None, l2_matrix =None, targ_vector=None, *, dimensionality=2):

        self.l2_matrix = l2_matrix      # regularization vector
        self.targ_vector = targ_vector  # target
        params = any(param is not None
                     for param in (scale, rotation, shear, translation))

        # these parameters get overwritten if a higher-D matrix is given
        self._coeffs = range(dimensionality * (dimensionality + 1))

        if params and matrix is not None:
            raise ValueError(&#34;You cannot specify the transformation matrix and&#34;
                             &#34; the implicit parameters at the same time.&#34;)
        if params and dimensionality &gt; 2:
            raise ValueError(&#39;Parameter input is only supported in 2D.&#39;)
        elif matrix is not None:
            if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:
                raise ValueError(&#34;Invalid shape of transformation matrix.&#34;)
            else:
                dimensionality = matrix.shape[0] - 1
                nparam = dimensionality * (dimensionality + 1)
            self._coeffs = range(nparam)
            self.params = matrix
        elif params:  # note: 2D only
            if scale is None:
                scale = (1, 1)
            if rotation is None:
                rotation = 0
            if shear is None:
                shear = 0
            if translation is None:
                translation = (0, 0)

            if np.isscalar(scale):
                sx = sy = scale
            else:
                sx, sy = scale

            self.params = np.array([
                [sx * math.cos(rotation), -sy * math.sin(rotation + shear), 0],
                [sx * math.sin(rotation),  sy * math.cos(rotation + shear), 0],
                [                      0,                                0, 1]
            ])
            self.params[0:2, 2] = translation
        else:
            # default to an identity transform
            self.params = np.eye(dimensionality + 1)


    @property
    def scale(self):
        return np.sqrt(np.sum(self.params ** 2, axis=0))[:self.dimensionality]

    @property
    def rotation(self):
        if self.dimensionality != 2:
            raise NotImplementedError(
                &#39;The rotation property is only implemented for 2D transforms.&#39;
            )
        return math.atan2(self.params[1, 0], self.params[0, 0])

    @property
    def shear(self):
        if self.dimensionality != 2:
            raise NotImplementedError(
                &#39;The shear property is only implemented for 2D transforms.&#39;
            )
        beta = math.atan2(- self.params[0, 1], self.params[1, 1])
        return beta - self.rotation

    @property
    def translation(self):
        return self.params[0:self.dimensionality, self.dimensionality]



        # Thise are functions used for different steps of image analysis and registration

def ShiftTransform0(matrix=None, translation=None):
    return EuclideanTransform(matrix=matrix, rotation = 0, translation = translation)

def ScaleShiftTransform0(matrix=None, scale=None, translation=None):
    return AffineTransform(matrix=matrix, scale=scale, rotation = 0, shear=0, translation = translation)


def kp_to_list(kp):
    &#39;&#39;&#39;
    Convert a keypont object to a list (so that it can be &#34;pickled&#34;).

    Returns
    pt, angle, size, response, class_id, octave
    (all extracted from corresponding cv2.KeyPoint() object attributes)
    &#39;&#39;&#39;
    x, y = kp.pt
    pt = float(x), float(y)
    angle = float(kp.angle) if kp.angle is not None else None
    size = float(kp.size) if kp.size is not None else None
    response = float(kp.response) if kp.response is not None else None
    class_id = int(kp.class_id) if kp.class_id is not None else None
    octave = int(kp.octave) if kp.octave is not None else None
    return pt, angle, size, response, class_id, octave

def list_to_kp(inp_list):
    &#39;&#39;&#39;
    Convert a list to a keypont object

    Parameters:
    inp_list : list
        List of Key-Point properties to initialize the following cv2.KeyPoint() object attributes:
        [pt, angle, size, response, class_id, octave]

    Returns:
    kp : Instance of cv2.KeyPoint() object
    &#39;&#39;&#39;
    kp = cv2.KeyPoint()
    kp.pt = inp_list[0]
    kp.angle = inp_list[1]
    kp.size = inp_list[2]
    kp.response = inp_list[3]
    kp.octave = inp_list[4]
    kp.class_id = inp_list[5]
    return kp


def evaluate_FIBSEM_frame(params):
    &#39;&#39;&#39;
    Evaluates single FIB-SEM frame and extract parameters: data min/max, milling rate, FOV center.

    1. Calculates the data range of the EM data ©G.Shtengel 04/2022 gleb.shtengel@gmail.com
    Calculates histogram of pixel intensities of of the loaded image
    with number of bins determined by parameter nbins (default = 256)
    and normalizes it to get the probability distribution function (PDF),
    from which a cumulative distribution function (CDF) is calculated.
    Then given the threshold_min, threshold_max parameters,
    the minimum and maximum values for the image are found by finding
    the intensities at which CDF= threshold_min and (1- threshold_max), respectively.

    2. Extracts WD, MillingYVoltage, center_x, center_y data from the header

    Parameters:
    ----------
    params =  fl, kwargs
        fl : str
            The string containing a full path to the EM data file.
        kwargs: dictioanry of kwargs:
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        image_name: string
            the name of the image to perform this operations (defaulut is &#39;RawImageA&#39;)
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF

    Returns:
        dmin, dmax: (float) minimum and maximum values of the data range.
    &#39;&#39;&#39;
    fl, kwargs = params
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
    thr_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    thr_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    frame = FIBSEM_frame(fl, ftype=ftype)
    if frame.EightBit ==1:
        dmin = uint8(0)
        dmax =  uint8(255)
    else:
        dmin, dmax = frame.get_image_min_max(image_name = &#39;RawImageA&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins)
    if ftype == 0:
        try:
            WD = frame.WD
            MillingYVoltage = frame.MillingYVoltage
        except:
            WD = 0
            MillingYVoltage = 0
        try:
            center_x = (frame.FirstPixelX + frame.XResolution/2.0)
            center_y = (frame.FirstPixelY + frame.YResolution/2.0)
        except:
            center_x = 0
            center_y = 0
    else:
        WD = 0
        MillingYVoltage = 0
        center_x = 0
        center_y = 0

    return dmin, dmax, WD, MillingYVoltage, center_x, center_y


def evaluate_FIBSEM_frames_dataset(fls, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Evaluates parameters of FIBSEM data set (Min/Max, Working Distance (WD), Milling Y Voltage (MV), FOV center positions).

    Parameters:
    use_DASK : boolean
        perform remote DASK computations
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails

    kwargs:
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    frame_inds : array
        Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
    data_dir : str
        data directory (path) for saving the data
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    sliding_minmax : boolean
        if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
        if False - same data_min_glob and data_max_glob will be used for all files
    fit_params : list
        Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
        Other options are:
            [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
            [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
    Mill_Volt_Rate_um_per_V : float
        Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.
    FIBSEM_Data_xlsx : str
        Filepath of the Excell file for the FIBSEM data set data to be saved (Data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)
    disp_res : bolean
        If True (default), intermediate messages and results will be displayed.

    Returns:
    list of 9 parameters: FIBSEM_Data_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding, mill_rate_WD, mill_rate_MV, center_x, center_y
        FIBSEM_Data_xlsx : str
            path to Excel file with the FIBSEM data
        data_min_glob : float
            min data value for I8 conversion (open CV SIFT requires I8)
        data_man_glob : float
            max data value for I8 conversion (open CV SIFT requires I8)
        data_min_sliding : float array
            min data values (one per file) for I8 conversion
        data_max_sliding : float array
            max data values (one per file) for I8 conversion

        mill_rate_WD : float array
            Milling rate calculated based on Working Distance (WD)
        mill_rate_MV : float array
            Milling rate calculated based on Milling Y Voltage (MV)
        center_x : float array
            FOV Center X-coordinate extrated from the header data
        center_y : float array
            FOV Center Y-coordinate extrated from the header data
    &#39;&#39;&#39;

    nfrs = len(fls)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    frame_inds = kwargs.get(&#34;frame_inds&#34;, np.arange(len(fls)))
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, True)
    fit_params =  kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, 31.235258870176065)
    kwargs[&#39;Mill_Volt_Rate_um_per_V&#39;] = Mill_Volt_Rate_um_per_V

    FIBSEM_Data_xlsx = kwargs.get(&#39;FIBSEM_data_xlsx&#39;, &#39;FIBSEM_Data.xlsx&#39;)
    FIBSEM_Data_xlsx_path = os.path.join(data_dir, FIBSEM_Data_xlsx)
    disp_res = kwargs.get(&#34;disp_res&#34;, False)

    frame = FIBSEM_frame(fls[0], ftype=ftype)
    if frame.EightBit == 1 and ftype == 1:
        if disp_res:
            print(&#39;Original data is 8-bit, no need to find Min and Max for 8-bit conversion&#39;)
        data_min_glob = uint8(0)
        data_max_glob =  uint8(255)
        data_min_sliding = np.zeros(nfrs, dtype=uint8)
        data_max_sliding = np.zeros(nfrs, dtype=uint8)+ uint8(255)
        data_minmax_glob = np.zeros((nfrs, 2), dtype=uint8)
        data_minmax_glob[1, :] = uint8(255)
        mill_rate_WD = np.zeros(nfrs, dtype=float)
        mill_rate_MV = np.zeros(nfrs, dtype=float)
        center_x = np.zeros(nfrs, dtype=float)
        center_y = np.zeros(nfrs, dtype=float)

    else:
        params_s2 = [[fl, kwargs] for fl in fls]

        if use_DASK:
            if disp_res:
                print(&#39;Using DASK distributed&#39;)
            futures = DASK_client.map(evaluate_FIBSEM_frame, params_s2, retries = DASK_client_retries)
            results_s2 = np.array(DASK_client.gather(futures))
        else:
            if disp_res:
                print(&#39;Using Local Computation&#39;)
            results_s2 = np.zeros((nfrs, 6))
            for j, param_s2 in enumerate(tqdm(params_s2, desc=&#39;Evaluating FIB-SEM frames (data min/max, mill rate, FOV shifts): &#39;), display = disp_res):
                results_s2[j, :] = evaluate_FIBSEM_frame(param_s2)

        data_minmax_glob = results_s2[:, 0:2]
        data_min_glob, trash = get_min_max_thresholds(data_minmax_glob[:, 0], thr_min = threshold_min, thr_max = threshold_max, nbins = nbins, disp_res=False)
        trash, data_max_glob = get_min_max_thresholds(data_minmax_glob[:, 1], thr_min = threshold_min, thr_max = threshold_max, nbins = nbins, disp_res=False)
        data_min_sliding = savgol_filter(data_minmax_glob[:, 0].astype(double), min([fit_params[1], fit_params[1]]), fit_params[2])
        data_max_sliding = savgol_filter(data_minmax_glob[:, 1].astype(double), min([fit_params[1], fit_params[1]]), fit_params[2])
        mill_rate_WD = results_s2[:, 2]
        mill_rate_MV = results_s2[:, 3]
        center_x = results_s2[:, 4]
        center_y = results_s2[:, 5]

    if disp_res:
        print(&#39;Saving the FIBSEM dataset statistics (Min/Max, Mill Rate, FOV Shifts into the file: &#39;, FIBSEM_Data_xlsx_path)
        # Create a Pandas Excel writer using XlsxWriter as the engine.
    xlsx_writer = pd.ExcelWriter(FIBSEM_Data_xlsx_path, engine=&#39;xlsxwriter&#39;)
    columns=[&#39;Frame&#39;, &#39;Min&#39;, &#39;Max&#39;, &#39;Sliding Min&#39;, &#39;Sliding Max&#39;, &#39;Working Distance (mm)&#39;, &#39;Milling Y Voltage (V)&#39;, &#39;FOV X Center (Pix)&#39;, &#39;FOV Y Center (Pix)&#39;]
    minmax_df = pd.DataFrame(np.vstack((frame_inds.T,
        data_minmax_glob.T,
        data_min_sliding.T,
        data_max_sliding.T,
        mill_rate_WD.T,
        mill_rate_MV.T,
        center_x.T,
        center_y.T)).T, columns = columns, index = None)
    minmax_df.to_excel(xlsx_writer, index=None, sheet_name=&#39;FIBSEM Data&#39;)
    kwargs_info = pd.DataFrame([kwargs]).T   # prepare to be save in transposed format
    kwargs_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;kwargs Info&#39;)
    xlsx_writer.save()

    return FIBSEM_Data_xlsx_path, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding, mill_rate_WD, mill_rate_MV, center_x, center_y


# Routines to extract Key-Points and Descriptors

def extract_keypoints_descr_files(params):
    &#39;&#39;&#39;
    Extracts Key-Points and Descriptors (single image) for SIFT procedure.
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

    Parameters:
    -----------
    params = fl, dmin, dmax, kwargs
        fl : str
            image filename (full path)
        dmin : float
            min data value for I8 conversion (open CV SIFT requires I8)
        dmax : float
            max data value for I8 conversion (open CV SIFT requires I8)
        kwargs:
        -------
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        thr_min : float
            CDF threshold for determining the minimum data value
        thr_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction.
        kp_max_num : int
            Max number of key-points to be matched.
            Key-points in every frame are indexed (in descending order)
            by the strength of the response. Only kp_max_num is kept for
            further processing.
            Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
        SIFT_nfeatures : int
            SIFT libary default is 0. The number of best features to retain.
            The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
        SIFT_nOctaveLayers : int
            SIFT libary default  is 3. The number of layers in each octave.
            3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
        SIFT_contrastThreshold : double
            SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
            The larger the threshold, the less features are produced by the detector.
            The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
            When nOctaveLayers is set to default and if you want to use the value used in
            D. Lowe paper (0.03), set this argument to 0.09.
        SIFT_edgeThreshold : double
            SIFT libary default  is 10. The threshold used to filter out edge-like features.
            Note that the its meaning is different from the contrastThreshold,
            i.e. the larger the edgeThreshold, the less features are filtered out
            (more features are retained).
        SIFT_sigma : double
            SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
            If your image is captured with a weak camera with soft lenses, you might want to reduce the number.

    Returns:
        fnm : str
            path to the file containing Key-Points and Descriptors
    &#39;&#39;&#39;
    fl, dmin, dmax, kwargs = params
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    thr_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    thr_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, 10000)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])

    SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, 0)
    SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, 3)
    SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.04)
    SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, 10)
    SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, 1.6)

    #sift = cv2.xfeatures2d.SIFT_create(nfeatures=SIFT_nfeatures, nOctaveLayers=SIFT_nOctaveLayers, edgeThreshold=SIFT_edgeThreshold, contrastThreshold=SIFT_contrastThreshold, sigma=SIFT_sigma)
    sift = cv2.SIFT_create(nfeatures=SIFT_nfeatures, nOctaveLayers=SIFT_nOctaveLayers, edgeThreshold=SIFT_edgeThreshold, contrastThreshold=SIFT_contrastThreshold, sigma=SIFT_sigma)
    img, d1, d2 = FIBSEM_frame(fl, ftype=ftype).RawImageA_8bit_thresholds(thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = dmin, data_max = dmax, nbins=256)
    # extract keypoints and descriptors for both images

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = -1
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = -1

    kps, dess = sift.detectAndCompute(img[yi_eval:ya_eval, xi_eval:xa_eval], None)
    if kp_max_num != -1 and (len(kps) &gt; kp_max_num):
        kp_ind = np.argsort([-kp.response for kp in kps])[0:kp_max_num]
        kps = np.array(kps)[kp_ind]
        dess = np.array(dess)[kp_ind]
    if xi_eval &gt;0 or yi_eval&gt;0:   # add shifts to ke-pint coordinates to convert them to full image coordinated
        for kp in kps:
            kp.pt = kp.pt + np.array((xi_eval, yi_eval))
    #key_points = [KeyPoint(kp) for kp in kps]
    key_points = [kp_to_list(kp) for kp in kps]
    kpd = [key_points, dess]
    fnm = os.path.splitext(fl)[0] + &#39;_kpdes.bin&#39;
    pickle.dump(kpd, open(fnm, &#39;wb&#39;)) # converts array to binary and writes to output
    #pickle.dump(dess, open(fnm, &#39;wb&#39;)) # converts array to binary and writes to output
    return fnm

def extract_keypoints_dataset(fls, data_minmax, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Extracts Key-Points and Descriptors for SIFT procedure for all images (files) in the dataset.
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

    Parameters:
    -----------
    params = fl, data_minmax, kwargs

    fls : str array
        array of image filenames (full paths)
    data_minmax : list of 5 parameters
        minmax_xlsx : str
            path to Excel file with Min/Max data
        data_min_glob : float
            min data value for I8 conversion (open CV SIFT requires I8)
        data_min_sliding : float array
            min data values (one per file) for I8 conversion
        data_max_sliding : float array
            max data values (one per file) for I8 conversion
        data_minmax_glob : 2D float array
            min and max data values without sliding averaging
    DASK_client : DASK client object
        DASK client (needs to be initialized and running by this time)

    kwargs:
    sliding_minmax : boolean
        if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
        if False - same data_min_glob and data_max_glob will be used for all files
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    thr_min : float
        CDF threshold for determining the minimum data value
    thr_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    kp_max_num : int
        Max number of key-points to be matched.
        Key-points in every frame are indexed (in descending order)
        by the strength of the response. Only kp_max_num is kept for
        further processing.
        Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
    SIFT_nfeatures : int
        SIFT libary default is 0. The number of best features to retain.
        The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
    SIFT_nOctaveLayers : int
        SIFT libary default  is 3. The number of layers in each octave.
        3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
    SIFT_contrastThreshold : double
        SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
        The larger the threshold, the less features are produced by the detector.
        The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
        When nOctaveLayers is set to default and if you want to use the value used in
        D. Lowe paper (0.03), set this argument to 0.09.
    SIFT_edgeThreshold : double
        SIFT libary default  is 10. The threshold used to filter out edge-like features.
        Note that the its meaning is different from the contrastThreshold,
        i.e. the larger the edgeThreshold, the less features are filtered out
        (more features are retained).
    SIFT_sigma : double
        SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
        If your image is captured with a weak camera with soft lenses, you might want to reduce the number.

    Returns:
    fnms : str array
        array of paths to the files containing Key-Points and Descriptors
    &#39;&#39;&#39;
    minmax_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding  = data_minmax
    sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, True)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    if sliding_minmax:
        params_s3 = [[dts3[0], dts3[1], dts3[2], kwargs] for dts3 in zip(fls, data_min_sliding, data_max_sliding)]
    else:
        params_s3 = [[fl, data_min_glob, data_max_glob, kwargs] for fl in fls]
    if use_DASK:
        print(&#39;Using DASK distributed&#39;)
        futures_s3 = DASK_client.map(extract_keypoints_descr_files, params_s3, retries = DASK_client_retries)
        fnms = DASK_client.gather(futures_s3)
    else:
        print(&#39;Using Local Computation&#39;)
        fnms = []
        for j, param_s3 in enumerate(tqdm(params_s3, desc=&#39;Extracting Key Points and Descriptors: &#39;)):
            fnms.append(extract_keypoints_descr_files(param_s3))
    return fnms


def estimate_kpts_transform_error(src_pts, dst_pts, transform_matrix):
    &#34;&#34;&#34;
    Estimate the transformation error for key-point pairs and known transformation matrix.
    ©G.Shtengel, 09/2021. gleb.shtengel@gmail.com

    Image transformation matrix in a form:
        A = [[a0  a1   a2]
             [b0   b1  b2]
             [0   0    1]]
     Thransofrmation is supposed to be in a form:
     Xnew = a0 * Xoriginal + a1 * Yoriginal + a2
     Ynew = b0 * Xoriginal + b1 * Yoriginal + b2
     source and destination points are pairs of coordinates (2xN array)
     errors are estimated as norm(dest_pts - A*src_pts) so that least square regression can be performed

    Returns:
        np.linalg.norm(dst_pts - src_pts_transformed, ord=2, axis=1)
    &#34;&#34;&#34;
    src_pts_transformed = src_pts @ transform_matrix[0:2, 0:2].T + transform_matrix[0:2, 2]
    return np.linalg.norm(dst_pts - src_pts_transformed, ord=2, axis=1)


def determine_transformation_matrix(src_pts, dst_pts, TransformType, drmax = 2, max_iter = 100):
    &#39;&#39;&#39;
    Determine the transformation matrix.
    ©G.Shtengel, 09/2021. gleb.shtengel@gmail.com

    Determine the transformation matrix in a form:
            A = [[a0  a1   a2]
                 [b0   b1  b2]
                 [0   0    1]]
    based on the given source and destination points using linear regression such that the error is minimized for
    sum(dst_pts - A*src_pts).

    For each matched pair of keypoins the error is calculated as err[j] = dst_pts[j] - A*src_pts[j]
    The iterative procedure throws away the matched keypoint pair with worst error on every iteration
    untill the worst error falls below drmax or the max number of iterations is reached.

    Returns
    transform_matrix, kpts, error_abs_mean, iteration
    &#39;&#39;&#39;
    transform_matrix = np.eye(3,3)
    iteration = 1
    max_error = drmax * 2.0
    errors = []
    while iteration &lt;= max_iter and max_error &gt; drmax:
        # determine the new transformation matrix
        if TransformType == ShiftTransform:
            transform_matrix[0:2, 2] = np.mean(np.array(dst_pts.astype(float) - src_pts.astype(float)), axis=0)

        if TransformType == XScaleShiftTransform:
            n, d = src_pts.shape
            xsrc = np.array(src_pts)[:,0].astype(float)
            ysrc = np.array(src_pts)[:,1].astype(float)
            xdst = np.array(dst_pts)[:,0].astype(float)
            ydst = np.array(dst_pts)[:,1].astype(float)
            s00 = np.sum(xsrc)
            s01 = np.sum(xdst)
            sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
            #sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xdst*xdst) - s01*s01)
            #s10 = np.sum(ysrc)
            #s11 = np.sum(ydst)
            #sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)
            sy = 1.00 # force sy=1 if there are not enough keypoints points
            # spread over wide range of y-range (and y-range is small) to determine y-scale accuartely
            tx = np.mean(xdst) - sx * np.mean(xsrc)
            ty = np.mean(ydst) - sy * np.mean(ysrc)
            transform_matrix = np.array([[sx, 0,  tx],
                                         [0,  sy, ty],
                                         [0,  0,  1]])

        if TransformType == ScaleShiftTransform:
            n, d = src_pts.shape
            xsrc = np.array(src_pts)[:,0].astype(float)
            ysrc = np.array(src_pts)[:,1].astype(float)
            xdst = np.array(dst_pts)[:,0].astype(float)
            ydst = np.array(dst_pts)[:,1].astype(float)
            s00 = np.sum(xsrc)
            s01 = np.sum(xdst)
            sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
            s10 = np.sum(ysrc)
            s11 = np.sum(ydst)
            sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)
            tx = np.mean(xdst) - sx * np.mean(xsrc)
            ty = np.mean(ydst) - sy * np.mean(ysrc)
            transform_matrix = np.array([[sx, 0,  tx],
                                         [0,  sy, ty],
                                         [0,  0,  1]])

        if TransformType == AffineTransform:
            #estimate_scale = True
            #transform_matrix = _umeyama(src_pts, dst_pts, estimate_scale)
            tform = AffineTransform()
            tform.estimate(src_pts, dst_pts)
            transform_matrix = tform.params

        if TransformType == RegularizedAffineTransform:
            tform = AffineTransform()
            tform.estimate(src_pts, dst_pts)  # regularization parameters are already part of estimate procedure
            # this is implemented this way because the other code - RANSAC does not work otherwise
            transform_matrix = tform.params

        # estimate transformation errors and find outliers
        errs = estimate_kpts_transform_error(src_pts, dst_pts, transform_matrix)
        max_error = np.max(errs)
        ind = np.argmax(errs)
        src_pts = np.delete(src_pts, ind, axis=0)
        dst_pts = np.delete(dst_pts, ind, axis=0)
        #print(&#39;Iteration {:d}, max_error={:.2f} &#39;.format(iteration, max_error), (iteration &lt;= max_iter), (max_error &gt; drmax))
        iteration +=1
    kpts = [src_pts, dst_pts]
    error_abs_mean = np.mean(np.abs(np.delete(errs, ind, axis=0)))
    return transform_matrix, kpts, error_abs_mean, iteration


def determine_transformations_files(params_dsf):
    &#39;&#39;&#39; Determine the transformation matrix from two sets of Key-Points and Descriptors.
    ©G.Shtengel, 09/2021. gleb.shtengel@gmail.com

    This is a faster version of the procedure - it loads the keypoints and matches for each frame from files.
    params_dsf = fnm_1, fnm_2, kwargs
    where
    fnm_1 - keypoints for the first image (source)
    fnm_2 - keypoints for the first image (destination)
    and kwargs must include:
    TransformType - transformation type to be used (ShiftTransform, XScaleShiftTransform, ScaleShiftTransform, AffineTransform, RegularizedAffineTransform)
    BF_Matcher -  if True - use BF matcher, otherwise use FLANN matcher for keypoint matching
    solver - a string indicating which solver to use:
    &#39;LinReg&#39; will use Linear Regression with iterative &#34;Throwing out the Worst Residual&#34; Heuristic
    &#39;RANSAC&#39; will use RANSAC (Random Sample Consensus) algorithm.
    Lowe_Ratio_Threshold - threshold for Lowe&#39;s Ratio Test
    drmax - in the case of &#39;LinReg&#39; - outlier threshold for iterative regression
           - in the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
    max_iter - max number of iterations
    save_matches - if True - save the matched keypoints into a binary dump file

    Returns:
    transform_matrix, fnm_matches, kpts, error_abs_mean, iteration
    &#39;&#39;&#39;
    fnm_1, fnm_2, kwargs = params_dsf

    ftype = kwargs.get(&#34;ftype&#34;, 0)
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)    # threshold for Lowe&#39;s Ratio Test

    if TransformType == RegularizedAffineTransform:

        def estimate(self, src, dst):
            self.params = determine_regularized_affine_transform(src, dst, l2_matrix, targ_vector)
        RegularizedAffineTransform.estimate = estimate

    kpp1s, des1 = pickle.load(open(fnm_1, &#39;rb&#39;))
    kpp2s, des2 = pickle.load(open(fnm_2, &#39;rb&#39;))

    kp1 = [list_to_kp(kpp1) for kpp1 in kpp1s]     # this converts a list of lists to a list of keypoint objects to be used by a matcher later
    kp2 = [list_to_kp(kpp2) for kpp2 in kpp2s]     # same for the second frame

    # establish matches
    if BFMatcher:    # if BFMatcher==True - use BF (Brute Force) matcher
        # This procedure uses BF (Brute-Force) Matcher.
        # BF matcher takes the descriptor of one feature in the first image and matches it with all other features
        # in second image using some distance calculation. The closest match in teh second image is returned.
        # For BF matcher, first we have to create the cv.DescriptorMatcher object with BFMatcher as type.
        # It takes two optional params:
        #
        # First parameter one is NormType. It specifies the distance measurement to be used. By default, it is L2.
        # It is good for SIFT, SURF, etc. (L1 is also there).
        # For binary string-based descriptors like ORB, BRIEF, BRISK, etc., Hamming should be used,
        # which uses Hamming distance as measurement. If ORB is using WTA_K of 3 or 4, Hamming2 should be used.
        #
        # Second parameter is boolean variable, CrossCheck which is false by default.
        # If it is true, Matcher returns only those matches with value (i,j)
        # such that i-th descriptor in set A has j-th descriptor in set B as the best match and vice-versa.
        # That is, the two features in both sets should match each other.
        # It provides consistant result, and is a good alternative to ratio test proposed by D.Lowe in SIFT paper.
        # http://amroamroamro.github.io/mexopencv/opencv_contrib/SURF_descriptor.html
        bf = cv2.BFMatcher()
        matches = bf.knnMatch(des1,des2,k=2)
    else:            # otherwise - use FLANN matcher
        # This procedure uses FLANN (Fast Library for Approximate Nearest Neighbors) Matcher (FlannBasedMatcher):
        # https://docs.opencv.org/3.4/d5/d6f/tutorial_feature_flann_matcher.html
        #
        # FLANN contains a collection of algorithms optimized for fast nearest neighbor search in large datasets
        # and for high dimensional features. It works faster than BFMatcher for large datasets.
        # For FlannBasedMatcher, it accepts two sets of options which specifies the algorithm to be used, its related parameters etc.
        # First one is Index. For various algorithms, the information to be passed is explained in FLANN docs.
        # http://amroamroamro.github.io/mexopencv/opencv_contrib/SURF_descriptor.html
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
        search_params = dict(checks = 50)
        flann = cv2.FlannBasedMatcher(index_params, search_params)
        matches = flann.knnMatch(des1, des2, k=2)

    # Lowe&#39;s Ratio test
    good = []
    for m, n in matches:
        if m.distance &lt; Lowe_Ratio_Threshold * n.distance:
            good.append(m)

    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1, 2)
    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1, 2)

    if solver == &#39;LinReg&#39;:
        # Determine the transformation matrix via iterative liear regression
        transform_matrix, kpts, error_abs_mean, iteration = determine_transformation_matrix(src_pts, dst_pts, TransformType, drmax = drmax, max_iter = max_iter)
        n_kpts = len(kpts[0])
    else:  # the other option is solver = &#39;RANSAC&#39;
        try:
            min_samples = len(src_pts)//20
            model, inliers = ransac((src_pts, dst_pts),
                TransformType, min_samples=min_samples,
                residual_threshold=drmax, max_trials=10000)
            n_inliers = np.sum(inliers)
            inlier_keypoints_left = [cv2.KeyPoint(point[0], point[1], 1) for point in src_pts[inliers]]
            inlier_keypoints_right = [cv2.KeyPoint(point[0], point[1], 1) for point in dst_pts[inliers]]
            placeholder_matches = [cv2.DMatch(idx, idx, 1) for idx in range(n_inliers)]
            src_pts_ransac = np.float32([ inlier_keypoints_left[m.queryIdx].pt for m in placeholder_matches ]).reshape(-1, 2)
            dst_pts_ransac = np.float32([ inlier_keypoints_right[m.trainIdx].pt for m in placeholder_matches ]).reshape(-1, 2)
            #non_nan_inds = ~np.isnan(src_pts_ransac) * ~np.isnan(dst_pts_ransac)
            #src_pts_ransac = src_pts_ransac[non_nan_inds]
            #dst_pts_ransac = dst_pts_ransac[non_nan_inds]
            kpts = [src_pts_ransac, dst_pts_ransac]
            # find shift parameters
            transform_matrix = model.params
            iteration = len(src_pts)- len(src_pts_ransac)
            error_abs_mean = np.mean(np.abs(estimate_kpts_transform_error(src_pts_ransac, dst_pts_ransac, transform_matrix)))
        except:
            transform_matrix = array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])
            kpts = [[], []]
            error_abs_mean = np.nan
            iteration = np.nan
    if save_matches:
        fnm_matches = fnm_2.replace(&#39;_kpdes.bin&#39;, &#39;_matches.bin&#39;)
        pickle.dump(kpts, open(fnm_matches, &#39;wb&#39;))
    else:
        fnm_matches = &#39;&#39;
    return transform_matrix, fnm_matches, kpts, error_abs_mean, iteration


def determine_transformations_dataset(fnms, DASK_client, **kwargs):
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    params_s4 = []
    for j, fnm in enumerate(fnms[:-1]):
        fname1 = fnms[j]
        fname2 = fnms[j+1]
        params_s4.append([fname1, fname2, kwargs])
    if use_DASK:
        print(&#39;Using DASK distributed&#39;)
        futures4 = DASK_client.map(determine_transformations_files, params_s4, retries = DASK_client_retries)
        #determine_transformations_files returns (transform_matrix, fnm_matches, kpts, iteration)
        results_s4 = DASK_client.gather(futures4)
    else:
        print(&#39;Using Local Computation&#39;)
        results_s4 = []
        for param_s4 in tqdm(params_s4, desc = &#39;Extracting Transformation Parameters: &#39;):
            results_s4.append(determine_transformations_files(param_s4))
    return results_s4


def build_filename(fname, **kwargs):
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    dtp = kwargs.get(&#34;dtp&#34;, int16)                             #  int16 or uint8
    threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)
    zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, 1)             # binning factor in z-direction (milling direction). Default is 1
    preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
    fit_params =  kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # If True, the linear slope will be subtracted from the cumulative shifts.
    subtract_FOVtrend_from_fit = kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])

    pad_edges =  kwargs.get(&#34;pad_edges&#34;, True)
    suffix =  kwargs.get(&#34;suffix&#34;, &#39;&#39;)

    frame = FIBSEM_frame(fname, ftype=ftype)
    dformat_read = &#39;I8&#39; if frame.EightBit else &#39;I16&#39;

    if dtp == int16:
        dformat_save = &#39;I16&#39;
        fnm_reg = &#39;Registered_I16.mrc&#39;
    else:
        dformat_save = &#39;I8&#39;
        fnm_reg = &#39;Registered_I8.mrc&#39;

    if zbin_factor&gt;1:
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_zbin{:d}.mrc&#39;.format(zbin_factor))

    fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, (&#39;_&#39; + TransformType.__name__ + &#39;_&#39; + solver + &#39;.mrc&#39;))

    fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_drmax{:.1f}.mrc&#39;.format(drmax))

    if preserve_scales:
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_const_scls_&#39;+fit_params[0]+&#39;.mrc&#39;)

    if np.any(subtract_linear_fit):
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_shift_subtr.mrc&#39;)

    if pad_edges:
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_padded.mrc&#39;)

    if len(suffix)&gt;0:
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_&#39; + suffix + &#39;.mrc&#39;)
    return fnm_reg, dtp


def find_fit(tr_matr_cum, fit_params):
    fit_method = fit_params[0]
    if fit_method == &#39;SG&#39;:  # perform Savitsky-Golay fitting with parameters
        ws, porder = fit_params[1:3]         # window size 701, polynomial order 3
        s00_fit = savgol_filter(tr_matr_cum[:, 0, 0].astype(double), ws, porder)
        s01_fit = savgol_filter(tr_matr_cum[:, 0, 1].astype(double), ws, porder)
        s10_fit = savgol_filter(tr_matr_cum[:, 1, 0].astype(double), ws, porder)
        s11_fit = savgol_filter(tr_matr_cum[:, 1, 1].astype(double), ws, porder)
    else:
        fr = np.arange(0, len(tr_matr_cum), dtype=np.double)
        if fit_method == &#39;PF&#39;:  # perform polynomial fitting with parameters
            porder = fit_params[1]         # polynomial order
            s00_coeffs = np.polyfit(fr, tr_matr_cum[:, 0, 0].astype(double), porder)
            s00_fit = np.polyval(s00_coeffs, fr)
            s01_coeffs = np.polyfit(fr, tr_matr_cum[:, 0, 1].astype(double), porder)
            s01_fit = np.polyval(s01_coeffs, fr)
            s10_coeffs = np.polyfit(fr, tr_matr_cum[:, 1, 0].astype(double), porder)
            s10_fit = np.polyval(s10_coeffs, fr)
            s11_coeffs = np.polyfit(fr, tr_matr_cum[:, 1, 1].astype(double), porder)
            s11_fit = np.polyval(s11_coeffs, fr)

        else:   # otherwise perform linear fit with origin point tied to 1 for Sxx and Syy and to 0 for Sxy and Syx
            slp00 = -1.0 * (np.sum(fr)-np.dot(tr_matr_cum[:, 0, 0],fr))/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
            s00_fit = 1.0 + slp00 * fr
            slp11 = -1.0 * (np.sum(fr)-np.dot(tr_matr_cum[:, 1, 1],fr))/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
            s11_fit = 1.0 + slp11 * fr
            slp01 = np.dot(tr_matr_cum[:, 0, 1],fr)/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
            s01_fit = slp01 * fr
            slp10 = np.dot(tr_matr_cum[:, 1, 0],fr)/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
            s10_fit = slp10 * fr

    tr_matr_cum_new = tr_matr_cum.copy()
    tr_matr_cum_new[:, 0, 0] = tr_matr_cum[:, 0, 0].astype(double) + 1.0 - s00_fit
    tr_matr_cum_new[:, 0, 1] = tr_matr_cum[:, 0, 1].astype(double) - s01_fit
    tr_matr_cum_new[:, 1, 0] = tr_matr_cum[:, 1, 0].astype(double) - s10_fit
    tr_matr_cum_new[:, 1, 1] = tr_matr_cum[:, 1, 1].astype(double) + 1.0 - s11_fit
    s_fits = [s00_fit, s01_fit, s10_fit, s11_fit]
    return tr_matr_cum_new, s_fits


def process_transf_matrix(transformation_matrix, FOVtrend_x, FOVtrend_y, fnms_matches, npts, error_abs_mean, **kwargs):
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)

    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)

    preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
    fit_params =  kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # The linear slopes along X- and Y- directions (respectively) will be subtracted from the cumulative shifts.
    subtract_FOVtrend_from_fit = kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])

    #print(&#34;subtract_linear_fit:&#34;, subtract_linear_fit)
    pad_edges =  kwargs.get(&#34;pad_edges&#34;, True)

    tr_matr_cum = transformation_matrix.copy()
    prev_mt = np.eye(3,3)
    for j, cur_mt in enumerate(tqdm(transformation_matrix, desc=&#39;Calculating Cummilative Transformation Matrix&#39;)):
        if any(np.isnan(cur_mt)):
            print(&#39;Frame: {:d} has ill-defined transformation matrix, will use identity transformation instead:&#39;.format(j))
            print(cur_mt)
        else:
            prev_mt = np.matmul(cur_mt, prev_mt)
        tr_matr_cum[j] = prev_mt
    # Now insert identity matrix for the zero frame which does not need to be trasformed

    tr_matr_cum = np.insert(tr_matr_cum, 0, np.eye(3,3), axis=0)

    fr = np.arange(0, len(tr_matr_cum), dtype=np.double)
    s00_cum_orig = tr_matr_cum[:, 0, 0].astype(np.double)
    s01_cum_orig = tr_matr_cum[:, 0, 1].astype(np.double)
    s10_cum_orig = tr_matr_cum[:, 1, 0].astype(np.double)
    s11_cum_orig = tr_matr_cum[:, 1, 1].astype(np.double)
    Xshift_cum_orig = tr_matr_cum[:, 0, 2].astype(np.double)
    Yshift_cum_orig = tr_matr_cum[:, 1, 2].astype(np.double)

    if preserve_scales:  # in case of ScaleShift Transform WITH scale perservation
        #print(&#39;Recalculating the transformation matrix for preserved scales&#39;)
        tr_matr_cum, s_fits = find_fit(tr_matr_cum, fit_params)
        s00_fit, s01_fit, s10_fit, s11_fit = s_fits
        txs = np.zeros(len(tr_matr_cum), dtype=float)
        tys = np.zeros(len(tr_matr_cum), dtype=float)

        for j, fnm_matches in enumerate(tqdm(fnms_matches, desc=&#39;Recalculating the shifts for preserved scales: &#39;)):
            try:
                src_pts, dst_pts = pickle.load(open(fnm_matches, &#39;rb&#39;))

                txs[j+1] = np.mean(tr_matr_cum[j, 0, 0] * dst_pts[:, 0] + tr_matr_cum[j, 0, 1] * dst_pts[:, 1]
                                   - tr_matr_cum[j+1, 0, 0] * src_pts[:, 0] - tr_matr_cum[j+1, 0, 1] * src_pts[:, 1])
                tys[j+1] = np.mean(tr_matr_cum[j, 1, 1] * dst_pts[:, 1] + tr_matr_cum[j, 1, 0] * dst_pts[:, 0]
                                   - tr_matr_cum[j+1, 1, 1] * src_pts[:, 1] - tr_matr_cum[j+1, 1, 0] * src_pts[:, 0])
            except:
                txs[j+1] = 0.0
                tys[j+1] = 0.0
        txs_cum = np.cumsum(txs)
        tys_cum = np.cumsum(tys)
        tr_matr_cum[:, 0, 2] = txs_cum
        tr_matr_cum[:, 1, 2] = tys_cum

    Xshift_cum = tr_matr_cum[:, 0, 2].copy()
    Yshift_cum = tr_matr_cum[:, 1, 2].copy()

    # Subtract linear trends from offsets
    if subtract_linear_fit[0]:
        fr = np.arange(0, len(Xshift_cum))
        if subtract_FOVtrend_from_fit[0]:
            pX = np.polyfit(fr, Xshift_cum+FOVtrend_x, 1)
        else:
            pX = np.polyfit(fr, Xshift_cum, 1)
        Xfit = np.polyval(pX, fr)
        Xshift_residual = Xshift_cum - Xfit
        #Xshift_residual0 = -np.polyval(pX, 0.0)
    else:
        Xshift_residual = Xshift_cum.copy()
        Xfit = np.zeros(len(Xshift_cum))

    if subtract_linear_fit[1]:
        fr = np.arange(0, len(Yshift_cum))
        if subtract_FOVtrend_from_fit[1]:
            pY = np.polyfit(fr, Yshift_cum+FOVtrend_y, 1)
        else:
            pY = np.polyfit(fr, Yshift_cum, 1)
        Yfit = np.polyval(pY, fr)
        Yshift_residual = Yshift_cum - Yfit
        #Yshift_residual0 = -np.polyval(pY, 0.0)
    else:
        Yshift_residual = Yshift_cum.copy()
        Yfit = np.zeros(len(Yshift_cum))

    # define new cumulative transformation matrix where the offests may have linear slopes subtracted
    tr_matr_cum[:, 0, 2] = Xshift_residual
    tr_matr_cum[:, 1, 2] = Yshift_residual

    # save the data
    default_bin_file = os.path.join(data_dir, fnm_reg.replace(&#39;.mrc&#39;, &#39;_transf_matrix.bin&#39;))
    transf_matrix_bin_file = kwargs.get(&#34;dump_filename&#34;, default_bin_file)
    transf_matrix_xlsx_file = default_bin_file.replace(&#39;.bin&#39;, &#39;.xlsx&#39;)

    xlsx_writer = pd.ExcelWriter(transf_matrix_xlsx_file, engine=&#39;xlsxwriter&#39;)
    columns=[&#39;T00 (Sxx)&#39;, &#39;T01 (Sxy)&#39;, &#39;T02 (Tx)&#39;,
                 &#39;T10 (Syx)&#39;, &#39;T11 (Syy)&#39;, &#39;T12 (Ty)&#39;,
                 &#39;T20 (0.0)&#39;, &#39;T21 (0.0)&#39;, &#39;T22 (1.0)&#39;]
    tr_mx_dt = pd.DataFrame(transformation_matrix.reshape((len(transformation_matrix), 9)), columns = columns, index = None)
    tr_mx_dt.to_excel(xlsx_writer, index=None, sheet_name=&#39;Orig. Transformation Matrix&#39;)

    tr_mx_cum_dt = pd.DataFrame(tr_matr_cum.reshape((len(tr_matr_cum), 9)), columns = columns, index = None)
    tr_mx_cum_dt.to_excel(xlsx_writer, index=None, sheet_name=&#39;Cum. Transformation Matrix&#39;)

    columns_shifts=[&#39;s00_cum_orig&#39;, &#39;s00_fit&#39;, &#39;s11_cum_orig&#39;, &#39;s11_fit&#39;, &#39;s01_cum_orig&#39;, &#39;s01_fit&#39;, &#39;s10_cum_orig&#39;, &#39;s10_fit&#39;, &#39;Xshift_cum_orig&#39;, &#39;Yshift_cum_orig&#39;, &#39;Xshift_cum&#39;, &#39;Yshift_cum&#39;, &#39;Xfit&#39;, &#39;Yfit&#39;]
    shifts_dt = pd.DataFrame(np.vstack((s00_cum_orig, s00_fit, s11_cum_orig, s11_fit, s01_cum_orig, s01_fit, s10_cum_orig, s10_fit, Xshift_cum_orig, Yshift_cum_orig, Xshift_cum, Yshift_cum, Xfit, Yfit)).T, columns = columns_shifts, index = None)
    shifts_dt.to_excel(xlsx_writer, index=None, sheet_name=&#39;Intermediate Results&#39;)

    columns_reg_stat = [&#39;Npts&#39;, &#39;Mean Abs Error&#39;]
    reg_stat_dt = pd.DataFrame(np.vstack((npts, error_abs_mean)).T, columns = columns_reg_stat, index = None)
    reg_stat_dt.to_excel(xlsx_writer, index=None, sheet_name=&#39;Reg. Stat. Info&#39;)

    kwargs_info = pd.DataFrame([kwargs]).T   # prepare to be save in transposed format
    kwargs_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;kwargs Info&#39;)

    xlsx_writer.save()

    DumpObject = [kwargs, npts, error_abs_mean,
              transformation_matrix, s00_cum_orig, s11_cum_orig, s00_fit, s11_fit,
              tr_matr_cum, s01_cum_orig, s10_cum_orig, s01_fit, s10_fit,
              Xshift_cum_orig, Yshift_cum_orig, Xshift_cum, Yshift_cum, Yshift_cum, Xfit, Yfit]
    with open(transf_matrix_bin_file,&#34;wb&#34;) as f:
        pickle.dump(DumpObject, f)

    return tr_matr_cum, transf_matrix_xlsx_file


def determine_pad_offsets_old(shape, tr_matr):
    ysz, xsz = shape
    xmins = np.zeros(len(tr_matr))
    xmaxs = xmins.copy()
    ymins = xmins.copy()
    ymaxs = xmins.copy()
    corners = np.array([[0,0], [0, ysz], [xsz, 0], [xsz, ysz]])
    for j, trm in enumerate(tqdm(tr_matr, desc = &#39;Determining the pad offsets&#39;)):
        a = (trm[0:2, 0:2] @ corners.T).T + trm[0:2, 2]
        xmins[j] = np.min(a[:, 0])
        xmaxs[j] = np.max(a[:, 0])
        ymins[j] = np.min(a[:, 1])
        ymaxs[j] = np.max(a[:, 1])
        xmin = np.min((np.min(xmins), 0.0))
        xmax = np.max(xmaxs)-xsz
        ymin = np.min((np.min(ymins), 0.0))
        ymax = np.max(ymaxs)-ysz
    return xmin, xmax, ymin, ymax


def determine_pad_offsets(shape, tr_matr):
    ysz, xsz = shape
    corners = np.array([[0.0, 0.0, 1.0], [0.0, ysz, 1.0], [xsz, 0.0, 1.0], [xsz, ysz, 1.0]])
    a = np.array(tr_matr)[:, 0:2, :] @ corners.T
    xc = a[:, 0, :].ravel()
    yc = a[:, 1, :].ravel()
    xmin = np.min((np.min(xc), 0.0))
    xmax = np.max(xc)-xsz
    ymin = np.min((np.min(yc), 0.0))
    ymax = np.max(yc)-ysz
    return xmin, xmax, ymin, ymax


def SIFT_find_keypoints_dataset(fr, **kwargs):
    &#39;&#39;&#39;
    Evaluate SIFT key point discovery for a test frame (fr). ©G.Shtengel 08/2022 gleb.shtengel@gmail.com

    Parameters:
    fr : str
        filename for the data frame to be used for SIFT key point discovery evaluation

    kwargs
    ---------
    data_dir : str
        data directory (path)
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    fnm_reg : str
        filename for the final registed dataset
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.

    SIFT_nfeatures : int
        SIFT libary default is 0. The number of best features to retain.
        The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
    SIFT_nOctaveLayers : int
        SIFT libary default  is 3. The number of layers in each octave.
        3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
    SIFT_contrastThreshold : double
        SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
        The larger the threshold, the less features are produced by the detector.
        The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
        When nOctaveLayers is set to default and if you want to use the value used in
        D. Lowe paper (0.03), set this argument to 0.09.
    SIFT_edgeThreshold : double
        SIFT libary default  is 10. The threshold used to filter out edge-like features.
        Note that the its meaning is different from the contrastThreshold,
        i.e. the larger the edgeThreshold, the less features are filtered out
        (more features are retained).
    SIFT_sigma : double
        SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
        If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check

    Returns:
    dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts
    &#39;&#39;&#39;

    ftype = kwargs.get(&#34;ftype&#34;, 0)
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)   # threshold for Lowe&#39;s Ratio Test
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.04)

    frame = FIBSEM_frame(fr, ftype=ftype)
    if ftype == 0:
        if frame.FileVersion &gt; 8 :
            Sample_ID = frame.Sample_ID.strip(&#39;\x00&#39;)
        else:
            Sample_ID = frame.Notes[0:16]
    else:
        Sample_ID = frame.Sample_ID
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, Sample_ID)

    print(Sample_ID)

    #if save_res_png :
    #    frame.display_images()

    img = np.ravel(frame.RawImageA)
    fsz=12
    fszl=11
    dmin, dmax = frame.get_image_min_max(image_name = &#39;RawImageA&#39;, thr_min=threshold_min, thr_max=threshold_max, nbins=nbins)
    xi = dmin-(np.abs(dmax-dmin)/10)
    xa = dmax+(np.abs(dmax-dmin)/10)

    fig, axs = subplots(2,1, figsize=(6,6))
    fig.suptitle(Sample_ID + &#39;,  thr_min={:.0e}, thr_max={:.0e}, contrastThreshold={:.3f}, kp_max_num={:d}, comp.time={:.1f}sec&#39;.format(threshold_min, threshold_max, SIFT_contrastThreshold, kp_max_num, comp_time), fontsize=fszl)

    hist, bins, patches = axs[0].hist(img, bins = nbins)
    axs[0].set_xlim(xi, xa)
    axs[0].plot([dmin, dmin], [0, np.max(hist)], &#39;r&#39;, linestyle = &#39;--&#39;)
    axs[0].plot([dmax, dmax], [0, np.max(hist)], &#39;g&#39;, linestyle = &#39;--&#39;)
    axs[0].set_ylabel(&#39;Count&#39;, fontsize = fsz)
    pdf = hist / (frame.XResolution * frame.YResolution)
    cdf = np.cumsum(pdf)
    xCDF = bins[0:-1]+(bins[1]-bins[0])/2.0
    xthr = [xCDF[0], xCDF[-1]]
    ythr_min = [threshold_min, threshold_min]
    y1thr_max = [1-threshold_max, 1-threshold_max]

    axs[1].plot(xCDF, cdf, label=&#39;CDF&#39;)
    axs[1].plot(xthr, ythr_min, &#39;r&#39;, label=&#39;thr_min={:.5f}&#39;.format(threshold_min))
    axs[1].plot([dmin, dmin], [0, 1], &#39;r&#39;, linestyle = &#39;--&#39;, label = &#39;data_min={:.1f}&#39;.format(dmin))
    axs[1].plot(xthr, y1thr_max, &#39;g&#39;, label=&#39;1.0 - thr_max = {:.5f}&#39;.format(1-threshold_max))
    axs[1].plot([dmax, dmax], [0, 1], &#39;g&#39;, linestyle = &#39;--&#39;, label = &#39;data_max={:.1f}&#39;.format(dmax))
    axs[1].set_xlabel(&#39;Intensity Level&#39;, fontsize = fsz)
    axs[1].set_ylabel(&#39;CDF&#39;, fontsize = fsz)
    axs[1].set_xlim(xi, xa)
    axs[1].legend(loc=&#39;center&#39;, fontsize=fsz)
    axs[0].set_title(&#39;Data Min and Max with thr_min={:.0e},  thr_max={:.0e}&#39;.format(threshold_min, threshold_max), fontsize = fsz)
    for ax in ravel(axs):
        ax.grid(True)

    t0 = time.time()
    params1 = [fr, dmin, dmax, kwargs]
    fnm_1 = extract_keypoints_descr_files(params1)

    t1 = time.time()
    comp_time = (t1-t0)
    #print(&#39;Time to compute: {:.1f}sec&#39;.format(comp_time))

    xfsz = 3 * (np.int(7 * frame.XResolution / np.max([frame.XResolution, frame.YResolution]))+1)
    yfsz = 3 * (np.int(7 * frame.YResolution / np.max([frame.XResolution, frame.YResolution]))+2)
    fig2, ax = subplots(1,1, figsize=(xfsz,yfsz))
    fig2.subplots_adjust(left=0.0, bottom=0.25*(1-frame.YResolution/frame.XResolution), right=1.0, top=1.0)
    symsize = 2
    fsize = 12
    img2 = FIBSEM_frame(fr, ftype=ftype).RawImageA
    ax.imshow(img2, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
    ax.axis(False)

    kpp1s, des1 = pickle.load(open(fnm_1, &#39;rb&#39;))
    kp1 = [list_to_kp(kpp1) for kpp1 in kpp1s]     # this converts a list of lists to a list of keypoint objects to be used by a matcher later
    src_pts = np.float32([ kp.pt for kp in kp1 ]).reshape(-1, 2)
    x, y = src_pts.T
    print(&#39;Extracted {:d} keyponts&#39;.format(len(kp1)))
    # the code below is for vector map. vectors have origin coordinates x and y, and vector projections xs and ys.
    vec_field = ax.scatter(x,y, s=0.02, marker=&#39;o&#39;, c=&#39;r&#39;)
    ax.text(0.01, 1.1-0.13*frame.YResolution/frame.XResolution, Sample_ID + &#39;, thr_min={:.0e}, thr_max={:.0e}, SIFT_nfeatures={:d}&#39;.format(threshold_min, threshold_max, SIFT_nfeatures), fontsize=fsize, transform=ax.transAxes)
    if save_res_png :
        png_name = os.path.splitext(fr)[0] + &#39;_SIFT_kpts_eval_&#39;+&#39;_thr_min{:.5f}_thr_max{:.5f}.png&#39;.format(threshold_min, threshold_max)
        fig2.savefig(png_name, dpi=300)
    return(dmin, dmax, comp_time, src_pts)


# This is a function used for selecting proper threshold and kp_max_num parameters for SIFT processing
def SIFT_evaluation_dataset(fs, **kwargs):
    &#39;&#39;&#39;
    Evaluate SIFT settings and perfromance of few test frames (fs). ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

    Parameters:
    fs : array of str
        filenames for the data frames to be used for SIFT evaluation

    kwargs
    ---------
    data_dir : str
        data directory (path)
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    fnm_reg : str
        filename for the final registed dataset
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
    TransformType : object reference
        Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
        Choose from the following options:
            ShiftTransform - only x-shift and y-shift
            XScaleShiftTransform  -  x-scale, x-shift, y-shift
            ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
            AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
            RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
    l2_matrix : 2D float array
        matrix of regularization (shrinkage) parameters
    targ_vector = 1D float array
        target vector for regularization
    solver : str
        Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
    drmax : float
        In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
        In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
    max_iter : int
        Max number of iterations in the iterative procedure above (RANSAC or LinReg)
    BFMatcher : boolean
        If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches : boolean
        If True, matches will be saved into individual files
    kp_max_num : int
        Max number of key-points to be matched.
        Key-points in every frame are indexed (in descending order) by the strength of the response.
        Only kp_max_num is kept for further processing.
        Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
    SIFT_nfeatures : int
        SIFT libary default is 0. The number of best features to retain.
        The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
    SIFT_nOctaveLayers : int
        SIFT libary default  is 3. The number of layers in each octave.
        3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
    SIFT_contrastThreshold : double
        SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
        The larger the threshold, the less features are produced by the detector.
        The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
        When nOctaveLayers is set to default and if you want to use the value used in
        D. Lowe paper (0.03), set this argument to 0.09.
    SIFT_edgeThreshold : double
        SIFT libary default  is 10. The threshold used to filter out edge-like features.
        Note that the its meaning is different from the contrastThreshold,
        i.e. the larger the edgeThreshold, the less features are filtered out
        (more features are retained).
    SIFT_sigma : double
        SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
        If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check

    Returns:
    dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts
    &#39;&#39;&#39;
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)   # threshold for Lowe&#39;s Ratio Test
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)
    SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.04)

    frame = FIBSEM_frame(fs[0], ftype=ftype)
    if ftype == 0:
        if frame.FileVersion &gt; 8 :
            Sample_ID = frame.Sample_ID.strip(&#39;\x00&#39;)
        else:
            Sample_ID = frame.Notes[0:16]
    else:
        Sample_ID = frame.Sample_ID
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, Sample_ID)

    print(Sample_ID)

    #if save_res_png :
    #    frame.display_images()

    img = np.ravel(frame.RawImageA)
    fsz=12
    fszl=11
    dmin, dmax = frame.get_image_min_max(image_name = &#39;RawImageA&#39;, thr_min=threshold_min, thr_max=threshold_max, nbins=nbins)
    xi = dmin-(np.abs(dmax-dmin)/10)
    xa = dmax+(np.abs(dmax-dmin)/10)

    fig, axs = subplots(2,2, figsize=(12,8))
    fig.suptitle(Sample_ID + &#39;,  thr_min={:.0e}, thr_max={:.0e}, contrastThreshold={:.3f}, kp_max_num={:d}&#39;.format(threshold_min, threshold_max, SIFT_contrastThreshold, kp_max_num), fontsize=fszl)

    hist, bins, patches = axs[0,0].hist(img, bins = nbins)
    axs[0,0].set_xlim(xi, xa)
    axs[0,0].plot([dmin, dmin], [0, np.max(hist)], &#39;r&#39;, linestyle = &#39;--&#39;)
    axs[0,0].plot([dmax, dmax], [0, np.max(hist)], &#39;g&#39;, linestyle = &#39;--&#39;)
    axs[0,0].set_ylabel(&#39;Count&#39;, fontsize = fsz)
    pdf = hist / (frame.XResolution * frame.YResolution)
    cdf = np.cumsum(pdf)
    xCDF = bins[0:-1]+(bins[1]-bins[0])/2.0
    xthr = [xCDF[0], xCDF[-1]]
    ythr_min = [threshold_min, threshold_min]
    y1thr_max = [1-threshold_max, 1-threshold_max]

    axs[1,0].plot(xCDF, cdf, label=&#39;CDF&#39;)
    axs[1,0].plot(xthr, ythr_min, &#39;r&#39;, label=&#39;thr_min={:.5f}&#39;.format(threshold_min))
    axs[1,0].plot([dmin, dmin], [0, 1], &#39;r&#39;, linestyle = &#39;--&#39;, label = &#39;data_min={:.1f}&#39;.format(dmin))
    axs[1,0].plot(xthr, y1thr_max, &#39;g&#39;, label=&#39;1.0 - thr_max = {:.5f}&#39;.format(1-threshold_max))
    axs[1,0].plot([dmax, dmax], [0, 1], &#39;g&#39;, linestyle = &#39;--&#39;, label = &#39;data_max={:.1f}&#39;.format(dmax))
    axs[1,0].set_xlabel(&#39;Intensity Level&#39;, fontsize = fsz)
    axs[1,0].set_ylabel(&#39;CDF&#39;, fontsize = fsz)
    axs[1,0].set_xlim(xi, xa)
    axs[1,0].legend(loc=&#39;center&#39;, fontsize=fsz)
    axs[0,0].set_title(&#39;Data Min and Max with thr_min={:.0e},  thr_max={:.0e}&#39;.format(threshold_min, threshold_max), fontsize = fsz)

    minmax = []
    for f in fs:
        minmax.append(FIBSEM_frame(f, ftype=ftype).get_image_min_max(image_name = &#39;RawImageA&#39;, thr_min=threshold_min, thr_max=threshold_max, nbins=nbins))
    dmin = np.min(np.array(minmax))
    dmax = np.max(np.array(minmax))
    #print(&#39;data range: &#39;, dmin, dmax)

    t0 = time.time()
    #print(&#39;File1: &#39;,fs[0], dmin, dmax, kwargs)
    params1 = [fs[0], dmin, dmax, kwargs]
    fnm_1 = extract_keypoints_descr_files(params1)
    #print(&#39;File2: &#39;,fs[1], dmin, dmax, kwargs)
    params2 = [fs[1], dmin, dmax, kwargs]
    fnm_2 = extract_keypoints_descr_files(params2)

    params_dsf = [fnm_1, fnm_2, kwargs]
    transform_matrix, fnm_matches, kpts, error_abs_mean, iteration = determine_transformations_files(params_dsf)
    n_matches = len(kpts[0])

    src_pts_filtered, dst_pts_filtered = kpts

    src_pts_transformed = src_pts_filtered @ transform_matrix[0:2, 0:2].T + transform_matrix[0:2, 2]
    xshifts = (dst_pts_filtered - src_pts_transformed)[:,0]
    yshifts = (dst_pts_filtered - src_pts_transformed)[:,1]

    t1 = time.time()
    comp_time = (t1-t0)
    #print(&#39;Time to compute: {:.1f}sec&#39;.format(comp_time))

    axx = axs[0,1]
    hst = axx.hist(xshifts, bins=64)
    axx.set_xlabel(&#39;SIFT: X Error (pixels)&#39;)
    axx.text(0.05, 0.9, &#39;mean={:.3f}&#39;.format(np.mean(xshifts)), transform=axx.transAxes, fontsize=fsz)
    axx.text(0.05, 0.8, &#39;median={:.3f}&#39;.format(np.median(xshifts)), transform=axx.transAxes, fontsize=fsz)
    axx.set_title(&#39;data range: {:.1f} ÷ {:.1f}&#39;.format(dmin, dmax), fontsize=fsz)
    axy = axs[1,1]
    hst = axy.hist(yshifts, bins=64)
    axy.set_xlabel(&#39;SIFT: Y Error (pixels)&#39;)
    axy.text(0.05, 0.9, &#39;mean={:.3f}&#39;.format(np.mean(yshifts)), transform=axy.transAxes, fontsize=fsz)
    axy.text(0.05, 0.8, &#39;median={:.3f}&#39;.format(np.median(yshifts)), transform=axy.transAxes, fontsize=fsz)
    axt=axx  # print Transformation Matrix data over axx plot
    axt.text(0.65, 0.8, &#39;Transf. Matrix:&#39;, transform=axt.transAxes, fontsize=fsz)
    axt.text(0.55, 0.7, &#39;{:.4f} {:.4f} {:.4f}&#39;.format(transform_matrix[0,0], transform_matrix[0,1], transform_matrix[0,2]), transform=axt.transAxes, fontsize=fsz-1)
    axt.text(0.55, 0.6, &#39;{:.4f} {:.4f} {:.4f}&#39;.format(transform_matrix[1,0], transform_matrix[1,1], transform_matrix[1,2]), transform=axt.transAxes, fontsize=fsz-1)

    for ax in ravel(axs):
        ax.grid(True)

    fig.suptitle(Sample_ID + &#39;,  thr_min={:.0e}, thr_max={:.0e}, kp_max_num={:d}, comp.time={:.1f}sec&#39;.format(threshold_min, threshold_max, kp_max_num, comp_time), fontsize=fszl)

    if TransformType == RegularizedAffineTransform:
        tstr = [&#39;{:d}&#39;.format(x) for x in targ_vector]
        otext =  TransformType.__name__ + &#39;, λ= {:.1e}, t=[&#39;.format(l2_matrix[0,0]) + &#39;, &#39;.join(tstr) + &#39;], &#39; + solver + &#39;, #of matches={:d}&#39;.format(n_matches)
    else:
        otext = TransformType.__name__ + &#39;, &#39; + solver + &#39;, #of matches={:d}&#39;.format(n_matches)

    axs[0,0].text(0.01, 1.14, otext, fontsize=fszl, transform=axs[0,0].transAxes)
    if save_res_png :
        png_name = os.path.splitext(fs[0])[0] + &#39;_SIFT_eval_&#39;+TransformType.__name__ + &#39;_&#39; + solver +&#39;_thr_min{:.5f}_thr_max{:.5f}.png&#39;.format(threshold_min, threshold_max)
        fig.savefig(png_name, dpi=300)

    xfsz = np.int(7 * frame.XResolution / np.max([frame.XResolution, frame.YResolution]))+1
    yfsz = np.int(7 * frame.YResolution / np.max([frame.XResolution, frame.YResolution]))+2
    fig2, ax = subplots(1,1, figsize=(xfsz,yfsz))
    fig2.subplots_adjust(left=0.0, bottom=0.25*(1-frame.YResolution/frame.XResolution), right=1.0, top=1.0)
    symsize = 2
    fsize = 12
    img2 = FIBSEM_frame(fs[-1], ftype=ftype).RawImageA
    ax.imshow(img2, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
    ax.axis(False)
    x, y = dst_pts_filtered.T
    M = sqrt(xshifts*xshifts+yshifts*yshifts)
    xs = xshifts
    ys = yshifts

    # the code below is for vector map. vectors have origin coordinates x and y, and vector projections xs and ys.
    vec_field = ax.quiver(x,y,xs,ys,M, scale=40, width =0.003, cmap=&#39;jet&#39;)
    cbar = fig2.colorbar(vec_field, pad=0.05, shrink=0.70, orientation = &#39;horizontal&#39;, format=&#34;%.1f&#34;)
    cbar.set_label(&#39;SIFT Shift Amplitude (pix)&#39;, fontsize=fsize)

    ax.text(0.01, 1.1-0.13*frame.YResolution/frame.XResolution, Sample_ID + &#39;, thr_min={:.0e}, thr_max={:.0e}, kp_max_num={:d},  #of matches={:d}&#39;.format(threshold_min, threshold_max, kp_max_num, n_matches), fontsize=fsize, transform=ax.transAxes)

    if save_res_png :
        fig2_fnm = os.path.join(data_dir, &#39;SIFT_vmap_&#39;+TransformType.__name__ + &#39;_&#39; + solver +&#39;_thr_min{:.0e}_thr_max{:.0e}_kp_max{:d}.png&#39;.format(threshold_min, threshold_max, kp_max_num))
        fig2.savefig(fig2_fnm, dpi=300)

    return(dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts)


def save_inlens_data(fname):
    tfr = FIBSEM_frame(fname)
    tfr.save_images_tif(&#39;A&#39;)
    return fname


def transform_chunk_of_frames(frame_filenames, xsz, ysz, ftype,
                        flatten_image, image_correction_file,
                        perfrom_transformation, tr_matrices, shift_matrix, inv_shift_matrix,
                        xi, xa, yi, ya,
                        ImgB_fraction=0.0,
                        invert_data=False,
                        int_order=1,
                        flipY=False):
    &#39;&#39;&#39;
    Transform Chunk of Frames and average into a single transformed frames. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters
    frame_filenames : list of strings
        Filenames (Full paths) of FIB-SEM frame files for every frame in frame_inds
    xsz  :  int
        X-size (pixels)
    ysz  :  int
        Y-size (pixels)
    ftype : int
        File Type. 0 for Shan&#39;s .dat files, 1 for tif files
    flatten_image : bolean
        perform image flattening
    image_correction_file : str
        full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
    xi : int
        Low X-axis bound for placing the transformed frame into the image before transformation
    xa : int
        High X-axis bound for placing the transformed frame into the image before transformation
    yi : int
        Low Y-axis bound for placing the transformed frame into the image before transformation
    ya : int
        High Y-axis bound for placing the transformed frame into the image before transformation
    perfrom_transformation  : boolean
        perform transformation
    tr_matrices : list of 2D (or 3d array)
        Transformation matrix for every frame in frame_inds.
    shift_matrix : 2d array
        shift matrix
    inv_shift_matrix : 2d array
        inverse shift matrix.
    ImgB_fraction : float
        Fractional weight of Image B for fused images, default is 0
    invert_data : boolean
        Invert data, default is False.
    int_order : int
        Default is 1. Interpolation order (0: Nearest-neighbor, 1: Bi-linear (default), 2: Bi-quadratic, 3: Bi-cubic, 4: Bi-quartic, 5: Bi-quintic)
    flipY : boolean
        Flip output along Y-axis, default is False.

    Returns
    &#39;&#39;&#39;
    transformed_img = np.zeros((ysz, xsz), dtype=float)
    frame_img = np.zeros((ysz, xsz), dtype=float)
    num_frames = len(frame_filenames)
    for frame_filename, tr_matrix in zip(frame_filenames, tr_matrices):
        frame = FIBSEM_frame(frame_filename, ftype=ftype)

        if ImgB_fraction &lt; 1e-5:
            #image = frame.RawImageA.astype(float)
            if flatten_image:
                image = (frame.flatten_image(image_correction_file = image_correction_file)[0]).astype(float)
            else:
                image = frame.RawImageA.astype(float)
        else:
            if flatten_image:
                flattened_images = frame.flatten_image(image_correction_file = image_correction_file)
                flattened_RawImageA = flattened_images[0].astype(float)
                if len(flattened_images)&gt;1:
                    flattened_RawImageB = flattened_images[1].astype(float)
                else:
                    flattened_RawImageB = frame.RawImageB.astype(float)
                image = flattened_RawImageA* (1.0 - ImgB_fraction) + flattened_RawImageB * ImgB_fraction
            else:
                image = frame.RawImageA.astype(float) * (1.0 - ImgB_fraction) + frame.RawImageB.astype(float) * ImgB_fraction

        if invert_data:
            frame_img[yi:ya, xi:xa] = np.negative(image)
            &#39;&#39;&#39;
            if frame.EightBit==0:
                frame_img[yi:ya, xi:xa] = np.negative(image)
            else:
                frame_img[yi:ya, xi:xa]  =  uint8(255) - image
            &#39;&#39;&#39;
        else:
            frame_img[yi:ya, xi:xa]  = image

        if perfrom_transformation:
            transf = ProjectiveTransform(matrix = shift_matrix @ (tr_matrix @ inv_shift_matrix))
            frame_img_reg = warp(frame_img, transf, order = int_order,  preserve_range=True)
        else:
            frame_img_reg = frame_img.copy()

        transformed_img = transformed_img + frame_img_reg

    if num_frames &gt; 1:
        transformed_img = transformed_img/num_frames

    if flipY:
        transformed_img = np.flip(transformed_img, axis=0)
    &#39;&#39;&#39;
    if frame.EightBit==1:
        transformed_img = np.clip(np.round(transformed_img) , 0, 255)
    &#39;&#39;&#39;

    return transformed_img


def transform_and_save_chunk_of_frames(chunk_of_frame_parametrs):
    &#39;&#39;&#39;
    Transform Chunk of Frames and save into a single transformed frame. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters
    chunk_of_frame_parametrs : list of following parameters
        [save_filename, frame_filenames, tr_matrices, tr_args]

    save_filename : path
        Filename for saving the transformed frame
    frame_filenames : list of strings
        Filenames (Full paths) of FIB-SEM frame files for every frame in frame_inds
    tr_matrices : list of 2D (or 3d array)
        Transformation matrix for every frame in frame_inds.

    tr_args : list of lowwowing parameters:
        tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, flatten_image, image_correction_file, perfrom_transformation, shift_matrix, inv_shift_matrix, ftype, dtp]

    ImgB_fraction : float
        Fractional weight of Image B for fused images, default is 0
    xsz  :  int
        X-size (pixels)
    ysz  :  int
        Y-size (pixels)
    xi : int
        Low X-axis bound for placing the transformed frame into the image before transformation
    xa : int
        High X-axis bound for placing the transformed frame into the image before transformation
    yi : int
        Low Y-axis bound for placing the transformed frame into the image before transformation
    ya : int
        High Y-axis bound for placing the transformed frame into the image before transformation
    int_order : int
        Default is 1. Interpolation order (0: Nearest-neighbor, 1: Bi-linear (default), 2: Bi-quadratic, 3: Bi-cubic, 4: Bi-quartic, 5: Bi-quintic)
    invert_data : boolean
        Invert data, default is False.
    flipY : boolean
        Flip output along Y-axis, default is False.
    flatten_image : bolean
        perform image flattening
    image_correction_file : str
        full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
    perfrom_transformation  : boolean
        perform transformation
    shift_matrix : 2d array
        shift matrix
    inv_shift_matrix : 2d array
        inverse shift matrix.
    ftype : int
        File Type. 0 for Shan&#39;s .dat files, 1 for tif files
    dtp : data type
        Python data type for saving. Deafult is int16, the other option currently is uint8.

    Returns
    &#39;&#39;&#39;
    save_filename, frame_filenames, tr_matrices, tr_args = chunk_of_frame_parametrs
    ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, flatten_image, image_correction_file, perfrom_transformation, shift_matrix, inv_shift_matrix, ftype, dtp = tr_args
    num_frames = len(frame_filenames)
    transformed_img = np.zeros((ysz, xsz), dtype=float)
    frame_img = np.zeros((ysz, xsz), dtype=float)

    for frame_filename, tr_matrix in zip(frame_filenames, tr_matrices):
        frame = FIBSEM_frame(frame_filename, ftype=ftype)

        if ImgB_fraction &lt; 1e-5:
            #image = frame.RawImageA.astype(float)
            if flatten_image:
                image = (frame.flatten_image(image_correction_file = image_correction_file)[0]).astype(float)
            else:
                image = frame.RawImageA.astype(float)
        else:
            if flatten_image:
                flattened_images = frame.flatten_image(image_correction_file = image_correction_file)
                flattened_RawImageA = flattened_images[0].astype(float)
                if len(flattened_images)&gt;1:
                    flattened_RawImageB = flattened_images[1].astype(float)
                else:
                    flattened_RawImageB = frame.RawImageB.astype(float)
                image = flattened_RawImageA* (1.0 - ImgB_fraction) + flattened_RawImageB * ImgB_fraction
            else:
                image = frame.RawImageA.astype(float) * (1.0 - ImgB_fraction) + frame.RawImageB.astype(float) * ImgB_fraction

        if invert_data:
            frame_img[yi:ya, xi:xa] = np.negative(image)
            &#39;&#39;&#39;
            if frame.EightBit==0:
                frame_img[yi:ya, xi:xa] = np.negative(image)
            else:
                frame_img[yi:ya, xi:xa]  =  uint8(255) - image
            &#39;&#39;&#39;
        else:
            frame_img[yi:ya, xi:xa]  = image

        if perfrom_transformation:
            transf = ProjectiveTransform(matrix = shift_matrix @ (tr_matrix @ inv_shift_matrix))
            frame_img_reg = warp(frame_img, transf, order = int_order,  preserve_range=True)
        else:
            frame_img_reg = frame_img.copy()

        transformed_img = transformed_img + frame_img_reg

    if num_frames &gt; 1:
        transformed_img = transformed_img/num_frames

    if flipY:
        transformed_img = np.flip(transformed_img, axis=0)

    tiff.imsave(save_filename, transformed_img.astype(dtp))

    return save_filename


def transform_two_chunks(params):
    &#39;&#39;&#39;
    Transforms two chunks of EM frames and evaluates registration

    Parameters:
    params: list
    params = [chunk0_frames, chunk1_frames, fls, tr_matr_cum_residual, shift_matrix, inv_shift_matrix, xi_eval, xa_eval, yi_eval, ya_eval, save_frame_png, frame_number, filename_frame_png, tr_args]
        chunk0_frames : array of int
            indecis of the frames in the first chunk
        chunk1_frames : array of int
            indecis of the frames in the second chunk
        fls : array of str
            Filenames (full paths) of the ioriginal data frames
        tr_matr_cum_residual : 3d array
            Transformation matrix
        shift_matrix : 2d array
            shift matrix
        inv_shift_matrix : 2d array
            inverse shift matrix
    tr_args : list
        tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, zbin_factor, flatten_image, image_correction_file, perfrom_transformation, ftype]

    Returns:
    registration_stat : list
        registration
    &#39;&#39;&#39;
    # first, unpack the parameters
    chunk0_filenames, chunk1_filenames, chunk0_tr_matrices, chunk1_tr_matrices, shift_matrix, inv_shift_matrix, xi_eval, xa_eval, yi_eval, ya_eval, save_frame_png, frame_number, filename_frame_png, tr_args = params
    ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, zbin_factor, flatten_image, image_correction_file, perfrom_transformation, ftype = tr_args
    binned_fr_img0 = transform_chunk_of_frames(chunk0_filenames, xsz, ysz, ftype,
                                flatten_image, image_correction_file,
                                perfrom_transformation, chunk0_tr_matrices, shift_matrix, inv_shift_matrix,
                                xi, xa, yi, ya,
                                ImgB_fraction = ImgB_fraction,
                                invert_data = invert_data,
                                int_order = int_order,
                                flipY=flipY)
    binned_fr_img1 = transform_chunk_of_frames(chunk1_filenames, xsz, ysz, ftype,
                                flatten_image, image_correction_file,
                                perfrom_transformation, chunk1_tr_matrices, shift_matrix, inv_shift_matrix,
                                xi, xa, yi, ya,
                                ImgB_fraction = ImgB_fraction,
                                invert_data = invert_data,
                                int_order = int_order,
                                flipY=flipY)

    I1 = binned_fr_img0[yi_eval:ya_eval, xi_eval:xa_eval]
    I2 = binned_fr_img1[yi_eval:ya_eval, xi_eval:xa_eval]
    fr_mean = np.abs(I1/2.0 + I2/2.0)
    image_nsad =  np.mean(np.abs(I1-I2))/(np.mean(fr_mean)-np.amin(fr_mean))
    image_ncc = Two_Image_NCC_SNR(I1, I2)[0]
    image_mi = mutual_information_2d(I1.ravel(), I2.ravel(), sigma=1.0, bin=2048, normalized=True)
    if save_frame_png:
        yshape, xshape = binned_fr_img0.shape
        fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
        fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
        dmin, dmax = get_min_max_thresholds(I1)
        ax.imshow(binned_fr_img0, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.text(0.06, 0.95, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(frame_number, image_nsad, image_ncc, image_mi), color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
        rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        ax.axis(&#39;off&#39;)
        fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
        plt.close(fig)
    return [image_nsad, image_ncc, image_mi]


def analyze_registration_frames(DASK_client, frame_filenames, **kwargs):
    &#39;&#39;&#39;
    Transform and save FIB-SEM data set. A new vesion, with variable zbin_factor option. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters
    DASK_client :
    frame_filenames : list of strings
        List of filenames (one for each transformed / z-binned frame)

    kwargs
    ---------
    use_DASK : boolean
        perform remote DASK computations
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    save_registration_summary : boolean
        If True (default), the rgistration analysis will be saved
    data_dir : str
        data directory (path)
    frame_inds : int array
        Array of frame indecis. If not set or set to np.array((-1)), all frames will be analyzed
    fnm_reg : str
        filename for the final registed dataset
    npts : array or list of int
        Numbers of Keypoints used for registration
    error_abs_mean : array or list of float
        mean abs error between registered key-points
    eval_bounds : list of [xi_eval, xa_eval, yi_eval, ya_eval] lists of int
        Evaluation boundaries for analysis
    eval_metrics : list of str
        list of evaluation metrics to use. default is [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;]
    save_sample_frames_png : bolean
        If True, sample frames with superimposed eval box and registration analysis data will be saved into png files
    sample_frame_inds : list of int
        list of sample frame indecis
    save_registration_summary : boolean
        If True, the registration summary is saved into XLSX file
    disp_res : bolean
        If True (default), intermediate messages and results will be displayed.

    Returns:
    reg_summary, reg_summary_xlsx
        reg_summary : pandas DataFrame
        reg_summary = pd.DataFrame(np.vstack((npts, error_abs_mean, image_nsad, image_ncc, image_mi)
        reg_summary_xlsx : name of the XLSX workbook containing the data
    &#39;&#39;&#39;

    use_DASK = kwargs.get(&#34;use_DASK&#34;, True)  # do not use DASK the data is to be saved
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)

    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    fpath_reg = os.path.join(data_dir, fnm_reg)

    save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, True)
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    save_registration_summary = kwargs.get(&#39;save_registration_summary&#39;, True)
    dump_filename = kwargs.get(&#39;dump_filename&#39;, &#39;&#39;)

    frame_inds_default = np.arange(len(frame_filenames))
    frame_inds = np.array(kwargs.get(&#34;frame_inds&#34;, frame_inds_default))
    nfrs = len(frame_inds)                                                   # number of source images(frames) before z-binning
    sample_frame_inds = kwargs.get(&#34;sample_frame_inds&#34;, [frame_inds[nfrs//10], frame_inds[nfrs//2], frame_inds[nfrs//10*9-1]])
    npts = kwargs.get(&#34;npts&#34;, False)
    error_abs_mean = kwargs.get(&#34;error_abs_mean&#34;, False)

    first_frame = tiff.imread(frame_filenames[frame_inds[0]])
    ya, xa = first_frame.shape
    eval_bounds = kwargs.get(&#34;eval_bounds&#34;, [[0, xa, 0, ya]]*nfrs)
    eval_metrics = kwargs.get(&#39;eval_metrics&#39;, [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;])

    params_frames = []
    for j, frame_ind in enumerate(tqdm(frame_inds[0:-1], desc=&#39;Setting up parameter sets&#39;, display=disp_res)):
        params_frames.append([frame_filenames[frame_ind], frame_filenames[frame_ind+1], eval_bounds[j], eval_metrics])

    if use_DASK:
        if disp_res:
            print(&#39;Will perform distributed computations using DASK&#39;)
        if disp_res:
            print(&#39;Starting DASK jobs&#39;)
        futures_til = DASK_client.map(Two_Image_Analysis, params_frames, retries = DASK_client_retries)
        results_til = DASK_client.gather(futures_til)
        image_metrics = np.array(results_til)  # 2D array  np.array([[image_nsad, image_ncc, image_mi]])
        if disp_res:
            print(&#39;Finished DASK jobs&#39;)

    else:   # if DASK is not used - perform local computations
        if disp_res:
            print(&#39;Will perform local computations&#39;)
        image_metrics = np.zeros((nfrs-1, len(eval_metrics)), dtype=float)
        for j, params_frame in enumerate(tqdm(params_frames, desc = &#39;Analyzing frame pairs&#39;, display = disp_res)):
            image_metrics[j, :] = Two_Image_Analysis(params_frame)
        results_til = []

    # save sample frames
    if save_sample_frames_png:
        for frame_ind in sample_frame_inds:
            filename_frame_png = os.path.splitext(fpath_reg)[0]+&#39;_sample_image_frame{:d}.png&#39;.format(frame_ind)
            fr_img = tiff.imread(frame_filenames[frame_ind]).astype(float)
            yshape, xshape = fr_img.shape
            fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
            fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
            xi_eval, xa_eval, yi_eval, ya_eval = eval_bounds[frame_ind]
            dmin, dmax = get_min_max_thresholds(fr_img[yi_eval:ya_eval, xi_eval:xa_eval], disp_res=False)
            ax.imshow(fr_img, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
            sample_text = &#39;Frame={:d}&#39;.format(frame_ind)
            for k, metric in enumerate(eval_metrics):
                sample_text = sample_text + &#39;,  &#39;+ metric + &#39;={:.3f}&#39;.format(image_metrics[frame_ind, k])
            ax.text(0.06, 0.95, sample_text, color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
            rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
            ax.add_patch(rect_patch)
            ax.axis(&#39;off&#39;)
            fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
            plt.close(fig)

    columns = [&#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;] + eval_metrics
    reg_summary = pd.DataFrame(np.vstack((frame_inds[1:].T, np.array(eval_bounds)[frame_inds[1:], :].T, np.array(image_metrics).T)).T, columns = columns, index = None)
    if npts:
        reg_summary[&#39;Npts&#39;] = npts
        columns = columns + [&#39;Npts&#39;]
    if error_abs_mean:
        reg_summary[&#39;Mean Abs Error&#39;] = error_abs_mean
        columns = columns + [&#39;Mean Abs Error&#39;]

    if save_registration_summary:
        registration_summary_xlsx = fpath_reg.replace(&#39;.mrc&#39;, &#39;_RegistrationQuality.xlsx&#39;)
        if disp_res:
            print(&#39;Saving the Registration Quality Statistics into the file: &#39;, registration_summary_xlsx)
        # Create a Pandas Excel writer using XlsxWriter as the engine.
        xlsx_writer = pd.ExcelWriter(registration_summary_xlsx, engine=&#39;xlsxwriter&#39;)
        reg_summary.to_excel(xlsx_writer, index=None, sheet_name=&#39;Registration Quality Statistics&#39;)
        Stack_info = pd.DataFrame([{&#39;Stack Filename&#39; : fnm_reg, &#39;dump_filename&#39; : dump_filename}]).T # prepare to be save in transposed format
        try:
            del kwargs[&#39;eval_bounds&#39;]
        except:
            pass
        SIFT_info = pd.DataFrame([kwargs]).T   # prepare to be save in transposed format
        Stack_info = Stack_info.append(SIFT_info)
        Stack_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;Stack Info&#39;)
        xlsx_writer.save()
    else:
        registration_summary_xlsx = &#39;Registration data not saved&#39;

    return reg_summary, registration_summary_xlsx


def transform_and_save_frames(DASK_client, frame_inds, fls, tr_matr_cum_residual, **kwargs):
    &#39;&#39;&#39;frank power supply
    Transform and save FIB-SEM data set. A new vesion, with variable zbin_factor option. ©G.Shtengel 01/2023 gleb.shtengel@gmail.com

    Parameters
    DASK_client : DASK client
    frame_inds : int array
        Array of frame indecis. If not set or set to np.array((-1)), all frames will be transformed
    fls : array of strings
        full array of filenames
    tr_matr_cum_residual : array
        transformation matrix

    kwargs
    ---------
    use_DASK : boolean
        perform remote DASK computations
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    data_dir : str
        data directory (path)
    ImgB_fraction : float
        fractional ratio of Image B to be used for constructing the fuksed image:
        ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.
    flipY : boolean
        If True, the data will be flipped along Y-axis. Default is False.
    zbin_factor : int
        binning factor along Z-axis
    perfrom_transformation : boolean
        If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed.
    int_order : int
        The order of interpolation. 1: Bi-linear
    flatten_image : bolean
        perform image flattening
    image_correction_file : str
        full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
    invert_data : boolean
        If True - the data is inverted.
    dtp  : dtype
        Python data type for saving. Deafult is int16, the other option currently is uint8.
    disp_res : bolean
        Default is False

    Returns:
    registered_filenames : list of filenames (one for each transformed / z-binned frame)

    &#39;&#39;&#39;
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    test_frame = FIBSEM_frame(fls[0], ftype=ftype)

    save_transformed_dataset = kwargs.get(&#34;save_transformed_dataset&#34;, True)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)  # do not use DASK the data is to be saved
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    dump_filename = kwargs.get(&#39;dump_filename&#39;, &#39;&#39;)
    ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, 0.0)         # fusion fraction. In case if Img B is present, the fused image
                                                            # for each frame will be constructed ImgF = (1.0-ImgB_fraction)*ImgA + ImgB_fraction*ImgB
    if test_frame.DetB == &#39;None&#39;:
        ImgB_fraction=0.0

    XResolution = kwargs.get(&#34;XResolution&#34;, test_frame.XResolution)
    YResolution = kwargs.get(&#34;YResolution&#34;, test_frame.YResolution)
    pad_edges =  kwargs.get(&#34;pad_edges&#34;, True)
    flipY = kwargs.get(&#34;flipY&#34;, False)
    zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, 1)
    perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True)
    int_order = kwargs.get(&#34;int_order&#34;, 1)                  # The order of interpolation. 1: Bi-linear
    flatten_image = kwargs.get(&#34;flatten_image&#34;, False)
    image_correction_file = kwargs.get(&#34;image_correction_file&#34;, &#39;&#39;)
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    dtp = kwargs.get(&#34;dtp&#34;, int16)  # Python data type for saving. Deafult is int16, the other option currently is uint8.
    disp_res = kwargs.get(&#34;disp_res&#34;, False)
    nfrs = len(frame_inds)                                                   # number of source images(frames) before z-binning
    end_frame = ((frame_inds[0]+len(frame_inds)-1)//zbin_factor+1)*zbin_factor
    st_frames = np.arange(frame_inds[0], end_frame, zbin_factor)             # starting frame for each z-bin
    nfrs_zbinned = len(st_frames)                                            # number of frames after z-ninning

    frames_new = np.arange(nfrs_zbinned-1)

    if pad_edges and perfrom_transformation:
        shape = [YResolution, XResolution]
        xmn, xmx, ymn, ymx = determine_pad_offsets(shape, tr_matr_cum_residual)
        padx = int(xmx - xmn)
        pady = int(ymx - ymn)
        xi = int(np.max([xmx, 0]))
        yi = int(np.max([ymx, 0]))
        # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
        # so that the transformed images are not clipped.
        # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
        # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
        # those are calculated below base on the amount of padding calculated above
        shift_matrix = np.array([[1.0, 0.0, xi],
                                 [0.0, 1.0, yi],
                                 [0.0, 0.0, 1.0]])
        inv_shift_matrix = np.linalg.inv(shift_matrix)
    else:
        padx = 0
        pady = 0
        xi = 0
        yi = 0
        shift_matrix = np.eye(3,3)
        inv_shift_matrix = np.eye(3,3)

    fpath_reg = os.path.join(data_dir, fnm_reg)
    xsz = XResolution + padx
    xa = xi + XResolution
    ysz = YResolution + pady
    ya = yi + YResolution

    &#39;&#39;&#39;
    transform_and_save_chunk_of_frames(save_filename, frame_filenames, tr_matrices, tr_args):
    chunk_of_frame_parametrs = save_filename, frame_filenames, tr_matrices_cum_residual, tr_args
    tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, flatten_image, image_correction_file, perfrom_transformation, shift_matrix, inv_shift_matrix, ftype, dtp]
    process_frames = np.arange(st_frame, min(st_frame+zbin_factor, (frame_inds[-1]+1)))
    chunk_of_frame_parametrs_dataset.append([save_filename, process_frames, np.array(tr_matr_cum_residual)[process_frames], tr_args])

    &#39;&#39;&#39;
    tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, flatten_image, image_correction_file, perfrom_transformation, shift_matrix, inv_shift_matrix, ftype, dtp]
    chunk_of_frame_parametrs_dataset = []
    for j, st_frame in enumerate(tqdm(st_frames, desc=&#39;Setting up DASK parameter sets&#39;, display=disp_res)):
        save_filename = os.path.join(os.path.split(fls[st_frame])[0],&#39;Registered_Frame_{:d}.tif&#39;.format(j))
        process_frames = np.arange(st_frame, min(st_frame+zbin_factor, (frame_inds[-1]+1)))
        chunk_of_frame_parametrs_dataset.append([save_filename, np.array(fls)[process_frames], np.array(tr_matr_cum_residual)[process_frames], tr_args])

    if use_DASK:
        if disp_res:
            print(&#39;Starting DASK jobs&#39;)
        futures_td = DASK_client.map(transform_and_save_chunk_of_frames, chunk_of_frame_parametrs_dataset, retries = DASK_client_retries)
        registered_filenames = np.array(DASK_client.gather(futures_td))
        if disp_res:
            print(&#39;Finished DASK jobs&#39;)
    else:   # if DASK is not used - perform local computations
        if disp_res:
            print(&#39;Will perform local computations&#39;)
        registered_filenames = []
        for chunk_of_frame_parametrs in tqdm(chunk_of_frame_parametrs_dataset, desc = &#39;Transforming and saving frame chunks&#39;, display = disp_res):
            registered_filenames.append(transform_and_save_chunk_of_frames(chunk_of_frame_parametrs))

    return registered_filenames


def save_data_stack(FIBSEMstack, **kwargs):
    &#39;&#39;&#39;
    Saves the dataset into a file.

    Parameters
        FIBSEMstack : 3D array (may be DASK array)
            Data set to be saved

    kwargs
    ---------
        data_dir : str
            data directory for saving the data
        fnm_reg : str
            filename for the final registed dataset
        fnm_types : list of strings
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is &#39;mrc&#39;. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        voxel_size : rec array of 3 elemets
            voxel size in nm
        dtp  : dtype
            Python data type for saving. Deafult is int16, the other option currently is uint8.
        disp_res : bolean
            Display messages and intermediate results

    Returns:
        fnms_saved : list of strings
            Paths to the files where the data set was saved.

    &#39;&#39;&#39;
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registered_set.mrc&#39;)
    fpath_reg = os.path.join(data_dir, fnm_reg)
    fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
    voxel_size_default = np.rec.array((8.0, 8.0, 8.0), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
    voxel_size = kwargs.get(&#34;voxel_size&#34;, voxel_size_default)
    dtp = kwargs.get(&#34;dtp&#34;, int16)
    disp_res  = kwargs.get(&#34;disp_res&#34;, False )
    nz, ny, nx = FIBSEMstack.shape
    if disp_res:
        print(&#39;The resulting stack shape will be  nx={:d}, ny={:d}, nz={:d},  data type:&#39;.format(nx, ny, nz), dtp)
        print(&#39;Voxel Size (nm): {:2f} x {:2f} x {:2f}&#39;.format(voxel_size.x, voxel_size.y, voxel_size.z))

    fnms_saved = []
    if len(fnm_types)&gt;0:
        for fnm_type in fnm_types:
            # save dataset at HDF5 file
            if fnm_type == &#39;h5&#39;:
                fpath_reg_h5 = fpath_reg.replace(&#39;.mrc&#39;, &#39;.h5&#39;)
                try:
                    os.remove(fpath_reg_h5)
                except:
                    pass
                fnms_saved.append(fpath_reg_h5)
                if disp_res:
                    print(&#39;Saving dataset into Big Data Viewer HDF5 file: &#39;, fpath_reg_h5)
                bdv_writer = npy2bdv.BdvWriter(fpath_reg_h5, nchannels=1, blockdim=((1, 256, 256),))
                bdv_writer.append_view(stack=FIBSEMstack,
                       virtual_stack_dim=(nz,ny,nx),
                       time=0, channel=0,
                       voxel_size_xyz=(voxel_size.x, voxel_size.y, voxel_size.z),
                       voxel_units=&#39;nm&#39;)
                bdv_writer.write_xml()
                bdv_writer.close()
            if fnm_type == &#39;mrc&#39;:
                if disp_res:
                    print(&#39;Saving dataset into MRC file: &#39;, fpath_reg)
                fnms_saved.append(fpath_reg)
                &#39;&#39;&#39;
                mode 0 -&gt; uint8
                mode 1 -&gt; int16
                &#39;&#39;&#39;
                if dtp==int16:
                    mrc_mode = 1
                else:
                    mrc_mode = 0
                # Make a new, empty memory-mapped MRC file
                mrc = mrcfile.new_mmap(fpath_reg, shape=(nz, ny, nx), mrc_mode=mrc_mode, overwrite=True)
                voxel_size_angstr = voxel_size.copy()
                voxel_size_angstr.x = voxel_size_angstr.x * 10.0
                voxel_size_angstr.y = voxel_size_angstr.y * 10.0
                voxel_size_angstr.z = voxel_size_angstr.z * 10.0
                #mrc.header.cella = voxel_size_angstr
                mrc.voxel_size = voxel_size_angstr
                for j, FIBSEMframe in enumerate(tqdm(FIBSEMstack, desc = &#39;Saving Frames into MRC File: &#39;, display = disp_res)):
                    mrc.data[j,:,:] = FIBSEMframe.astype(dtp)
                mrc.close()
    else:
        print(&#39;Registered data set is NOT saved into a file&#39;)
    return fnms_saved


def check_for_nomatch_frames_dataset(fls, fnms, fnms_matches,
                                     transformation_matrix,
                                     error_abs_mean, npts,
                                     thr_npt, **kwargs):
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)

    inds_zeros = np.squeeze(np.argwhere(npts &lt; thr_npt ))
    print(&#39;Frames with no matches to the next frame:  &#39;, np.array(inds_zeros))
    frames_to_remove = []
    if shape(inds_zeros)!=():
        for ind0 in inds_zeros:
            if ind0 &lt; (len(fls)-2) and npts[ind0+1] &lt; thr_npt:
                frames_to_remove.append(ind0+1)
                print(&#39;Frame to remove: {:d} : &#39;.format(ind0+1) + &#39;, File: &#39; + fls[ind0+1])
                frame_to_remove  = FIBSEM_frame(fls[ind0+1], ftype=ftype)
                frame_to_remove.save_snapshot(dpi=300)
        print(&#39;Frames to remove:  &#39;, frames_to_remove)

    if len(frames_to_remove) == 0:
        print(&#39;No frames selected for removal&#39;)
    else:
        # create copies of the original arrays
        fnms_orig = fnms.copy()
        fls_orig = fls.copy()
        error_abs_mean_orig = error_abs_mean.copy()
        tr_matrix_orig = transformation_matrix.copy()

        # go through the frames to be removed and remove the frames from the list and then re-calculate the shift for new neighbours.
        for j,fr in enumerate(tqdm(frames_to_remove, desc = &#39;Removing frames and finding shifts for new sequential frames&#39;)):
            frj = fr-j # to account for the fact that every time we remove a frame the array shrinks and indicis reset
            print(&#39;Removing the frame {:d}&#39;.format(frj))
            print(fls[frj])
            fls = np.delete(fls, frj)
            fnms = np.delete(fnms, frj)
            fnms_matches = np.delete(fnms_matches, frj)
            error_abs_mean = np.delete(error_abs_mean, frj)
            transformation_matrix = np.delete(transformation_matrix, frj, axis = 0)
            npts = np.delete(npts, frj, axis = 0)
            fname1 = fnms[frj-1]
            fname2 = fnms[frj]
            new_step4_res = determine_transformations_files([fname1, fname2, kwargs])
            npts[frj-1] = np.array(len(new_step4_res[2][0]))
            error_abs_mean[frj-1] = new_step4_res[3]
            transformation_matrix[frj-1] = np.array(new_step4_res[0])
        print(&#39;Mean Number of Keypoints :&#39;, np.mean(npts).astype(int))
    return frames_to_remove, fls, fnms, fnms_matches, error_abs_mean, npts, transformation_matrix



class FIBSEM_dataset:
    &#34;&#34;&#34;
    A class representing a FIB-SEM data set
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com
    Contains the info/settings on the FIB-SEM dataset and the procedures that can be performed on it.

    Attributes
    ----------
    fls : array of str
        filenames for the individual data frames in the set
    data_dir : str
        data directory (path)
    Sample_ID : str
            Sample ID
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    PixelSize : float
        pixel size in nm. This is inherited from FIBSEM_frame object. Default is 8.0
    voxel_size : rec.array(( float,  float,  float), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
        voxel size in nm. Default is isotropic (PixelSize, PixelSize, PixelSize)
    Scaling : 2D array of floats
        scaling parameters allowing to convert I16 data into actual electron counts
    fnm_reg : str
        filename for the final registed dataset
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    sliding_minmax : boolean
        if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
        if False - same data_min_glob and data_max_glob will be used for all files
    TransformType : object reference
        Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
        Choose from the following options:
            ShiftTransform - only x-shift and y-shift
            XScaleShiftTransform  -  x-scale, x-shift, y-shift
            ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
            AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
            RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
    l2_matrix : 2D float array
        matrix of regularization (shrinkage) parameters
    targ_vector = 1D float array
        target vector for regularization
    solver : str
        Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
    drmax : float
        In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
        In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
    max_iter : int
        Max number of iterations in the iterative procedure above (RANSAC or LinReg)
    BFMatcher : boolean
        If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches : boolean
        If True, matches will be saved into individual files
    kp_max_num : int
        Max number of key-points to be matched.
        Key-points in every frame are indexed (in descending order) by the strength of the response.
        Only kp_max_num is kept for further processing.
        Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
    SIFT_nfeatures : int
        SIFT libary default is 0. The number of best features to retain.
        The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
    SIFT_nOctaveLayers : int
        SIFT libary default  is 3. The number of layers in each octave.
        3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
    SIFT_contrastThreshold : double
        SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
        The larger the threshold, the less features are produced by the detector.
        The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
        When nOctaveLayers is set to default and if you want to use the value used in
        D. Lowe paper (0.03), set this argument to 0.09.
    SIFT_edgeThreshold : double
        SIFT libary default  is 10. The threshold used to filter out edge-like features.
        Note that the its meaning is different from the contrastThreshold,
        i.e. the larger the edgeThreshold, the less features are filtered out
        (more features are retained).
    SIFT_sigma : double
        SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
        If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    dtp : Data Type
        Python data type for saving. Deafult is int16, the other option currently is uint8.
    zbin_factor : int
        binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
    flipY : boolean
        If True, the data will be flipped along Y-axis. Default is False.
    preserve_scales : boolean
        If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
    fit_params : list
        Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
        Other options are:
            [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
            [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
    int_order : int
        The order of interpolation (when transforming the data).
            The order has to be in the range 0-5:
                0: Nearest-neighbor
                1: Bi-linear (default)
                2: Bi-quadratic
                3: Bi-cubic
                4: Bi-quartic
                5: Bi-quintic
    subtract_linear_fit : [boolean, boolean]
        List of two Boolean values for two directions: X- and Y-.
        If True, the linear slopes along X- and Y- directions (respectively)
        will be subtracted from the cumulative shifts.
        This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.
    ImgB_fraction : float
            fractional ratio of Image B to be used for constructing the fuksed image:
            ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction
    evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.

    Methods
    -------
    SIFT_evaluation(eval_fls = [], **kwargs):
        Evaluate SIFT settings and perfromance of few test frames (eval_fls).

    convert_raw_data_to_tif_files(DASK_client = &#39;&#39;, **kwargs):
        Convert binary &#34;.dat&#34; files into &#34;.tif&#34; files

    evaluate_FIBSEM_statistics(self, DASK_client, **kwargs):
        Evaluates parameters of FIBSEM data set (data Min/Max, Working Distance, Milling Y Voltage, FOV center positions).

    extract_keypoints(DASK_client, **kwargs):
        Extract Key-Points and Descriptors

    determine_transformations(DASK_client, **kwargs):
        Determine transformation matrices for sequential frame pairs

    process_transformation_matrix(**kwargs):
        Calculate cumulative transformation matrix

    save_parameters(**kwargs):
        Save transformation attributes and parameters (including transformation matrices)

    check_for_nomatch_frames(thr_npt, **kwargs):
        Check for frames with low number of Key-Point matches,m exclude them and re-calculate the cumulative transformation matrix

    transform_and_save(DASK_client, **kwargs):
        Transform the frames using the cumulative transformation matrix and save the data set into .mrc file

    show_eval_box(**kwargs):
        Show the box used for evaluating the registration quality

    estimate_SNRs(**kwargs):
        Estimate SNRs in Image A and Image B based on single-image SNR calculation.

    evaluate_ImgB_fractions(ImgB_fractions, frame_inds, **kwargs):
        Calculate NCC and SNR vs Image B fraction over a set of frames.
    &#34;&#34;&#34;

    def __init__(self, fls, **kwargs):
        &#34;&#34;&#34;
        Initializes an instance of  FIBSEM_dataset object. ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

        Parameters
        ----------
        fls : array of str
            filenames for the individual data frames in the set
        data_dir : str
            data directory (path)

        kwargs
        ---------
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        Sample_ID : str
                Sample ID
        PixelSize : float
            pixel size in nm. Default is 8.0
        Scaling : 2D array of floats
            scaling parameters allowing to convert I16 data into actual electron counts
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        sliding_minmax : boolean
            if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
            if False - same data_min_glob and data_max_glob will be used for all files
        TransformType : object reference
            Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
            Choose from the following options:
                ShiftTransform - only x-shift and y-shift
                XScaleShiftTransform  -  x-scale, x-shift, y-shift
                ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        kp_max_num : int
            Max number of key-points to be matched.
            Key-points in every frame are indexed (in descending order) by the strength of the response.
            Only kp_max_num is kept for further processing.
            Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
        SIFT_nfeatures : int
            SIFT libary default is 0. The number of best features to retain.
            The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
        SIFT_nOctaveLayers : int
            SIFT libary default  is 3. The number of layers in each octave.
            3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
        SIFT_contrastThreshold : double
            SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
            The larger the threshold, the less features are produced by the detector.
            The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
            When nOctaveLayers is set to default and if you want to use the value used in
            D. Lowe paper (0.03), set this argument to 0.09.
        SIFT_edgeThreshold : double
            SIFT libary default  is 10. The threshold used to filter out edge-like features.
            Note that the its meaning is different from the contrastThreshold,
            i.e. the larger the edgeThreshold, the less features are filtered out
            (more features are retained).
        SIFT_sigma : double
            SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
            If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        dtp : Data Type
            Python data type for saving. Deafult is int16, the other option currently is uint8.
        zbin_factor : int
            binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
        preserve_scales : boolean
            If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
        fit_params : list
            Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
            Other options are:
                [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
                [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
        int_order : int
            The order of interpolation (when transforming the data).
                The order has to be in the range 0-5:
                    0: Nearest-neighbor
                    1: Bi-linear (default)
                    2: Bi-quadratic
                    3: Bi-cubic
                    4: Bi-quartic
                    5: Bi-quintic
        subtract_linear_fit : [boolean, boolean]
            List of two Boolean values for two directions: X- and Y-.
            If True, the linear slopes along X- and Y- directions (respectively)
            will be subtracted from the cumulative shifts.
            This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.
        disp_res : boolean
            If False, the intermediate printouts will be suppressed
        &#34;&#34;&#34;

        disp_res = kwargs.get(&#39;disp_res&#39;, True)
        self.fls = fls
        self.fnms = [os.path.splitext(fl)[0] + &#39;_kpdes.bin&#39; for fl in fls]
        self.nfrs = len(fls)
        if disp_res:
            print(&#39;Total Number of frames: &#39;, self.nfrs)
        self.data_dir = kwargs.get(&#39;data_dir&#39;, os.getcwd())
        self.ftype = kwargs.get(&#34;ftype&#34;, 0) # ftype=0 - Shan Xu&#39;s binary format  ftype=1 - tif files
        mid_frame = FIBSEM_frame(fls[self.nfrs//2], ftype = self.ftype)
        self.XResolution = kwargs.get(&#34;XResolution&#34;, mid_frame.XResolution)
        self.YResolution = kwargs.get(&#34;YResolution&#34;, mid_frame.YResolution)
        self.Scaling = kwargs.get(&#34;Scaling&#34;, mid_frame.Scaling)
        if hasattr(mid_frame, &#39;PixelSize&#39;):
            self.PixelSize = kwargs.get(&#34;PixelSize&#34;, mid_frame.PixelSize)
        else:
            self.PixelSize = kwargs.get(&#34;PixelSize&#34;, 8.0)
        self.voxel_size = np.rec.array((self.PixelSize,  self.PixelSize,  self.PixelSize), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
        if hasattr(self, &#39;YResolution&#39;):
            YResolution_default = self.YResolution
        else:
            YResolution_default = FIBSEM_frame(self.fls[len(self.fls)//2]).YResolution
        YResolution = kwargs.get(&#34;YResolution&#34;, YResolution_default)

        test_frame = FIBSEM_frame(fls[0], ftype=self.ftype)
        self.DetA = test_frame.DetA
        self.DetB = test_frame.DetB
        self.ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, 0.0)
        if self.DetB == &#39;None&#39;:
            ImgB_fraction = 0.0
        self.Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
        self.EightBit = kwargs.get(&#34;EightBit&#34;, 1)
        self.use_DASK = kwargs.get(&#34;use_DASK&#34;, True)
        self.DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        self.threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
        self.threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
        self.nbins = kwargs.get(&#34;nbins&#34;, 256)
        self.sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, True)
        self.TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
        self.tr_matr_cum_residual = [np.eye(3,3) for i in np.arange(self.nfrs)]  # placeholder - identity transformation matrix
        l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
        l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
        l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
        l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
        self.l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
        self.targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
        self.solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
        self.drmax = kwargs.get(&#34;drmax&#34;, 2.0)
        self.max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
        self.BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        self.save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
        self.kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
        self.SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, 0)
        self.SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, 3)
        self.SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.04)
        self.SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, 10)
        self.SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, 1.6)
        self.save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)
        self.zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, 1)         # binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
        self.eval_metrics = kwargs.get(&#39;eval_metrics&#39;, [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;])
        self.fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
        self.flipY = kwargs.get(&#34;flipY&#34;, False)                     # If True, the registered data will be flipped along Y axis
        self.preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, True) # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
        self.fit_params =  kwargs.get(&#34;fit_params&#34;, False)          # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                                    # window size 701, polynomial order 3

        self.int_order = kwargs.get(&#34;int_order&#34;, False)             #     The order of interpolation. The order has to be in the range 0-5:
                                                                    #    - 0: Nearest-neighbor
                                                                    #    - 1: Bi-linear (default)
                                                                    #    - 2: Bi-quadratic
                                                                    #    - 3: Bi-cubic
                                                                    #    - 4: Bi-quartic
                                                                    #    - 5: Bi-quintic
        self.subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # If True, the linear slope will be subtracted from the cumulative shifts.
        self.subtract_FOVtrend_from_fit = kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])
        self.FOVtrend_x = np.zeros(len(fls))
        self.FOVtrend_y = np.zeros(len(fls))
        self.pad_edges =  kwargs.get(&#34;pad_edges&#34;, True)
        build_fnm_reg, build_dtp = build_filename(fls[0], **kwargs)
        self.fnm_reg = kwargs.get(&#34;fnm_reg&#34;, build_fnm_reg)
        self.dtp = kwargs.get(&#34;dtp&#34;, build_dtp)
        if disp_res:
            print(&#39;Registered data will be saved into: &#39;, self.fnm_reg)


        kwargs.update({&#39;data_dir&#39; : self.data_dir, &#39;fnm_reg&#39; : self.fnm_reg, &#39;dtp&#39; : self.dtp})

        if kwargs.get(&#34;recall_parameters&#34;, False):
            dump_filename = kwargs.get(&#34;dump_filename&#34;, &#39;&#39;)
            try:
                dump_data = pickle.load(open(dump_filename, &#39;rb&#39;))
                dump_loaded = True
            except Exception as ex1:
                dump_loaded = False
                if disp_res:
                    print(&#39;Failed to open Parameter dump filename: &#39;, dump_filename)
                    print(ex1.message)

            if dump_loaded:
                try:
                    for key in tqdm(dump_data, desc=&#39;Recalling the data set parameters&#39;):
                        setattr(self, key, dump_data[key])
                except Exception as ex2:
                    if disp_res:
                        print(&#39;Parameter dump filename: &#39;, dump_filename)
                        print(&#39;Failed to restore the object parameters&#39;)
                        print(ex2.message)



    def SIFT_evaluation(self, eval_fls = [], **kwargs):
        &#39;&#39;&#39;
        Evaluate SIFT settings and perfromance of few test frames (eval_fls). ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

        Parameters:
        eval_fls : array of str
            filenames for the data frames to be used for SIFT evaluation

        kwargs
        ---------
        data_dir : str
            data directory (path)
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        fnm_reg : str
            filename for the final registed dataset
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        TransformType : object reference
            Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
            Choose from the following options:
                ShiftTransform - only x-shift and y-shift
                XScaleShiftTransform  -  x-scale, x-shift, y-shift
                ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        kp_max_num : int
            Max number of key-points to be matched.
            Key-points in every frame are indexed (in descending order) by the strength of the response.
            Only kp_max_num is kept for further processing.
            Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
        SIFT_nfeatures : int
            SIFT libary default is 0. The number of best features to retain.
            The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
        SIFT_nOctaveLayers : int
            SIFT libary default  is 3. The number of layers in each octave.
            3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
        SIFT_contrastThreshold : double
            SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
            The larger the threshold, the less features are produced by the detector.
            The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
            When nOctaveLayers is set to default and if you want to use the value used in
            D. Lowe paper (0.03), set this argument to 0.09.
        SIFT_edgeThreshold : double
            SIFT libary default  is 10. The threshold used to filter out edge-like features.
            Note that the its meaning is different from the contrastThreshold,
            i.e. the larger the edgeThreshold, the less features are filtered out
            (more features are retained).
        SIFT_sigma : double
            SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
            If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check

        Returns:
        dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts
        &#39;&#39;&#39;
        if len(eval_fls) == 0:
            eval_fls = [self.fls[self.nfrs//2], self.fls[self.nfrs//2+1]]
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        threshold_min = kwargs.get(&#34;threshold_min&#34;, self.threshold_min)
        threshold_max = kwargs.get(&#34;threshold_max&#34;, self.threshold_max)
        nbins = kwargs.get(&#34;nbins&#34;, self.nbins)
        TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
        l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
        targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
        solver = kwargs.get(&#34;solver&#34;, self.solver)
        drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
        max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
        kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
        SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, self.SIFT_nfeatures)
        SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, self.SIFT_nOctaveLayers)
        SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, self.SIFT_contrastThreshold)
        SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, self.SIFT_edgeThreshold)
        SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, self.SIFT_sigma)
        Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)
        BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
        save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])

        SIFT_evaluation_kwargs = {&#39;ftype&#39; : ftype,
                                &#39;Sample_ID&#39; : Sample_ID,
                                &#39;data_dir&#39; : data_dir,
                                &#39;fnm_reg&#39; : fnm_reg,
                                &#39;threshold_min&#39; : threshold_min,
                                &#39;threshold_max&#39; : threshold_max,
                                &#39;nbins&#39; : nbins,
                                &#39;evaluation_box&#39; : evaluation_box,
                                &#39;TransformType&#39; : TransformType,
                                &#39;l2_matrix&#39; : l2_matrix,
                                &#39;targ_vector&#39; : targ_vector,
                                &#39;solver&#39; : solver,
                                &#39;drmax&#39; : drmax,
                                &#39;max_iter&#39; : max_iter,
                                &#39;kp_max_num&#39; : kp_max_num,
                                &#39;SIFT_Transform&#39; : TransformType,
                                &#39;SIFT_nfeatures&#39; : SIFT_nfeatures,
                                &#39;SIFT_nOctaveLayers&#39; : SIFT_nOctaveLayers,
                                &#39;SIFT_contrastThreshold&#39; : SIFT_contrastThreshold,
                                &#39;SIFT_edgeThreshold&#39; : SIFT_edgeThreshold,
                                &#39;SIFT_sigma&#39; : SIFT_sigma,
                                &#39;Lowe_Ratio_Threshold&#39; : Lowe_Ratio_Threshold,
                                &#39;BFMatcher&#39; : BFMatcher,
                                &#39;save_matches&#39; : save_matches,
                                &#39;save_res_png&#39;  : save_res_png}

        dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts = SIFT_evaluation_dataset(eval_fls, **SIFT_evaluation_kwargs)
        src_pts_filtered, dst_pts_filtered = kpts
        print(&#39;Transformation Matrix determined using &#39;+ TransformType.__name__ +&#39; using &#39; + solver + &#39; solver&#39;)
        print(transform_matrix)
        print(&#39;{:d} keypoint matches were detected with {:.1f} pixel outlier threshold&#39;.format(n_matches, drmax))
        print(&#39;Number of iterations: {:d}&#39;.format(iteration))
        return dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts


    def convert_raw_data_to_tif_files(self, DASK_client = &#39;&#39;, **kwargs):
        &#39;&#39;&#39;
        Convert binary &#34;.dat&#34; files into &#34;.tif&#34; files.

        Parameters:
        DASK_client : instance of the DASK client object

        kwargs
        ---------
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        &#39;&#39;&#39;
        if hasattr(self, &#34;use_DASK&#34;):
            use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
        else:
            use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
        if hasattr(self, &#34;DASK_client_retries&#34;):
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
        else:
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        if self.ftype ==0 :
            print(&#39;Step 2a: Creating &#34;*InLens.tif&#34; files using DASK distributed&#39;)
            t00 = time.time()
            if use_DASK:
                try:
                    futures = DASK_client.map(save_inlens_data, self.fls, retries = DASK_client_retries)
                    fls_new = np.array(DASK_client.gather(futures))
                except:
                    fls_new = []
                    for fl in tqdm(self.fls, desc = &#39;Converting .dat data files into .tif format&#39;):
                            fls_new.append(save_inlens_data(fl))
            else:
                fls_new = []
                for fl in tqdm(self.fls, desc = &#39;Converting .dat data files into .tif format&#39;):
                    fls_new.append(save_inlens_data(fl))

            t01 = time.time()
            print(&#39;Step 2a: Elapsed time: {:.2f} seconds&#39;.format(t01 - t00))
            print(&#39;Step 2a: Quick check if all files were converted: &#39;, np.array_equal(self.fls, fls_new))
        else:
            print(&#39;Step 2a: data is already in TIF format&#39;)


    def evaluate_FIBSEM_statistics(self, DASK_client, **kwargs):
        &#39;&#39;&#39;
        Evaluates parameters of FIBSEM data set (Min/Max, Working Distance (WD), Milling Y Voltage (MV), FOV center positions).

        Parameters:
        use_DASK : boolean
            perform remote DASK computations
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails

        kwargs:
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        frame_inds : array
            Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
        data_dir : str
            data directory (path)  for saving the data
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        sliding_minmax : boolean
            if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
            if False - same data_min_glob and data_max_glob will be used for all files
        fit_params : list
            Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
            Other options are:
                [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
                [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
        Mill_Volt_Rate_um_per_V : float
            Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.
        FIBSEM_Data_xlsx : str
            Filepath of the Excell file for the FIBSEM data set data to be saved (Data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)
        disp_res : bolean
            If True (default), intermediate messages and results will be displayed.

        Returns:
        list of 9 parameters: FIBSEM_Data_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding, mill_rate_WD, mill_rate_MV, center_x, center_y
            FIBSEM_Data_xlsx : str
                path to Excel file with the FIBSEM data
            data_min_glob : float
                min data value for I8 conversion (open CV SIFT requires I8)
            data_man_glob : float
                max data value for I8 conversion (open CV SIFT requires I8)
            data_min_sliding : float array
                min data values (one per file) for I8 conversion
            data_max_sliding : float array
                max data values (one per file) for I8 conversion

            mill_rate_WD : float array
                Milling rate calculated based on Working Distance (WD)
            mill_rate_MV : float array
                Milling rate calculated based on Milling Y Voltage (MV)
            center_x : float array
                FOV Center X-coordinate extrated from the header data
            center_y : float array
                FOV Center Y-coordinate extrated from the header data
        &#39;&#39;&#39;
        if hasattr(self, &#34;use_DASK&#34;):
            use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
        else:
            use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
        if hasattr(self, &#34;DASK_client_retries&#34;):
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
        else:
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        frame_inds = kwargs.get(&#34;frame_inds&#34;, np.arange(len(self.fls)))
        data_dir = self.data_dir
        threshold_min = kwargs.get(&#34;threshold_min&#34;, self.threshold_min)
        threshold_max = kwargs.get(&#34;threshold_max&#34;, self.threshold_max)
        nbins = kwargs.get(&#34;nbins&#34;, self.nbins)
        sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, self.sliding_minmax)
        fit_params = kwargs.get(&#34;fit_params&#34;, self.fit_params)

        if hasattr(self, &#39;Mill_Volt_Rate_um_per_V&#39;):
            Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, self.Mill_Volt_Rate_um_per_V)
        else:
            Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, 31.235258870176065)
        FIBSEM_Data_xlsx = kwargs.get(&#39;FIBSEM_Data_xlsx&#39;, &#39;FIBSEM_Data_xlsx.xlsx&#39;)
        disp_res = kwargs.get(&#39;disp_res&#39;, True)

        local_kwargs = {&#39;use_DASK&#39; : use_DASK,
                        &#39;DASK_client_retries&#39; : DASK_client_retries,
                        &#39;ftype&#39; : ftype,
                        &#39;frame_inds&#39; : frame_inds,
                        &#39;data_dir&#39; : data_dir,
                        &#39;threshold_min&#39; : threshold_min,
                        &#39;threshold_max&#39; : threshold_max,
                        &#39;nbins&#39; : nbins,
                        &#39;sliding_minmax&#39; : sliding_minmax,
                        &#39;fit_params&#39; : fit_params,
                        &#39;Mill_Volt_Rate_um_per_V&#39; : Mill_Volt_Rate_um_per_V,
                        &#39;FIBSEM_Data_xlsx&#39; : FIBSEM_Data_xlsx,
                        &#39;disp_res&#39; : disp_res}

        if disp_res:
            print(&#39;Evaluating the parameters of FIBSEM data set (data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)&#39;)
        self.FIBSEM_Data = evaluate_FIBSEM_frames_dataset(self.fls, DASK_client, **local_kwargs)
        self.data_minmax = self.FIBSEM_Data[0:5]
        WD = self.FIBSEM_Data[5]
        MillingYVoltage = self.FIBSEM_Data[6]

        apert = np.min((51, len(self.FIBSEM_Data[7])-1))
        self.FOVtrend_x = savgol_filter(self.FIBSEM_Data[7]*1.0, apert, 1) - self.FIBSEM_Data[7][0]
        self.FOVtrend_y = savgol_filter(self.FIBSEM_Data[8]*1.0, apert, 1) - self.FIBSEM_Data[8][0]

        WD_fit_coef = np.polyfit(frame_inds, WD, 1)
        rate_WD = WD_fit_coef[0]*1.0e6

        MV_fit_coef = np.polyfit(frame_inds, MillingYVoltage, 1)
        rate_MV = MV_fit_coef[0]*Mill_Volt_Rate_um_per_V*-1.0e3

        Z_pixel_size_WD = rate_WD
        Z_pixel_size_MV = rate_MV

        if ftype == 0:
            if disp_res:
                if self.zbin_factor &gt; 1:
                    print(&#39;Z pixel (after {:d}-x Z-binning) = {:.2f} nm - based on WD data&#39;.format(self.zbin_factor, Z_pixel_size_WD*self.zbin_factor))
                    print(&#39;Z pixel (after {:d}-x Z-binning) = {:.2f} nm - based on Milling Voltage data&#39;.format(self.zbin_factor, Z_pixel_size_MV*self.zbin_factor))
                else:
                    print(&#39;Z pixel = {:.2f} nm  - based on WD data&#39;.format(Z_pixel_size_WD))
                    print(&#39;Z pixel = {:.2f} nm  - based on Milling Voltage data&#39;.format(Z_pixel_size_MV))

            self.voxel_size = np.rec.array((self.PixelSize,  self.PixelSize,  Z_pixel_size_WD), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
        else:
            if disp_res:
                print(&#39;No milling rate data is available, isotropic voxel size is set to {:.2f} nm&#39;.format(self.PixelSize))
            self.voxel_size = np.rec.array((self.PixelSize,  self.PixelSize,  Z_pixel_size_WD), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])

        return self.FIBSEM_Data


    def extract_keypoints(self, DASK_client, **kwargs):
        &#39;&#39;&#39;
        Extract Key-Points and Descriptors

        Parameters:
        DASK_client : instance of the DASK client object

        kwargs
        ---------
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        EightBit : int
            0 - 16-bit data, 1: 8-bit data
        fnm_reg : str
            filename for the final registed dataset
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        sliding_minmax : boolean
            if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
            if False - same data_min_glob and data_max_glob will be used for all files
        data_minmax : list of 5 parameters
            minmax_xlsx : str
                path to Excel file with Min/Max data
            data_min_glob : float
                min data value for I8 conversion (open CV SIFT requires I8)
            data_min_sliding : float array
                min data values (one per file) for I8 conversion
            data_max_sliding : float array
                max data values (one per file) for I8 conversion
            data_minmax_glob : 2D float array
                min and max data values without sliding averaging
        kp_max_num : int
            Max number of key-points to be matched.
            Key-points in every frame are indexed (in descending order) by the strength of the response.
            Only kp_max_num is kept for further processing.
            Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)

        Returns:
        fnms : array of str
            filenames for binary files kontaining Key-Point and Descriptors for each frame
        &#39;&#39;&#39;
        if len(self.fls) == 0:
            print(&#39;Data set not defined, perform initialization first&#39;)
            fnms = []
        else:
            if hasattr(self, &#34;use_DASK&#34;):
                use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
            else:
                use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
            if hasattr(self, &#34;DASK_client_retries&#34;):
                DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
            else:
                DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
            ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
            data_dir = self.data_dir
            fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
            threshold_min = kwargs.get(&#34;threshold_min&#34;, self.threshold_min)
            threshold_max = kwargs.get(&#34;threshold_max&#34;, self.threshold_max)
            nbins = kwargs.get(&#34;nbins&#34;, self.nbins)
            sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, self.sliding_minmax)
            data_minmax = kwargs.get(&#34;data_minmax&#34;, self.data_minmax)
            kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)

            SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, self.SIFT_nfeatures)
            SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, self.SIFT_nOctaveLayers)
            SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, self.SIFT_contrastThreshold)
            SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, self.SIFT_edgeThreshold)
            SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, self.SIFT_sigma)

            minmax_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding = data_minmax
            kpt_kwargs = {&#39;ftype&#39; : ftype,
                        &#39;threshold_min&#39; : threshold_min,
                        &#39;threshold_max&#39; : threshold_max,
                        &#39;nbins&#39; : nbins,
                        &#39;kp_max_num&#39; : kp_max_num,
                        &#39;SIFT_nfeatures&#39; : SIFT_nfeatures,
                        &#39;SIFT_nOctaveLayers&#39; : SIFT_nOctaveLayers,
                        &#39;SIFT_contrastThreshold&#39; : SIFT_contrastThreshold,
                        &#39;SIFT_edgeThreshold&#39; : SIFT_edgeThreshold,
                        &#39;SIFT_sigma&#39; : SIFT_sigma}

            if sliding_minmax:
                params_s3 = [[dts3[0], dts3[1], dts3[2], kpt_kwargs] for dts3 in zip(self.fls, data_min_sliding, data_max_sliding)]
            else:
                params_s3 = [[fl, data_min_glob, data_max_glob, kpt_kwargs] for fl in self.fls]
            if use_DASK:
                print(&#39;Using DASK distributed&#39;)
                futures_s3 = DASK_client.map(extract_keypoints_descr_files, params_s3, retries = DASK_client_retries)
                fnms = DASK_client.gather(futures_s3)
            else:
                print(&#39;Using Local Computation&#39;)
                fnms = []
                for j, param_s3 in enumerate(tqdm(params_s3, desc=&#39;Extracting Key Points and Descriptors: &#39;)):
                    fnms.append(extract_keypoints_descr_files(param_s3))

            self.fnms = fnms
        return fnms


    def determine_transformations(self, DASK_client, **kwargs):
        &#39;&#39;&#39;
        Determine transformation matrices for sequential frame pairs

        Parameters:
        DASK_client : instance of the DASK client object

        kwargs
        ---------
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        TransformType : object reference
                Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
                Choose from the following options:
                    ShiftTransform - only x-shift and y-shift
                    XScaleShiftTransform  -  x-scale, x-shift, y-shift
                    ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                    AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                    RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        Lowe_Ratio_Threshold : float
            threshold for Lowe&#39;s Ratio Test
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check


        Returns:
        results_s4 : array of lists containing the reults:
            results_s4 = [transformation_matrix, fnm_matches, npt, error_abs_mean]
            transformation_matrix : 2D float array
                transformation matrix for each sequential frame pair
            fnm_matches : str
                filename containing the matches used to determin the transformation for the par of frames
            npts : int
                number of matches
            error_abs_mean : float
                mean abs error of registration for all matched Key-Points
        &#39;&#39;&#39;
        if len(self.fnms) == 0:
            print(&#39;No data on individual key-point data files, peform key-point search&#39;)
            results_s4 = []
        else:
            if hasattr(self, &#34;use_DASK&#34;):
                use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
            else:
                use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
            if hasattr(self, &#34;DASK_client_retries&#34;):
                DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
            else:
                DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
            ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
            TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
            l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
            targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
            solver = kwargs.get(&#34;solver&#34;, self.solver)
            drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
            max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
            kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
            Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)   # threshold for Lowe&#39;s Ratio Test
            BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
            save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
            save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
            dt_kwargs = {&#39;ftype&#39; : ftype,
                            &#39;TransformType&#39; : TransformType,
                            &#39;l2_matrix&#39; : l2_matrix,
                            &#39;targ_vector&#39;: targ_vector,
                            &#39;solver&#39; : solver,
                            &#39;drmax&#39; : drmax,
                            &#39;max_iter&#39; : max_iter,
                            &#39;BFMatcher&#39; : BFMatcher,
                            &#39;save_matches&#39; : save_matches,
                            &#39;kp_max_num&#39; : kp_max_num,
                            &#39;Lowe_Ratio_Threshold&#39; : Lowe_Ratio_Threshold}

            params_s4 = []
            for j, fnm in enumerate(self.fnms[:-1]):
                fname1 = self.fnms[j]
                fname2 = self.fnms[j+1]
                params_s4.append([fname1, fname2, dt_kwargs])
            if use_DASK:
                print(&#39;Using DASK distributed&#39;)
                futures4 = DASK_client.map(determine_transformations_files, params_s4, retries = DASK_client_retries)
                #determine_transformations_files returns (transform_matrix, fnm_matches, kpts, iteration)
                results_s4 = DASK_client.gather(futures4)
            else:
                print(&#39;Using Local Computation&#39;)
                results_s4 = []
                for param_s4 in tqdm(params_s4, desc = &#39;Extracting Transformation Parameters: &#39;):
                    results_s4.append(determine_transformations_files(param_s4))
            #determine_transformations_files returns (transform_matrix, fnm_matches, kpts, errors, iteration)
            self.transformation_matrix = np.nan_to_num(np.array([result[0] for result in results_s4]))
            self.fnms_matches = [result[1] for result in results_s4]
            self.error_abs_mean = np.nan_to_num(np.array([result[3] for result in results_s4]))
            self.npts = np.nan_to_num(np.array([len(result[2][0])  for result in results_s4]))
            print(&#39;Mean Number of Keypoints :&#39;, np.mean(self.npts).astype(np.int16))
        return results_s4


    def process_transformation_matrix(self, **kwargs):
        &#39;&#39;&#39;
        Calculate cumulative transformation matrix

        kwargs
        ---------
        data_dir : str
            data directory (path)
        fnm_reg : str
            filename for the final registed dataset
        Sample_ID : str
            Sample ID
        TransformType : object reference
                Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
                Choose from the following options:
                    ShiftTransform - only x-shift and y-shift
                    XScaleShiftTransform  -  x-scale, x-shift, y-shift
                    ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                    AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                    RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        preserve_scales : boolean
            If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
        fit_params : list
            Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
            Other options are:
                [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
                [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
        subtract_linear_fit : [boolean, boolean]
            List of two Boolean values for two directions: X- and Y-.
            If True, the linear slopes along X- and Y- directions (respectively)
            will be subtracted from the cumulative shifts.
            This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.

        Returns:
        tr_matr_cum_residual, tr_matr_cum_xlsx_file : list of 2D arrays of float and the filename of the XLSX file with the transf matrix results
            Cumulative transformation matrices
        &#39;&#39;&#39;
        if len(self.transformation_matrix) == 0:
            print(&#39;No data on individual key-point matches, peform key-point search / matching first&#39;)
            self.tr_matr_cum_residual = []
        else:
            data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
            fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
            TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
            SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, self.SIFT_nfeatures)
            SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, self.SIFT_nOctaveLayers)
            SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, self.SIFT_contrastThreshold)
            SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, self.SIFT_edgeThreshold)
            SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, self.SIFT_sigma)
            Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
            l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
            targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
            solver = kwargs.get(&#34;solver&#34;, self.solver)
            drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
            max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
            BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
            save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
            kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
            save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
            preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, self.preserve_scales)
            fit_params =  kwargs.get(&#34;fit_params&#34;, self.fit_params)
            subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, self.subtract_linear_fit)
            subtract_FOVtrend_from_fit =  kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, self.subtract_FOVtrend_from_fit)
            pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)

            TM_kwargs = {&#39;fnm_reg&#39; : fnm_reg,
                            &#39;data_dir&#39; : data_dir,
                            &#39;TransformType&#39; : TransformType,
                            &#39;SIFT_nfeatures&#39; : SIFT_nfeatures,
                            &#39;SIFT_nOctaveLayers&#39; : SIFT_nOctaveLayers,
                            &#39;SIFT_contrastThreshold&#39; : SIFT_contrastThreshold,
                            &#39;SIFT_edgeThreshold&#39; : SIFT_edgeThreshold,
                            &#39;SIFT_sigma&#39; : SIFT_sigma,
                            &#39;Sample_ID&#39; : Sample_ID,
                            &#39;l2_matrix&#39; : l2_matrix,
                            &#39;targ_vector&#39;: targ_vector,
                            &#39;solver&#39; : solver,
                            &#39;drmax&#39; : drmax,
                            &#39;max_iter&#39; : max_iter,
                            &#39;BFMatcher&#39; : BFMatcher,
                            &#39;save_matches&#39; : save_matches,
                            &#39;kp_max_num&#39; : kp_max_num,
                            &#39;save_res_png &#39; : save_res_png ,
                            &#39;preserve_scales&#39; : preserve_scales,
                            &#39;fit_params&#39; : fit_params,
                            &#39;subtract_linear_fit&#39; : subtract_linear_fit,
                            &#39;subtract_FOVtrend_from_fit&#39; : subtract_FOVtrend_from_fit,
                            &#39;pad_edges&#39; : pad_edges}
            self.tr_matr_cum_residual, self.transf_matrix_xlsx_file = process_transf_matrix(self.transformation_matrix,
                                             self.FOVtrend_x,
                                             self.FOVtrend_y,
                                             self.fnms_matches,
                                             self.npts,
                                             self.error_abs_mean,
                                             **TM_kwargs)
        return self.tr_matr_cum_residual, self.transf_matrix_xlsx_file

    def save_parameters(self, **kwargs):
        &#39;&#39;&#39;
        Save transformation attributes and parameters (including transformation matrices).

        kwargs:
        -------
        dump_filename : string
            String containing the name of the binary dump for saving all attributes of the current istance of the FIBSEM_dataset object.


        Returns:
        dump_filename : string
        &#39;&#39;&#39;
        default_dump_filename = os.path.join(self.data_dir, self.fnm_reg.replace(&#39;.mrc&#39;, &#39;_params.bin&#39;))
        dump_filename = kwargs.get(&#34;dump_filename&#34;, default_dump_filename)

        pickle.dump(self.__dict__, open(dump_filename, &#39;wb&#39;))

        npts_fnm = dump_filename.replace(&#39;_params.bin&#39;, &#39;_Npts_Errs_data.csv&#39;)
        Tr_matrix_xls_fnm = dump_filename.replace(&#39;_params.bin&#39;, &#39;_Transform_Matrix_data.csv&#39;)

        # Save the keypoint statistics into a CSV file
        columns=[&#39;Npts&#39;, &#39;Mean Abs Error&#39;]
        npdt = pd.DataFrame(np.vstack((self.npts, self.error_abs_mean)).T, columns = columns, index = None)
        npdt.to_csv(npts_fnm, index = None)

        # Save the X-Y shift data and keypoint statistics into a CSV file
        columns=[&#39;T00 (Sxx)&#39;, &#39;T01 (Sxy)&#39;, &#39;T02 (Tx)&#39;,
                 &#39;T10 (Syx)&#39;, &#39;T11 (Syy)&#39;, &#39;T12 (Ty)&#39;,
                 &#39;T20 (0.0)&#39;, &#39;T21 (0.0)&#39;, &#39;T22 (1.0)&#39;]
        tr_mx_dt = pd.DataFrame(self.transformation_matrix.reshape((len(self.transformation_matrix), 9)), columns = columns, index = None)
        tr_mx_dt.to_csv(Tr_matrix_xls_fnm, index = None)
        return dump_filename

    def check_for_nomatch_frames(self, thr_npt, **kwargs):
        &#39;&#39;&#39;
        Calculate cumulative transformation matrix

        Parameters:
        -----------
        thr_npt : int
            minimum number of matches. If the pair has less than this - it is reported as &#34;suspicious&#34; and is excluded.

        kwargs
        ---------
        data_dir : str
            data directory (path)
        fnm_reg : str
            filename for the final registed dataset
        Sample_ID : str
            Sample ID
        TransformType : object reference
                Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
                Choose from the following options:
                    ShiftTransform - only x-shift and y-shift
                    XScaleShiftTransform  -  x-scale, x-shift, y-shift
                    ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                    AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                    RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        preserve_scales : boolean
            If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
        fit_params : list
            Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
            Other options are:
                [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
                [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
        subtract_linear_fit : [boolean, boolean]
            List of two Boolean values for two directions: X- and Y-.
            If True, the linear slopes along X- and Y- directions (respectively)
            will be subtracted from the cumulative shifts.
            This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.

        Returns:
        tr_matr_cum_residual : list of 2D arrays of float
            Cumulative transformation matrices
        &#39;&#39;&#39;
        self.thr_npt = thr_npt
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
        targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
        solver = kwargs.get(&#34;solver&#34;, self.solver)
        drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
        max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
        BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
        save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
        kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, self.preserve_scales)
        fit_params =  kwargs.get(&#34;fit_params&#34;, self.fit_params)
        subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, self.subtract_linear_fit)
        subtract_FOVtrend_from_fit =  kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, self.subtract_FOVtrend_from_fit)
        pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)

        res_nomatch_check = check_for_nomatch_frames_dataset(self.fls, self.fnms, self.fnms_matches,
                                     self.transformation_matrix, self.error_abs_mean, self.npts,
                                     thr_npt,
                                     data_dir = self.data_dir, fnm_reg = self.fnm_reg)
        frames_to_remove, self.fls, self.fnms, self.fnms_matches, self.error_abs_mean, self.npts, self.transformation_matrix = res_nomatch_check

        if len(frames_to_remove) &gt; 0:
            TM_kwargs = {&#39;fnm_reg&#39; : fnm_reg,
                            &#39;data_dir&#39; : data_dir,
                            &#39;TransformType&#39; : TransformType,
                            &#39;Sample_ID&#39; : Sample_ID,
                            &#39;l2_matrix&#39; : l2_matrix,
                            &#39;targ_vector&#39;: targ_vector,
                            &#39;solver&#39; : solver,
                            &#39;drmax&#39; : drmax,
                            &#39;max_iter&#39; : max_iter,
                            &#39;BFMatcher&#39; : BFMatcher,
                            &#39;save_matches&#39; : save_matches,
                            &#39;kp_max_num&#39; : kp_max_num,
                            &#39;save_res_png &#39; : save_res_png ,
                            &#39;preserve_scales&#39; : preserve_scales,
                            &#39;fit_params&#39; : fit_params,
                            &#39;subtract_linear_fit&#39; : subtract_linear_fit,
                            &#39;subtract_FOVtrend_from_fit&#39; : subtract_FOVtrend_from_fit,
                            &#39;pad_edges&#39; : pad_edges}
            self.tr_matr_cum_residual, self.transf_matrix_xlsx_file = process_transf_matrix(self.transformation_matrix,
                                             self.FOVtrend_x,
                                             self.FOVtrend_y,
                                             self.fnms_matches,
                                             self.npts,
                                             self.error_abs_mean,
                                             **TM_kwargs)
        return self.tr_matr_cum_residual, self.transf_matrix_xlsx_file


    def transform_and_save(self, DASK_client, save_transformed_dataset=True, save_registration_summary=True, frame_inds=np.array((-1)), **kwargs):
        &#39;&#39;&#39;
        Transform the frames using the cumulative transformation matrix and save the data set into .mrc and/or .h5 file

        Parameters
        DASK_client : instance of the DASK client object
        save_transformed_dataset : boolean
            If true, the transformed data set will be saved into MRC file
        save_registration_summary : bolean
            If True, the registration analysis data will be saved into XLSX file
        frame_inds : int array (or list)
            Array of frame indecis. If not set or set to np.array((-1)), all frames will be transformed

        kwargs
        ---------
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        data_dir : str
            data directory (path)
        fnm_types : list of strings
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is &#39;mrc&#39;. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        fnm_reg : str
            filename for the final registed dataset
        ImgB_fraction : float
            fractional ratio of Image B to be used for constructing the fuksed image:
            ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction
        add_offset : boolean
            If True - the Dark Oount offset will be added before saving to make values positive (set True if saving into BigDataViewer HDF5 - it uses UI16 data format)
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        perfrom_transformation : boolean
            If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed.
        invert_data : boolean
            If True - the data is inverted.
        flatten_image : bolean
            perform image flattening
        image_correction_file : str
            full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
        flipY : boolean
            If True, the data will be flipped along Y-axis. Default is False.
        zbin_factor : int
            binning factor along Z-axis
        eval_metrics : list of str
            list of evaluation metrics to use. default is [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;]
        fnm_types : list of strings
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is &#39;mrc&#39;. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        sliding_evaluation_box : boolean
            if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
        start_evaluation_box : list of 4 int
            see above
        stop_evaluation_box : list of 4 int
            see above
        save_sample_frames_png : bolean
            If True, sample frames with superimposed eval box and registration analysis data will be saved into png files
        dtp  : dtype
            Python data type for saving. Deafult is int16, the other option currently is uint8.
        disp_res : bolean
            If True (default), intermediate messages and results will be displayed.

        Returns:
        reg_summary, reg_summary_xlsx
            reg_summary : pandas DataFrame
            reg_summary = pd.DataFrame(np.vstack((npts, error_abs_mean, image_nsad, image_ncc, image_mi)
            reg_summary_xlsx : name of the XLSX workbook containing the data
        &#39;&#39;&#39;
        if (frame_inds == np.array((-1))).all():
            frame_inds = np.arange(len(self.fls))

        if hasattr(self, &#34;use_DASK&#34;):
            use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
        else:
            use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
        if hasattr(self, &#34;DASK_client_retries&#34;):
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
        else:
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        if hasattr(self, &#39;XResolution&#39;):
            XResolution_default = self.XResolution
        else:
            XResolution_default = FIBSEM_frame(self.fls[len(self.fls)//2]).XResolution
        XResolution = kwargs.get(&#34;XResolution&#34;, XResolution_default)
        if hasattr(self, &#39;YResolution&#39;):
            YResolution_default = self.YResolution
        else:
            YResolution_default = FIBSEM_frame(self.fls[len(self.fls)//2]).YResolution
        YResolution = kwargs.get(&#34;YResolution&#34;, YResolution_default)

        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        if hasattr(self, &#39;fnm_types&#39;):
            fnm_types = kwargs.get(&#34;fnm_types&#34;, self.fnm_types)
        else:
            fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
        ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, self.ImgB_fraction)
        if self.DetB == &#39;None&#39;:
            ImgB_fraction = 0.0
        if hasattr(self, &#39;add_offset&#39;):
            add_offset = kwargs.get(&#34;add_offset&#34;, self.add_offset)
        else:
            add_offset = kwargs.get(&#34;add_offset&#34;, False)
        save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, True)
        pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        if hasattr(self, &#39;eval_metrics&#39;):
            eval_metrics =  kwargs.get(&#34;eval_metrics&#34;, self.eval_metrics)
        else:
            eval_metrics = kwargs.get(&#39;eval_metrics&#39;, [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;])
        if hasattr(self, &#39;zbin_factor&#39;):
            zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, self.zbin_factor)         # binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
        else:
            zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, 1)
        if hasattr(self, &#39;voxel_size&#39;):
            voxel_size = kwargs.get(&#34;voxel_size&#34;, self.voxel_size)
        else:
            voxel_size_default = np.rec.array((self.PixelSize,  self.PixelSize,  self.PixelSize), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
            voxel_size = kwargs.get(&#34;voxel_size&#34;, voxel_size_default)
        voxel_size_zbinned = voxel_size.copy()
        voxel_size_zbinned.z = voxel_size.z * zbin_factor
        if hasattr(self, &#39;flipY&#39;):
            flipY = kwargs.get(&#34;flipY&#34;, self.flipY)
        else:
            flipY = kwargs.get(&#34;flipY&#34;, False)
        if hasattr(self, &#39;dump_filename&#39;):
            dump_filename = kwargs.get(&#34;dump_filename&#34;, self.dump_filename)
        else:
            dump_filename = kwargs.get(&#34;dump_filename&#34;, &#39;&#39;)
        int_order = kwargs.get(&#34;int_order&#34;, self.int_order)
        preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, self.preserve_scales)
        if hasattr(self, &#39;flatten_image&#39;):
            flatten_image = kwargs.get(&#34;flatten_image&#34;, self.flatten_image)
        else:
            flatten_image = kwargs.get(&#34;flatten_image&#34;, False)
        if hasattr(self, &#39;image_correction_file&#39;):
            image_correction_file = kwargs.get(&#34;image_correction_file&#34;, self.image_correction_file)
        else:
            image_correction_file = kwargs.get(&#34;image_correction_file&#34;, &#39;&#39;)
        perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True)  and hasattr(self, &#39;tr_matr_cum_residual&#39;)
        if hasattr(self, &#39;invert_data&#39;):
            invert_data = kwargs.get(&#34;invert_data&#34;, self.invert_data)
        else:
            invert_data = kwargs.get(&#34;invert_data&#34;, False)
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
        start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
        stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
        disp_res  = kwargs.get(&#34;disp_res&#34;, True )
        dtp = kwargs.get(&#34;dtp&#34;, int16)  # Python data type for saving. Deafult is int16, the other option currently is uint8.

        save_kwargs = {&#39;fnm_types&#39; : fnm_types,
                        &#39;fnm_reg&#39; : fnm_reg,
                        &#39;use_DASK&#39; : use_DASK,
                        &#39;DASK_client_retries&#39; : DASK_client_retries,
                        &#39;ftype&#39; : ftype,
                        &#39;XResolution&#39; : XResolution,
                        &#39;YResolution&#39; : YResolution,
                        &#39;data_dir&#39; : data_dir,
                        &#39;voxel_size&#39; : voxel_size_zbinned,
                        &#39;pad_edges&#39; : pad_edges,
                        &#39;ImgB_fraction&#39; : ImgB_fraction,
                        &#39;save_res_png &#39; : save_res_png ,
                        &#39;dump_filename&#39; : dump_filename,
                        &#39;dtp&#39; : dtp,
                        &#39;zbin_factor&#39; : zbin_factor,
                        &#39;eval_metrics&#39; : eval_metrics,
                        &#39;flipY&#39; : flipY,
                        &#39;int_order&#39; : int_order,
                        &#39;preserve_scales&#39; : preserve_scales,
                        &#39;flatten_image&#39; : flatten_image,
                        &#39;image_correction_file&#39; : image_correction_file,
                        &#39;perfrom_transformation&#39; : perfrom_transformation,
                        &#39;invert_data&#39; : invert_data,
                        &#39;evaluation_box&#39; : evaluation_box,
                        &#39;sliding_evaluation_box&#39; : sliding_evaluation_box,
                        &#39;start_evaluation_box&#39; : start_evaluation_box,
                        &#39;stop_evaluation_box&#39; : stop_evaluation_box,
                        &#39;save_sample_frames_png&#39; : save_sample_frames_png,
                        &#39;save_registration_summary&#39; : save_registration_summary,
                        &#39;disp_res&#39; : disp_res}

        # first, transform, bin and save frame chunks into individual tif files
        if disp_res:
            print(&#39;Transforming and Saving Intermediate Registered Frames&#39;)
        registered_filenames = transform_and_save_frames(DASK_client, frame_inds, self.fls, self.tr_matr_cum_residual, **save_kwargs)

        frame0 = tiff.imread(registered_filenames[0])
        ny, nx = np.shape(frame0)
        if disp_res:
            print(&#39;Analyzing Registration Quality&#39;)
        if pad_edges and perfrom_transformation:
            xmn, xmx, ymn, ymx = determine_pad_offsets([ny, nx], self.tr_matr_cum_residual)
            padx = int(xmx - xmn)
            pady = int(ymx - ymn)
            xi = int(np.max([xmx, 0]))
            yi = int(np.max([ymx, 0]))
        else:
            padx = 0
            pady = 0
            xi = 0
            yi = 0
        if sliding_evaluation_box:
            dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
            dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
        else:
            dx_eval = 0
            dy_eval = 0

        eval_bounds = []
        for j, registered_filename in enumerate(tqdm(registered_filenames, desc=&#39;Setting up evaluation bounds&#39;, display = disp_res)):
            if sliding_evaluation_box:
                xi_eval = xi + start_evaluation_box[2] + dx_eval*j//nz
                yi_eval = yi + start_evaluation_box[0] + dy_eval*j//nz
                if start_evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + start_evaluation_box[3]
                else:
                    xa_eval = nx
                if start_evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + start_evaluation_box[1]
                else:
                    ya_eval = ny
            else:
                xi_eval = xi + evaluation_box[2]
                if evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + evaluation_box[3]
                else:
                    xa_eval = nx
                yi_eval = yi + evaluation_box[0]
                if evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + evaluation_box[1]
                else:
                    ya_eval = ny
            eval_bounds.append([xi_eval, xa_eval, yi_eval, ya_eval])

        save_kwargs[&#39;eval_bounds&#39;] = eval_bounds

        reg_summary, reg_summary_xlsx = analyze_registration_frames(DASK_client, registered_filenames, **save_kwargs)

        if save_transformed_dataset:
            if add_offset:
                offset = self.Scaling[1, 0] * (1.0-ImgB_fraction) + self.Scaling[1, 1] * ImgB_fraction
            if disp_res:
                print(&#34;Creating Dask Array Stack&#34;)
            # now build dask array of the transformed dataset
            # read the first file to get the shape and dtype (ASSUMING THAT ALL FILES SHARE THE SAME SHAPE/TYPE)
            lazy_imread = dask.delayed(tiff.imread)  # lazy reader
            lazy_arrays = [lazy_imread(fn) for fn in registered_filenames]
            dask_arrays = [ da.from_delayed(delayed_reader, shape=frame0.shape, dtype=frame0.dtype)   for delayed_reader in lazy_arrays]
            # Stack infividual frames into one large dask.array
            if add_offset:
                FIBSEMstack = da.stack(dask_arrays, axis=0) - offset
            else:
                FIBSEMstack = da.stack(dask_arrays, axis=0)
            #nz, ny, nx = FIBSEMstack.shape
            fnms_saved = save_data_stack(FIBSEMstack, **save_kwargs)
        else:
            if disp_res:
                print(&#39;Registered data set is NOT saved into a file&#39;)

        # Remove Intermediate Registered Frame Files
        for registered_filename in tqdm(registered_filenames, desc=&#39;Removing Intermediate Registered Frame Files: &#39;, display = disp_res):
            try:
                os.remove(registered_filename)
            except:
                pass

        return reg_summary, reg_summary_xlsx


    def show_eval_box(self, **kwargs):
        &#39;&#39;&#39;
        Show the box used for evaluating the registration quality

        kwargs
        ---------
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        sliding_evaluation_box : boolean
            if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
        start_evaluation_box : list of 4 int
            see above
        stop_evaluation_box : list of 4 int
            see above
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        data_dir : str
            data directory (path)
        fnm_reg : str
            filename for the final registed dataset
        Sample_ID : str
            Sample ID
        int_order : int
            The order of interpolation (when transforming the data).
                The order has to be in the range 0-5:
                    0: Nearest-neighbor
                    1: Bi-linear (default)
                    2: Bi-quadratic
                    3: Bi-cubic
                    4: Bi-quartic
                    5: Bi-quintic
        perfrom_transformation : boolean
            If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed
        invert_data : boolean
            If True - the data is inverted
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.
        flipY : boolean
            If True, the data will be flipped along Y-axis. Default is False.
        frame_inds : array or list of int
            Array or list oif frame indecis to use to display the evaluation box.
            Default are [nfrs//10, nfrs//2, nfrs//10*9]
        &#39;&#39;&#39;
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
        start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
        stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        int_order = kwargs.get(&#34;int_order&#34;, self.int_order)
        perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True) and hasattr(self, &#39;tr_matr_cum_residual&#39;)
        invert_data =  kwargs.get(&#34;invert_data&#34;, False)
        flipY = kwargs.get(&#34;flipY&#34;, False)
        pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        fls = self.fls
        nfrs = len(fls)
        default_indecis = [nfrs//10, nfrs//2, nfrs//10*9]
        frame_inds = kwargs.get(&#34;frame_inds&#34;, default_indecis)

        for j in frame_inds:
            frame = FIBSEM_frame(fls[j], ftype=ftype)
            if pad_edges and perfrom_transformation:
                shape = [frame.YResolution, frame.XResolution]
                xmn, xmx, ymn, ymx = determine_pad_offsets(shape, self.tr_matr_cum_residual)
                padx = np.int16(xmx - xmn)
                pady = np.int16(ymx - ymn)
                xi = np.int16(np.max([xmx, 0]))
                yi = np.int16(np.max([ymx, 0]))
                # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
                # so that the transformed images are not clipped.
                # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
                # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
                # those are calculated below base on the amount of padding calculated above
                shift_matrix = np.array([[1.0, 0.0, xi],
                                         [0.0, 1.0, yi],
                                         [0.0, 0.0, 1.0]])
                inv_shift_matrix = np.linalg.inv(shift_matrix)
            else:
                padx = 0
                pady = 0
                xi = 0
                yi = 0
                shift_matrix = np.eye(3,3)
                inv_shift_matrix = np.eye(3,3)

            xsz = frame.XResolution + padx
            xa = xi + frame.XResolution
            ysz = frame.YResolution + pady
            ya = yi + frame.YResolution

            xi_eval = xi + evaluation_box[2]
            if evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + evaluation_box[3]
            else:
                xa_eval = xa
            yi_eval = yi + evaluation_box[0]
            if evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + evaluation_box[1]
            else:
                ya_eval = ya

            if sliding_evaluation_box:
                dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
                dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
            else:
                dx_eval = 0
                dy_eval = 0

            frame_img = np.zeros((ysz, xsz))

            if invert_data:
                frame_img[yi:ya, xi:xa] = np.negative(frame.RawImageA.astype(float))
                &#39;&#39;&#39;
                if frame.EightBit==0:
                    frame_img[yi:ya, xi:xa] = np.negative(frame.RawImageA)
                else:
                    frame_img[yi:ya, xi:xa]  =  uint8(255) - frame.RawImageA
                &#39;&#39;&#39;
            else:
                frame_img[yi:ya, xi:xa]  = frame.RawImageA.astype(float)

            if perfrom_transformation:
                transf = ProjectiveTransform(matrix = shift_matrix @ (self.tr_matr_cum_residual[j] @ inv_shift_matrix))
                frame_img_reg = warp(frame_img, transf, order = int_order, preserve_range=True)
            else:
                frame_img_reg = frame_img.copy()

            if flipY:
                frame_img_reg = np.flip(frame_img_reg, axis=0)

            if sliding_evaluation_box:
                xi_eval = start_evaluation_box[2] + dx_eval*j//nfrs
                yi_eval = start_evaluation_box[0] + dy_eval*j//nfrs
                if start_evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + start_evaluation_box[3]
                else:
                    xa_eval = xsz
                if start_evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + start_evaluation_box[1]
                else:
                    ya_eval = ysz

            vmin, vmax = get_min_max_thresholds(frame_img_reg[yi_eval:ya_eval, xi_eval:xa_eval], disp_res=False)
            fig, ax = subplots(1,1, figsize=(10.0, 11.0*ysz/xsz))
            ax.imshow(frame_img_reg, cmap=&#39;Greys&#39;, vmin=vmin, vmax=vmax)
            ax.grid(True, color = &#34;cyan&#34;)
            ax.set_title(fls[j])
            rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=2.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
            ax.add_patch(rect_patch)
            if save_res_png :
                fig.savefig(os.path.splitext(fls[j])[0]+&#39;_evaluation_box.png&#39;, dpi=300)


    def estimate_SNRs(self, **kwargs):
        &#39;&#39;&#39;
        Estimate SNRs in Image A and Image B based on single-image SNR calculation.

        kwargs
        ---------
        frame_inds : list of int
            List oif frame indecis to use to display the evaluation box.
            Default are [nfrs//10, nfrs//2, nfrs//10*9]
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        sliding_evaluation_box : boolean
            if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
        start_evaluation_box : list of 4 int
            see above
        stop_evaluation_box : list of 4 int
            see above
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        data_dir : str
            data directory (path)
        Sample_ID : str
            Sample ID
        ImgB_fraction : float
            Optional fractional weight of Image B to use for constructing the fused image: FusedImage = ImageA*(1.0-ImgB_fraction) + ImageB*ImgB_fraction
            If not provided, the value determined from rSNR ratios will be used.
        invert_data : boolean
            If True - the data is inverted
        perfrom_transformation : boolean
            If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.
        extrapolate_signal : boolean
            extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.
        flipY : boolean
            If True, the data will be flipped along Y-axis. Default is False.

        &#39;&#39;&#39;
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
        start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
        stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        int_order = kwargs.get(&#34;int_order&#34;, self.int_order)
        invert_data =  kwargs.get(&#34;invert_data&#34;, False)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, False )
        ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, 0.00 )
        flipY = kwargs.get(&#34;flipY&#34;, False)
        pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)
        perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True) and hasattr(self, &#39;tr_matr_cum_residual&#39;)
        extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)

        fls = self.fls
        nfrs = len(fls)
        default_indecis = [nfrs//10, nfrs//2, nfrs//10*9]
        frame_inds = kwargs.get(&#34;frame_inds&#34;, default_indecis)

        test_frame = FIBSEM_frame(fls[0], ftype=ftype)

        xi = 0
        yi = 0
        xsz = test_frame.XResolution
        xa = xi + xsz
        ysz = test_frame.YResolution
        ya = yi + ysz

        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        frame_img = np.zeros((ysz, xsz))
        xSNRAs=[]
        ySNRAs=[]
        rSNRAs=[]
        xSNRBs=[]
        ySNRBs=[]
        rSNRBs=[]

        for j in tqdm(frame_inds, desc=&#39;Analyzing Auto-Correlation SNRs &#39;):

            frame = FIBSEM_frame(fls[j], ftype=ftype)
            if pad_edges and perfrom_transformation:
                shape = [frame.YResolution, frame.XResolution]
                xmn, xmx, ymn, ymx = determine_pad_offsets(shape, self.tr_matr_cum_residual)
                padx = np.int16(xmx - xmn)
                pady = np.int16(ymx - ymn)
                xi = np.int16(np.max([xmx, 0]))
                yi = np.int16(np.max([ymx, 0]))
                # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
                # so that the transformed images are not clipped.
                # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
                # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
                # those are calculated below base on the amount of padding calculated above
                shift_matrix = np.array([[1.0, 0.0, xi],
                                         [0.0, 1.0, yi],
                                         [0.0, 0.0, 1.0]])
                inv_shift_matrix = np.linalg.inv(shift_matrix)
            else:
                padx = 0
                pady = 0
                xi = 0
                yi = 0
                shift_matrix = np.eye(3,3)
                inv_shift_matrix = np.eye(3,3)

            xsz = frame.XResolution + padx
            xa = xi + frame.XResolution
            ysz = frame.YResolution + pady
            ya = yi + frame.YResolution

            xi_eval = xi + evaluation_box[2]
            if evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + evaluation_box[3]
            else:
                xa_eval = xa
            yi_eval = yi + evaluation_box[0]
            if evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + evaluation_box[1]
            else:
                ya_eval = ya

            if sliding_evaluation_box:
                dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
                dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
            else:
                dx_eval = 0
                dy_eval = 0

            frame_imgA = np.zeros((ysz, xsz))
            if self.DetB != &#39;None&#39;:
                frame_imgB = np.zeros((ysz, xsz))

            if invert_data:
                frame_imgA[yi:ya, xi:xa] = np.negative(frame.RawImageA.astype(float))
                if self.DetB != &#39;None&#39;:
                    frame_imgB[yi:ya, xi:xa] = np.negative(frame.RawImageB.astype(float))
            else:
                frame_imgA[yi:ya, xi:xa]  = frame.RawImageA.astype(float)
                if self.DetB != &#39;None&#39;:
                    frame_imgB[yi:ya, xi:xa]  = frame.RawImageB.astype(float)

            if perfrom_transformation:
                transf = ProjectiveTransform(matrix = shift_matrix @ (self.tr_matr_cum_residual[j] @ inv_shift_matrix))
                frame_imgA_reg = warp(frame_imgA, transf, order = int_order, preserve_range=True)
                if self.DetB != &#39;None&#39;:
                    frame_imgB_reg = warp(frame_imgB, transf, order = int_order, preserve_range=True)
            else:
                frame_imgA_reg = frame_imgA.copy()
                if self.DetB != &#39;None&#39;:
                    frame_imgB_reg = frame_imgB.copy()

            if flipY:
                frame_imgA_reg = np.flip(frame_imgA_reg, axis=0)
                if self.DetB != &#39;None&#39;:
                    frame_imgB_reg = np.flip(frame_imgB_reg, axis=0)

            if sliding_evaluation_box:
                xi_eval = start_evaluation_box[2] + dx_eval*j//nfrs
                yi_eval = start_evaluation_box[0] + dy_eval*j//nfrs
                if start_evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + start_evaluation_box[3]
                else:
                    xa_eval = xsz
                if start_evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + start_evaluation_box[1]
                else:
                    ya_eval = ysz

            &#39;&#39;&#39;
            if invert_data:
                if test_frame.EightBit==0:
                    frame_imgA = np.negative(FIBSEM_frame(fls[j], ftype=ftype).RawImageA)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB = np.negative(FIBSEM_frame(fls[j], ftype=ftype).RawImageB)
                else:
                    frame_imgA  =  uint8(255) - FIBSEM_frame(fls[j], ftype=ftype).RawImageA
                    if self.DetB != &#39;None&#39;:
                        frame_imgB  =  uint8(255) - FIBSEM_frame(fls[j], ftype=ftype).RawImageB

            else:
                frame_imgA  = FIBSEM_frame(fls[j], ftype=ftype).RawImageA
                if self.DetB != &#39;None&#39;:
                    frame_imgB  = FIBSEM_frame(fls[j], ftype=ftype).RawImageB

            if flipY:
                frame_imgA = np.flip(frame_imgA, axis=0)
                frame_imgB = np.flip(frame_imgB, axis=0)
            &#39;&#39;&#39;
            frame_imgA_eval = frame_imgA_reg[yi_eval:ya_eval, xi_eval:xa_eval]
            SNR_png = os.path.splitext(os.path.split(fls[j])[1])[0] + &#39;.png&#39;
            SNR_png_fname = os.path.join(data_dir, SNR_png)
            ImageA_xSNR, ImageA_ySNR, ImageA_rSNR= Single_Image_SNR(frame_imgA_eval, extrapolate_signal=extrapolate_signal, save_res_png=save_res_png,
                                                                        res_fname = SNR_png_fname.replace(&#39;.png&#39;, &#39;_ImgA_SNR.png&#39;),
                                                                        img_label=&#39;Image A, frame={:d}&#39;.format(j))
            xSNRAs.append(ImageA_xSNR)
            ySNRAs.append(ImageA_ySNR)
            rSNRAs.append(ImageA_rSNR)
            if self.DetB != &#39;None&#39;:
                frame_imgB_eval = frame_imgB_reg[yi_eval:ya_eval, xi_eval:xa_eval]
                ImageB_xSNR, ImageB_ySNR, ImageB_rSNR = Single_Image_SNR(frame_imgB_eval, extrapolate_signal=extrapolate_signal, save_res_png=save_res_png,
                                                                            res_fname = SNR_png_fname.replace(&#39;.png&#39;, &#39;_ImgB_SNR.png&#39;),
                                                                            img_label=&#39;Image B, frame={:d}&#39;.format(j))
                xSNRBs.append(ImageB_xSNR)
                ySNRBs.append(ImageB_ySNR)
                rSNRBs.append(ImageB_rSNR)

        fig, ax = subplots(1,1, figsize = (6,4))
        ax.plot(frame_inds, xSNRAs, &#39;r+&#39;, label=&#39;Image A x-SNR&#39;)
        ax.plot(frame_inds, ySNRAs, &#39;b+&#39;, label=&#39;Image A y-SNR&#39;)
        ax.plot(frame_inds, rSNRAs, &#39;g+&#39;, label=&#39;Image A r-SNR&#39;)
        if self.DetB != &#39;None&#39;:
            ax.plot(frame_inds, xSNRBs, &#39;rx&#39;, linestyle=&#39;dotted&#39;, label=&#39;Image B x-SNR&#39;)
            ax.plot(frame_inds, ySNRBs, &#39;bx&#39;, linestyle=&#39;dotted&#39;, label=&#39;Image B y-SNR&#39;)
            ax.plot(frame_inds, rSNRBs, &#39;gx&#39;, linestyle=&#39;dotted&#39;, label=&#39;Image B r-SNR&#39;)
            ImgB_fraction_xSNR = np.mean(np.array(xSNRBs)/(np.array(xSNRAs) + np.array(xSNRBs)))
            ImgB_fraction_ySNR = np.mean(np.array(ySNRBs)/(np.array(ySNRAs) + np.array(ySNRBs)))
            ImgB_fraction_rSNR = np.mean(np.array(rSNRBs)/(np.array(rSNRAs) + np.array(rSNRBs)))
            if ImgB_fraction &lt; 1e-9:
                ImgB_fraction = ImgB_fraction_rSNR
            ax.text(0.1, 0.5, &#39;ImgB fraction (x-SNR) = {:.4f}&#39;.format(ImgB_fraction_xSNR), color=&#39;r&#39;, transform=ax.transAxes)
            ax.text(0.1, 0.42, &#39;ImgB fraction (y-SNR) = {:.4f}&#39;.format(ImgB_fraction_ySNR), color=&#39;b&#39;, transform=ax.transAxes)
            ax.text(0.1, 0.34, &#39;ImgB fraction (r-SNR) = {:.4f}&#39;.format(ImgB_fraction_rSNR), color=&#39;g&#39;, transform=ax.transAxes)

            xSNRFs=[]
            ySNRFs=[]
            rSNRFs=[]
            for j in tqdm(frame_inds, desc=&#39;Re-analyzing Auto-Correlation SNRs for fused image&#39;):
                frame = FIBSEM_frame(fls[j], ftype=ftype)
                if pad_edges and perfrom_transformation:
                    shape = [frame.YResolution, frame.XResolution]
                    xmn, xmx, ymn, ymx = determine_pad_offsets(shape, self.tr_matr_cum_residual)
                    padx = np.int16(xmx - xmn)
                    pady = np.int16(ymx - ymn)
                    xi = np.int16(np.max([xmx, 0]))
                    yi = np.int16(np.max([ymx, 0]))
                    # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
                    # so that the transformed images are not clipped.
                    # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
                    # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
                    # those are calculated below base on the amount of padding calculated above
                    shift_matrix = np.array([[1.0, 0.0, xi],
                                             [0.0, 1.0, yi],
                                             [0.0, 0.0, 1.0]])
                    inv_shift_matrix = np.linalg.inv(shift_matrix)
                else:
                    padx = 0
                    pady = 0
                    xi = 0
                    yi = 0
                    shift_matrix = np.eye(3,3)
                    inv_shift_matrix = np.eye(3,3)

                xsz = frame.XResolution + padx
                xa = xi + frame.XResolution
                ysz = frame.YResolution + pady
                ya = yi + frame.YResolution

                xi_eval = xi + evaluation_box[2]
                if evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + evaluation_box[3]
                else:
                    xa_eval = xa
                yi_eval = yi + evaluation_box[0]
                if evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + evaluation_box[1]
                else:
                    ya_eval = ya

                if sliding_evaluation_box:
                    dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
                    dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
                else:
                    dx_eval = 0
                    dy_eval = 0

                frame_imgA = np.zeros((ysz, xsz))
                if self.DetB != &#39;None&#39;:
                    frame_imgB = np.zeros((ysz, xsz))

                if invert_data:
                    frame_imgA[yi:ya, xi:xa] = np.negative(frame.RawImageA.astype(float))
                    if self.DetB != &#39;None&#39;:
                        frame_imgB[yi:ya, xi:xa] = np.negative(frame.RawImageB.astype(float))
                else:
                    frame_imgA[yi:ya, xi:xa]  = frame.RawImageA.astype(float)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB[yi:ya, xi:xa]  = frame.RawImageB.astype(float)

                if perfrom_transformation:
                    transf = ProjectiveTransform(matrix = shift_matrix @ (self.tr_matr_cum_residual[j] @ inv_shift_matrix))
                    frame_imgA_reg = warp(frame_imgA, transf, order = int_order, preserve_range=True)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB_reg = warp(frame_imgB, transf, order = int_order, preserve_range=True)
                else:
                    frame_imgA_reg = frame_imgA.copy()
                    if self.DetB != &#39;None&#39;:
                        frame_imgB_reg = frame_imgB.copy()

                if flipY:
                    frame_imgA_reg = np.flip(frame_imgA_reg, axis=0)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB_reg = np.flip(frame_imgB_reg, axis=0)

                if sliding_evaluation_box:
                    xi_eval = start_evaluation_box[2] + dx_eval*j//nfrs
                    yi_eval = start_evaluation_box[0] + dy_eval*j//nfrs
                    if start_evaluation_box[3] &gt; 0:
                        xa_eval = xi_eval + start_evaluation_box[3]
                    else:
                        xa_eval = xsz
                    if start_evaluation_box[1] &gt; 0:
                        ya_eval = yi_eval + start_evaluation_box[1]
                    else:
                        ya_eval = ysz

                frame_imgA_eval = frame_imgA_reg[yi_eval:ya_eval, xi_eval:xa_eval]
                frame_imgB_eval = frame_imgB_reg[yi_eval:ya_eval, xi_eval:xa_eval]

                frame_imgF_eval = frame_imgA_eval * (1.0 - ImgB_fraction) + frame_imgB_eval * ImgB_fraction
                ImageF_xSNR, ImageF_ySNR, ImageF_rSNR = Single_Image_SNR(frame_imgF_eval, extrapolate_signal=extrapolate_signal, save_res_png=save_res_png,
                                                                        res_fname = SNR_png_fname.replace(&#39;.png&#39;, &#39;_ImgB_fr{:.3f}_SNR.png&#39;.format(ImgB_fraction)),
                                                                        img_label=&#39;Fused, ImB_fr={:.4f}, frame={:d}&#39;.format(ImgB_fraction, j))
                xSNRFs.append(ImageF_xSNR)
                ySNRFs.append(ImageF_ySNR)
                rSNRFs.append(ImageF_rSNR)

            ax.plot(frame_inds, xSNRFs, &#39;rd&#39;, linestyle=&#39;dashed&#39;, label=&#39;Fused Image x-SNR&#39;)
            ax.plot(frame_inds, ySNRFs, &#39;bd&#39;, linestyle=&#39;dashed&#39;, label=&#39;Fused Image y-SNR&#39;)
            ax.plot(frame_inds, rSNRFs, &#39;gd&#39;, linestyle=&#39;dashed&#39;, label=&#39;Fused Image r-SNR&#39;)

        else:
            ImgB_fraction_xSNR = 0.0
            ImgB_fraction_ySNR = 0.0
            ImgB_fraction_rSNR = 0.0
        ax.grid(True)
        ax.legend()
        ax.set_title(Sample_ID + &#39;  &#39; + data_dir, fontsize=8)
        ax.set_xlabel(&#39;Frame&#39;)
        ax.set_ylabel(&#39;SNR&#39;)
        if save_res_png :
            fig_filename = os.path.join(data_dir, os.path.splitext(fnm_reg)[0]+&#39;SNR_evaluation_mult_frame.png&#39;)
            fig.savefig(fig_filename, dpi=300)

        return ImgB_fraction_xSNR, ImgB_fraction_ySNR, ImgB_fraction_rSNR


    def evaluate_ImgB_fractions(self, ImgB_fractions, frame_inds, **kwargs):
        &#39;&#39;&#39;
        Calculate NCC and SNR vs Image B fraction over a set of frames.

        ImgB_fractions : list
            List of fractions to estimate the NCC and SNR
        frame_inds : int array
            array of frame indices to perform NCC / SNR evaluation

        kwargs
        ---------
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        extrapolate_signal : boolean
            extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        data_dir : str
            data directory (path)
        Sample_ID : str
            Sample ID

        invert_data : boolean
            If True - the data is inverted
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.


        Returns
        SNRimpr_max_position, SNRimpr_max, ImgB_fractions, SNRs
        &#39;&#39;&#39;
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        if hasattr(self, &#39;invert_data&#39;):
            invert_data = kwargs.get(&#34;invert_data&#34;, self.invert_data)
        else:
            invert_data = kwargs.get(&#34;invert_data&#34;, False)
        if hasattr(self, &#39;flipY&#39;):
            flipY = kwargs.get(&#34;flipY&#34;, self.flipY)
        else:
            flipY = kwargs.get(&#34;flipY&#34;, flipY)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, False )
        extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)

        nbr = len(ImgB_fractions)
        kwargs[&#39;zbin_factor&#39;] = 1

        test_frame = FIBSEM_frame(self.fls[frame_inds[0]])

        xi = 0
        yi = 0
        xsz = test_frame.XResolution
        xa = xi + xsz
        ysz = test_frame.YResolution
        ya = yi + ysz

        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        br_results = []
        xSNRFs=[]
        ySNRFs=[]
        rSNRFs=[]

        for ImgB_fraction in tqdm(ImgB_fractions, desc=&#39;Evaluating Img B fractions&#39;):
            kwargs[&#39;ImgB_fraction&#39;] = ImgB_fraction
            kwargs[&#39;disp_res&#39;] = False
            kwargs[&#39;evaluation_box&#39;] = evaluation_box
            kwargs[&#39;flipY&#39;] = flipY
            kwargs[&#39;invert_data&#39;] = invert_data
            DASK_client = &#39;&#39;
            kwargs[&#39;disp_res&#39;] = False
            br_res, br_res_xlsx = self.transform_and_save(DASK_client,
                                                                save_transformed_dataset=False,
                                                                save_registration_summary=False,
                                                                frame_inds=frame_inds,
                                                                use_DASK=False,
                                                                save_sample_frames_png=False,
                                                                eval_metrics = [&#39;NCC&#39;],
                                                                **kwargs)
            br_results.append(br_res)

            if invert_data:
                if test_frame.EightBit==0:
                    frame_imgA = np.negative(test_frame.RawImageA)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB = np.negative(test_frame.RawImageB)
                else:
                    frame_imgA  =  uint8(255) - test_frame.RawImageA
                    if self.DetB != &#39;None&#39;:
                        frame_imgB  =  uint8(255) - test_frame.RawImageB
            else:
                frame_imgA  = test_frame.RawImageA
                if self.DetB != &#39;None&#39;:
                    frame_imgB  = test_frame.RawImageB
            if flipY:
                frame_imgA = np.flip(frame_imgA, axis=0)
                frame_imgB = np.flip(frame_imgB, axis=0)

            frame_imgA_eval = frame_imgA[yi_eval:ya_eval, xi_eval:xa_eval]
            frame_imgB_eval = frame_imgB[yi_eval:ya_eval, xi_eval:xa_eval]

            frame_imgF_eval = frame_imgA_eval * (1.0 - ImgB_fraction) + frame_imgB_eval * ImgB_fraction
            ImageF_xSNR, ImageF_ySNR, ImageF_rSNR = Single_Image_SNR(frame_imgF_eval,
                                                                    extrapolate_signal=extrapolate_signal,
                                                                    disp_res=False,
                                                                    save_res_png=False,
                                                                    res_fname = &#39;&#39;,
                                                                    img_label=&#39;&#39;)
            xSNRFs.append(ImageF_xSNR)
            ySNRFs.append(ImageF_ySNR)
            rSNRFs.append(ImageF_rSNR)

        fig, axs = subplots(4,1, figsize=(6,11))
        fig.subplots_adjust(left=0.12, bottom=0.06, right=0.99, top=0.96, wspace=0.25, hspace=0.24)
        try:
            ncc0 = (br_results[0])[&#39;NCC&#39;]
        except:
            ncc0 = (br_results[0])[&#39;Image NCC&#39;]
        SNR0 = ncc0 / (1-ncc0)
        SNRimpr_cc = []
        SNRs = []

        for j, (ImgB_fraction, br_result) in enumerate(zip(ImgB_fractions, br_results)):
            my_col = get_cmap(&#34;gist_rainbow_r&#34;)((nbr-j)/(nbr-1))
            try:
                ncc = br_result[&#39;NCC&#39;]
            except:
                ncc = br_result[&#39;Image NCC&#39;]
            SNR = ncc / (1.0-ncc)
            frames_local = br_result[&#39;Frame&#39;]
            axs[0].plot(frames_local, SNR, color=my_col, label = &#39;ImgB fraction = {:.2f}&#39;.format(ImgB_fraction))
            axs[1].plot(frames_local, SNR/SNR0, color=my_col, label = &#39;ImgB fraction = {:.2f}&#39;.format(ImgB_fraction))
            SNRimpr_cc.append(np.mean(SNR/SNR0))
            SNRs.append(np.mean(SNR))

        SNRimpr_ac = np.array(rSNRFs) / rSNRFs[0]

        SNRimpr_cc_max = np.max(SNRimpr_cc)
        SNRimpr_cc_max_ind = np.argmax(SNRimpr_cc)
        ImgB_fraction_max = ImgB_fractions[SNRimpr_cc_max_ind]
        xi = max(0, (SNRimpr_cc_max_ind-3))
        xa = min((SNRimpr_cc_max_ind+3), len(ImgB_fractions))
        ImgB_fr_range = ImgB_fractions[xi : xa]
        SNRimpr_cc_range = SNRimpr_cc[xi : xa]
        popt = np.polyfit(ImgB_fr_range, SNRimpr_cc_range, 2)
        SNRimpr_cc_fit_max_pos = -0.5 * popt[1] / popt[0]
        ImgB_fr_fit_cc = np.linspace(ImgB_fr_range[0], ImgB_fr_range[-1], 21)
        SNRimpr_cc_fit = np.polyval(popt, ImgB_fr_fit_cc)
        if popt[0] &lt; 0 and SNRimpr_cc_fit_max_pos &gt; ImgB_fractions[0] and SNRimpr_cc_fit_max_pos&lt;ImgB_fractions[-1]:
            SNRimpr_cc_max_position = SNRimpr_cc_fit_max_pos
            SNRimpr_cc_max = np.polyval(popt, SNRimpr_cc_max_position)
        else:
            SNRimpr_cc_max_position = ImgB_fraction_max

        SNRimpr_ac_max = np.max(SNRimpr_ac)
        SNRimpr_ac_max_ind = np.argmax(SNRimpr_ac)
        ImgB_fraction_max = ImgB_fractions[SNRimpr_ac_max_ind]
        xi = max(0, (SNRimpr_ac_max_ind-3))
        xa = min((SNRimpr_ac_max_ind+3), len(ImgB_fractions))
        ImgB_fr_range = ImgB_fractions[xi : xa]
        SNRimpr_ac_range = SNRimpr_ac[xi : xa]
        popt = np.polyfit(ImgB_fr_range, SNRimpr_ac_range, 2)
        SNRimpr_ac_fit_max_pos = -0.5 * popt[1] / popt[0]
        ImgB_fr_fit_ac = np.linspace(ImgB_fr_range[0], ImgB_fr_range[-1], 21)
        SNRimpr_ac_fit = np.polyval(popt, ImgB_fr_fit_ac)
        if popt[0] &lt; 0 and SNRimpr_ac_fit_max_pos &gt; ImgB_fractions[0] and SNRimpr_ac_fit_max_pos&lt;ImgB_fractions[-1]:
            SNRimpr_ac_max_position = SNRimpr_ac_fit_max_pos
            SNRimpr_ac_max = np.polyval(popt, SNRimpr_ac_max_position)
        else:
            SNRimpr_ac_max_position = ImgB_fraction_max

        fs=10
        axs[0].grid(True)
        axs[0].set_ylabel(&#39;Frame-to-Frame SNR&#39;, fontsize=fs)
        axs[0].set_xlabel(&#39;Frame&#39;, fontsize=fs)
        axs[0].legend(fontsize=fs-1)
        axs[0].set_title(Sample_ID + &#39;  &#39; + data_dir, fontsize=fs)
        axs[1].grid(True)
        axs[1].set_ylabel(&#39;Frame-to-Frame SNR Improvement&#39;, fontsize=fs)
        axs[1].set_xlabel(&#39;Frame&#39;, fontsize=fs)

        axs[2].plot(ImgB_fractions, rSNRFs, &#39;rd&#39;, label=&#39;Data (auto-correlation)&#39;)
        axs[2].grid(True)
        axs[2].set_ylabel(&#39;Auto-Corr SNR&#39;, fontsize=fs)

        axs[3].plot(ImgB_fractions, SNRimpr_cc, &#39;cs&#39;, label=&#39;Data (cross-corr.)&#39;)
        axs[3].plot(ImgB_fr_fit_cc, SNRimpr_cc_fit, &#39;b&#39;, label=&#39;Fit (cross-corr.)&#39;)
        axs[3].plot(SNRimpr_cc_max_position, SNRimpr_cc_max, &#39;bx&#39;, markersize=10, label=&#39;Max SNR Impr. (cc)&#39;)
        axs[3].text(0.4, 0.35, &#39;Max CC SNR Improvement={:.3f}&#39;.format(SNRimpr_cc_max), transform=axs[3].transAxes, fontsize=fs)
        axs[3].text(0.4, 0.25, &#39;@ Img B Fraction ={:.3f}&#39;.format(SNRimpr_cc_max_position), transform=axs[3].transAxes, fontsize=fs)
        axs[3].plot(ImgB_fractions, SNRimpr_ac, &#39;rd&#39;, label=&#39;Data (auto-corr.)&#39;)
        axs[3].plot(ImgB_fr_fit_ac, SNRimpr_ac_fit, &#39;magenta&#39;, label=&#39;Fit (auto-corr.)&#39;)
        axs[3].plot(SNRimpr_ac_max_position, SNRimpr_ac_max, &#39;mx&#39;, markersize=10, label=&#39;Max SNR Impr. (ac)&#39;)
        axs[3].text(0.4, 0.15, &#39;Max AC SNR Improvement={:.3f}&#39;.format(SNRimpr_ac_max), transform=axs[3].transAxes, fontsize=fs)
        axs[3].text(0.4, 0.05, &#39;@ Img B Fraction ={:.3f}&#39;.format(SNRimpr_ac_max_position), transform=axs[3].transAxes, fontsize=fs)

        axs[3].legend(fontsize=fs-2, loc=&#39;upper left&#39;)
        axs[3].grid(True)
        axs[3].set_ylabel(&#39;Mean SNR improvement&#39;, fontsize=fs)
        axs[3].set_xlabel(&#39;Image B fraction&#39;, fontsize=fs)

        if save_res_png :
            fname_image = os.path.join(data_dir, os.path.splitext(fnm_reg)[0]+&#39;_SNR_vs_ImgB_ratio_evaluation.png&#39;)
            fig.savefig(fname_image, dpi=300)

        return SNRimpr_cc_max_position, SNRimpr_cc_max, ImgB_fractions, SNRs, rSNRFs</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.Perform_2D_fit"><code class="name flex">
<span>def <span class="ident">Perform_2D_fit</span></span>(<span>img, estimator, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Bin the image and then perform 2D polynomial (currently only 2D parabolic) fit on the binned image.
©G.Shtengel 04/2022 gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>2D array</code></dt>
<dd>original image</dd>
<dt><strong><code>estimator</code></strong> :&ensp;<code>RANSACRegressor(),</code></dt>
<dd>LinearRegression(),
TheilSenRegressor(),
HuberRegressor()</dd>
<dt>kwargs:</dt>
<dt><strong><code>image_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Image name (for display purposes)</dd>
<dt><strong><code>bins</code></strong> :&ensp;<code>int</code></dt>
<dd>binsize for image binning. If not provided, bins=10</dd>
<dt><strong><code>Analysis_ROIs</code></strong> :&ensp;<code>list</code> of <code>lists: [[left, right, top, bottom]]</code></dt>
<dd>list of coordinates (indices) for each of the ROI's - the boundaries of the image subset to evaluate the parabolic fit.</dd>
<dt><strong><code>calc_corr</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True - the full image correction is calculated</dd>
<dt>ignore_Y
: boolean</dt>
<dt>If True - the parabolic fit to only X is perfromed</dt>
<dt><strong><code>disp_res</code></strong> :&ensp;<code>boolean</code></dt>
<dd>(default is False) - to plot/ display the results</dd>
<dt><strong><code>save_res_png</code></strong> :&ensp;<code>boolean</code></dt>
<dd>save the analysis output into a PNG file (default is False)</dd>
<dt><strong><code>res_fname</code></strong> :&ensp;<code>string</code></dt>
<dd>filename for the result image ('Image_Flattening.png')</dd>
<dt><strong><code>Xsect</code></strong> :&ensp;<code>int</code></dt>
<dd>X - coordinate for Y-crossection</dd>
<dt><strong><code>Ysect</code></strong> :&ensp;<code>int</code></dt>
<dd>Y - coordinate for X-crossection</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt>Returns:</dt>
<dt><strong><code>intercept</code></strong>, <strong><code>coefs</code></strong>, <strong><code>mse</code></strong>, <strong><code>img_correction_array</code></strong></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Perform_2D_fit(img, estimator, **kwargs):
    &#39;&#39;&#39;
    Bin the image and then perform 2D polynomial (currently only 2D parabolic) fit on the binned image.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Parameters
    ----------
    img : 2D array
        original image
    estimator : RANSACRegressor(),
                LinearRegression(),
                TheilSenRegressor(),
                HuberRegressor()
    kwargs:
    image_name : str
        Image name (for display purposes)
    bins : int
        binsize for image binning. If not provided, bins=10
    Analysis_ROIs : list of lists: [[left, right, top, bottom]]
        list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the parabolic fit.
    calc_corr : boolean
        If True - the full image correction is calculated
    ignore_Y  : boolean
        If True - the parabolic fit to only X is perfromed
    disp_res : boolean
        (default is False) - to plot/ display the results
    save_res_png : boolean
        save the analysis output into a PNG file (default is False)
    res_fname : string
        filename for the result image (&#39;Image_Flattening.png&#39;)
    Xsect : int
        X - coordinate for Y-crossection
    Ysect : int
        Y - coordinate for X-crossection
    dpi : int

    Returns:
    intercept, coefs, mse, img_correction_array
    &#39;&#39;&#39;
    image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
    ysz, xsz = img.shape
    calc_corr = kwargs.get(&#34;calc_corr&#34;, False)
    ignore_Y = kwargs.get(&#34;ignore_Y&#34;, False)
    Xsect = kwargs.get(&#34;Xsect&#34;, xsz//2)
    Ysect = kwargs.get(&#34;Ysect&#34;, ysz//2)
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    bins = kwargs.get(&#34;bins&#34;, 10) #bins = 10
    Analysis_ROIs = kwargs.get(&#34;Analysis_ROIs&#34;, [])
    save_res_png = kwargs.get(&#34;save_res_png&#34;, False)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;Image_Flattening.png&#39;)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    img_binned = img[0:ysz//bins*bins, 0:xsz//bins*bins].astype(float).reshape(ysz//bins, bins, xsz//bins, bins).sum(3).sum(1)/bins/bins
    if len(Analysis_ROIs)&gt;0:
            Analysis_ROIs_binned = [[ind//bins for ind in Analysis_ROI] for Analysis_ROI in Analysis_ROIs]
    else:
        Analysis_ROIs_binned = []
    vmin, vmax = get_min_max_thresholds(img_binned, disp_res=False)
    yszb, xszb = img_binned.shape
    yb, xb = np.indices(img_binned.shape)
    if len(Analysis_ROIs_binned)&gt;0:
        img_1D_list = []
        xb_1d_list = []
        yb_1d_list = []
        for Analysis_ROI_binned in Analysis_ROIs_binned:
            #Analysis_ROI : list of [left, right, top, bottom]
            img_1D_list = img_1D_list + img_binned[Analysis_ROI_binned[2]:Analysis_ROI_binned[3], Analysis_ROI_binned[0]:Analysis_ROI_binned[1]].ravel().tolist()
            xb_1d_list  = xb_1d_list + xb[Analysis_ROI_binned[2]:Analysis_ROI_binned[3], Analysis_ROI_binned[0]:Analysis_ROI_binned[1]].ravel().tolist()
            yb_1d_list  = yb_1d_list + yb[Analysis_ROI_binned[2]:Analysis_ROI_binned[3], Analysis_ROI_binned[0]:Analysis_ROI_binned[1]].ravel().tolist()
        img_1D = np.array(img_1D_list)
        xb_1d = np.array(xb_1d_list)
        yb_1d = np.array(yb_1d_list)
    else:
        img_1D = img_binned.ravel()
        xb_1d = xb.ravel()
        yb_1d = yb.ravel()

    img_binned_1D = img_binned.ravel()
    X_binned = np.vstack((xb.ravel(), yb.ravel())).T
    X = np.vstack((xb_1d, yb_1d)).T

    ysz, xsz = img.shape
    yf, xf = np.indices(img.shape)
    xf_1d = xf.ravel()/bins
    yf_1d = yf.ravel()/bins
    Xf = np.vstack((xf_1d, yf_1d)).T

    model = make_pipeline(PolynomialFeatures(2), estimator)

    if ignore_Y:
        ymean = np.mean(yb_1d)
        yb_1d_flat = yb_1d*0.0+ymean
        X_yflat = np.vstack((xb_1d, yb_1d_flat)).T
        model.fit(X_yflat, img_1D)

    else:
        model.fit(X, img_1D)

    model = make_pipeline(PolynomialFeatures(2), estimator)
    model.fit(X, img_1D)
    if hasattr(model[-1], &#39;estimator_&#39;):
        if ignore_Y:
            model[-1].estimator_.coef_[0] = model[-1].estimator_.coef_[0] + model[-1].estimator_.coef_[2]*ymean + model[-1].estimator_.coef_[5]*ymean*ymean
            model[-1].estimator_.coef_[1] = model[-1].estimator_.coef_[1] + model[-1].estimator_.coef_[4]*ymean
            model[-1].estimator_.coef_[2] = 0.0
            model[-1].estimator_.coef_[4] = 0.0
            model[-1].estimator_.coef_[5] = 0.0
        coefs = model[-1].estimator_.coef_
        intercept = model[-1].estimator_.intercept_
    else:
        if ignore_Y:
            model[-1].coef_[0] = model[-1].coef_[0] + model[-1].coef_[2]*ymean + model[-1].coef_[5]*ymean*ymean
            model[-1].coef_[1] = model[-1].coef_[1] + model[-1].coef_[4]*ymean
            model[-1].coef_[2] = 0.0
            model[-1].coef_[4] = 0.0
            model[-1].coef_[5] = 0.0
        coefs = model[-1].coef_
        intercept = model[-1].intercept_
    img_fit_1d = model.predict(X)
    scr = model.score(X, img_1D)
    mse = mean_squared_error(img_fit_1d, img_1D)
    img_fit = model.predict(X_binned).reshape(yszb, xszb)
    if calc_corr:
        img_correction_array = np.mean(img_fit_1d) / model.predict(Xf).reshape(ysz, xsz)
    else:
        img_correction_array = img * 0.0

    if disp_res:
        print(&#39;Estimator coefficients ( 1  x  y  x^2  x*y  y^2): &#39;, coefs)
        print(&#39;Estimator intercept: &#39;, intercept)

        fig, axs = subplots(2,2, figsize = (12, 8))
        axs[0, 0].imshow(img_binned, cmap=&#39;Greys&#39;, vmin=vmin, vmax=vmax)
        axs[0, 0].grid(True)
        axs[0, 0].plot([Xsect//bins, Xsect//bins], [0, yszb], &#39;lime&#39;, linewidth = 0.5)
        axs[0, 0].plot([0, xszb], [Ysect//bins, Ysect//bins], &#39;cyan&#39;, linewidth = 0.5)
        if len(Analysis_ROIs_binned)&gt;0:
            col_ROIs = &#39;yellow&#39;
            axs[0, 0].text(0.3, 0.9, &#39;with Analysis ROIs&#39;, color=col_ROIs, transform=axs[0, 0].transAxes)
            for Analysis_ROI_binned in Analysis_ROIs_binned:
            #Analysis_ROI : list of [left, right, top, bottom]
                xi, xa, yi, ya = Analysis_ROI_binned
                ROI_patch = patches.Rectangle((xi,yi),abs(xa-xi)-2,abs(ya-yi)-2, linewidth=0.75, edgecolor=col_ROIs,facecolor=&#39;none&#39;)
                axs[0, 0].add_patch(ROI_patch)

        axs[0, 0].set_xlim((0, xszb))
        axs[0, 0].set_ylim((yszb, 0))
        axs[0, 0].set_title(&#39;{:d}-x Binned Raw:&#39;.format(bins) + image_name)

        axs[0, 1].imshow(img_fit, cmap=&#39;Greys&#39;, vmin=vmin, vmax=vmax)
        axs[0, 1].grid(True)
        axs[0, 1].plot([Xsect//bins, Xsect//bins], [0, yszb], &#39;lime&#39;, linewidth = 0.5)
        axs[0, 1].plot([0, xszb], [Ysect//bins, Ysect//bins], &#39;cyan&#39;, linewidth = 0.5)
        axs[0, 1].set_xlim((0, xszb))
        axs[0, 1].set_ylim((yszb,0))
        axs[0, 1].set_title(&#39;{:d}-x Binned Fit: &#39;.format(bins) + image_name)

        axs[1, 0].plot(img[Ysect, :],&#39;b&#39;, label = image_name, linewidth =0.5)
        axs[1, 0].plot(xb[0,:]*bins, img_binned[Ysect//bins, :],&#39;cyan&#39;, label = &#39;Binned &#39;+ image_name)
        axs[1, 0].plot(xb[0,:]*bins, img_fit[Ysect//bins, :], &#39;yellow&#39;, linewidth=4, linestyle=&#39;--&#39;, label = &#39;Fit: &#39;+ image_name)
        axs[1, 0].legend()
        axs[1, 0].grid(True)
        axs[1, 0].set_xlabel(&#39;X-coordinate&#39;)

        axs[1, 1].plot(img[:, Xsect],&#39;g&#39;, label = image_name, linewidth =0.5)
        axs[1, 1].plot(yb[:, 0]*bins, img_binned[:, Xsect//bins],&#39;lime&#39;, label = &#39;Binned &#39;+image_name)
        axs[1, 1].plot(yb[:, 0]*bins, img_fit[:, Xsect//bins], &#39;yellow&#39;, linewidth=4, linestyle=&#39;--&#39;, label = &#39;Fit: &#39;+ image_name)
        axs[1, 1].legend()
        axs[1, 1].grid(True)
        axs[1, 1].set_xlabel(&#39;Y-coordinate&#39;)
        if save_res_png:
            fig.savefig(res_fname, dpi=dpi)
            print(&#39;results saved into the file: &#39;+res_fname)
    return intercept, coefs, mse, img_correction_array</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.SIFT_evaluation_dataset"><code class="name flex">
<span>def <span class="ident">SIFT_evaluation_dataset</span></span>(<span>fs, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate SIFT settings and perfromance of few test frames (fs). ©G.Shtengel 10/2021 gleb.shtengel@gmail.com</p>
<p>Parameters:
fs : array of str
filenames for the data frames to be used for SIFT evaluation</p>
<h2 id="kwargs">Kwargs</h2>
<p>data_dir : str
data directory (path)
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
fnm_reg : str
filename for the final registed dataset
threshold_min : float
CDF threshold for determining the minimum data value
threshold_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF
evaluation_box : list of 4 int
evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
TransformType : object reference
Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
Choose from the following options:
ShiftTransform - only x-shift and y-shift
XScaleShiftTransform
-
x-scale, x-shift, y-shift
ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
AffineTransform -
full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
l2_matrix : 2D float array
matrix of regularization (shrinkage) parameters
targ_vector = 1D float array
target vector for regularization
solver : str
Solver used for SIFT ('RANSAC' or 'LinReg')
drmax : float
In the case of 'RANSAC' - Maximum distance for a data point to be classified as an inlier.
In the case of 'LinReg' - outlier threshold for iterative regression
max_iter : int
Max number of iterations in the iterative procedure above (RANSAC or LinReg)
BFMatcher : boolean
If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
save_matches : boolean
If True, matches will be saved into individual files
kp_max_num : int
Max number of key-points to be matched.
Key-points in every frame are indexed (in descending order) by the strength of the response.
Only kp_max_num is kept for further processing.
Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
SIFT_nfeatures : int
SIFT libary default is 0. The number of best features to retain.
The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
SIFT_nOctaveLayers : int
SIFT libary default
is 3. The number of layers in each octave.
3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
SIFT_contrastThreshold : double
SIFT libary default
is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
The larger the threshold, the less features are produced by the detector.
The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
When nOctaveLayers is set to default and if you want to use the value used in
D. Lowe paper (0.03), set this argument to 0.09.
SIFT_edgeThreshold : double
SIFT libary default
is 10. The threshold used to filter out edge-like features.
Note that the its meaning is different from the contrastThreshold,
i.e. the larger the edgeThreshold, the less features are filtered out
(more features are retained).
SIFT_sigma : double
SIFT library default is 1.6.
The sigma of the Gaussian applied to the input image at the octave #0.
If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check</p>
<p>Returns:
dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def SIFT_evaluation_dataset(fs, **kwargs):
    &#39;&#39;&#39;
    Evaluate SIFT settings and perfromance of few test frames (fs). ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

    Parameters:
    fs : array of str
        filenames for the data frames to be used for SIFT evaluation

    kwargs
    ---------
    data_dir : str
        data directory (path)
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    fnm_reg : str
        filename for the final registed dataset
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
    TransformType : object reference
        Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
        Choose from the following options:
            ShiftTransform - only x-shift and y-shift
            XScaleShiftTransform  -  x-scale, x-shift, y-shift
            ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
            AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
            RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
    l2_matrix : 2D float array
        matrix of regularization (shrinkage) parameters
    targ_vector = 1D float array
        target vector for regularization
    solver : str
        Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
    drmax : float
        In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
        In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
    max_iter : int
        Max number of iterations in the iterative procedure above (RANSAC or LinReg)
    BFMatcher : boolean
        If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches : boolean
        If True, matches will be saved into individual files
    kp_max_num : int
        Max number of key-points to be matched.
        Key-points in every frame are indexed (in descending order) by the strength of the response.
        Only kp_max_num is kept for further processing.
        Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
    SIFT_nfeatures : int
        SIFT libary default is 0. The number of best features to retain.
        The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
    SIFT_nOctaveLayers : int
        SIFT libary default  is 3. The number of layers in each octave.
        3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
    SIFT_contrastThreshold : double
        SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
        The larger the threshold, the less features are produced by the detector.
        The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
        When nOctaveLayers is set to default and if you want to use the value used in
        D. Lowe paper (0.03), set this argument to 0.09.
    SIFT_edgeThreshold : double
        SIFT libary default  is 10. The threshold used to filter out edge-like features.
        Note that the its meaning is different from the contrastThreshold,
        i.e. the larger the edgeThreshold, the less features are filtered out
        (more features are retained).
    SIFT_sigma : double
        SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
        If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check

    Returns:
    dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts
    &#39;&#39;&#39;
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)   # threshold for Lowe&#39;s Ratio Test
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)
    SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.04)

    frame = FIBSEM_frame(fs[0], ftype=ftype)
    if ftype == 0:
        if frame.FileVersion &gt; 8 :
            Sample_ID = frame.Sample_ID.strip(&#39;\x00&#39;)
        else:
            Sample_ID = frame.Notes[0:16]
    else:
        Sample_ID = frame.Sample_ID
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, Sample_ID)

    print(Sample_ID)

    #if save_res_png :
    #    frame.display_images()

    img = np.ravel(frame.RawImageA)
    fsz=12
    fszl=11
    dmin, dmax = frame.get_image_min_max(image_name = &#39;RawImageA&#39;, thr_min=threshold_min, thr_max=threshold_max, nbins=nbins)
    xi = dmin-(np.abs(dmax-dmin)/10)
    xa = dmax+(np.abs(dmax-dmin)/10)

    fig, axs = subplots(2,2, figsize=(12,8))
    fig.suptitle(Sample_ID + &#39;,  thr_min={:.0e}, thr_max={:.0e}, contrastThreshold={:.3f}, kp_max_num={:d}&#39;.format(threshold_min, threshold_max, SIFT_contrastThreshold, kp_max_num), fontsize=fszl)

    hist, bins, patches = axs[0,0].hist(img, bins = nbins)
    axs[0,0].set_xlim(xi, xa)
    axs[0,0].plot([dmin, dmin], [0, np.max(hist)], &#39;r&#39;, linestyle = &#39;--&#39;)
    axs[0,0].plot([dmax, dmax], [0, np.max(hist)], &#39;g&#39;, linestyle = &#39;--&#39;)
    axs[0,0].set_ylabel(&#39;Count&#39;, fontsize = fsz)
    pdf = hist / (frame.XResolution * frame.YResolution)
    cdf = np.cumsum(pdf)
    xCDF = bins[0:-1]+(bins[1]-bins[0])/2.0
    xthr = [xCDF[0], xCDF[-1]]
    ythr_min = [threshold_min, threshold_min]
    y1thr_max = [1-threshold_max, 1-threshold_max]

    axs[1,0].plot(xCDF, cdf, label=&#39;CDF&#39;)
    axs[1,0].plot(xthr, ythr_min, &#39;r&#39;, label=&#39;thr_min={:.5f}&#39;.format(threshold_min))
    axs[1,0].plot([dmin, dmin], [0, 1], &#39;r&#39;, linestyle = &#39;--&#39;, label = &#39;data_min={:.1f}&#39;.format(dmin))
    axs[1,0].plot(xthr, y1thr_max, &#39;g&#39;, label=&#39;1.0 - thr_max = {:.5f}&#39;.format(1-threshold_max))
    axs[1,0].plot([dmax, dmax], [0, 1], &#39;g&#39;, linestyle = &#39;--&#39;, label = &#39;data_max={:.1f}&#39;.format(dmax))
    axs[1,0].set_xlabel(&#39;Intensity Level&#39;, fontsize = fsz)
    axs[1,0].set_ylabel(&#39;CDF&#39;, fontsize = fsz)
    axs[1,0].set_xlim(xi, xa)
    axs[1,0].legend(loc=&#39;center&#39;, fontsize=fsz)
    axs[0,0].set_title(&#39;Data Min and Max with thr_min={:.0e},  thr_max={:.0e}&#39;.format(threshold_min, threshold_max), fontsize = fsz)

    minmax = []
    for f in fs:
        minmax.append(FIBSEM_frame(f, ftype=ftype).get_image_min_max(image_name = &#39;RawImageA&#39;, thr_min=threshold_min, thr_max=threshold_max, nbins=nbins))
    dmin = np.min(np.array(minmax))
    dmax = np.max(np.array(minmax))
    #print(&#39;data range: &#39;, dmin, dmax)

    t0 = time.time()
    #print(&#39;File1: &#39;,fs[0], dmin, dmax, kwargs)
    params1 = [fs[0], dmin, dmax, kwargs]
    fnm_1 = extract_keypoints_descr_files(params1)
    #print(&#39;File2: &#39;,fs[1], dmin, dmax, kwargs)
    params2 = [fs[1], dmin, dmax, kwargs]
    fnm_2 = extract_keypoints_descr_files(params2)

    params_dsf = [fnm_1, fnm_2, kwargs]
    transform_matrix, fnm_matches, kpts, error_abs_mean, iteration = determine_transformations_files(params_dsf)
    n_matches = len(kpts[0])

    src_pts_filtered, dst_pts_filtered = kpts

    src_pts_transformed = src_pts_filtered @ transform_matrix[0:2, 0:2].T + transform_matrix[0:2, 2]
    xshifts = (dst_pts_filtered - src_pts_transformed)[:,0]
    yshifts = (dst_pts_filtered - src_pts_transformed)[:,1]

    t1 = time.time()
    comp_time = (t1-t0)
    #print(&#39;Time to compute: {:.1f}sec&#39;.format(comp_time))

    axx = axs[0,1]
    hst = axx.hist(xshifts, bins=64)
    axx.set_xlabel(&#39;SIFT: X Error (pixels)&#39;)
    axx.text(0.05, 0.9, &#39;mean={:.3f}&#39;.format(np.mean(xshifts)), transform=axx.transAxes, fontsize=fsz)
    axx.text(0.05, 0.8, &#39;median={:.3f}&#39;.format(np.median(xshifts)), transform=axx.transAxes, fontsize=fsz)
    axx.set_title(&#39;data range: {:.1f} ÷ {:.1f}&#39;.format(dmin, dmax), fontsize=fsz)
    axy = axs[1,1]
    hst = axy.hist(yshifts, bins=64)
    axy.set_xlabel(&#39;SIFT: Y Error (pixels)&#39;)
    axy.text(0.05, 0.9, &#39;mean={:.3f}&#39;.format(np.mean(yshifts)), transform=axy.transAxes, fontsize=fsz)
    axy.text(0.05, 0.8, &#39;median={:.3f}&#39;.format(np.median(yshifts)), transform=axy.transAxes, fontsize=fsz)
    axt=axx  # print Transformation Matrix data over axx plot
    axt.text(0.65, 0.8, &#39;Transf. Matrix:&#39;, transform=axt.transAxes, fontsize=fsz)
    axt.text(0.55, 0.7, &#39;{:.4f} {:.4f} {:.4f}&#39;.format(transform_matrix[0,0], transform_matrix[0,1], transform_matrix[0,2]), transform=axt.transAxes, fontsize=fsz-1)
    axt.text(0.55, 0.6, &#39;{:.4f} {:.4f} {:.4f}&#39;.format(transform_matrix[1,0], transform_matrix[1,1], transform_matrix[1,2]), transform=axt.transAxes, fontsize=fsz-1)

    for ax in ravel(axs):
        ax.grid(True)

    fig.suptitle(Sample_ID + &#39;,  thr_min={:.0e}, thr_max={:.0e}, kp_max_num={:d}, comp.time={:.1f}sec&#39;.format(threshold_min, threshold_max, kp_max_num, comp_time), fontsize=fszl)

    if TransformType == RegularizedAffineTransform:
        tstr = [&#39;{:d}&#39;.format(x) for x in targ_vector]
        otext =  TransformType.__name__ + &#39;, λ= {:.1e}, t=[&#39;.format(l2_matrix[0,0]) + &#39;, &#39;.join(tstr) + &#39;], &#39; + solver + &#39;, #of matches={:d}&#39;.format(n_matches)
    else:
        otext = TransformType.__name__ + &#39;, &#39; + solver + &#39;, #of matches={:d}&#39;.format(n_matches)

    axs[0,0].text(0.01, 1.14, otext, fontsize=fszl, transform=axs[0,0].transAxes)
    if save_res_png :
        png_name = os.path.splitext(fs[0])[0] + &#39;_SIFT_eval_&#39;+TransformType.__name__ + &#39;_&#39; + solver +&#39;_thr_min{:.5f}_thr_max{:.5f}.png&#39;.format(threshold_min, threshold_max)
        fig.savefig(png_name, dpi=300)

    xfsz = np.int(7 * frame.XResolution / np.max([frame.XResolution, frame.YResolution]))+1
    yfsz = np.int(7 * frame.YResolution / np.max([frame.XResolution, frame.YResolution]))+2
    fig2, ax = subplots(1,1, figsize=(xfsz,yfsz))
    fig2.subplots_adjust(left=0.0, bottom=0.25*(1-frame.YResolution/frame.XResolution), right=1.0, top=1.0)
    symsize = 2
    fsize = 12
    img2 = FIBSEM_frame(fs[-1], ftype=ftype).RawImageA
    ax.imshow(img2, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
    ax.axis(False)
    x, y = dst_pts_filtered.T
    M = sqrt(xshifts*xshifts+yshifts*yshifts)
    xs = xshifts
    ys = yshifts

    # the code below is for vector map. vectors have origin coordinates x and y, and vector projections xs and ys.
    vec_field = ax.quiver(x,y,xs,ys,M, scale=40, width =0.003, cmap=&#39;jet&#39;)
    cbar = fig2.colorbar(vec_field, pad=0.05, shrink=0.70, orientation = &#39;horizontal&#39;, format=&#34;%.1f&#34;)
    cbar.set_label(&#39;SIFT Shift Amplitude (pix)&#39;, fontsize=fsize)

    ax.text(0.01, 1.1-0.13*frame.YResolution/frame.XResolution, Sample_ID + &#39;, thr_min={:.0e}, thr_max={:.0e}, kp_max_num={:d},  #of matches={:d}&#39;.format(threshold_min, threshold_max, kp_max_num, n_matches), fontsize=fsize, transform=ax.transAxes)

    if save_res_png :
        fig2_fnm = os.path.join(data_dir, &#39;SIFT_vmap_&#39;+TransformType.__name__ + &#39;_&#39; + solver +&#39;_thr_min{:.0e}_thr_max{:.0e}_kp_max{:d}.png&#39;.format(threshold_min, threshold_max, kp_max_num))
        fig2.savefig(fig2_fnm, dpi=300)

    return(dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.SIFT_find_keypoints_dataset"><code class="name flex">
<span>def <span class="ident">SIFT_find_keypoints_dataset</span></span>(<span>fr, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate SIFT key point discovery for a test frame (fr). ©G.Shtengel 08/2022 gleb.shtengel@gmail.com</p>
<p>Parameters:
fr : str
filename for the data frame to be used for SIFT key point discovery evaluation</p>
<h2 id="kwargs">Kwargs</h2>
<p>data_dir : str
data directory (path)
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
fnm_reg : str
filename for the final registed dataset
threshold_min : float
CDF threshold for determining the minimum data value
threshold_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF
evaluation_box : list of 4 int
evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.</p>
<p>SIFT_nfeatures : int
SIFT libary default is 0. The number of best features to retain.
The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
SIFT_nOctaveLayers : int
SIFT libary default
is 3. The number of layers in each octave.
3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
SIFT_contrastThreshold : double
SIFT libary default
is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
The larger the threshold, the less features are produced by the detector.
The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
When nOctaveLayers is set to default and if you want to use the value used in
D. Lowe paper (0.03), set this argument to 0.09.
SIFT_edgeThreshold : double
SIFT libary default
is 10. The threshold used to filter out edge-like features.
Note that the its meaning is different from the contrastThreshold,
i.e. the larger the edgeThreshold, the less features are filtered out
(more features are retained).
SIFT_sigma : double
SIFT library default is 1.6.
The sigma of the Gaussian applied to the input image at the octave #0.
If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check</p>
<p>Returns:
dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def SIFT_find_keypoints_dataset(fr, **kwargs):
    &#39;&#39;&#39;
    Evaluate SIFT key point discovery for a test frame (fr). ©G.Shtengel 08/2022 gleb.shtengel@gmail.com

    Parameters:
    fr : str
        filename for the data frame to be used for SIFT key point discovery evaluation

    kwargs
    ---------
    data_dir : str
        data directory (path)
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    fnm_reg : str
        filename for the final registed dataset
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.

    SIFT_nfeatures : int
        SIFT libary default is 0. The number of best features to retain.
        The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
    SIFT_nOctaveLayers : int
        SIFT libary default  is 3. The number of layers in each octave.
        3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
    SIFT_contrastThreshold : double
        SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
        The larger the threshold, the less features are produced by the detector.
        The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
        When nOctaveLayers is set to default and if you want to use the value used in
        D. Lowe paper (0.03), set this argument to 0.09.
    SIFT_edgeThreshold : double
        SIFT libary default  is 10. The threshold used to filter out edge-like features.
        Note that the its meaning is different from the contrastThreshold,
        i.e. the larger the edgeThreshold, the less features are filtered out
        (more features are retained).
    SIFT_sigma : double
        SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
        If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check

    Returns:
    dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts
    &#39;&#39;&#39;

    ftype = kwargs.get(&#34;ftype&#34;, 0)
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)   # threshold for Lowe&#39;s Ratio Test
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.04)

    frame = FIBSEM_frame(fr, ftype=ftype)
    if ftype == 0:
        if frame.FileVersion &gt; 8 :
            Sample_ID = frame.Sample_ID.strip(&#39;\x00&#39;)
        else:
            Sample_ID = frame.Notes[0:16]
    else:
        Sample_ID = frame.Sample_ID
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, Sample_ID)

    print(Sample_ID)

    #if save_res_png :
    #    frame.display_images()

    img = np.ravel(frame.RawImageA)
    fsz=12
    fszl=11
    dmin, dmax = frame.get_image_min_max(image_name = &#39;RawImageA&#39;, thr_min=threshold_min, thr_max=threshold_max, nbins=nbins)
    xi = dmin-(np.abs(dmax-dmin)/10)
    xa = dmax+(np.abs(dmax-dmin)/10)

    fig, axs = subplots(2,1, figsize=(6,6))
    fig.suptitle(Sample_ID + &#39;,  thr_min={:.0e}, thr_max={:.0e}, contrastThreshold={:.3f}, kp_max_num={:d}, comp.time={:.1f}sec&#39;.format(threshold_min, threshold_max, SIFT_contrastThreshold, kp_max_num, comp_time), fontsize=fszl)

    hist, bins, patches = axs[0].hist(img, bins = nbins)
    axs[0].set_xlim(xi, xa)
    axs[0].plot([dmin, dmin], [0, np.max(hist)], &#39;r&#39;, linestyle = &#39;--&#39;)
    axs[0].plot([dmax, dmax], [0, np.max(hist)], &#39;g&#39;, linestyle = &#39;--&#39;)
    axs[0].set_ylabel(&#39;Count&#39;, fontsize = fsz)
    pdf = hist / (frame.XResolution * frame.YResolution)
    cdf = np.cumsum(pdf)
    xCDF = bins[0:-1]+(bins[1]-bins[0])/2.0
    xthr = [xCDF[0], xCDF[-1]]
    ythr_min = [threshold_min, threshold_min]
    y1thr_max = [1-threshold_max, 1-threshold_max]

    axs[1].plot(xCDF, cdf, label=&#39;CDF&#39;)
    axs[1].plot(xthr, ythr_min, &#39;r&#39;, label=&#39;thr_min={:.5f}&#39;.format(threshold_min))
    axs[1].plot([dmin, dmin], [0, 1], &#39;r&#39;, linestyle = &#39;--&#39;, label = &#39;data_min={:.1f}&#39;.format(dmin))
    axs[1].plot(xthr, y1thr_max, &#39;g&#39;, label=&#39;1.0 - thr_max = {:.5f}&#39;.format(1-threshold_max))
    axs[1].plot([dmax, dmax], [0, 1], &#39;g&#39;, linestyle = &#39;--&#39;, label = &#39;data_max={:.1f}&#39;.format(dmax))
    axs[1].set_xlabel(&#39;Intensity Level&#39;, fontsize = fsz)
    axs[1].set_ylabel(&#39;CDF&#39;, fontsize = fsz)
    axs[1].set_xlim(xi, xa)
    axs[1].legend(loc=&#39;center&#39;, fontsize=fsz)
    axs[0].set_title(&#39;Data Min and Max with thr_min={:.0e},  thr_max={:.0e}&#39;.format(threshold_min, threshold_max), fontsize = fsz)
    for ax in ravel(axs):
        ax.grid(True)

    t0 = time.time()
    params1 = [fr, dmin, dmax, kwargs]
    fnm_1 = extract_keypoints_descr_files(params1)

    t1 = time.time()
    comp_time = (t1-t0)
    #print(&#39;Time to compute: {:.1f}sec&#39;.format(comp_time))

    xfsz = 3 * (np.int(7 * frame.XResolution / np.max([frame.XResolution, frame.YResolution]))+1)
    yfsz = 3 * (np.int(7 * frame.YResolution / np.max([frame.XResolution, frame.YResolution]))+2)
    fig2, ax = subplots(1,1, figsize=(xfsz,yfsz))
    fig2.subplots_adjust(left=0.0, bottom=0.25*(1-frame.YResolution/frame.XResolution), right=1.0, top=1.0)
    symsize = 2
    fsize = 12
    img2 = FIBSEM_frame(fr, ftype=ftype).RawImageA
    ax.imshow(img2, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
    ax.axis(False)

    kpp1s, des1 = pickle.load(open(fnm_1, &#39;rb&#39;))
    kp1 = [list_to_kp(kpp1) for kpp1 in kpp1s]     # this converts a list of lists to a list of keypoint objects to be used by a matcher later
    src_pts = np.float32([ kp.pt for kp in kp1 ]).reshape(-1, 2)
    x, y = src_pts.T
    print(&#39;Extracted {:d} keyponts&#39;.format(len(kp1)))
    # the code below is for vector map. vectors have origin coordinates x and y, and vector projections xs and ys.
    vec_field = ax.scatter(x,y, s=0.02, marker=&#39;o&#39;, c=&#39;r&#39;)
    ax.text(0.01, 1.1-0.13*frame.YResolution/frame.XResolution, Sample_ID + &#39;, thr_min={:.0e}, thr_max={:.0e}, SIFT_nfeatures={:d}&#39;.format(threshold_min, threshold_max, SIFT_nfeatures), fontsize=fsize, transform=ax.transAxes)
    if save_res_png :
        png_name = os.path.splitext(fr)[0] + &#39;_SIFT_kpts_eval_&#39;+&#39;_thr_min{:.5f}_thr_max{:.5f}.png&#39;.format(threshold_min, threshold_max)
        fig2.savefig(png_name, dpi=300)
    return(dmin, dmax, comp_time, src_pts)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform0"><code class="name flex">
<span>def <span class="ident">ScaleShiftTransform0</span></span>(<span>matrix=None, scale=None, translation=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ScaleShiftTransform0(matrix=None, scale=None, translation=None):
    return AffineTransform(matrix=matrix, scale=scale, rotation = 0, shear=0, translation = translation)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform0"><code class="name flex">
<span>def <span class="ident">ShiftTransform0</span></span>(<span>matrix=None, translation=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ShiftTransform0(matrix=None, translation=None):
    return EuclideanTransform(matrix=matrix, rotation = 0, translation = translation)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.Single_Image_Noise_ROIs"><code class="name flex">
<span>def <span class="ident">Single_Image_Noise_ROIs</span></span>(<span>img, Noise_ROIs, Hist_ROI, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyses the noise statistics in the selected ROI's of the EM data
©G.Shtengel 04/2022 gleb.shtengel@gmail.com</p>
<p>Performs following:
1.
Smooth the image by 2D convolution with a given kernel.
2.
Determine "Noise" as difference between the original raw and smoothed data.
3.
Build a histogram of Smoothed Image.
4.
For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
5.
Plot the dependence of the noise variance vs. image intensity.
6.
One of the parameters is a DarkCount. If it is not explicitly defined as input parameter, it will be set to 0.
7.
The equation is determined for a line that passes through the points Intensity=DarkCount and Noise Variance = 0 and is a best fit for
the [Mean Intensity, Noise Variance] points determined for each ROI (Step 1 above).
8.
The data is plotted. Following values of SNR are defined from the slope of the line in Step 7:
a.
PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity at the histogram peak determined in the Step 3.
b.
MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
c.
DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance), where Max and Min Intensity are determined by
corresponding cumulative threshold parameters, and Noise Variance is taken at the intensity in the middle of the range
(Min Intensity + Max Intensity)/2.0.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>2D array</code></dt>
<dd>original image</dd>
<dt><strong><code>Noise_ROIs</code></strong> :&ensp;<code>list</code> of <code>lists: [[left, right, top, bottom]]</code></dt>
<dd>list of coordinates (indices) for each of the ROI's - the boundaries of the image subset to evaluate the noise.</dd>
<dt><strong><code>Hist_ROI</code></strong> :&ensp;<code>list [left, right, top, bottom]</code></dt>
<dd>coordinates (indices) of the boundaries of the image subset to evaluate the real data histogram.</dd>
<dt>kwargs:</dt>
<dt><strong><code>DarkCount</code></strong> :&ensp;<code>float</code></dt>
<dd>the value of the Intensity Data at 0.</dd>
<dt><strong><code>kernel</code></strong> :&ensp;<code>2D float array</code></dt>
<dd>a kernel to perform 2D smoothing convolution.</dd>
<dt><strong><code>nbins_disp</code></strong> :&ensp;<code>int</code></dt>
<dd>(default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.</dd>
<dt><strong><code>thresholds_disp</code></strong> :&ensp;<code>list [thr_min_disp, thr_max_disp]</code></dt>
<dd>(default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.</dd>
<dt><strong><code>nbins_analysis</code></strong> :&ensp;<code>int</code></dt>
<dd>(default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.</dd>
<dt><strong><code>thresholds_analysis</code></strong> :&ensp;<code>list [thr_min_analysis, thr_max_analysis]</code></dt>
<dd>(default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.</dd>
<dt><strong><code>nbins_analysis</code></strong> :&ensp;<code>int</code></dt>
<dd>(default 256) number of histogram bins for building the data histogram in Step 5.</dd>
<dt><strong><code>disp_res</code></strong> :&ensp;<code>boolean</code></dt>
<dd>(default is False) - to plot/ display the results</dd>
<dt><strong><code>save_res_png</code></strong> :&ensp;<code>boolean</code></dt>
<dd>save the analysis output into a PNG file (default is True)</dd>
<dt><strong><code>res_fname</code></strong> :&ensp;<code>string</code></dt>
<dd>filename for the sesult image ('SNR_result.png')</dd>
<dt><strong><code>img_label</code></strong> :&ensp;<code>string</code></dt>
<dd>optional image label</dd>
<dt><strong><code>Notes</code></strong> :&ensp;<code>string</code></dt>
<dd>optional additional notes</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt>Returns:</dt>
<dt><strong><code>mean_vals</code></strong>, <strong><code>var_vals</code></strong>, <strong><code>NF_slope</code></strong>, <strong><code>PSNR</code></strong>, <strong><code>MSNR</code></strong>, <strong><code>DSNR</code></strong></dt>
<dd>mean_vals and var_vals are the Mean Intensity and Noise Variance values for the Noise_ROIs (Step 1)
NF_slope is the slope of the linear fit curve (Step 4)
PSNR and DSNR are Peak and Dynamic SNR's (Step 6)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Single_Image_Noise_ROIs(img, Noise_ROIs, Hist_ROI, **kwargs):
    &#39;&#39;&#39;
    Analyses the noise statistics in the selected ROI&#39;s of the EM data
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Performs following:
    1.  Smooth the image by 2D convolution with a given kernel.
    2.  Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
    3.  Build a histogram of Smoothed Image.
    4.  For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
    5.  Plot the dependence of the noise variance vs. image intensity.
    6.  One of the parameters is a DarkCount. If it is not explicitly defined as input parameter, it will be set to 0.
    7.  The equation is determined for a line that passes through the points Intensity=DarkCount and Noise Variance = 0 and is a best fit for
        the [Mean Intensity, Noise Variance] points determined for each ROI (Step 1 above).
    8.  The data is plotted. Following values of SNR are defined from the slope of the line in Step 7:
        a.  PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity at the histogram peak determined in the Step 3.
        b.  MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
        c.  DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance), where Max and Min Intensity are determined by
            corresponding cumulative threshold parameters, and Noise Variance is taken at the intensity in the middle of the range
            (Min Intensity + Max Intensity)/2.0.

    Parameters
    ----------
    img : 2D array
        original image
    Noise_ROIs : list of lists: [[left, right, top, bottom]]
        list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the noise.
    Hist_ROI : list [left, right, top, bottom]
        coordinates (indices) of the boundaries of the image subset to evaluate the real data histogram.

    kwargs:
    DarkCount : float
        the value of the Intensity Data at 0.
    kernel : 2D float array
        a kernel to perform 2D smoothing convolution.
    nbins_disp : int
        (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
    thresholds_disp : list [thr_min_disp, thr_max_disp]
        (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
    nbins_analysis : int
        (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
    thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
        (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
    nbins_analysis : int
         (default 256) number of histogram bins for building the data histogram in Step 5.
    disp_res : boolean
        (default is False) - to plot/ display the results
    save_res_png : boolean
        save the analysis output into a PNG file (default is True)
    res_fname : string
        filename for the sesult image (&#39;SNR_result.png&#39;)
    img_label : string
        optional image label
    Notes : string
        optional additional notes
    dpi : int

    Returns:
    mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR
        mean_vals and var_vals are the Mean Intensity and Noise Variance values for the Noise_ROIs (Step 1)
        NF_slope is the slope of the linear fit curve (Step 4)
        PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 6)
    &#39;&#39;&#39;
    st = 1.0/np.sqrt(2.0)
    def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
    def_kernel = def_kernel/def_kernel.sum()
    kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
    DarkCount = kwargs.get(&#34;DarkCount&#34;, 0)
    nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
    thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
    nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
    thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;Noise_Analysis_ROIs.png&#39;)
    img_label = kwargs.get(&#34;img_label&#34;, &#39;&#39;)
    Notes = kwargs.get(&#34;Notes&#34;, &#39;&#39;)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    fs=11
    img_filtered = convolve2d(img, kernel, mode=&#39;same&#39;)
    range_disp = get_min_max_thresholds(img_filtered, thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res=False)

    xi, xa, yi, ya = Hist_ROI
    img_hist = img[yi:ya, xi:xa]
    img_hist_filtered = img_filtered[yi:ya, xi:xa]

    range_analysis = get_min_max_thresholds(img_hist_filtered, thr_min = thresholds_analysis[0], thr_max = thresholds_analysis[1], nbins = nbins_analysis, disp_res=False)
    if disp_res:
        print(&#39;The EM data range for noise analysis: {:.1f} - {:.1f},  DarkCount={:.1f}&#39;.format(range_analysis[0], range_analysis[1], DarkCount))
    bins_analysis = np.linspace(range_analysis[0], range_analysis[1], nbins_analysis)

    xy_ratio = img.shape[1]/img.shape[0]
    xsz = 15
    ysz = xsz*3.5/xy_ratio

    n_ROIs = len(Noise_ROIs)+1
    mean_vals = np.zeros(n_ROIs)
    var_vals = np.zeros(n_ROIs)
    mean_vals[0] = DarkCount

    if disp_res:
        fig = plt.figure(figsize=(xsz,ysz))
        axs0 = fig.add_subplot(3,1,1)
        axs1 = fig.add_subplot(3,1,2)
        axs2 = fig.add_subplot(3,3,7)
        axs3 = fig.add_subplot(3,3,8)
        axs4 = fig.add_subplot(3,3,9)
        fig.subplots_adjust(left=0.01, bottom=0.06, right=0.99, top=0.95, wspace=0.25, hspace=0.10)

        axs0.text(0.01, 1.13, res_fname + &#39;,   &#39; +  Notes, transform=axs0.transAxes, fontsize=fs-3)
        axs0.imshow(img, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs0.axis(False)
        axs0.set_title(&#39;Original Image: &#39; + img_label, color=&#39;r&#39;, fontsize=fs+1)
        Hist_patch = patches.Rectangle((xi,yi),abs(xa-xi)-2,abs(ya-yi)-2, linewidth=1.0, edgecolor=&#39;white&#39;,facecolor=&#39;none&#39;)
        axs1.add_patch(Hist_patch)

        axs2.imshow(img_hist_filtered, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs2.axis(False)
        axs2.set_title(&#39;Smoothed ROI&#39;, fontsize=fs+1)

    if disp_res:
        hist, bins, hist_patches = axs3.hist(img_hist_filtered.ravel(), range=range_disp, bins = nbins_disp)
    else:
        hist, bins = np.histogram(img_hist_filtered.ravel(), range=range_disp, bins = nbins_disp)

    bin_centers = np.array(bins[1:] - (bins[1]-bins[0])/2.0)
    hist_center_ind = np.argwhere((bin_centers&gt;range_analysis[0]) &amp; (bin_centers&lt;range_analysis[1]))
    hist_smooth = savgol_filter(np.array(hist), (nbins_disp//10)*2+1, 7)
    I_peak = bin_centers[hist_smooth.argmax()]
    I_mean = np.mean(img)
    C_peak = hist_smooth.max()

    if disp_res:
        axs3.plot(bin_centers[hist_center_ind], hist_smooth[hist_center_ind], color=&#39;grey&#39;, linestyle=&#39;dashed&#39;, linewidth=2)
        Ipeak_lbl = &#39;$I_{peak}$&#39; +&#39;={:.1f}&#39;.format(I_peak)
        axs3.plot(I_peak, C_peak, &#39;rd&#39;, label = Ipeak_lbl)
        axs3.set_title(&#39;Histogram of the Smoothed ROI&#39;, fontsize=fs+1)
        axs3.grid(True)
        axs3.set_xlabel(&#39;Smoothed ROI Image Intensity&#39;, fontsize=fs+1)
        for hist_patch in np.array(hist_patches)[bin_centers&lt;range_analysis[0]]:
            hist_patch.set_facecolor(&#39;lime&#39;)
        for hist_patch in np.array(hist_patches)[bin_centers&gt;range_analysis[1]]:
            hist_patch.set_facecolor(&#39;red&#39;)
        ylim3=array(axs3.get_ylim())
        I_min, I_max = range_analysis
        axs3.plot([I_min, I_min],[ylim3[0]-1000, ylim3[1]], color=&#39;lime&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{min}$&#39; +&#39;={:.1f}&#39;.format(I_min))
        axs3.plot([I_max, I_max],[ylim3[0]-1000, ylim3[1]], color=&#39;red&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{max}$&#39; +&#39;={:.1f}&#39;.format(I_max))
        axs3.set_ylim(ylim3)
        axs3.legend(loc=&#39;upper right&#39;, fontsize=fs+1)
        axs1.imshow(img_filtered, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs1.axis(False)
        axs1.set_title(&#39;Smoothed Image&#39;)
        axs4.plot(DarkCount, 0, &#39;d&#39;, color = &#39;black&#39;, label=&#39;Dark Count&#39;)

    for j, ROI in enumerate(tqdm(Noise_ROIs, desc = &#39;analyzing ROIs&#39;)):
        xi, xa, yi, ya = ROI
        img_ROI = img[yi:ya, xi:xa]
        img_ROI_filtered = img_filtered[yi:ya, xi:xa]

        imdiff = (img_ROI - img_ROI_filtered)
        x = np.mean(img_ROI)
        y = np.var(imdiff)
        mean_vals[j+1] = x
        var_vals[j+1] = y

        if disp_res:
            patch_col = get_cmap(&#34;gist_rainbow_r&#34;)((j)/(n_ROIs))
            rect_patch = patches.Rectangle((xi,yi),abs(xa-xi)-2,abs(ya-yi)-2, linewidth=0.5, edgecolor=patch_col,facecolor=&#39;none&#39;)
            axs0.add_patch(rect_patch)
            axs4.plot(x, y, &#39;d&#39;, color = patch_col) #, label=&#39;patch {:d}&#39;.format(j))

    NF_slope = np.mean(var_vals[1:]/(mean_vals[1:]-DarkCount))
    mean_val_fit = np.array([mean_vals.min(), mean_vals.max()])
    var_val_fit = (mean_val_fit-DarkCount)*NF_slope
    VAR = (I_peak - DarkCount) * NF_slope
    VAR_at_mean = ((I_max + I_min) /2.0 - DarkCount) * NF_slope
    PSNR = (I_peak - DarkCount)/np.sqrt(VAR)
    MSNR = (I_mean - DarkCount)/np.sqrt((I_mean - DarkCount)*NF_slope)
    DSNR = (I_max - I_min)/np.sqrt(VAR_at_mean)

    if disp_res:
        axs4.grid(True)
        axs4.set_title(&#39;Noise Distribution&#39;, fontsize=fs+1)
        axs4.set_xlabel(&#39;ROI Image Intensity Mean&#39;, fontsize=fs+1)
        axs4.set_ylabel(&#39;ROI Image Intensity Variance&#39;, fontsize=fs+1)
        axs4.plot(mean_val_fit, var_val_fit, color=&#39;orange&#39;, label=&#39;Fit:  y = (x {:.1f}) * {:.2f}&#39;.format(DarkCount, NF_slope))
        axs4.legend(loc = &#39;upper left&#39;, fontsize=fs+2)
        ylim4=array(axs4.get_ylim())
        V_min = (I_min-DarkCount)*NF_slope
        V_max = (I_max-DarkCount)*NF_slope
        V_peak = (I_peak-DarkCount)*NF_slope
        axs4.plot([I_min, I_min],[ylim4[0], V_min], color=&#39;lime&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{min}$&#39; +&#39;={:.1f}&#39;.format(I_min))
        axs4.plot([I_max, I_max],[ylim4[0], V_max], color=&#39;red&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{max}$&#39; +&#39;={:.1f}&#39;.format(I_max))
        axs4.plot([I_peak, I_peak],[ylim4[0], V_peak], color=&#39;black&#39;, linestyle=&#39;dashed&#39;, label=&#39;$I_{peak}$&#39; +&#39;={:.1f}&#39;.format(I_peak))
        axs4.set_ylim(ylim4)
        txt1 = &#39;Peak Intensity:  {:.1f}&#39;.format(I_peak)
        axs4.text(0.05, 0.65, txt1, transform=axs4.transAxes, fontsize=fs+1)
        txt2 = &#39;Variance={:.1f}, STD={:.1f}&#39;.format(VAR, np.sqrt(VAR))
        axs4.text(0.05, 0.55, txt2, transform=axs4.transAxes, fontsize=fs+1)
        txt3 = &#39;PSNR = {:.2f}&#39;.format(PSNR)
        axs4.text(0.05, 0.45, txt3, transform=axs4.transAxes, fontsize=fs+1)
        txt3 = &#39;DSNR = {:.2f}&#39;.format(DSNR)
        axs4.text(0.05, 0.35, txt3, transform=axs4.transAxes, fontsize=fs+1)
        if save_res_png:
            fig.savefig(res_fname, dpi=dpi)
            print(&#39;results saved into the file: &#39;+res_fname)

    return mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.Single_Image_Noise_Statistics"><code class="name flex">
<span>def <span class="ident">Single_Image_Noise_Statistics</span></span>(<span>img, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyses the noise statistics of the EM data image.
©G.Shtengel 04/2022 gleb.shtengel@gmail.com</p>
<p>Performs following:
1. Smooth the image by 2D convolution with a given kernel.
2. Determine "Noise" as difference between the original raw and smoothed data.
3. Build a histogram of Smoothed Image.
4. For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
5. Plot the dependence of the noise variance vs. image intensity.
6. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
it will be set to 0
7. The equation is determined for a line that passes through the point:
Intensity=DarkCount and Noise Variance = 0
and is a best fit for the [Mean Intensity, Noise Variance] points
determined for each ROI (Step 1 above).
8. The data is plotted. Two values of SNR are defined from the slope of the line in Step 4:
PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity
at the histogram peak determined in the Step 5.
MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
where Max and Min Intensity are determined by corresponding cummulative
threshold parameters, and Noise Variance is taken at the intensity
in the middle of the range (Min Intensity + Max Intensity)/2.0</p>
<h2 id="parameters">Parameters</h2>
<pre><code>img : 2d array

kwargs:
evaluation_box : list of 4 int
    evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
    if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
DarkCount : float
    the value of the Intensity Data at 0.
kernel : 2D float array
    a kernel to perfrom 2D smoothing convolution.
nbins_disp : int
    (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
thresholds_disp : list [thr_min_disp, thr_max_disp]
    (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
nbins_analysis : int
    (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
    (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
nbins_analysis : int
     (default 256) number of histogram bins for building the data histogram in Step 5.
disp_res : boolean
    (default is False) - to plot/ display the results
save_res_png : boolean
    save the analysis output into a PNG file (default is True)
res_fname : string
    filename for the result image ('Noise_Analysis.png')
img_label : string
    optional image label
Notes : string
    optional additional notes
dpi : int
</code></pre>
<dl>
<dt>Returns:</dt>
<dt><strong><code>mean_vals</code></strong>, <strong><code>var_vals</code></strong>, <strong><code>I0</code></strong>, <strong><code>PSNR</code></strong>, <strong><code>DSNR</code></strong>, <strong><code>popt</code></strong>, <strong><code>result</code></strong></dt>
<dd>mean_vals and var_vals are the Mean Intensity and Noise Variance values for Step5, I0 is zero intercept (should be close to DarkCount)
PSNR and DSNR are Peak and Dynamic SNR's (Step 8)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Single_Image_Noise_Statistics(img, **kwargs):
    &#39;&#39;&#39;
    Analyses the noise statistics of the EM data image.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Performs following:
    1. Smooth the image by 2D convolution with a given kernel.
    2. Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
    3. Build a histogram of Smoothed Image.
    4. For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
    5. Plot the dependence of the noise variance vs. image intensity.
    6. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
        it will be set to 0
    7. The equation is determined for a line that passes through the point:
            Intensity=DarkCount and Noise Variance = 0
            and is a best fit for the [Mean Intensity, Noise Variance] points
            determined for each ROI (Step 1 above).
    8. The data is plotted. Two values of SNR are defined from the slope of the line in Step 4:
        PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity
            at the histogram peak determined in the Step 5.
        MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
        DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
            where Max and Min Intensity are determined by corresponding cummulative
            threshold parameters, and Noise Variance is taken at the intensity
            in the middle of the range (Min Intensity + Max Intensity)/2.0

    Parameters
    ----------
        img : 2d array

        kwargs:
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        DarkCount : float
            the value of the Intensity Data at 0.
        kernel : 2D float array
            a kernel to perfrom 2D smoothing convolution.
        nbins_disp : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
        thresholds_disp : list [thr_min_disp, thr_max_disp]
            (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
        nbins_analysis : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
        thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
            (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
        nbins_analysis : int
             (default 256) number of histogram bins for building the data histogram in Step 5.
        disp_res : boolean
            (default is False) - to plot/ display the results
        save_res_png : boolean
            save the analysis output into a PNG file (default is True)
        res_fname : string
            filename for the result image (&#39;Noise_Analysis.png&#39;)
        img_label : string
            optional image label
        Notes : string
            optional additional notes
        dpi : int

    Returns:
    mean_vals, var_vals, I0, PSNR, DSNR, popt, result
        mean_vals and var_vals are the Mean Intensity and Noise Variance values for Step5, I0 is zero intercept (should be close to DarkCount)
        PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 8)
    &#39;&#39;&#39;
    st = 1.0/np.sqrt(2.0)
    def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    def_kernel = def_kernel/def_kernel.sum()
    kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
    DarkCount = kwargs.get(&#34;DarkCount&#34;, 0)
    nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
    thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
    nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
    thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;Noise_Analysis.png&#39;)
    image_name = kwargs.get(&#34;image_name&#34;, &#39;&#39;)
    Notes = kwargs.get(&#34;Notes&#34;, &#39;&#39;)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    xi = 0
    yi = 0
    ysz, xsz = img.shape
    xa = xi + xsz
    ya = yi + ysz

    xi_eval = xi + evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = xa
    yi_eval = yi + evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ya

    img = img[yi_eval:ya_eval, xi_eval:xa_eval]
    img_filtered = convolve2d(img, kernel, mode=&#39;same&#39;)[1:-1, 1:-1]
    img = img[1:-1, 1:-1]

    range_disp = get_min_max_thresholds(img_filtered, thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res = disp_res)
    if disp_res:
        print(&#39;The EM data range for display:            {:.1f} - {:.1f}&#39;.format(range_disp[0], range_disp[1]))
    range_analysis = get_min_max_thresholds(img_filtered, thr_min = thresholds_analysis[0], thr_max = thresholds_analysis[1], nbins = nbins_analysis, disp_res = disp_res)
    if disp_res:
        print(&#39;The EM data range for noise analysis:     {:.1f} - {:.1f}&#39;.format(range_analysis[0], range_analysis[1]))
    bins_analysis = np.linspace(range_analysis[0], range_analysis[1], nbins_analysis)

    imdiff = (img-img_filtered)
    range_imdiff = get_min_max_thresholds(imdiff, thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res = disp_res)

    xy_ratio = img.shape[1]/img.shape[0]
    xsz = 15
    ysz = xsz/1.5*xy_ratio
    if disp_res:
        fs=11
        fig, axss = subplots(2,3, figsize=(xsz,ysz),  gridspec_kw={&#34;height_ratios&#34; : [1,1]})
        fig.subplots_adjust(left=0.07, bottom=0.06, right=0.99, top=0.92, wspace=0.15, hspace=0.10)
        axs = axss.ravel()
        axs[0].text(-0.1, 1.1, res_fname + &#39;,       &#39; +  Notes, transform=axs[0].transAxes, fontsize=fs)

        axs[0].imshow(img, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs[0].axis(False)
        axs[0].set_title(&#39;Original Image: &#39; + image_name, color=&#39;r&#39;, fontsize=fs+1)

        axs[1].imshow(img_filtered, cmap=&#34;Greys&#34;, vmin = range_disp[0], vmax = range_disp[1])
        axs[1].axis(False)
        axs[1].set_title(&#39;Smoothed Image&#39;)
        Low_mask = img*0.0+255.0
        High_mask = Low_mask.copy()
        Low_mask[img_filtered &gt; range_analysis[0]] = np.nan
        axs[1].imshow(Low_mask, cmap=&#34;brg_r&#34;)
        High_mask[img_filtered &lt; range_analysis[1]] = np.nan
        axs[1].imshow(High_mask, cmap=&#34;gist_rainbow&#34;)


        axs[2].imshow(imdiff, cmap=&#34;Greys&#34;, vmin = range_imdiff[0], vmax = range_imdiff[1])
        axs[2].axis(False)
        axs[2].set_title(&#39;Image Difference&#39;, fontsize=fs+1)

    if disp_res:
        hist, bins, patches = axs[4].hist(img_filtered.ravel(), range=range_disp, bins = nbins_disp)
    else:
        hist, bins = np.histogram(img_hist_filtered.ravel(), range=range_disp, bins = nbins_disp)
    bin_centers = np.array(bins[1:] - (bins[1]-bins[0])/2.0)
    hist_center_ind = np.argwhere((bin_centers&gt;range_analysis[0]) &amp; (bin_centers&lt;range_analysis[1]))
    hist_smooth = savgol_filter(np.array(hist), (nbins_disp//10)*2+1, 7)
    I_peak = bin_centers[hist_smooth.argmax()]
    C_peak = hist_smooth.max()
    Ipeak_lbl = &#39;$I_{peak}$&#39; +&#39;={:.1f}&#39;.format(I_peak)

    if disp_res:
        axs[4].plot(bin_centers[hist_center_ind], hist_smooth[hist_center_ind], color=&#39;grey&#39;, linestyle=&#39;dashed&#39;, linewidth=2)
        axs[4].plot(I_peak, C_peak, &#39;rd&#39;, label = Ipeak_lbl)
        axs[4].legend(loc=&#39;upper left&#39;, fontsize=fs+1)
        axs[4].set_title(&#39;Histogram of the Smoothed Image&#39;, fontsize=fs+1)
        axs[4].grid(True)
        axs[4].set_xlabel(&#39;Image Intensity&#39;, fontsize=fs+1)
        for patch in np.array(patches)[bin_centers&lt;range_analysis[0]]:
            patch.set_facecolor(&#39;lime&#39;)
        for patch in np.array(patches)[bin_centers&gt;range_analysis[1]]:
            patch.set_facecolor(&#39;red&#39;)
        ylim4=array(axs[4].get_ylim())
        axs[4].plot([range_analysis[0], range_analysis[0]],[ylim4[0]-1000, ylim4[1]], color=&#39;lime&#39;, linestyle=&#39;dashed&#39;, label=&#39;Ilow&#39;)
        axs[4].plot([range_analysis[1], range_analysis[1]],[ylim4[0]-1000, ylim4[1]], color=&#39;red&#39;, linestyle=&#39;dashed&#39;, label=&#39;Ihigh&#39;)
        axs[4].set_ylim(ylim4)
        txt1 = &#39;Smoothing Kernel&#39;
        axs[4].text(0.69, 0.955, txt1, transform=axs[4].transAxes, backgroundcolor=&#39;white&#39;, fontsize=fs-1)
        txt2 = &#39;{:.3f}  {:.3f}  {:.3f}&#39;.format(kernel[0,0], kernel[0,1], kernel[0,2])
        axs[4].text(0.69, 0.910, txt2, transform=axs[4].transAxes, backgroundcolor=&#39;white&#39;, fontsize=fs-2)
        txt3 = &#39;{:.3f}  {:.3f}  {:.3f}&#39;.format(kernel[1,0], kernel[1,1], kernel[1,2])
        axs[4].text(0.69, 0.865, txt3, transform=axs[4].transAxes, backgroundcolor=&#39;white&#39;, fontsize=fs-2)
        txt3 = &#39;{:.3f}  {:.3f}  {:.3f}&#39;.format(kernel[2,0], kernel[2,1], kernel[2,2])
        axs[4].text(0.69, 0.820, txt3, transform=axs[4].transAxes, backgroundcolor=&#39;white&#39;, fontsize=fs-2)

    if disp_res:
        hist, bins, patches = axs[5].hist(imdiff.ravel(), bins = nbins_disp)
        axs[5].grid(True)
        axs[5].set_title(&#39;Histogram of the Difference Map&#39;, fontsize=fs+1)
        axs[5].set_xlabel(&#39;Image Difference&#39;, fontsize=fs+1)
    else:
        hist, bins = np.histogram(imdiff.ravel(), bins = nbins_disp)

    ind_new = np.digitize(img_filtered, bins_analysis)
    result = np.array([(np.mean(img_filtered[ind_new == j]), np.var(imdiff[ind_new == j]))  for j in range(1, nbins_analysis)])
    non_nan_ind = np.argwhere(np.invert(np.isnan(result[:, 0])))
    mean_vals = np.squeeze(result[non_nan_ind, 0])
    var_vals = np.squeeze(result[non_nan_ind, 1])
    try:
        popt = np.polyfit(mean_vals, var_vals, 1)
        if disp_res:
            print(&#39;popt: &#39;, popt)

        I_array = np.array((range_analysis[0], range_analysis[1], I_peak))
        if disp_res:
            print(&#39;I_array: &#39;, I_array)
        Var_array = np.polyval(popt, I_array)
        Var_peak = Var_array[2]
    except:
        if disp_res:
            print(&#34;np.polyfit could not converge&#34;)
        popt = np.array([np.var(imdiff)/np.mean(img_filtered-DarkCount), 0])
        I_array = np.array((range_analysis[0], range_analysis[1], I_peak))
        Var_peak = np.var(imdiff)
    var_fit = np.polyval(popt, mean_vals)
    I0 = -popt[1]/popt[0]
    Slope_header = np.mean(var_vals/(mean_vals-DarkCount))
    var_fit_header = (mean_vals-DarkCount) * Slope_header
    if disp_res:
        axs[3].plot(mean_vals, var_vals, &#39;r.&#39;, label=&#39;data&#39;)
        axs[3].plot(mean_vals, var_fit, &#39;b&#39;, label=&#39;linear fit: {:.1f}*x + {:.1f}&#39;.format(popt[0], popt[1]))
        axs[3].plot(mean_vals, var_fit_header, &#39;magenta&#39;, label=&#39;linear fit with header offset&#39;)
        axs[3].grid(True)
        axs[3].set_title(&#39;Noise Distribution&#39;, fontsize=fs+1)
        axs[3].set_xlabel(&#39;Image Intensity Mean&#39;, fontsize=fs+1)
        axs[3].set_ylabel(&#39;Image Intensity Variance&#39;, fontsize=fs+1)
        ylim3=array(axs[3].get_ylim())
        lbl_low = &#39;$I_{low}$&#39;+&#39;, thr={:.1e}&#39;.format(thresholds_analysis[0])
        lbl_high = &#39;$I_{high}$&#39;+&#39;, thr={:.1e}&#39;.format(thresholds_analysis[1])
        axs[3].plot([range_analysis[0], range_analysis[0]],[ylim3[0]-1000, ylim3[1]], color=&#39;lime&#39;, linestyle=&#39;dashed&#39;, label=lbl_low)
        axs[3].plot([range_analysis[1], range_analysis[1]],[ylim3[0]-1000, ylim3[1]], color=&#39;red&#39;, linestyle=&#39;dashed&#39;, label=lbl_high)
        axs[3].legend(loc=&#39;upper center&#39;, fontsize=fs+1)
        axs[3].set_ylim(ylim3)

    PSNR = (I_peak-I0)/np.sqrt(Var_peak)
    PSNR_header = (I_peak-DarkCount)/np.sqrt(Var_peak)
    DSNR = (range_analysis[1]-range_analysis[0])/np.sqrt(Var_peak)
    if disp_res:
        print(&#39;Var at peak: {:.1f}&#39;.format(Var_peak))
        print(&#39;PSNR={:.2f}, PSNR_header={:.2f}, DSNR={:.2f}&#39;.format(PSNR, PSNR_header, DSNR))

        txt1 = &#39;Histogram Peak:  &#39; + Ipeak_lbl
        axs[3].text(0.25, 0.27, txt1, transform=axs[3].transAxes, fontsize=fs+1)
        txt2 = &#39;DSNR = &#39; +&#39;$(I_{high}$&#39; +&#39;$ - I_{low})$&#39; +&#39;/&#39;+&#39;$σ_{peak}$&#39; + &#39; = {:.2f}&#39;.format(DSNR)
        axs[3].text(0.25, 0.22, txt2, transform=axs[3].transAxes, fontsize=fs+1)

        txt3 = &#39;Zero Intercept:    &#39; +&#39;$I_{0}$&#39; +&#39;={:.1f}&#39;.format(I0)
        axs[3].text(0.25, 0.17, txt3, transform=axs[3].transAxes, color=&#39;blue&#39;, fontsize=fs+1)
        txt4 = &#39;PSNR = &#39; +&#39;$(I_{peak}$&#39; +&#39;$ - I_{0})$&#39; +&#39;/&#39;+&#39;$σ_{peak}$&#39; + &#39; = {:.2f}&#39;.format(PSNR)
        axs[3].text(0.25, 0.12, txt4, transform=axs[3].transAxes, color=&#39;blue&#39;, fontsize=fs+1)

        txt5 = &#39;Header Zero Int.:    &#39; +&#39;$I_{0}$&#39; +&#39;={:.1f}&#39;.format(DarkCount)
        axs[3].text(0.25, 0.07, txt5, transform=axs[3].transAxes, color=&#39;magenta&#39;, fontsize=fs+1)
        txt6 = &#39;PSNR = &#39; +&#39;$(I_{peak}$&#39; +&#39;$ - I_{0})$&#39; +&#39;/&#39;+&#39;$σ_{peak}$&#39; + &#39; = {:.2f}&#39;.format(PSNR_header)
        axs[3].text(0.25, 0.02, txt6, transform=axs[3].transAxes, color=&#39;magenta&#39;, fontsize=fs+1)

        if save_res_png:
            fig.savefig(res_fname, dpi=300)
            print(&#39;results saved into the file: &#39;+res_fname)
    return mean_vals, var_vals, I0, PSNR, DSNR, popt, result</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.Single_Image_SNR"><code class="name flex">
<span>def <span class="ident">Single_Image_SNR</span></span>(<span>img, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimates SNR based on a single image.
©G.Shtengel 04/2022 gleb.shtengel@gmail.com
Calculates SNR of a single image base on auto-correlation analysis after [1].</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>2D array</code></dt>
<dd>&nbsp;</dd>
<dt>kwargs:</dt>
<dt><strong><code>edge_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>fraction of the full autocetrrelation range used to calculate the "mean value" (default is 0.10)</dd>
<dt><strong><code>extrapolate_signal</code></strong> :&ensp;<code>boolean</code></dt>
<dd>extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True</dd>
<dt><strong><code>disp_res</code></strong> :&ensp;<code>boolean</code></dt>
<dd>display results (plots) (default is True)</dd>
<dt><strong><code>save_res_png</code></strong> :&ensp;<code>boolean</code></dt>
<dd>save the analysis output into a PNG file (default is True)</dd>
<dt><strong><code>res_fname</code></strong> :&ensp;<code>string</code></dt>
<dd>filename for the result image ('SNR_result.png')</dd>
<dt><strong><code>img_label</code></strong> :&ensp;<code>string</code></dt>
<dd>optional image label</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code></dt>
<dd>dots-per-inch resolution for the output image</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xSNR, ySNR, rSNR </code></dt>
<dd>float, float, float
SNR determined using the method in [1] along X- and Y- directions.
If there is a direction with slow varying data - that direction provides more accurate SNR estimate
Y-streaks in typical FIB-SEM data provide slow varying Y-component because streaks
usually get increasingly worse with increasing Y.
So for typical FIB-SEM data use ySNR</dd>
</dl>
<p>[1] J. T. L. Thong et al, Single-image signal-to-noise ratio estimation. Scanning, 328–336 (2001).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Single_Image_SNR(img, **kwargs):
    &#39;&#39;&#39;
    Estimates SNR based on a single image.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com
    Calculates SNR of a single image base on auto-correlation analysis after [1].

    Parameters
    ---------
    img : 2D array

    kwargs:
    edge_fraction : float
        fraction of the full autocetrrelation range used to calculate the &#34;mean value&#34; (default is 0.10)
    extrapolate_signal : boolean
        extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
    disp_res : boolean
        display results (plots) (default is True)
    save_res_png : boolean
        save the analysis output into a PNG file (default is True)
    res_fname : string
        filename for the result image (&#39;SNR_result.png&#39;)
    img_label : string
        optional image label
    dpi : int
        dots-per-inch resolution for the output image

    Returns:
        xSNR, ySNR, rSNR : float, float, float
            SNR determined using the method in [1] along X- and Y- directions.
            If there is a direction with slow varying data - that direction provides more accurate SNR estimate
            Y-streaks in typical FIB-SEM data provide slow varying Y-component because streaks
            usually get increasingly worse with increasing Y.
            So for typical FIB-SEM data use ySNR

    [1] J. T. L. Thong et al, Single-image signal-to-noise ratio estimation. Scanning, 328–336 (2001).
    &#39;&#39;&#39;
    edge_fraction = kwargs.get(&#34;edge_fraction&#34;, 0.10)
    extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
    thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
    save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;SNR_results.png&#39;)
    img_label = kwargs.get(&#34;img_label&#34;, &#39;Orig. Image&#39;)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    #first make image size even
    ysz, xsz = img.shape
    img = img[0:ysz//2*2, 0:xsz//2*2]
    ysz, xsz = img.shape

    xy_ratio = xsz/ysz
    data_FT = fftshift(fftn(ifftshift(img-img.mean())))
    data_FC = (np.multiply(data_FT,np.conj(data_FT)))/xsz/ysz
    data_ACR = np.abs(fftshift(fftn(ifftshift(data_FC))))
    data_ACR_peak = data_ACR[ysz//2, xsz//2]
    data_ACR_log = np.log(data_ACR)
    data_ACR = data_ACR / data_ACR_peak
    radial_ACR = radial_profile(data_ACR, [xsz//2, ysz//2])
    r_ACR = np.concatenate((radial_ACR[::-1], radial_ACR[1:-1]))

    #rsz = xsz
    rsz = len(r_ACR)
    rcr = np.linspace(-rsz//2, rsz//2-1, rsz)
    xcr = np.linspace(-xsz//2, xsz//2-1, xsz)
    ycr = np.linspace(-ysz//2, ysz//2-1, ysz)

    xl = xcr[xsz//2-2:xsz//2]
    xacr_left = data_ACR[ysz//2, (xsz//2-2):(xsz//2)]
    xc = xcr[xsz//2]
    xr = xcr[xsz//2+1 : xsz//2+3]
    xacr_right = data_ACR[ysz//2, (xsz//2+1):(xsz//2+3)]
    if extrapolate_signal:
        xNFacl = xacr_left[0] + (xc-xl[0])/(xl[1]-xl[0])*(xacr_left[1]-xacr_left[0])
        xNFacr = xacr_right[0] + (xc-xr[0])/(xr[1]-xr[0])*(xacr_right[1]-xacr_right[0])
    else:
        xNFacl = xacr_left[1]
        xNFacr = xacr_right[0]
    x_left = xcr[xsz//2-2:xsz//2+1]
    xacr_left = np.concatenate((xacr_left, np.array([xNFacl])))
    x_right = xcr[xsz//2 : xsz//2+3]
    xacr_right = np.concatenate((np.array([xNFacr]), xacr_right))

    yl = ycr[ysz//2-2:ysz//2]
    yacr_left = data_ACR[(ysz//2-2):(ysz//2), xsz//2]
    yc = ycr[ysz//2]
    yr = ycr[ysz//2+1 : ysz//2+3]
    yacr_right = data_ACR[(ysz//2+1):(ysz//2+3), xsz//2]
    if extrapolate_signal:
        yNFacl = yacr_left[0] + (yc-yl[0])/(yl[1]-yl[0])*(yacr_left[1]-yacr_left[0])
        yNFacr = yacr_right[0] + (yc-yr[0])/(yr[1]-yr[0])*(yacr_right[1]-yacr_right[0])
    else:
        yNFacl = yacr_left[1]
        yNFacr = yacr_right[0]
    y_left = ycr[ysz//2-2:ysz//2+1]
    yacr_left = np.concatenate((yacr_left, np.array([yNFacl])))
    y_right = ycr[ysz//2 : ysz//2+3]
    yacr_right = np.concatenate((np.array([yNFacr]), yacr_right))

    rl = rcr[rsz//2-2:rsz//2]
    racr_left = r_ACR[(rsz//2-2):(rsz//2)]
    rc = rcr[rsz//2]
    rr = rcr[rsz//2+1 : rsz//2+3]
    racr_right = r_ACR[(rsz//2+1):(rsz//2+3)]
    if extrapolate_signal:
        rNFacl = racr_left[0] + (rc-rl[0])/(rl[1]-rl[0])*(racr_left[1]-racr_left[0])
        rNFacr = racr_right[0] + (rc-rr[0])/(rr[1]-rr[0])*(racr_right[1]-racr_right[0])
    else:
        rNFacl = racr_left[1]
        rNFacr = racr_right[0]
    r_left = rcr[rsz//2-2:rsz//2+1]
    racr_left = np.concatenate((racr_left, np.array([rNFacl])))
    r_right = rcr[rsz//2 : rsz//2+3]
    racr_right = np.concatenate((np.array([rNFacr]), racr_right))

    x_acr = data_ACR[ysz//2, xsz//2]
    x_noise_free_acr = xacr_right[0]
    xedge = int32(xsz*edge_fraction)
    x_mean_value = np.mean(data_ACR[ysz//2, 0:xedge])
    xx_mean_value = np.linspace(-xsz//2, (-xsz//2+xedge-1), xedge)
    yedge = int32(ysz*edge_fraction)
    y_acr = data_ACR[ysz//2, xsz//2]
    y_noise_free_acr = yacr_right[0]
    y_mean_value = np.mean(data_ACR[0:yedge, xsz//2])
    yy_mean_value = np.linspace(-ysz//2, (-ysz//2+yedge-1), yedge)
    redge = int32(rsz*edge_fraction)
    r_acr = data_ACR[ysz//2, xsz//2]
    r_noise_free_acr = racr_right[0]
    r_mean_value = np.mean(r_ACR[0:redge])
    rr_mean_value = np.linspace(-rsz//2, (-rsz//2+redge-1), redge)

    xSNR = (x_noise_free_acr-x_mean_value)/(x_acr - x_noise_free_acr)
    ySNR = (y_noise_free_acr-y_mean_value)/(y_acr - y_noise_free_acr)
    rSNR = (r_noise_free_acr-r_mean_value)/(r_acr - r_noise_free_acr)
    if disp_res:
        fs=12

        if xy_ratio &lt; 2.5:
            fig, axs = subplots(1,4, figsize = (20, 5))
        else:
            fig = plt.figure(figsize = (20, 5))
            ax0 = fig.add_subplot(2, 2, 1)
            ax1 = fig.add_subplot(2, 2, 3)
            ax2 = fig.add_subplot(1, 4, 3)
            ax3 = fig.add_subplot(1, 4, 4)
            axs = [ax0, ax1, ax2, ax3]
        fig.subplots_adjust(left=0.03, bottom=0.06, right=0.99, top=0.92, wspace=0.25, hspace=0.10)

        range_disp = get_min_max_thresholds(img, thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res=False)
        axs[0].imshow(img, cmap=&#39;Greys&#39;, vmin=range_disp[0], vmax=range_disp[1])
        axs[0].grid(True)
        axs[0].set_title(img_label)
        if save_res_png:
            axs[0].text(0, 1.1 + (xy_ratio-1.0)/20.0, res_fname, transform=axs[0].transAxes)
        axs[1].imshow(data_ACR_log, extent=[-xsz//2-1, xsz//2, -ysz//2-1, ysz//2])
        axs[1].grid(True)
        axs[1].set_title(&#39;Autocorrelation (log scale)&#39;)

        axs[2].plot(xcr, data_ACR[ysz//2, :], &#39;r&#39;, linewidth =0.5, label=&#39;X&#39;)
        axs[2].plot(ycr, data_ACR[:, xsz//2], &#39;b&#39;, linewidth =0.5, label=&#39;Y&#39;)
        axs[2].plot(rcr, r_ACR, &#39;g&#39;, linewidth =0.5, label=&#39;R&#39;)
        axs[2].plot(xx_mean_value, xx_mean_value*0 + x_mean_value, &#39;r--&#39;, linewidth =2.0, label=&#39;&lt;X&gt;={:.5f}&#39;.format(x_mean_value))
        axs[2].plot(yy_mean_value, yy_mean_value*0 + y_mean_value, &#39;b--&#39;, linewidth =2.0, label=&#39;&lt;Y&gt;={:.5f}&#39;.format(y_mean_value))
        axs[2].plot(rr_mean_value, rr_mean_value*0 + r_mean_value, &#39;g--&#39;, linewidth =2.0, label=&#39;&lt;R&gt;={:.5f}&#39;.format(r_mean_value))
        axs[2].grid(True)
        axs[2].legend()
        axs[2].set_title(&#39;Normalized autocorr. cross-sections&#39;)
        axs[3].plot(xcr, data_ACR[ysz//2, :], &#39;rx&#39;, label=&#39;X data&#39;)
        axs[3].plot(ycr, data_ACR[:, xsz//2], &#39;bd&#39;, label=&#39;Y data&#39;)
        axs[3].plot(rcr, r_ACR, &#39;g+&#39;, ms=10, label=&#39;R data&#39;)
        axs[3].plot(xcr[xsz//2], data_ACR[ysz//2, xsz//2], &#39;md&#39;, label=&#39;Peak: {:.4f}, {:.4f}&#39;.format(xcr[xsz//2], data_ACR[ysz//2, xsz//2]))
        axs[3].plot(x_left, xacr_left, &#39;r&#39;)
        axs[3].plot(x_right, xacr_right, &#39;r&#39;, label=&#39;X extrap.: {:.4f}, {:.4f}&#39;.format(x_right[0], xacr_right[0]))
        axs[3].plot(y_left, yacr_left, &#39;b&#39;)
        axs[3].plot(y_right, yacr_right, &#39;b&#39;, label=&#39;Y extrap.: {:.4f}, {:.4f}&#39;.format(y_right[0], yacr_right[0]))
        axs[3].plot(r_left, racr_left, &#39;g&#39;)
        axs[3].plot(r_right, racr_right, &#39;g&#39;, label=&#39;R extrap.: {:.4f}, {:.4f}&#39;.format(r_right[0], racr_right[0]))
        axs[3].text(0.03, 0.92, &#39;xSNR = {:.2f}&#39;.format(xSNR), color=&#39;r&#39;, transform=axs[3].transAxes, fontsize=fs)
        axs[3].text(0.03, 0.86, &#39;ySNR = {:.2f}&#39;.format(ySNR), color=&#39;b&#39;, transform=axs[3].transAxes, fontsize=fs)
        axs[3].text(0.03, 0.80, &#39;rSNR = {:.2f}&#39;.format(rSNR), color=&#39;g&#39;, transform=axs[3].transAxes, fontsize=fs)
        axs[3].grid(True)
        axs[3].legend()
        axs[3].set_xlim(-5,5)
        axs[3].set_title(&#39;Normalized autocorr. cross-sections&#39;)

        if save_res_png:
            #print(&#39;X:   ACR peak = {:.4f}, Noise-Free ACR Peak = {:.4f}, Squared Mean = {:.4f}&#39;.format(x_acr, x_noise_free_acr, x_mean_value ))
            #print(&#39;xSNR = {:.2f}&#39;.format(xSNR))
            #print(&#39;Y:   ACR peak = {:.4f}, Noise-Free ACR Peak = {:.4f}, Squared Mean = {:.4f}&#39;.format(y_acr, y_noise_free_acr, y_mean_value ))
            #print(&#39;ySNR = {:.4f}&#39;.format(ySNR))
            #print(&#39;R:   ACR peak = {:.4f}, Noise-Free ACR Peak = {:.4f}, Squared Mean = {:.4f}&#39;.format(r_acr, r_noise_free_acr, r_mean_value ))
            #print(&#39;rSNR = {:.4f}&#39;.format(rSNR))
            fig.savefig(res_fname, dpi=dpi)
            print(&#39;Saved the results into the file: &#39;, res_fname)

    return xSNR, ySNR, rSNR</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.Two_Image_Analysis"><code class="name flex">
<span>def <span class="ident">Two_Image_Analysis</span></span>(<span>params)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyzes the registration
quality between two frames (for DASK registration analysis)</p>
<p>Parameterss:
params : list of params
params = [frame1_filename, frame2_filename, eval_bounds, eval_metrics]
eval_bounds = [xi,
xa, yi, ya]
eval_metrics = ['NSAD', 'NCC', 'NMI', 'FSC']</p>
<p>Returns
results : list of results</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Two_Image_Analysis(params):
    &#39;&#39;&#39;
    Analyzes the registration  quality between two frames (for DASK registration analysis)

    Parameterss:
    params : list of params
        params = [frame1_filename, frame2_filename, eval_bounds, eval_metrics]
        eval_bounds = [xi,  xa, yi, ya]
        eval_metrics = [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;]

    Returns
        results : list of results
    &#39;&#39;&#39;
    frame1_filename, frame2_filename, eval_bounds, eval_metrics = params
    xi_eval,  xa_eval, yi_eval, ya_eval = eval_bounds


    I1 = tiff.imread(frame1_filename)
    I1c = I1[yi_eval:ya_eval, xi_eval:xa_eval]
    I2 = tiff.imread(frame2_filename)
    I2c = I2[yi_eval:ya_eval, xi_eval:xa_eval]
    fr_mean = abs(I1c/2.0 + I2c/2.0)
    dy, dx = shape(I2c)

    results = []
    for metric in eval_metrics:
        if metric == &#39;NSAD&#39;:
            results.append(mean(abs(I1c-I2c))/(np.mean(fr_mean)-np.amin(fr_mean)))
        if metric == &#39;NCC&#39;:
            results.append(Two_Image_NCC_SNR(I1c, I2c)[0])
        if metric == &#39;NMI&#39;:
            results.append(mutual_information_2d(I1c.ravel(), I2c.ravel(), sigma=1.0, bin=2048, normalized=True))
        if metric == &#39;FSC&#39;:
            #SNRt is SNR threshold for determining the resolution bandwidth
            # force square images for FSC
            if dx != dy:
                d=min((dx//2, dy//2))
                results.append(Two_Image_FSC(I1c[dy//2-d:dy//2+d, dx//2-d:dx//2+d], I2c[dy//2-d:dy//2+d, dx//2-d:dx//2+d], SNRt=0.143, disp_res=False)[4])
            else:
                results.append(Two_Image_FSC(I1c, I2c, SNRt=0.143, disp_res=False)[4])

    return results</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.Two_Image_FSC"><code class="name flex">
<span>def <span class="ident">Two_Image_FSC</span></span>(<span>img1, img2, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform Fourier Shell Correlation to determine the image resolution, after [1]. ©G.Shtengel, 10/2019. gleb.shtengel@gmail.com
FSC is determined from radially averaged foirier cross-correlation (with optional selection of range of angles for radial averaging).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img1</code></strong> :&ensp;<code>2D array</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>img2</code></strong> :&ensp;<code>2D array</code></dt>
<dd>&nbsp;</dd>
<dt>kwargs:</dt>
<dt><strong><code>SNRt</code></strong> :&ensp;<code>float</code></dt>
<dd>SNR threshold for determining the resolution bandwidth</dd>
<dt><strong><code>astart</code></strong> :&ensp;<code>float</code></dt>
<dd>Start angle for radial averaging. Default is 0</dd>
<dt><strong><code>astop</code></strong> :&ensp;<code>float</code></dt>
<dd>Stop angle for radial averaging. Default is 90</dd>
<dt><strong><code>symm</code></strong> :&ensp;<code>int</code></dt>
<dd>Symmetry factor (how many times Start and stop angle intervalks are repeated within 360 deg). Default is 4.</dd>
<dt><strong><code>disp_res</code></strong> :&ensp;<code>boolean</code></dt>
<dd>display results (plots) (default is False)</dd>
<dt><strong><code>ax</code></strong> :&ensp;<code>axis object (matplotlip)</code></dt>
<dd>to export the plot</dd>
<dt><strong><code>save_res_png</code></strong> :&ensp;<code>boolean</code></dt>
<dd>save results into PNG file (default is False)</dd>
<dt><strong><code>res_fname</code></strong> :&ensp;<code>string</code></dt>
<dd>filename for the result image ('SNR_result.png')</dd>
<dt><strong><code>img_labels</code></strong> :&ensp;<code>[string, string]</code></dt>
<dd>optional image labels</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code></dt>
<dd>dots-per-inch resolution for the output image</dd>
<dt><strong><code>pixel</code></strong> :&ensp;<code>float</code></dt>
<dd>optional pixel size in nm. If not provided, will be ignored.
if provided, second axis will be added on top with inverse pixels</dd>
<dt><strong><code>xrange</code></strong> :&ensp;<code>[float, float]</code></dt>
<dd>range of x axis in FSC plot in inverse pixels
if not provided [0, 0.5] range will be used</dd>
</dl>
<p>Returns FSC_sp_frequencies, FSC_data, x2, T, FSC_bw
FSC_sp_frequencies : float array
Spatial Frequency (/Nyquist) - for FSC plot
FSC_data: float array
x2 : float array
Spatial Frequency (/Nyquist) - for threshold line plot
T : float array
threshold line plot
FSC_bw : float
the value of FSC determined as an intersection of smoothed data threshold</p>
<p>[1]. M. van Heela, and M. Schatzb, "Fourier shell correlation threshold criteria," Journal of Structural Biology 151, 250-262 (2005)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Two_Image_FSC(img1, img2, **kwargs):
    &#39;&#39;&#39;
    Perform Fourier Shell Correlation to determine the image resolution, after [1]. ©G.Shtengel, 10/2019. gleb.shtengel@gmail.com
    FSC is determined from radially averaged foirier cross-correlation (with optional selection of range of angles for radial averaging).

    Parameters
    ---------
    img1 : 2D array
    img2 : 2D array

    kwargs:
    SNRt : float
        SNR threshold for determining the resolution bandwidth
    astart : float
        Start angle for radial averaging. Default is 0
    astop : float
        Stop angle for radial averaging. Default is 90
    symm : int
        Symmetry factor (how many times Start and stop angle intervalks are repeated within 360 deg). Default is 4.
    disp_res : boolean
        display results (plots) (default is False)
    ax : axis object (matplotlip)
        to export the plot
    save_res_png : boolean
        save results into PNG file (default is False)
    res_fname : string
        filename for the result image (&#39;SNR_result.png&#39;)
    img_labels : [string, string]
        optional image labels
    dpi : int
        dots-per-inch resolution for the output image
    pixel : float
        optional pixel size in nm. If not provided, will be ignored.
        if provided, second axis will be added on top with inverse pixels
    xrange : [float, float]
        range of x axis in FSC plot in inverse pixels
        if not provided [0, 0.5] range will be used

    Returns FSC_sp_frequencies, FSC_data, x2, T, FSC_bw
        FSC_sp_frequencies : float array
            Spatial Frequency (/Nyquist) - for FSC plot
        FSC_data: float array
        x2 : float array
            Spatial Frequency (/Nyquist) - for threshold line plot
        T : float array
            threshold line plot
        FSC_bw : float
            the value of FSC determined as an intersection of smoothed data threshold

    [1]. M. van Heela, and M. Schatzb, &#34;Fourier shell correlation threshold criteria,&#34; Journal of Structural Biology 151, 250-262 (2005)
    &#39;&#39;&#39;
    SNRt = kwargs.get(&#34;SNRt&#34;, 0.1)
    astart = kwargs.get(&#34;astart&#34;, 0.0)
    astop = kwargs.get(&#34;astop&#34;, 90.0)
    symm = kwargs.get(&#34;symm&#34;, 4)
    disp_res = kwargs.get(&#34;disp_res&#34;, False)
    ax = kwargs.get(&#34;ax&#34;, &#39;&#39;)
    save_res_png = kwargs.get(&#34;save_res_png&#34;, False)
    res_fname = kwargs.get(&#34;res_fname&#34;, &#39;FSC_results.png&#39;)
    img_labels = kwargs.get(&#34;img_labels&#34;, [&#39;Image 1&#39;, &#39;Image 2&#39;])
    dpi = kwargs.get(&#34;dpi&#34;, 300)
    pixel = kwargs.get(&#34;pixel&#34;, 0.0)
    xrange = kwargs.get(&#34;xrange&#34;, [0, 0.5])

    #Check whether the inputs dimensions match and the images are square
    if disp_res:
        if ( np.shape(img1) != np.shape(img2) ) :
            print(&#39;input images must have the same dimensions&#39;)
        if ( np.shape(img1)[0] != np.shape(img1)[1]) :
            print(&#39;input images must be squares&#39;)
    I1 = fftshift(fftn(ifftshift(img1)))  # I1 and I2 store the FFT of the images to be used in the calcuation for the FSC
    I2 = fftshift(fftn(ifftshift(img2)))

    C_imre = np.multiply(I1,np.conj(I2))
    C12_ar = abs(np.multiply((I1+I2),np.conj(I1+I2)))
    y0,x0 = argmax2d(C12_ar)
    C1 = radial_profile_select_angles(abs(np.multiply(I1,np.conj(I1))), [x0,y0], astart, astop, symm)
    C2 = radial_profile_select_angles(abs(np.multiply(I2,np.conj(I2))), [x0,y0], astart, astop, symm)
    C  = radial_profile_select_angles(np.real(C_imre), [x0,y0], astart, astop, symm) + 1j * radial_profile_select_angles(np.imag(C_imre), [x0,y0], astart, astop, symm)

    FSC_data = abs(C)/np.sqrt(abs(np.multiply(C1,C2)))
    &#39;&#39;&#39;
    T is the SNR threshold calculated accoring to the input SNRt, if nothing is given
    a default value of 0.1 is used.

    x2 contains the normalized spatial frequencies
    &#39;&#39;&#39;
    r = np.arange(1+np.shape(img1)[0])
    n = 2*np.pi*r
    n[0] = 1
    eps = np.finfo(float).eps
    t1 = np.divide(np.ones(np.shape(n)),n+eps)
    t2 = SNRt + 2*np.sqrt(SNRt)*t1 + np.divide(np.ones(np.shape(n)),np.sqrt(n))
    t3 = SNRt + 2*np.sqrt(SNRt)*t1 + 1
    T = np.divide(t2,t3)
    #FSC_sp_frequencies = np.arange(np.shape(C)[0])/(np.shape(img1)[0]/sqrt(2.0))
    #x2 = r/(np.shape(img1)[0]/sqrt(2.0))
    FSC_sp_frequencies = np.arange(np.shape(C)[0])/(np.shape(img1)[0])
    x2 = r/(np.shape(img1)[0])
    FSC_data_smooth = smooth(FSC_data, 20)
    FSC_bw = find_BW(FSC_sp_frequencies, FSC_data_smooth, SNRt)
    &#39;&#39;&#39;
    If the disp_res input is set to True, an output plot is generated.
    &#39;&#39;&#39;
    if disp_res:
        if ax==&#39;&#39;:
            fig = plt.figure(figsize=(8,12))
            axs0 = fig.add_subplot(3,2,1)
            axs1 = fig.add_subplot(3,2,2)
            axs2 = fig.add_subplot(3,2,3)
            axs3 = fig.add_subplot(3,2,4)
            ax = fig.add_subplot(3,1,3)
            fig.subplots_adjust(left=0.01, bottom=0.06, right=0.99, top=0.975, wspace=0.25, hspace=0.10)
            vmin1, vmax1 = get_min_max_thresholds(img1, disp_res=False)
            vmin2, vmax2 = get_min_max_thresholds(img2, disp_res=False)
            axs0.imshow(img1, cmap=&#39;Greys&#39;, vmin=vmin1, vmax=vmax1)
            axs1.imshow(img2, cmap=&#39;Greys&#39;, vmin=vmin2, vmax=vmax2)
            x = np.linspace(0, 1.41, 500)
            axs2.set_xlim(-1,1)
            axs2.set_ylim(-1,1)
            axs2.imshow(np.log(abs(I1)), extent=[-1, 1, -1, 1], cmap = &#39;Greys_r&#39;)
            axs3.set_xlim(-1,1)
            axs3.set_ylim(-1,1)
            axs3.imshow(np.log(abs(I2)), extent=[-1, 1, -1, 1], cmap = &#39;Greys_r&#39;)
            for i in np.arange(symm):
                ai = np.radians(astart + 360.0/symm*i)
                aa = np.radians(astop + 360.0/symm*i)
                axs2.plot(x * np.cos(ai), x * np.sin(ai), color=&#39;orange&#39;, linewidth = 0.5)
                axs3.plot(x * np.cos(ai), x * np.sin(ai), color=&#39;orange&#39;, linewidth = 0.5)
                axs2.plot(x * np.cos(aa), x * np.sin(aa), color=&#39;orange&#39;, linewidth = 0.5)
                axs3.plot(x * np.cos(aa), x * np.sin(aa), color=&#39;orange&#39;, linewidth = 0.5)
            ttls = img_labels.copy()
            ttls.append(&#39;FFT of &#39;+img_labels[0])
            ttls.append(&#39;FFT of &#39;+img_labels[1])
            for axi, ttl in zip([axs0, axs1, axs2, axs3], ttls):
                axi.grid(False)
                axi.axis(False)
                axi.set_title(ttl)

    if disp_res or ax != &#39;&#39;:
        ax.plot(FSC_sp_frequencies, FSC_data, label = &#39;FSC data&#39;, color=&#39;r&#39;)
        ax.plot(FSC_sp_frequencies, FSC_data_smooth, label = &#39;FSC data smoothed&#39;, color=&#39;b&#39;)
        ax.plot(x2, x2*0.0+SNRt, &#39;--&#39;, label = &#39;Threshold SNR = {:.3f}&#39;.format(SNRt), color=&#39;m&#39;)
        if pixel&gt;1e-6:
            label = &#39;FSC BW = {:.3f} inv.pix., or {:.2f} nm&#39;.format(FSC_bw, pixel/FSC_bw)
        else:
            label = &#39;FSC BW = {:.3f}&#39;.format(FSC_bw)
        ax.plot(np.array((FSC_bw,FSC_bw)), np.array((0.0,1.0)), &#39;--&#39;, label = label, color = &#39;g&#39;)
        ax.set_xlim(xrange)
        ax.legend()
        ax.set_xlabel(&#39;Spatial Frequency (inverse pixels)&#39;)
        ax.set_ylabel(&#39;FSC Magnitude&#39;)
        ax.grid(True)
        if pixel&gt;1e-6:

            def forward(x):
                return x/pixel
            def inverse(x):
                return x*pixel
            secax = ax.secondary_xaxis(&#39;top&#39;, functions=(forward, inverse))
            secax.set_xlabel(&#39;Spatial Frequency ($nm^{-1}$)&#39;)

    if disp_res:
        ax.set_position([0.1, 0.05, 0.85, 0.28])
        print(&#39;FSC BW = {:.5f}&#39;.format(FSC_bw))
        if save_res_png:
            fig.savefig(res_fname, dpi=dpi)
            print(&#39;Saved the results into the file: &#39;, res_fname)
    return (FSC_sp_frequencies, FSC_data, x2, T, FSC_bw)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.Two_Image_NCC_SNR"><code class="name flex">
<span>def <span class="ident">Two_Image_NCC_SNR</span></span>(<span>img1, img2, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimates normalized cross-correlation and SNR of two images.
©G.Shtengel 04/2022 gleb.shtengel@gmail.com</p>
<p>Calculates SNR from cross-correlation of two images after [1, 2, 3].</p>
<p>Parameters</p>
<hr>
<p>img1 : 2D array
img2 : 2D array</p>
<p>kwargs:
zero_mean: boolean
if True, cross-correlation is zero-mean</p>
<p>Returns:
NCC, SNR : float, float
NCC - normalized cross-correlation coefficient
SNR - Signal-to-Noise ratio based on NCC</p>
<p>[1] J. Frank, L. AI-Ali, Signal-to-noise ratio of electron micrographs obtained by cross correlation. Nature 256, 4 (1975).
[2] J. Frank, in: Computer Processing of Electron Microscopic Images. Ed. P.W. Hawkes (Springer, Berlin, 1980).
[3] M. Radermacher, T. Ruiz, On cross-correlations, averages and noise in electron microscopy. Acta Crystallogr. Sect. F Struct. Biol. Commun. 75, 12–18 (2019).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Two_Image_NCC_SNR(img1, img2, **kwargs):
    &#39;&#39;&#39;
    Estimates normalized cross-correlation and SNR of two images.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Calculates SNR from cross-correlation of two images after [1, 2, 3].

    Parameters
    ---------
    img1 : 2D array
    img2 : 2D array

    kwargs:
    zero_mean: boolean
        if True, cross-correlation is zero-mean

    Returns:
        NCC, SNR : float, float
            NCC - normalized cross-correlation coefficient
            SNR - Signal-to-Noise ratio based on NCC

   [1] J. Frank, L. AI-Ali, Signal-to-noise ratio of electron micrographs obtained by cross correlation. Nature 256, 4 (1975).
   [2] J. Frank, in: Computer Processing of Electron Microscopic Images. Ed. P.W. Hawkes (Springer, Berlin, 1980).
   [3] M. Radermacher, T. Ruiz, On cross-correlations, averages and noise in electron microscopy. Acta Crystallogr. Sect. F Struct. Biol. Commun. 75, 12–18 (2019).

    &#39;&#39;&#39;
    zero_mean = kwargs.get(&#34;zero_mean&#34;, True)

    if img1.shape==img2.shape:
        ysz, xsz = img1.shape
        if zero_mean:
            img1 = img1- img1.mean()
            img2 = img2- img2.mean()
        xy = np.sum((img1.ravel()*img2.ravel()))/(xsz*ysz)
        xx = np.sum((img1.ravel()*img1.ravel()))/(xsz*ysz)
        yy = np.sum((img2.ravel()*img2.ravel()))/(xsz*ysz)
        NCC = xy / np.sqrt(xx*yy)
        SNR = NCC / (1-NCC)

    else:
        print(&#34;img1 and img2 shapes must be equal&#34;)
        NCC = 0.0
        SNR = 0.0

    return NCC, SNR</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.analyze_mrc_stack_registration"><code class="name flex">
<span>def <span class="ident">analyze_mrc_stack_registration</span></span>(<span>mrc_filename, DASK_client, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Read MRC stack and analyze registration - calculate NSAD, NCC, and MI.
©G.Shtengel, 04/2021. gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mrc_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>File name (full path) of the mrc stack to be analyzed</dd>
</dl>
<p>DASK client (needs to be initialized and running by this time)</p>
<dl>
<dt>kwargs:</dt>
<dt><strong><code>use_DASK</code></strong> :&ensp;<code>boolean</code></dt>
<dd>use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).</dd>
<dt><strong><code>DASK_client_retries</code></strong> :&ensp;<code>int (default to 0)</code></dt>
<dd>Number of allowed automatic retries if a task fails</dd>
<dt><strong><code>frame_inds</code></strong> :&ensp;<code>array</code></dt>
<dd>Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames</dd>
<dt><strong><code>invert_data</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, the data will be inverted</dd>
<dt><strong><code>evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.</dd>
<dt><strong><code>sliding_evaluation_box</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box</dd>
<dt><strong><code>start_evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>see above</dd>
<dt><strong><code>stop_evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>see above</dd>
<dt>save_res_png
: boolean</dt>
<dt>Save PNG images of the intermediate processing statistics and final registration quality check</dt>
<dt><strong><code>save_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the filename to save the results. If empty, mrc_filename+'_RegistrationQuality.csv' will be used</dd>
<dt><strong><code>save_sample_frames_png</code></strong> :&ensp;<code>bolean</code></dt>
<dd>If True, sample frames with superimposed eval box and registration analysis data will be saved into png files. Default is True</dd>
</dl>
<p>Returns reg_summary : PD data frame, registration_summary_xlsx : path to summary XLSX workbook</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def analyze_mrc_stack_registration(mrc_filename, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Read MRC stack and analyze registration - calculate NSAD, NCC, and MI.
    ©G.Shtengel, 04/2021. gleb.shtengel@gmail.com

    Parameters
    ---------
    mrc_filename : str
        File name (full path) of the mrc stack to be analyzed
    DASK client (needs to be initialized and running by this time)

    kwargs:
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    frame_inds : array
        Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
    invert_data : boolean
        If True, the data will be inverted
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    save_filename : str
        Path to the filename to save the results. If empty, mrc_filename+&#39;_RegistrationQuality.csv&#39; will be used
    save_sample_frames_png : bolean
        If True, sample frames with superimposed eval box and registration analysis data will be saved into png files. Default is True

    Returns reg_summary : PD data frame, registration_summary_xlsx : path to summary XLSX workbook
    &#39;&#39;&#39;
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    save_filename = kwargs.get(&#34;save_filename&#34;, mrc_filename )
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    registration_summary_xlsx = save_filename.replace(&#39;.mrc&#39;, &#39;_RegistrationQuality.xlsx&#39;)
    save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, True)

    if sliding_evaluation_box:
        print(&#39;Will use sliding (linearly) evaluation box&#39;)
        print(&#39;   Starting with box:  &#39;, start_evaluation_box)
        print(&#39;   Finishing with box: &#39;, stop_evaluation_box)
    else:
        print(&#39;Will use fixed evaluation box: &#39;, evaluation_box)

    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;)
    header = mrc_obj.header
    mrc_mode = header.mode
    nx, ny, nz = int32(header[&#39;nx&#39;]), int32(header[&#39;ny&#39;]), int32(header[&#39;nz&#39;])
    header_dict = {}
    for record in header.dtype.names: # create dictionary from the header data
        if (&#39;extra&#39; not in record) and (&#39;label&#39; not in record):
            header_dict[record] = header[record]
    &#39;&#39;&#39;
    mode 0 -&gt; uint8
    mode 1 -&gt; int16
    mode 2 -&gt; float32
    mode 4 -&gt; complex64
    mode 6 -&gt; uint16
    &#39;&#39;&#39;
    if mrc_mode==0:
        dt_mrc=uint8
    if mrc_mode==1:
        dt_mrc=int16
    if mrc_mode==2:
        dt_mrc=float32
    if mrc_mode==4:
        dt_mrc=complex64
    if mrc_mode==6:
        dt_mrc=uint16
    print(&#39;mrc_mode={:d} &#39;.format(mrc_mode), &#39;, dt_mrc=&#39;, dt_mrc)

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = nx
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ny
    evals = [xi_eval, xa_eval, yi_eval, ya_eval]

    frame_inds_default = np.arange(nz-1)+1
    frame_inds = np.array(kwargs.get(&#34;frame_inds&#34;, frame_inds_default))
    nf = frame_inds[-1]-frame_inds[0]+1
    if frame_inds[0]==0:
        frame_inds = frame_inds+1
    sample_frame_inds = [frame_inds[nf//10], frame_inds[nf//2], frame_inds[nf//10*9]]
    print(&#39;Will analyze regstrations in {:d} frames&#39;.format(len(frame_inds)))
    print(&#39;Will save the data into &#39; + registration_summary_xlsx)
    if sliding_evaluation_box:
        dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
        dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
    else:
        dx_eval = 0
        dy_eval = 0

    params_mrc_mult = []
    xi_evals = np.zeros(nf, dtype=int16)
    xa_evals = np.zeros(nf, dtype=int16)
    yi_evals = np.zeros(nf, dtype=int16)
    ya_evals = np.zeros(nf, dtype=int16)
    for j, fr in enumerate(frame_inds):
        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*(fr-frame_inds[0])//nf
            yi_eval = start_evaluation_box[0] + dy_eval*(fr-frame_inds[0])//nf
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny
            evals = [xi_eval, xa_eval, yi_eval, ya_eval]
        xi_evals[j] = xi_eval
        xa_evals[j] = xa_eval
        yi_evals[j] = yi_eval
        ya_evals[j] = ya_eval
        if fr in sample_frame_inds:
            save_frame_png = save_sample_frames_png
            filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame{:d}.png&#39;.format(fr)
        else:
            save_frame_png = False
            filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame.png&#39;
        params_mrc_mult.append([mrc_filename, fr, invert_data, evals, save_frame_png, filename_frame_png])

    if use_DASK:
        mrc_obj.close()
        print(&#39;Using DASK distributed&#39;)
        futures = DASK_client.map(evaluate_registration_two_frames, params_mrc_mult, retries = DASK_client_retries)
        dask_results = DASK_client.gather(futures)
        image_nsad = np.array([res[0] for res in dask_results])
        image_ncc = np.array([res[1] for res in dask_results])
        image_mi = np.array([res[2] for res in dask_results])
    else:
        print(&#39;Using Local Computation&#39;)
        image_nsad = np.zeros(nf, dtype=float)
        image_ncc = np.zeros(nf, dtype=float)
        image_mi = np.zeros(nf, dtype=float)
        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*frame_inds[0]//nf
            yi_eval = start_evaluation_box[0] + dy_eval*frame_inds[0]//nf
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny
        if invert_data:
            prev_frame = -1.0 * ((mrc_obj.data[frame_inds[0]-1, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float))
        else:
            prev_frame =(mrc_obj.data[frame_inds[0]-1, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float)
        for j, frame_ind in enumerate(tqdm(frame_inds, desc=&#39;Evaluating frame registration: &#39;)):
            if sliding_evaluation_box:
                xi_eval = start_evaluation_box[2] + dx_eval*j//nf
                yi_eval = start_evaluation_box[0] + dy_eval*j//nf
                if start_evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + start_evaluation_box[3]
                else:
                    xa_eval = nx
                if start_evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + start_evaluation_box[1]
                else:
                    ya_eval = ny

            if invert_data:
                curr_frame = -1.0 * ((mrc_obj.data[frame_ind, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float))
            else:
                curr_frame = (mrc_obj.data[frame_ind, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float)
            if use_cp:
                curr_frame_cp = cp.array(curr_frame)
                prev_frame_cp = cp.array(prev_frame)
                fr_mean = cp.abs(curr_frame_cp/2.0 + prev_frame_cp/2.0)
            else:
                fr_mean = abs(curr_frame/2.0 + prev_frame/2.0)

            image_ncc[j-1] = Two_Image_NCC_SNR(curr_frame, prev_frame)[0]
            if use_cp:
                image_nsad[j-1] =  cp.asnumpy(cp.mean(cp.abs(curr_frame_cp-prev_frame_cp))/(cp.mean(fr_mean)-cp.amin(fr_mean)))
                image_mi[j-1] = cp.asnumpy(mutual_information_2d_cp(prev_frame_cp.ravel(), curr_frame_cp.ravel(), sigma=1.0, bin=2048, normalized=True))
            else:
                image_nsad[j-1] =  mean(abs(curr_frame-prev_frame))/(np.mean(fr_mean)-np.amin(fr_mean))
                image_mi[j-1] = mutual_information_2d(prev_frame.ravel(), curr_frame.ravel(), sigma=1.0, bin=2048, normalized=True)
            prev_frame = curr_frame.copy()
            del curr_frame_cp, prev_frame_cp
            if (frame_ind in sample_frame_inds) and save_sample_frames_png:
                filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame{:d}.png&#39;.format(j)
                fr_img = (mrc_obj.data[frame_ind, :, :].astype(dt_mrc)).astype(float)
                yshape, xshape = fr_img.shape
                fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
                fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
                dmin, dmax = get_min_max_thresholds(fr_img[yi_eval:ya_eval, xi_eval:xa_eval])
                if invert_data:
                    ax.imshow(fr_img, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
                else:
                    ax.imshow(fr_img, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
                ax.text(0.06, 0.95, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(frame_ind, image_nsad[j-1], image_ncc[j-1], image_mi[j-1]), color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
                rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
                ax.add_patch(rect_patch)
                ax.axis(&#39;off&#39;)
                fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
                plt.close(fig)

        mrc_obj.close()

    nsads = [np.mean(image_nsad), np.median(image_nsad), np.std(image_nsad)]
    #image_ncc = image_ncc[1:-1]
    nccs = [np.mean(image_ncc), np.median(image_ncc), np.std(image_ncc)]
    nmis = [np.mean(image_mi), np.median(image_mi), np.std(image_mi)]

    print(&#39;Saving the Registration Quality Statistics into the file: &#39;, registration_summary_xlsx)
    xlsx_writer = pd.ExcelWriter(registration_summary_xlsx, engine=&#39;xlsxwriter&#39;)
    columns=[&#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;, &#39;Image NSAD&#39;, &#39;Image NCC&#39;, &#39;Image MI&#39;]
    reg_summary = pd.DataFrame(np.vstack((frame_inds, xi_evals, xa_evals, yi_evals, ya_evals, image_nsad, image_ncc, image_mi)).T, columns = columns, index = None)
    reg_summary.to_excel(xlsx_writer, index=None, sheet_name=&#39;Registration Quality Statistics&#39;)
    Stack_info = pd.DataFrame([{&#39;Stack Filename&#39; : mrc_filename, &#39;Sample_ID&#39; : Sample_ID, &#39;invert_data&#39; : invert_data}]).T # prepare to be save in transposed format
    header_info = pd.DataFrame([header_dict]).T
    Stack_info = Stack_info.append(header_info)
    Stack_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;Stack Info&#39;)
    xlsx_writer.save()

    return reg_summary, registration_summary_xlsx</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.analyze_registration_frames"><code class="name flex">
<span>def <span class="ident">analyze_registration_frames</span></span>(<span>DASK_client, frame_filenames, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform and save FIB-SEM data set. A new vesion, with variable zbin_factor option. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com</p>
<p>Parameters
DASK_client :
frame_filenames : list of strings
List of filenames (one for each transformed / z-binned frame)</p>
<h2 id="kwargs">Kwargs</h2>
<p>use_DASK : boolean
perform remote DASK computations
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails
save_registration_summary : boolean
If True (default), the rgistration analysis will be saved
data_dir : str
data directory (path)
frame_inds : int array
Array of frame indecis. If not set or set to np.array((-1)), all frames will be analyzed
fnm_reg : str
filename for the final registed dataset
npts : array or list of int
Numbers of Keypoints used for registration
error_abs_mean : array or list of float
mean abs error between registered key-points
eval_bounds : list of [xi_eval, xa_eval, yi_eval, ya_eval] lists of int
Evaluation boundaries for analysis
eval_metrics : list of str
list of evaluation metrics to use. default is ['NSAD', 'NCC', 'NMI', 'FSC']
save_sample_frames_png : bolean
If True, sample frames with superimposed eval box and registration analysis data will be saved into png files
sample_frame_inds : list of int
list of sample frame indecis
save_registration_summary : boolean
If True, the registration summary is saved into XLSX file
disp_res : bolean
If True (default), intermediate messages and results will be displayed.</p>
<p>Returns:
reg_summary, reg_summary_xlsx
reg_summary : pandas DataFrame
reg_summary = pd.DataFrame(np.vstack((npts, error_abs_mean, image_nsad, image_ncc, image_mi)
reg_summary_xlsx : name of the XLSX workbook containing the data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def analyze_registration_frames(DASK_client, frame_filenames, **kwargs):
    &#39;&#39;&#39;
    Transform and save FIB-SEM data set. A new vesion, with variable zbin_factor option. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters
    DASK_client :
    frame_filenames : list of strings
        List of filenames (one for each transformed / z-binned frame)

    kwargs
    ---------
    use_DASK : boolean
        perform remote DASK computations
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    save_registration_summary : boolean
        If True (default), the rgistration analysis will be saved
    data_dir : str
        data directory (path)
    frame_inds : int array
        Array of frame indecis. If not set or set to np.array((-1)), all frames will be analyzed
    fnm_reg : str
        filename for the final registed dataset
    npts : array or list of int
        Numbers of Keypoints used for registration
    error_abs_mean : array or list of float
        mean abs error between registered key-points
    eval_bounds : list of [xi_eval, xa_eval, yi_eval, ya_eval] lists of int
        Evaluation boundaries for analysis
    eval_metrics : list of str
        list of evaluation metrics to use. default is [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;]
    save_sample_frames_png : bolean
        If True, sample frames with superimposed eval box and registration analysis data will be saved into png files
    sample_frame_inds : list of int
        list of sample frame indecis
    save_registration_summary : boolean
        If True, the registration summary is saved into XLSX file
    disp_res : bolean
        If True (default), intermediate messages and results will be displayed.

    Returns:
    reg_summary, reg_summary_xlsx
        reg_summary : pandas DataFrame
        reg_summary = pd.DataFrame(np.vstack((npts, error_abs_mean, image_nsad, image_ncc, image_mi)
        reg_summary_xlsx : name of the XLSX workbook containing the data
    &#39;&#39;&#39;

    use_DASK = kwargs.get(&#34;use_DASK&#34;, True)  # do not use DASK the data is to be saved
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)

    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    fpath_reg = os.path.join(data_dir, fnm_reg)

    save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, True)
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    save_registration_summary = kwargs.get(&#39;save_registration_summary&#39;, True)
    dump_filename = kwargs.get(&#39;dump_filename&#39;, &#39;&#39;)

    frame_inds_default = np.arange(len(frame_filenames))
    frame_inds = np.array(kwargs.get(&#34;frame_inds&#34;, frame_inds_default))
    nfrs = len(frame_inds)                                                   # number of source images(frames) before z-binning
    sample_frame_inds = kwargs.get(&#34;sample_frame_inds&#34;, [frame_inds[nfrs//10], frame_inds[nfrs//2], frame_inds[nfrs//10*9-1]])
    npts = kwargs.get(&#34;npts&#34;, False)
    error_abs_mean = kwargs.get(&#34;error_abs_mean&#34;, False)

    first_frame = tiff.imread(frame_filenames[frame_inds[0]])
    ya, xa = first_frame.shape
    eval_bounds = kwargs.get(&#34;eval_bounds&#34;, [[0, xa, 0, ya]]*nfrs)
    eval_metrics = kwargs.get(&#39;eval_metrics&#39;, [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;])

    params_frames = []
    for j, frame_ind in enumerate(tqdm(frame_inds[0:-1], desc=&#39;Setting up parameter sets&#39;, display=disp_res)):
        params_frames.append([frame_filenames[frame_ind], frame_filenames[frame_ind+1], eval_bounds[j], eval_metrics])

    if use_DASK:
        if disp_res:
            print(&#39;Will perform distributed computations using DASK&#39;)
        if disp_res:
            print(&#39;Starting DASK jobs&#39;)
        futures_til = DASK_client.map(Two_Image_Analysis, params_frames, retries = DASK_client_retries)
        results_til = DASK_client.gather(futures_til)
        image_metrics = np.array(results_til)  # 2D array  np.array([[image_nsad, image_ncc, image_mi]])
        if disp_res:
            print(&#39;Finished DASK jobs&#39;)

    else:   # if DASK is not used - perform local computations
        if disp_res:
            print(&#39;Will perform local computations&#39;)
        image_metrics = np.zeros((nfrs-1, len(eval_metrics)), dtype=float)
        for j, params_frame in enumerate(tqdm(params_frames, desc = &#39;Analyzing frame pairs&#39;, display = disp_res)):
            image_metrics[j, :] = Two_Image_Analysis(params_frame)
        results_til = []

    # save sample frames
    if save_sample_frames_png:
        for frame_ind in sample_frame_inds:
            filename_frame_png = os.path.splitext(fpath_reg)[0]+&#39;_sample_image_frame{:d}.png&#39;.format(frame_ind)
            fr_img = tiff.imread(frame_filenames[frame_ind]).astype(float)
            yshape, xshape = fr_img.shape
            fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
            fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
            xi_eval, xa_eval, yi_eval, ya_eval = eval_bounds[frame_ind]
            dmin, dmax = get_min_max_thresholds(fr_img[yi_eval:ya_eval, xi_eval:xa_eval], disp_res=False)
            ax.imshow(fr_img, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
            sample_text = &#39;Frame={:d}&#39;.format(frame_ind)
            for k, metric in enumerate(eval_metrics):
                sample_text = sample_text + &#39;,  &#39;+ metric + &#39;={:.3f}&#39;.format(image_metrics[frame_ind, k])
            ax.text(0.06, 0.95, sample_text, color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
            rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
            ax.add_patch(rect_patch)
            ax.axis(&#39;off&#39;)
            fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
            plt.close(fig)

    columns = [&#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;] + eval_metrics
    reg_summary = pd.DataFrame(np.vstack((frame_inds[1:].T, np.array(eval_bounds)[frame_inds[1:], :].T, np.array(image_metrics).T)).T, columns = columns, index = None)
    if npts:
        reg_summary[&#39;Npts&#39;] = npts
        columns = columns + [&#39;Npts&#39;]
    if error_abs_mean:
        reg_summary[&#39;Mean Abs Error&#39;] = error_abs_mean
        columns = columns + [&#39;Mean Abs Error&#39;]

    if save_registration_summary:
        registration_summary_xlsx = fpath_reg.replace(&#39;.mrc&#39;, &#39;_RegistrationQuality.xlsx&#39;)
        if disp_res:
            print(&#39;Saving the Registration Quality Statistics into the file: &#39;, registration_summary_xlsx)
        # Create a Pandas Excel writer using XlsxWriter as the engine.
        xlsx_writer = pd.ExcelWriter(registration_summary_xlsx, engine=&#39;xlsxwriter&#39;)
        reg_summary.to_excel(xlsx_writer, index=None, sheet_name=&#39;Registration Quality Statistics&#39;)
        Stack_info = pd.DataFrame([{&#39;Stack Filename&#39; : fnm_reg, &#39;dump_filename&#39; : dump_filename}]).T # prepare to be save in transposed format
        try:
            del kwargs[&#39;eval_bounds&#39;]
        except:
            pass
        SIFT_info = pd.DataFrame([kwargs]).T   # prepare to be save in transposed format
        Stack_info = Stack_info.append(SIFT_info)
        Stack_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;Stack Info&#39;)
        xlsx_writer.save()
    else:
        registration_summary_xlsx = &#39;Registration data not saved&#39;

    return reg_summary, registration_summary_xlsx</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.analyze_tif_stack_registration"><code class="name flex">
<span>def <span class="ident">analyze_tif_stack_registration</span></span>(<span>tif_filename, DASK_client, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Read MRC stack and analyze registration - calculate NSAD, NCC, and MI.
©G.Shtengel, 08/2022. gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tif_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>File name (full path) of the mrc stack to be analyzed</dd>
</dl>
<p>DASK client (needs to be initialized and running by this time)</p>
<dl>
<dt>kwargs:</dt>
<dt><strong><code>use_DASK</code></strong> :&ensp;<code>boolean</code></dt>
<dd>use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).</dd>
<dt><strong><code>DASK_client_retries</code></strong> :&ensp;<code>int (default to 0)</code></dt>
<dd>Number of allowed automatic retries if a task fails</dd>
<dt><strong><code>frame_inds</code></strong> :&ensp;<code>array</code></dt>
<dd>Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames</dd>
<dt><strong><code>invert_data</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, the data will be inverted</dd>
<dt><strong><code>evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.</dd>
<dt><strong><code>sliding_evaluation_box</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box</dd>
<dt><strong><code>start_evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>see above</dd>
<dt><strong><code>stop_evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>see above</dd>
<dt>save_res_png
: boolean</dt>
<dt>Save PNG images of the intermediate processing statistics and final registration quality check</dt>
<dt><strong><code>save_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the filename to save the results. If empty, tif_filename+'_RegistrationQuality.csv' will be used</dd>
</dl>
<p>Returns reg_summary : PD data frame, registration_summary_xlsx : path to summary XLSX workbook</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def analyze_tif_stack_registration(tif_filename, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Read MRC stack and analyze registration - calculate NSAD, NCC, and MI.
    ©G.Shtengel, 08/2022. gleb.shtengel@gmail.com

    Parameters
    ---------
    tif_filename : str
        File name (full path) of the mrc stack to be analyzed
    DASK client (needs to be initialized and running by this time)

    kwargs:
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    frame_inds : array
        Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
    invert_data : boolean
        If True, the data will be inverted
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    save_filename : str
        Path to the filename to save the results. If empty, tif_filename+&#39;_RegistrationQuality.csv&#39; will be used

    Returns reg_summary : PD data frame, registration_summary_xlsx : path to summary XLSX workbook
    &#39;&#39;&#39;
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    save_filename = kwargs.get(&#34;save_filename&#34;, tif_filename )
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    registration_summary_xlsx = save_filename.replace(&#39;.mrc&#39;, &#39;_RegistrationQuality.xlsx&#39;)

    if sliding_evaluation_box:
        print(&#39;Will use sliding (linearly) evaluation box&#39;)
        print(&#39;   Starting with box:  &#39;, start_evaluation_box)
        print(&#39;   Finishing with box: &#39;, stop_evaluation_box)
    else:
        print(&#39;Will use fixed evaluation box: &#39;, evaluation_box)

    with tiff.TiffFile(tif_filename) as tif:
        tif_tags = {}
        for tag in tif.pages[0].tags.values():
            name, value = tag.name, tag.value
            tif_tags[name] = value
    #print(tif_tags)
    try:
        shape = eval(tif_tags[&#39;ImageDescription&#39;])
        nz, ny, nx = shape[&#39;shape&#39;]
    except:
        try:
            shape = eval(tif_tags[&#39;image_description&#39;])
            nz, ny, nx = shape[&#39;shape&#39;]
        except:
            fr0 = tiff.imread(tif_filename, key=0)
            ny, nx = np.shape(fr0)
            nz = eval(tif_tags[&#39;nimages&#39;])
    header_dict = {&#39;nx&#39; : nx, &#39;ny&#39; : ny, &#39;nz&#39; : nz }

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = nx
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ny
    evals = [xi_eval, xa_eval, yi_eval, ya_eval]

    frame_inds_default = np.arange(nz-1)+1
    frame_inds = np.array(kwargs.get(&#34;frame_inds&#34;, frame_inds_default))
    nf = frame_inds[-1]-frame_inds[0]+1
    if frame_inds[0]==0:
        frame_inds = frame_inds+1
    sample_frame_inds = [frame_inds[nf//10], frame_inds[nf//2], frame_inds[nf//10*9]]
    print(&#39;Will analyze regstrations in {:d} frames&#39;.format(len(frame_inds)))
    print(&#39;Will save the data into &#39; + registration_summary_xlsx)
    if sliding_evaluation_box:
        dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
        dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
    else:
        dx_eval = 0
        dy_eval = 0

    params_tif_mult = []
    xi_evals = np.zeros(nf, dtype=int16)
    xa_evals = np.zeros(nf, dtype=int16)
    yi_evals = np.zeros(nf, dtype=int16)
    ya_evals = np.zeros(nf, dtype=int16)
    for j, fr in enumerate(frame_inds):
        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*(fr-frame_inds[0])//nf
            yi_eval = start_evaluation_box[0] + dy_eval*(fr-frame_inds[0])//nf
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny
            evals = [xi_eval, xa_eval, yi_eval, ya_eval]
        xi_evals[j] = xi_eval
        xa_evals[j] = xa_eval
        yi_evals[j] = yi_eval
        ya_evals[j] = ya_eval
        if fr in sample_frame_inds:
            save_frame_png = save_sample_frames_png
            filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame{:d}.png&#39;.format(fr)
        else:
            save_frame_png = False
            filename_frame_png = os.path.splitext(save_filename)[0]+&#39;_sample_image_frame.png&#39;
        params_tif_mult.append([tif_filename, fr, invert_data, evals, save_frame_png, filename_frame_png])

    if use_DASK:
        print(&#39;Using DASK distributed&#39;)
        futures = DASK_client.map(evaluate_registration_two_frames_tif, params_tif_mult, retries = DASK_client_retries)
        dask_results = DASK_client.gather(futures)
        image_nsad = np.array([res[0] for res in dask_results])
        image_ncc = np.array([res[1] for res in dask_results])
        image_mi = np.array([res[2] for res in dask_results])
    else:
        print(&#39;Using Local Computation&#39;)
        image_nsad = np.zeros((nf), dtype=float)
        image_ncc = np.zeros((nf), dtype=float)
        image_mi = np.zeros((nf), dtype=float)
        results = []
        for params_tif_mult_pair in tqdm(params_tif_mult, desc=&#39;Evaluating frame registration: &#39;):
            print(params_tif_mult_pair)
            [tif_filename, fr, invert_data, evals] = params_tif_mult_pair
            print(fr)
            results.append(evaluate_registration_two_frames_tif(params_tif_mult_pair))
        image_nsad = np.array([res[0] for res in results])
        image_ncc = np.array([res[1] for res in results])
        image_mi = np.array([res[2] for res in results])

    nsads = [np.mean(image_nsad), np.median(image_nsad), np.std(image_nsad)]
    #image_ncc = image_ncc[1:-1]
    nccs = [np.mean(image_ncc), np.median(image_ncc), np.std(image_ncc)]
    nmis = [np.mean(image_mi), np.median(image_mi), np.std(image_mi)]

    print(&#39;Saving the Registration Quality Statistics into the file: &#39;, registration_summary_xlsx)
    xlsx_writer = pd.ExcelWriter(registration_summary_xlsx, engine=&#39;xlsxwriter&#39;)
    columns=[&#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;, &#39;Image NSAD&#39;, &#39;Image NCC&#39;, &#39;Image MI&#39;]
    reg_summary = pd.DataFrame(np.vstack((frame_inds, xi_evals, xa_evals, yi_evals, ya_evals, image_nsad, image_ncc, image_mi)).T, columns = columns, index = None)
    reg_summary.to_excel(xlsx_writer, index=None, sheet_name=&#39;Registration Quality Statistics&#39;)
    Stack_info = pd.DataFrame([{&#39;Stack Filename&#39; : tif_filename, &#39;Sample_ID&#39; : Sample_ID, &#39;invert_data&#39; : invert_data}]).T # prepare to be save in transposed format
    header_info = pd.DataFrame([header_dict]).T
    Stack_info = Stack_info.append(header_info)
    Stack_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;Stack Info&#39;)
    xlsx_writer.save()

    return reg_summary, registration_summary_xlsx</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.analyze_transformation_matrix"><code class="name flex">
<span>def <span class="ident">analyze_transformation_matrix</span></span>(<span>transformation_matrix, xf_filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyzes the transformation matrix created by FiJi-based workflow. ©G.Shtengel 10/2022 gleb.shtengel@gmail.com</p>
<p>Parameters:
transformation_matrix : array
Transformation matrix (read by read_transformation_matrix_from_xf_file above).
xf_filename : str
Full path to *.xf file containing the transformation matrix data</p>
<p>Returns:
tr_matr_cum : array
Cumulative transformation matrix</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def analyze_transformation_matrix(transformation_matrix, xf_filename):
    &#39;&#39;&#39;
    Analyzes the transformation matrix created by FiJi-based workflow. ©G.Shtengel 10/2022 gleb.shtengel@gmail.com

    Parameters:
    transformation_matrix : array
        Transformation matrix (read by read_transformation_matrix_from_xf_file above).
    xf_filename : str
        Full path to *.xf file containing the transformation matrix data

    Returns:
    tr_matr_cum : array
    Cumulative transformation matrix
    &#39;&#39;&#39;
    Xshift_orig = transformation_matrix[:, 0, 2]
    Yshift_orig = transformation_matrix[:, 1, 2]
    Xscale_orig = transformation_matrix[:, 0, 0]
    Yscale_orig = transformation_matrix[:, 1, 1]
    tr_matr_cum = transformation_matrix.copy()

    prev_mt = np.eye(3,3)
    for j, cur_mt in enumerate(tqdm(transformation_matrix, desc=&#39;Calculating Cummilative Transformation Matrix&#39;)):
        if any(np.isnan(cur_mt)):
            print(&#39;Frame: {:d} has ill-defined transformation matrix, will use identity transformation instead:&#39;.format(j))
            print(cur_mt)
        else:
            prev_mt = np.matmul(cur_mt, prev_mt)
        tr_matr_cum[j] = prev_mt
    # Now insert identity matrix for the zero frame which does not need to be trasformed
    tr_matr_cum_orig = tr_matr_cum.copy()

    s00_cum_orig = tr_matr_cum[:, 0, 0].copy()
    s11_cum_orig = tr_matr_cum[:, 1, 1].copy()
    s01_cum_orig = tr_matr_cum[:, 0, 1].copy()
    s10_cum_orig = tr_matr_cum[:, 1, 0].copy()

    Xshift_cum_orig = tr_matr_cum_orig[:, 0, 2]
    Yshift_cum_orig = tr_matr_cum_orig[:, 1, 2]


    #print(&#39;Recalculating Shifts&#39;)
    s00_cum_orig = tr_matr_cum[:, 0, 0]
    s11_cum_orig = tr_matr_cum[:, 1, 1]
    fr = np.arange(0, len(s00_cum_orig), dtype=float)
    s00_slp = -1.0 * (np.sum(fr)-np.dot(s00_cum_orig,fr))/np.dot(fr,fr) # find the slope of a linear fit with fiorced first scale=1
    s00_fit = 1.0 + s00_slp * fr
    s00_cum_new = s00_cum_orig + 1.0 - s00_fit
    s11_slp = -1.0 * (np.sum(fr)-np.dot(s11_cum_orig,fr))/np.dot(fr,fr) # find the slope of a linear fit with fiorced first scale=1
    s11_fit = 1.0 + s11_slp * fr
    s11_cum_new = s11_cum_orig + 1.0 - s11_fit

    s01_slp = np.dot(s01_cum_orig,fr)/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
    s01_fit = s01_slp * fr
    s01_cum_new = s01_cum_orig - s01_fit
    s10_slp = np.dot(s10_cum_orig,fr)/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
    s10_fit = s10_slp * fr
    s10_cum_new = s10_cum_orig - s10_fit

    Xshift_cum = tr_matr_cum[:, 0, 2]
    Yshift_cum = tr_matr_cum[:, 1, 2]

    subtract_linear_fit=True

    # Subtract linear trend from offsets
    if subtract_linear_fit:
        fr = np.arange(0, len(Xshift_cum) )
        pX = np.polyfit(fr, Xshift_cum, 1)
        Xfit = np.polyval(pX, fr)
        pY = np.polyfit(fr, Yshift_cum, 1)
        Yfit = np.polyval(pY, fr)
        Xshift_residual = Xshift_cum - Xfit
        Yshift_residual = Yshift_cum - Yfit
    else:
        Xshift_residual = Xshift_cum.copy()
        Yshift_residual = Yshift_cum.copy()

    # define new cum. transformation matrix where the offests may have linear slopes subtracted
    tr_matr_cum_residual = tr_matr_cum.copy()
    tr_matr_cum_residual[:, 0, 2] = Xshift_residual
    tr_matr_cum_residual[:, 1, 2] = Yshift_residual
    tr_matr_cum_residual[:, 0, 0] = s00_cum_new
    tr_matr_cum_residual[:, 1, 1] = s11_cum_new
    tr_matr_cum_residual[:, 0, 1] = s01_cum_new
    tr_matr_cum_residual[:, 1, 0] = s10_cum_new

    fs = 12
    fig5, axs5 = subplots(3,3, figsize=(18, 12), sharex=True)
    fig5.subplots_adjust(left=0.15, bottom=0.08, right=0.99, top=0.94)

    # plot scales
    axs5[0, 0].plot(Xscale_orig, &#39;r&#39;, label = &#39;Sxx fr.-to-fr.&#39;)
    axs5[0, 0].plot(Yscale_orig, &#39;b&#39;, label = &#39;Syy fr.-to-fr.&#39;)
    axs5[0, 0].set_title(&#39;Frame-to-Frame Scale Change&#39;, fontsize = fs + 1)
    axs5[1, 0].plot(tr_matr_cum_orig[:, 0, 0], &#39;r&#39;, linestyle=&#39;dotted&#39;, label = &#39;Sxx cum.&#39;)
    axs5[1, 0].plot(tr_matr_cum_orig[:, 1, 1], &#39;b&#39;, linestyle=&#39;dotted&#39;, label = &#39;Syy cum.&#39;)
    axs5[1, 0].plot(s00_fit, &#39;r&#39;, label = &#39;Sxx cum. - lin. fit&#39;)
    axs5[1, 0].plot(s11_fit, &#39;b&#39;, label = &#39;Syy cum. - lin. fit&#39;)
    axs5[1, 0].set_title(&#39;Cumulative Scale&#39;, fontsize = fs + 1)
    axs5[2, 0].plot(tr_matr_cum_residual[:, 0, 0], &#39;r&#39;, label = &#39;Sxx cum. - residual&#39;)
    axs5[2, 0].plot(tr_matr_cum_residual[:, 1, 1], &#39;b&#39;, label = &#39;Syy cum. - residual&#39;)
    axs5[2, 0].set_title(&#39;Residual Cumulative Scale&#39;, fontsize = fs + 1)
    axs5[2, 0].set_xlabel(&#39;Frame&#39;, fontsize = fs + 1)

    # plot shears
    axs5[0, 1].plot(transformation_matrix[:, 0, 1], &#39;r&#39;, label = &#39;Sxy fr.-to-fr.&#39;)
    axs5[0, 1].plot(transformation_matrix[:, 1, 0], &#39;b&#39;, label = &#39;Syx fr.-to-fr.&#39;)
    axs5[0, 1].set_title(&#39;Frame-to-Frame Shear&#39;, fontsize = fs + 1)
    axs5[1, 1].plot(tr_matr_cum_orig[:, 0, 1], &#39;r&#39;, linestyle=&#39;dotted&#39;, label = &#39;Sxy cum.&#39;)
    axs5[1, 1].plot(tr_matr_cum_orig[:, 1, 0], &#39;b&#39;, linestyle=&#39;dotted&#39;, label = &#39;Syx cum.&#39;)
    axs5[1, 1].plot(s01_fit, &#39;r&#39;, label = &#39;Sxy cum. - lin. fit&#39;)
    axs5[1, 1].plot(s10_fit, &#39;b&#39;, label = &#39;Syx cum. - lin. fit&#39;)
    axs5[1, 1].set_title(&#39;Cumulative Shear&#39;, fontsize = fs + 1)
    axs5[2, 1].plot(tr_matr_cum_residual[:, 0, 1], &#39;r&#39;, label = &#39;Sxy cum. - residual&#39;)
    axs5[2, 1].plot(tr_matr_cum_residual[:, 1, 0], &#39;b&#39;, label = &#39;Syx cum. - residual&#39;)
    axs5[2, 1].set_title(&#39;Residual Cumulative Shear&#39;, fontsize = fs + 1)
    axs5[2, 1].set_xlabel(&#39;Frame&#39;, fontsize = fs + 1)

    # plot shifts
    axs5[0, 2].plot(Xshift_orig, &#39;r&#39;, label = &#39;Tx fr.-to-fr.&#39;)
    axs5[0, 2].plot(Yshift_orig, &#39;b&#39;, label = &#39;Ty fr.-to-fr.&#39;)
    axs5[0, 2].set_title(&#39;Frame-to-Frame Shift&#39;, fontsize = fs + 1)
    axs5[1, 2].plot(Xshift_cum, &#39;r&#39;, linestyle=&#39;dotted&#39;, label = &#39;Tx cum.&#39;)
    axs5[1, 2].plot(Xfit, &#39;r&#39;, label = &#39;Tx cum. - lin. fit&#39;)
    axs5[1, 2].plot(Yshift_cum, &#39;b&#39;, linestyle=&#39;dotted&#39;, label = &#39;Ty cum.&#39;)
    axs5[1, 2].plot(Yfit, &#39;b&#39;, label = &#39;Ty cum. - lin. fit&#39;)
    axs5[1, 2].set_title(&#39;Cumulative Shift&#39;, fontsize = fs + 1)
    axs5[2, 2].plot(tr_matr_cum_residual[:, 0, 2], &#39;r&#39;, label = &#39;Tx cum. - residual&#39;)
    axs5[2, 2].plot(tr_matr_cum_residual[:, 1, 2], &#39;b&#39;, label = &#39;Ty cum. - residual&#39;)
    axs5[2, 2].set_title(&#39;Residual Cumulative Shift&#39;, fontsize = fs + 1)
    axs5[2, 2].set_xlabel(&#39;Frame&#39;, fontsize = fs + 1)

    for ax in axs5.ravel():
        ax.grid(True)
        ax.legend()
    fig5.suptitle(xf_filename, fontsize = fs + 2)
    fig5.savefig(xf_filename +&#39;_Transform_Summary.png&#39;, dpi=300)
    return tr_matr_cum</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.argmax2d"><code class="name flex">
<span>def <span class="ident">argmax2d</span></span>(<span>X)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def argmax2d(X):
    return np.unravel_index(X.argmax(), X.shape)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.beta"><code class="name flex">
<span>def <span class="ident">beta</span></span>(<span>a, b, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Beta distribution.</p>
<p>The Beta distribution is a special case of the Dirichlet distribution,
and is related to the Gamma distribution.
It has the probability
distribution function</p>
<p>[ (1 - x)^{\beta - 1}, ]
where the normalization, B, is the beta function,</p>
<p>[ (1 - t)^{\beta - 1} dt. ]
It is often seen in Bayesian inference and order statistics.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.beta</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Alpha, positive (&gt;0).</dd>
<dt><strong><code>b</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Beta, positive (&gt;0).</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>a</code> and <code>b</code> are both scalars.
Otherwise, <code>np.broadcast(a, b).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized beta distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.beta</code></dt>
<dd>which should be used for new code.</dd>
</dl></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.bin_crop_frames"><code class="name flex">
<span>def <span class="ident">bin_crop_frames</span></span>(<span>bin_crop_parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>Help function used by bin_crop_mrc_stack_DASK</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bin_crop_frames(bin_crop_parameters):
    &#39;&#39;&#39;
    Help function used by bin_crop_mrc_stack_DASK
    &#39;&#39;&#39;
    import logging
    logger = logging.getLogger(&#34;distributed.utils_perf&#34;)
    logger.setLevel(logging.ERROR)
    mrc_filename, save_filename, dtp, start_frame, stop_frame, xbin_factor, ybin_factor, zbin_factor, mode, xi, xa, yi, ya = bin_crop_parameters
    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;, permissive=True)
    if mode == &#39;mean&#39;:
        zbinnd_fr = np.mean(mrc_obj.data[start_frame:stop_frame, yi:ya, xi:xa], axis=0)
    else:
        zbinnd_fr = np.sum(mrc_obj.data[start_frame:stop_frame, yi:ya, xi:xa], axis=0)
    if (xbin_factor &gt; 1) or (ybin_factor &gt; 1):
        if mode == &#39;mean&#39;:
            zbinnd_fr = np.mean(np.mean(zbinnd_fr.reshape(ny_binned, ybin_factor, nx_binned, xbin_factor), axis=3), axis=1)
        else:
            zbinnd_fr = np.sum(np.sum(zbinnd_fr.reshape(ny_binned, ybin_factor, nx_binned, xbin_factor), axis=3), axis=1)
    tiff.imsave(save_filename, zbinnd_fr.astype(dtp))
    mrc_obj.close()
    return save_filename</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.bin_crop_mrc_stack"><code class="name flex">
<span>def <span class="ident">bin_crop_mrc_stack</span></span>(<span>mrc_filename, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Bins and crops a 3D mrc stack along X-, Y-, or Z-directions and saves it into MRC or HDF5 format. ©G.Shtengel 08/2022 gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters</h2>
<p>mrc_filename : str
name (full path) of the mrc file to be binned
**kwargs:
fnm_types : list of strings.
File type(s) for output data. Options are: ['h5', 'mrc'].
Defauls is ['mrc']. 'h5' is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
zbin_factor : int
binning factor in z-direction
xbin_factor : int
binning factor in x-direction
ybin_factor : int
binning factor in y-direction
mode
: str
Binning mode. Default is 'mean', other option is 'sum'
frmax : int
Maximum frame to bin. If not present, the entire file is binned
binned_copped_filename : str
name (full path) of the mrc file to save the results into. If not present, the new file name is constructed from the original by adding "_zbinXX" at the end.
xi : int
left edge of the crop
xa : int
right edge of the crop
yi : int
top edge of the crop
ya : int
bottom edge of the crop
fri : int
start frame
fra : int
stop frame</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>fnms_saved </code></dt>
<dd>list of str
Names of the new (binned and cropped) data files.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bin_crop_mrc_stack(mrc_filename, **kwargs):
    &#39;&#39;&#39;
    Bins and crops a 3D mrc stack along X-, Y-, or Z-directions and saves it into MRC or HDF5 format. ©G.Shtengel 08/2022 gleb.shtengel@gmail.com

    Parameters:
        mrc_filename : str
            name (full path) of the mrc file to be binned
    **kwargs:
        fnm_types : list of strings.
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is [&#39;mrc&#39;]. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        zbin_factor : int
            binning factor in z-direction
        xbin_factor : int
            binning factor in x-direction
        ybin_factor : int
            binning factor in y-direction
        mode  : str
            Binning mode. Default is &#39;mean&#39;, other option is &#39;sum&#39;
        frmax : int
            Maximum frame to bin. If not present, the entire file is binned
        binned_copped_filename : str
            name (full path) of the mrc file to save the results into. If not present, the new file name is constructed from the original by adding &#34;_zbinXX&#34; at the end.
        xi : int
            left edge of the crop
        xa : int
            right edge of the crop
        yi : int
            top edge of the crop
        ya : int
            bottom edge of the crop
        fri : int
            start frame
        fra : int
            stop frame
    Returns:
        fnms_saved : list of str
            Names of the new (binned and cropped) data files.
    &#39;&#39;&#39;
    fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
    xbin_factor = kwargs.get(&#34;xbin_factor&#34;, 1)      # binning factor in in x-direction
    ybin_factor = kwargs.get(&#34;ybin_factor&#34;, 1)      # binning factor in in y-direction
    zbin_factor = kwargs.get(&#34;zbin_factor&#34;, 1)      # binning factor in in z-direction

    mode = kwargs.get(&#39;mode&#39;, &#39;mean&#39;)                   # binning mode. Default is &#39;mean&#39;, other option is &#39;sum&#39;
    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;, permissive=True)
    header = mrc_obj.header
    &#39;&#39;&#39;
        mode 0 -&gt; uint8
        mode 1 -&gt; int16
        mode 2 -&gt; float32
        mode 4 -&gt; complex64
        mode 6 -&gt; uint16
    &#39;&#39;&#39;
    mrc_mode = mrc_obj.header.mode
    voxel_size_angstr = mrc_obj.voxel_size
    voxel_size_angstr_new = voxel_size_angstr.copy()
    voxel_size_angstr_new.x = voxel_size_angstr.x * xbin_factor
    voxel_size_angstr_new.y = voxel_size_angstr.y * ybin_factor
    voxel_size_angstr_new.z = voxel_size_angstr.z * zbin_factor
    voxel_size_new = voxel_size_angstr.copy()
    voxel_size_new.x = voxel_size_angstr_new.x / 10.0
    voxel_size_new.y = voxel_size_angstr_new.y / 10.0
    voxel_size_new.z = voxel_size_angstr_new.z / 10.0
    nx, ny, nz = int32(header[&#39;nx&#39;]), int32(header[&#39;ny&#39;]), int32(header[&#39;nz&#39;])
    frmax = kwargs.get(&#39;frmax&#39;, nz)
    xi = kwargs.get(&#39;xi&#39;, 0)
    xa = kwargs.get(&#39;xa&#39;, nx)
    yi = kwargs.get(&#39;yi&#39;, 0)
    ya = kwargs.get(&#39;ya&#39;, ny)
    fri = kwargs.get(&#39;fri&#39;, 0)
    fra = kwargs.get(&#39;fra&#39;, nz)
    nx_binned = (xa-xi)//xbin_factor
    ny_binned = (ya-yi)//ybin_factor
    xa = xi + nx_binned * xbin_factor
    ya = yi + ny_binned * ybin_factor
    binned_copped_filename_default = os.path.splitext(mrc_filename)[0] + &#39;_binned_croped.mrc&#39;
    binned_copped_filename = kwargs.get(&#39;binned_copped_filename&#39;, binned_copped_filename_default)
    binned_mrc_filename = os.path.splitext(binned_copped_filename)[0] + &#39;.mrc&#39;
    dt = type(mrc_obj.data[0,0,0])
    print(&#39;Source mrc_mode: {:d}, source data type:&#39;.format(mrc_mode), dt)
    print(&#39;Source Voxel Size (Angstroms): {:2f} x {:2f} x {:2f}&#39;.format(voxel_size_angstr.x, voxel_size_angstr.y, voxel_size_angstr.z))
    if mode == &#39;sum&#39;:
        mrc_mode = 1
        dt = int16
    print(&#39;Result mrc_mode: {:d}, source data type:&#39;.format(mrc_mode), dt)
    st_frames = np.arange(fri, fra, zbin_factor)
    print(&#39;New Data Set Shape:  {:d} x {:d} x {:d}&#39;.format(nx_binned, ny_binned, len(st_frames)))

    fnms_saved = []
    if &#39;mrc&#39; in fnm_types:
        fnms_saved.append(binned_mrc_filename)
        mrc_new = mrcfile.new_mmap(binned_mrc_filename, shape=(len(st_frames), ny_binned, nx_binned), mrc_mode=mrc_mode, overwrite=True)
        mrc_new.voxel_size = voxel_size_angstr_new
        #mrc_new.header.cella = voxel_size_angstr_new
        print(&#39;Result Voxel Size (Angstroms): {:2f} x {:2f} x {:2f}&#39;.format(voxel_size_angstr_new.x, voxel_size_angstr_new.y, voxel_size_angstr_new.z))
        desc = &#39;Saving the data stack into MRC file&#39;

    if &#39;h5&#39; in fnm_types:
        binned_h5_filename = os.path.splitext(binned_mrc_filename)[0] + &#39;.h5&#39;
        try:
            os.remove(binned_h5_filename)
        except:
            pass
        fnms_saved.append(binned_h5_filename)
        bdv_writer = npy2bdv.BdvWriter(binned_h5_filename, nchannels=1, blockdim=((1, 256, 256),))
        bdv_writer.append_view(stack=None, virtual_stack_dim=(len(st_frames),ny_binned,nx_binned),
                    time=0, channel=0,
                    voxel_size_xyz=(voxel_size_new.x, voxel_size_new.y, voxel_size_new.z), voxel_units=&#39;nm&#39;)
        if &#39;mrc&#39; in fnm_types:
            desc = &#39;Saving the data stack into MRC and H5 files&#39;
        else:
            desc = &#39;Saving the data stack into H5 file&#39;

    for j, st_frame in enumerate(tqdm(st_frames, desc=desc)):
        # need to fix this
        if mode == &#39;mean&#39;:
            zbinnd_fr = np.mean(mrc_obj.data[st_frame:min(st_frame+zbin_factor, nz-1), yi:ya, xi:xa], axis=0)
        else:
            zbinnd_fr = np.sum(mrc_obj.data[st_frame:min(st_frame+zbin_factor, nz-1), yi:ya, xi:xa], axis=0)
        if (xbin_factor &gt; 1) or (ybin_factor &gt; 1):
            if mode == &#39;mean&#39;:
                zbinnd_fr = np.mean(np.mean(zbinnd_fr.reshape(ny_binned, ybin_factor, nx_binned, xbin_factor), axis=3), axis=1)
            else:
                zbinnd_fr = np.sum(np.sum(zbinnd_fr.reshape(ny_binned, ybin_factor, nx_binned, xbin_factor), axis=3), axis=1)
        if &#39;mrc&#39; in fnm_types:
            mrc_new.data[j,:,:] = zbinnd_fr.astype(dt)
        if &#39;h5&#39; in fnm_types:
            bdv_writer.append_plane(plane=zbinnd_fr, z=j, time=0, channel=0)

    if &#39;mrc&#39; in fnm_types:
        mrc_new.close()

    if &#39;h5&#39; in fnm_types:
        bdv_writer.write_xml()
        bdv_writer.close()

    mrc_obj.close()

    return fnms_saved</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.bin_crop_mrc_stack_DASK"><code class="name flex">
<span>def <span class="ident">bin_crop_mrc_stack_DASK</span></span>(<span>DASK_client, mrc_filename, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Bins a 3D mrc stack along Z-direction (optional binning in X-Y plane as well) and crops it along X- and Y- directions. ©G.Shtengel 08/2022 gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters</h2>
<p>DASK_client
mrc_filename : str
name (full path) of the mrc file to be binned
**kwargs:
use_DASK : boolean
use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails
fnm_types : list of strings.
File type(s) for output data. Options are: ['h5', 'mrc'].
Defauls is ['mrc']. 'h5' is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
zbin_factor : int
binning factor in z-direction
xbin_factor : int
binning factor in x-direction
ybin_factor : int
binning factor in y-direction
mode
: str
Binning mode. Default is 'mean', other option is 'sum'
frmax : int
Maximum frame to bin. If not present, the entire file is binned
binned_copped_filename : str
name (full path) of the mrc file to save the results into. If not present, the new file name is constructed from the original by adding "_zbinXX" at the end.
xi : int
left edge of the crop
xa : int
right edge of the crop
yi : int
top edge of the crop
ya : int
bottom edge of the crop
fri : int
start frame
fra : int
stop frame
disp_res : bolean
Display messages and intermediate results</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>fnms_saved </code></dt>
<dd>list of str
Names of the new (binned and cropped) data files.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bin_crop_mrc_stack_DASK(DASK_client, mrc_filename, **kwargs):
    &#39;&#39;&#39;
    Bins a 3D mrc stack along Z-direction (optional binning in X-Y plane as well) and crops it along X- and Y- directions. ©G.Shtengel 08/2022 gleb.shtengel@gmail.com

    Parameters:
        DASK_client
        mrc_filename : str
            name (full path) of the mrc file to be binned
    **kwargs:
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        fnm_types : list of strings.
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is [&#39;mrc&#39;]. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        zbin_factor : int
            binning factor in z-direction
        xbin_factor : int
            binning factor in x-direction
        ybin_factor : int
            binning factor in y-direction
        mode  : str
            Binning mode. Default is &#39;mean&#39;, other option is &#39;sum&#39;
        frmax : int
            Maximum frame to bin. If not present, the entire file is binned
        binned_copped_filename : str
            name (full path) of the mrc file to save the results into. If not present, the new file name is constructed from the original by adding &#34;_zbinXX&#34; at the end.
        xi : int
            left edge of the crop
        xa : int
            right edge of the crop
        yi : int
            top edge of the crop
        ya : int
            bottom edge of the crop
        fri : int
            start frame
        fra : int
            stop frame
        disp_res : bolean
            Display messages and intermediate results
    Returns:
        fnms_saved : list of str
            Names of the new (binned and cropped) data files.
    &#39;&#39;&#39;
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 3)
    fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
    xbin_factor = kwargs.get(&#34;xbin_factor&#34;, 1)      # binning factor in in x-direction
    ybin_factor = kwargs.get(&#34;ybin_factor&#34;, 1)      # binning factor in in y-direction
    zbin_factor = kwargs.get(&#34;zbin_factor&#34;, 1)      # binning factor in in z-direction
    disp_res  = kwargs.get(&#34;disp_res&#34;, True )

    mode = kwargs.get(&#39;mode&#39;, &#39;mean&#39;)                   # binning mode. Default is &#39;mean&#39;, other option is &#39;sum&#39;
    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;, permissive=True)
    header = mrc_obj.header
    &#39;&#39;&#39;
        mode 0 -&gt; uint8
        mode 1 -&gt; int16
        mode 2 -&gt; float32
        mode 4 -&gt; complex64
        mode 6 -&gt; uint16
    &#39;&#39;&#39;
    mrc_mode = mrc_obj.header.mode
    #voxel_size_angstr = mrc_obj.header.cella
    voxel_size_angstr = mrc_obj.voxel_size
    voxel_size_angstr_new = voxel_size_angstr.copy()
    voxel_size_angstr_new.x = voxel_size_angstr.x * xbin_factor
    voxel_size_angstr_new.y = voxel_size_angstr.y * ybin_factor
    voxel_size_angstr_new.z = voxel_size_angstr.z * zbin_factor
    voxel_size_new = voxel_size_angstr.copy()
    voxel_size_new.x = voxel_size_angstr_new.x / 10.0
    voxel_size_new.y = voxel_size_angstr_new.y / 10.0
    voxel_size_new.z = voxel_size_angstr_new.z / 10.0
    nx, ny, nz = int32(header[&#39;nx&#39;]), int32(header[&#39;ny&#39;]), int32(header[&#39;nz&#39;])
    frmax = kwargs.get(&#39;frmax&#39;, nz)
    xi = kwargs.get(&#39;xi&#39;, 0)
    xa = kwargs.get(&#39;xa&#39;, nx)
    yi = kwargs.get(&#39;yi&#39;, 0)
    ya = kwargs.get(&#39;ya&#39;, ny)
    fri = kwargs.get(&#39;fri&#39;, 0)
    fra = kwargs.get(&#39;fra&#39;, nz)
    nx_binned = (xa-xi)//xbin_factor
    ny_binned = (ya-yi)//ybin_factor
    xa = xi + nx_binned * xbin_factor
    ya = yi + ny_binned * ybin_factor
    binned_copped_filename_default = os.path.splitext(mrc_filename)[0] + &#39;_binned_croped.mrc&#39;
    binned_copped_filename = kwargs.get(&#39;binned_copped_filename&#39;, binned_copped_filename_default)
    binned_mrc_filename = os.path.splitext(binned_copped_filename)[0] + &#39;.mrc&#39;
    dtp = type(mrc_obj.data[0,0,0])
    print(&#39;Source mrc_mode: {:d}, source data type:&#39;.format(mrc_mode), dtp)
    print(&#39;Source Voxel Size (Angstroms): {:2f} x {:2f} x {:2f}&#39;.format(voxel_size_angstr.x, voxel_size_angstr.y, voxel_size_angstr.z))
    if mode == &#39;sum&#39;:
        mrc_mode = 1
        dtp = int16
    print(&#39;Result mrc_mode: {:d}, source data type:&#39;.format(mrc_mode), dtp)

    st_frames = np.arange(fri, fra, zbin_factor)
    print(&#39;New Data Set Shape:  {:d} x {:d} x {:d}&#39;.format(nx_binned, ny_binned, len(st_frames)))

    bin_crop_parameters_dataset = []
    for j, st_frame in enumerate(tqdm(st_frames, desc=&#39;Setting up DASK parameter sets&#39;, display=disp_res)):
        save_filename = os.path.join(os.path.split(mrc_filename)[0],&#39;Binned_Cropped_Frame_{:d}.tif&#39;.format(j))
        start_frame = st_frame
        stop_frame = min(st_frame+zbin_factor, nz-1)
        bin_crop_parameters_dataset.append([mrc_filename, save_filename, dtp, start_frame, stop_frame, xbin_factor, ybin_factor, zbin_factor, mode, xi, xa, yi, ya])

    print(&#39;Binning / Cropping and Saving Intermediate Frames&#39;)
    if use_DASK:
        if disp_res:
            print(&#39;Starting DASK jobs&#39;)
        futures_bin_crop = DASK_client.map(bin_crop_frames, bin_crop_parameters_dataset, retries = DASK_client_retries)
        binned_cropped_filenames = np.array(DASK_client.gather(futures_bin_crop))
        if disp_res:
            print(&#39;Finished DASK jobs&#39;)
    else:   # if DASK is not used - perform local computations
        if disp_res:
            print(&#39;Will perform local computations&#39;)
        binned_cropped_filenames = []
        for bin_crop_parameters in tqdm(bin_crop_parameters_dataset, desc = &#39;Transforming and saving frame chunks&#39;, display = disp_res):
            binned_cropped_filenames.append(bin_crop_frames(bin_crop_parameters))

    print(&#34;Creating Dask Array Stack&#34;)
    # now build dask array of the transformed dataset
    # read the first file to get the shape and dtype (ASSUMING THAT ALL FILES SHARE THE SAME SHAPE/TYPE)
    frame0 = tiff.imread(binned_cropped_filenames[0])
    lazy_imread = dask.delayed(tiff.imread)  # lazy reader
    lazy_arrays = [lazy_imread(fn) for fn in binned_cropped_filenames]
    dask_arrays = [ da.from_delayed(delayed_reader, shape=frame0.shape, dtype=frame0.dtype)   for delayed_reader in lazy_arrays]
    # Stack infividual frames into one large dask.array
    FIBSEMstack = da.stack(dask_arrays, axis=0)
    nz, ny, nx = FIBSEMstack.shape

    print(&#39;Saving Intermediate Frames into Final Stacks&#39;)
    save_kwargs = {&#39;fnm_types&#39; : fnm_types,
                &#39;fnm_reg&#39; : binned_mrc_filename,
                &#39;voxel_size&#39; : voxel_size_new,
                &#39;dtp&#39; : dtp,
                &#39;disp_res&#39; : True}
    fnms_saved = save_data_stack(FIBSEMstack, **save_kwargs)

    for fnm in tqdm(binned_cropped_filenames, desc=&#39;Removing Intermediate Files: &#39;, display = disp_res):
        try:
            os.remove(os.path.join(fnm))
        except:
            pass

    return fnms_saved</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.binomial"><code class="name flex">
<span>def <span class="ident">binomial</span></span>(<span>n, p, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a binomial distribution.</p>
<p>Samples are drawn from a binomial distribution with specified
parameters, n trials and p probability of success where
n an integer &gt;= 0 and p is in the interval [0,1]. (n may be
input as a float, but it is truncated to an integer in use)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.binomial</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code> or <code>array_like</code> of <code>ints</code></dt>
<dd>Parameter of the distribution, &gt;= 0. Floats are also accepted,
but they will be truncated to integers.</dd>
<dt><strong><code>p</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Parameter of the distribution, &gt;= 0 and &lt;=1.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>n</code> and <code>p</code> are both scalars.
Otherwise, <code>np.broadcast(n, p).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized binomial distribution, where
each sample is equal to the number of successes over the n trials.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.binom</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>random.Generator.binomial</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the binomial distribution is</p>
<p>[
]
where :math:<code>n</code> is the number of trials, :math:<code>p</code> is the probability
of success, and :math:<code>N</code> is the number of successes.</p>
<p>When estimating the standard error of a proportion in a population by
using a random sample, the normal distribution works well unless the
product p<em>n &lt;=5, where p = population proportion estimate, and n =
number of samples, in which case the binomial distribution is used
instead. For example, a sample of 15 people shows 4 who are left
handed, and 11 who are right handed. Then p = 4/15 = 27%. 0.27</em>15 = 4,
so the binomial distribution should be used in this case.</p>
<h2 id="references">References</h2>
<p>.. [1] Dalgaard, Peter, "Introductory Statistics with R",
Springer-Verlag, 2002.
.. [2] Glantz, Stanton A. "Primer of Biostatistics.", McGraw-Hill,
Fifth Edition, 2002.
.. [3] Lentner, Marvin, "Elementary Applied Statistics", Bogden
and Quigley, 1972.
.. [4] Weisstein, Eric W. "Binomial Distribution." From MathWorld&ndash;A
Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/BinomialDistribution.html">http://mathworld.wolfram.com/BinomialDistribution.html</a>
.. [5] Wikipedia, "Binomial distribution",
<a href="https://en.wikipedia.org/wiki/Binomial_distribution">https://en.wikipedia.org/wiki/Binomial_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; n, p = 10, .5  # number of trials, probability of each trial
&gt;&gt;&gt; s = np.random.binomial(n, p, 1000)
# result of flipping a coin 10 times, tested 1000 times.
</code></pre>
<p>A real world example. A company drills 9 wild-cat oil exploration
wells, each with an estimated probability of success of 0.1. All nine
wells fail. What is the probability of that happening?</p>
<p>Let's do 20,000 trials of the model, and count the number that
generate zero positive results.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; sum(np.random.binomial(9, 0.1, 20000) == 0)/20000.
# answer = 0.38885, or 38%.
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.build_filename"><code class="name flex">
<span>def <span class="ident">build_filename</span></span>(<span>fname, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_filename(fname, **kwargs):
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    dtp = kwargs.get(&#34;dtp&#34;, int16)                             #  int16 or uint8
    threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)
    zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, 1)             # binning factor in z-direction (milling direction). Default is 1
    preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
    fit_params =  kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # If True, the linear slope will be subtracted from the cumulative shifts.
    subtract_FOVtrend_from_fit = kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])

    pad_edges =  kwargs.get(&#34;pad_edges&#34;, True)
    suffix =  kwargs.get(&#34;suffix&#34;, &#39;&#39;)

    frame = FIBSEM_frame(fname, ftype=ftype)
    dformat_read = &#39;I8&#39; if frame.EightBit else &#39;I16&#39;

    if dtp == int16:
        dformat_save = &#39;I16&#39;
        fnm_reg = &#39;Registered_I16.mrc&#39;
    else:
        dformat_save = &#39;I8&#39;
        fnm_reg = &#39;Registered_I8.mrc&#39;

    if zbin_factor&gt;1:
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_zbin{:d}.mrc&#39;.format(zbin_factor))

    fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, (&#39;_&#39; + TransformType.__name__ + &#39;_&#39; + solver + &#39;.mrc&#39;))

    fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_drmax{:.1f}.mrc&#39;.format(drmax))

    if preserve_scales:
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_const_scls_&#39;+fit_params[0]+&#39;.mrc&#39;)

    if np.any(subtract_linear_fit):
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_shift_subtr.mrc&#39;)

    if pad_edges:
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_padded.mrc&#39;)

    if len(suffix)&gt;0:
        fnm_reg = fnm_reg.replace(&#39;.mrc&#39;, &#39;_&#39; + suffix + &#39;.mrc&#39;)
    return fnm_reg, dtp</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.check_for_nomatch_frames_dataset"><code class="name flex">
<span>def <span class="ident">check_for_nomatch_frames_dataset</span></span>(<span>fls, fnms, fnms_matches, transformation_matrix, error_abs_mean, npts, thr_npt, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_for_nomatch_frames_dataset(fls, fnms, fnms_matches,
                                     transformation_matrix,
                                     error_abs_mean, npts,
                                     thr_npt, **kwargs):
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)

    inds_zeros = np.squeeze(np.argwhere(npts &lt; thr_npt ))
    print(&#39;Frames with no matches to the next frame:  &#39;, np.array(inds_zeros))
    frames_to_remove = []
    if shape(inds_zeros)!=():
        for ind0 in inds_zeros:
            if ind0 &lt; (len(fls)-2) and npts[ind0+1] &lt; thr_npt:
                frames_to_remove.append(ind0+1)
                print(&#39;Frame to remove: {:d} : &#39;.format(ind0+1) + &#39;, File: &#39; + fls[ind0+1])
                frame_to_remove  = FIBSEM_frame(fls[ind0+1], ftype=ftype)
                frame_to_remove.save_snapshot(dpi=300)
        print(&#39;Frames to remove:  &#39;, frames_to_remove)

    if len(frames_to_remove) == 0:
        print(&#39;No frames selected for removal&#39;)
    else:
        # create copies of the original arrays
        fnms_orig = fnms.copy()
        fls_orig = fls.copy()
        error_abs_mean_orig = error_abs_mean.copy()
        tr_matrix_orig = transformation_matrix.copy()

        # go through the frames to be removed and remove the frames from the list and then re-calculate the shift for new neighbours.
        for j,fr in enumerate(tqdm(frames_to_remove, desc = &#39;Removing frames and finding shifts for new sequential frames&#39;)):
            frj = fr-j # to account for the fact that every time we remove a frame the array shrinks and indicis reset
            print(&#39;Removing the frame {:d}&#39;.format(frj))
            print(fls[frj])
            fls = np.delete(fls, frj)
            fnms = np.delete(fnms, frj)
            fnms_matches = np.delete(fnms_matches, frj)
            error_abs_mean = np.delete(error_abs_mean, frj)
            transformation_matrix = np.delete(transformation_matrix, frj, axis = 0)
            npts = np.delete(npts, frj, axis = 0)
            fname1 = fnms[frj-1]
            fname2 = fnms[frj]
            new_step4_res = determine_transformations_files([fname1, fname2, kwargs])
            npts[frj-1] = np.array(len(new_step4_res[2][0]))
            error_abs_mean[frj-1] = new_step4_res[3]
            transformation_matrix[frj-1] = np.array(new_step4_res[0])
        print(&#39;Mean Number of Keypoints :&#39;, np.mean(npts).astype(int))
    return frames_to_remove, fls, fnms, fnms_matches, error_abs_mean, npts, transformation_matrix</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.chisquare"><code class="name flex">
<span>def <span class="ident">chisquare</span></span>(<span>df, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a chi-square distribution.</p>
<p>When <code>df</code> independent random variables, each with standard normal
distributions (mean 0, variance 1), are squared and summed, the
resulting distribution is chi-square (see Notes).
This distribution
is often used in hypothesis testing.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.chisquare</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Number of degrees of freedom, must be &gt; 0.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>df</code> is a scalar.
Otherwise,
<code>np.array(df).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized chi-square distribution.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>When <code>df</code> &lt;= 0 or when an inappropriate <code>size</code> (e.g. <code>size=-1</code>)
is given.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.chisquare</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The variable obtained by summing the squares of <code>df</code> independent,
standard normally distributed random variables:</p>
<p>[
]
is chi-square distributed, denoted</p>
<p>[
]
The probability density function of the chi-squared distribution is</p>
<p>[ x^{k/2 - 1} e^{-x/2}, ]
where :math:<code>\Gamma</code> is the gamma function,</p>
<p>[
]
References</p>
<hr>
<p>.. [1] NIST "Engineering Statistics Handbook"
<a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm">https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm</a></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.chisquare(2,4)
array([ 1.89920014,  9.00867716,  3.13710533,  5.62318272]) # random
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.choice"><code class="name flex">
<span>def <span class="ident">choice</span></span>(<span>a, size=None, replace=True, p=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a random sample from a given 1-D array</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.7.0</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.choice</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>1-D array-like</code> or <code>int</code></dt>
<dd>If an ndarray, a random sample is generated from its elements.
If an int, the random sample is generated as if it were <code>np.arange(a)</code></dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
Default is None, in which case a
single value is returned.</dd>
<dt><strong><code>replace</code></strong> :&ensp;<code>boolean</code>, optional</dt>
<dd>Whether the sample is with or without replacement. Default is True,
meaning that a value of <code>a</code> can be selected multiple times.</dd>
<dt><strong><code>p</code></strong> :&ensp;<code>1-D array-like</code>, optional</dt>
<dd>The probabilities associated with each entry in a.
If not given, the sample assumes a uniform distribution over all
entries in <code>a</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>samples</code></strong> :&ensp;<code>single item</code> or <code>ndarray</code></dt>
<dd>The generated random samples</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If a is an int and less than zero, if a or p are not 1-dimensional,
if a is an array-like of size 0, if p is not a vector of
probabilities, if a and p have different lengths, or if
replace=False and the sample size is greater than the population
size</dd>
</dl>
<h2 id="see-also">See Also</h2>
<p><code><a title="SIFT_gs.FIBSEM_SIFT_gs.randint" href="#SIFT_gs.FIBSEM_SIFT_gs.randint">RandomState.randint()</a></code>, <code><a title="SIFT_gs.FIBSEM_SIFT_gs.shuffle" href="#SIFT_gs.FIBSEM_SIFT_gs.shuffle">RandomState.shuffle()</a></code>, <code><a title="SIFT_gs.FIBSEM_SIFT_gs.permutation" href="#SIFT_gs.FIBSEM_SIFT_gs.permutation">RandomState.permutation()</a></code>
<code>random.Generator.choice: which should be used in new code</code></p>
<h2 id="notes">Notes</h2>
<p>Setting user-specified probabilities through <code>p</code> uses a more general but less
efficient sampler than the default. The general sampler produces a different sample
than the optimized sampler even if each element of <code>p</code> is 1 / len(a).</p>
<p>Sampling random rows from a 2-D array is not possible with this function,
but is possible with <code>Generator.choice</code> through its <code>axis</code> keyword.</p>
<h2 id="examples">Examples</h2>
<p>Generate a uniform random sample from np.arange(5) of size 3:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.choice(5, 3)
array([0, 3, 4]) # random
&gt;&gt;&gt; #This is equivalent to np.random.randint(0,5,3)
</code></pre>
<p>Generate a non-uniform random sample from np.arange(5) of size 3:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])
array([3, 3, 0]) # random
</code></pre>
<p>Generate a uniform random sample from np.arange(5) of size 3 without
replacement:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.choice(5, 3, replace=False)
array([3,1,0]) # random
&gt;&gt;&gt; #This is equivalent to np.random.permutation(np.arange(5))[:3]
</code></pre>
<p>Generate a non-uniform random sample from np.arange(5) of size
3 without replacement:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])
array([2, 3, 0]) # random
</code></pre>
<p>Any of the above can be repeated with an arbitrary array-like
instead of just integers. For instance:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']
&gt;&gt;&gt; np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])
array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random
      dtype='&lt;U11')
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.determine_pad_offsets"><code class="name flex">
<span>def <span class="ident">determine_pad_offsets</span></span>(<span>shape, tr_matr)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_pad_offsets(shape, tr_matr):
    ysz, xsz = shape
    corners = np.array([[0.0, 0.0, 1.0], [0.0, ysz, 1.0], [xsz, 0.0, 1.0], [xsz, ysz, 1.0]])
    a = np.array(tr_matr)[:, 0:2, :] @ corners.T
    xc = a[:, 0, :].ravel()
    yc = a[:, 1, :].ravel()
    xmin = np.min((np.min(xc), 0.0))
    xmax = np.max(xc)-xsz
    ymin = np.min((np.min(yc), 0.0))
    ymax = np.max(yc)-ysz
    return xmin, xmax, ymin, ymax</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.determine_pad_offsets_old"><code class="name flex">
<span>def <span class="ident">determine_pad_offsets_old</span></span>(<span>shape, tr_matr)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_pad_offsets_old(shape, tr_matr):
    ysz, xsz = shape
    xmins = np.zeros(len(tr_matr))
    xmaxs = xmins.copy()
    ymins = xmins.copy()
    ymaxs = xmins.copy()
    corners = np.array([[0,0], [0, ysz], [xsz, 0], [xsz, ysz]])
    for j, trm in enumerate(tqdm(tr_matr, desc = &#39;Determining the pad offsets&#39;)):
        a = (trm[0:2, 0:2] @ corners.T).T + trm[0:2, 2]
        xmins[j] = np.min(a[:, 0])
        xmaxs[j] = np.max(a[:, 0])
        ymins[j] = np.min(a[:, 1])
        ymaxs[j] = np.max(a[:, 1])
        xmin = np.min((np.min(xmins), 0.0))
        xmax = np.max(xmaxs)-xsz
        ymin = np.min((np.min(ymins), 0.0))
        ymax = np.max(ymaxs)-ysz
    return xmin, xmax, ymin, ymax</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.determine_regularized_affine_transform"><code class="name flex">
<span>def <span class="ident">determine_regularized_affine_transform</span></span>(<span>src_pts, dst_pts, l2_matrix=None, targ_vector=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate N-D affine transformation with regularization from a set of corresponding points.
©G.Shtengel 11/2021 gleb.shtengel@gmail.com</p>
<pre><code>We can determine the over-, well- and under-determined parameters
with the total least-squares method.
Number of source and destination coordinates must match.
The transformation is defined as:
    X = (a0*x + a1*y + a2)
    Y = (b0*x + b1*y + b2)
This is regularized Affine estimation - it is regularized so that the penalty is for deviation from a target (default target is rigid shift) transformation
a0 =1, a1=0, b0=1, b1=1 are parameters for target (shift) transform. Deviation from these is penalized.

The coefficients appear linearly so we can write
A x = B, where:
    A   = [[x y 1 0 0 0]
           [0 0 0 x y 1]]
    Htarget.T = [a0 a1 a2 b0 b1 b2]
    B.T = [X Y]

In case of ordinary least-squares (OLS) the solution of this system
of equations is:
H = np.linalg.inv(A.T @ A) @ A @ B

In case of least-squares with Tikhonov-like regularization:
H = np.linalg.inv(A.T @ A + Γ.T @ Γ) @ (A @ B + Γ.T @ Γ @ Htarget)
where Γ.T @ Γ (for simplicity will call it L2 vector) is regularization term and Htarget
is a target solution, deviation from which is minimized in L2 sense
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_regularized_affine_transform(src_pts, dst_pts, l2_matrix = None, targ_vector = None):
    &#34;&#34;&#34;
    Estimate N-D affine transformation with regularization from a set of corresponding points.
    ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

        We can determine the over-, well- and under-determined parameters
        with the total least-squares method.
        Number of source and destination coordinates must match.
        The transformation is defined as:
            X = (a0*x + a1*y + a2)
            Y = (b0*x + b1*y + b2)
        This is regularized Affine estimation - it is regularized so that the penalty is for deviation from a target (default target is rigid shift) transformation
        a0 =1, a1=0, b0=1, b1=1 are parameters for target (shift) transform. Deviation from these is penalized.

        The coefficients appear linearly so we can write
        A x = B, where:
            A   = [[x y 1 0 0 0]
                   [0 0 0 x y 1]]
            Htarget.T = [a0 a1 a2 b0 b1 b2]
            B.T = [X Y]

        In case of ordinary least-squares (OLS) the solution of this system
        of equations is:
        H = np.linalg.inv(A.T @ A) @ A @ B

        In case of least-squares with Tikhonov-like regularization:
        H = np.linalg.inv(A.T @ A + Γ.T @ Γ) @ (A @ B + Γ.T @ Γ @ Htarget)
        where Γ.T @ Γ (for simplicity will call it L2 vector) is regularization term and Htarget
        is a target solution, deviation from which is minimized in L2 sense
     &#34;&#34;&#34;

    src_matrix, src = _center_and_normalize_points_gs(src_pts)
    dst_matrix, dst = _center_and_normalize_points_gs(dst_pts)

    n, d = src.shape
    n2 = n*n   # normalization factor, so that shrinkage parameter does not depend on the number of points

    A = np.zeros((n * d, d * (d + 1)))
    # fill the A matrix with the appropriate block matrices; see docstring
    # for 2D example — this can be generalised to more blocks in the 3D and
    # higher-dimensional cases.
    for ddim in range(d):
        A[ddim*n : (ddim + 1) * n, ddim * (d + 1) : ddim * (d + 1) + d] = src
        A[ddim*n : (ddim + 1) * n, ddim * (d + 1) + d] = 1

    AtA = A.T @ A / n2

    if l2_matrix is None:
        l2 = 1.0e-5   # default shrinkage parameter
        l2_matrix = np.eye(2 * (d + 1)) * l2
        for ddim in range(d):
            ii = (d + 1) * (ddim + 1) - 1
            l2_matrix[ii,ii] = 0

    if targ_vector is None:
        targ_vector = np.zeros(2 * (d + 1))
        targ_vector[0] = 1
        targ_vector[4] = 1


    Hp = np.linalg.inv(AtA + l2_matrix) @ (A.T @ dst.T.ravel() / n2 + l2_matrix @ targ_vector)
    Hm = np.eye(d + 1)
    Hm[0:d, 0:d+1] = Hp.reshape(d, d + 1)
    H = np.linalg.inv(dst_matrix) @ Hm @ src_matrix
    return H</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.determine_transformation_matrix"><code class="name flex">
<span>def <span class="ident">determine_transformation_matrix</span></span>(<span>src_pts, dst_pts, TransformType, drmax=2, max_iter=100)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine the transformation matrix.
©G.Shtengel, 09/2021. gleb.shtengel@gmail.com</p>
<p>Determine the transformation matrix in a form:
A = [[a0
a1
a2]
[b0
b1
b2]
[0
0
1]]
based on the given source and destination points using linear regression such that the error is minimized for
sum(dst_pts - A*src_pts).</p>
<p>For each matched pair of keypoins the error is calculated as err[j] = dst_pts[j] - A*src_pts[j]
The iterative procedure throws away the matched keypoint pair with worst error on every iteration
untill the worst error falls below drmax or the max number of iterations is reached.</p>
<p>Returns
transform_matrix, kpts, error_abs_mean, iteration</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_transformation_matrix(src_pts, dst_pts, TransformType, drmax = 2, max_iter = 100):
    &#39;&#39;&#39;
    Determine the transformation matrix.
    ©G.Shtengel, 09/2021. gleb.shtengel@gmail.com

    Determine the transformation matrix in a form:
            A = [[a0  a1   a2]
                 [b0   b1  b2]
                 [0   0    1]]
    based on the given source and destination points using linear regression such that the error is minimized for
    sum(dst_pts - A*src_pts).

    For each matched pair of keypoins the error is calculated as err[j] = dst_pts[j] - A*src_pts[j]
    The iterative procedure throws away the matched keypoint pair with worst error on every iteration
    untill the worst error falls below drmax or the max number of iterations is reached.

    Returns
    transform_matrix, kpts, error_abs_mean, iteration
    &#39;&#39;&#39;
    transform_matrix = np.eye(3,3)
    iteration = 1
    max_error = drmax * 2.0
    errors = []
    while iteration &lt;= max_iter and max_error &gt; drmax:
        # determine the new transformation matrix
        if TransformType == ShiftTransform:
            transform_matrix[0:2, 2] = np.mean(np.array(dst_pts.astype(float) - src_pts.astype(float)), axis=0)

        if TransformType == XScaleShiftTransform:
            n, d = src_pts.shape
            xsrc = np.array(src_pts)[:,0].astype(float)
            ysrc = np.array(src_pts)[:,1].astype(float)
            xdst = np.array(dst_pts)[:,0].astype(float)
            ydst = np.array(dst_pts)[:,1].astype(float)
            s00 = np.sum(xsrc)
            s01 = np.sum(xdst)
            sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
            #sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xdst*xdst) - s01*s01)
            #s10 = np.sum(ysrc)
            #s11 = np.sum(ydst)
            #sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)
            sy = 1.00 # force sy=1 if there are not enough keypoints points
            # spread over wide range of y-range (and y-range is small) to determine y-scale accuartely
            tx = np.mean(xdst) - sx * np.mean(xsrc)
            ty = np.mean(ydst) - sy * np.mean(ysrc)
            transform_matrix = np.array([[sx, 0,  tx],
                                         [0,  sy, ty],
                                         [0,  0,  1]])

        if TransformType == ScaleShiftTransform:
            n, d = src_pts.shape
            xsrc = np.array(src_pts)[:,0].astype(float)
            ysrc = np.array(src_pts)[:,1].astype(float)
            xdst = np.array(dst_pts)[:,0].astype(float)
            ydst = np.array(dst_pts)[:,1].astype(float)
            s00 = np.sum(xsrc)
            s01 = np.sum(xdst)
            sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
            s10 = np.sum(ysrc)
            s11 = np.sum(ydst)
            sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)
            tx = np.mean(xdst) - sx * np.mean(xsrc)
            ty = np.mean(ydst) - sy * np.mean(ysrc)
            transform_matrix = np.array([[sx, 0,  tx],
                                         [0,  sy, ty],
                                         [0,  0,  1]])

        if TransformType == AffineTransform:
            #estimate_scale = True
            #transform_matrix = _umeyama(src_pts, dst_pts, estimate_scale)
            tform = AffineTransform()
            tform.estimate(src_pts, dst_pts)
            transform_matrix = tform.params

        if TransformType == RegularizedAffineTransform:
            tform = AffineTransform()
            tform.estimate(src_pts, dst_pts)  # regularization parameters are already part of estimate procedure
            # this is implemented this way because the other code - RANSAC does not work otherwise
            transform_matrix = tform.params

        # estimate transformation errors and find outliers
        errs = estimate_kpts_transform_error(src_pts, dst_pts, transform_matrix)
        max_error = np.max(errs)
        ind = np.argmax(errs)
        src_pts = np.delete(src_pts, ind, axis=0)
        dst_pts = np.delete(dst_pts, ind, axis=0)
        #print(&#39;Iteration {:d}, max_error={:.2f} &#39;.format(iteration, max_error), (iteration &lt;= max_iter), (max_error &gt; drmax))
        iteration +=1
    kpts = [src_pts, dst_pts]
    error_abs_mean = np.mean(np.abs(np.delete(errs, ind, axis=0)))
    return transform_matrix, kpts, error_abs_mean, iteration</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.determine_transformations_dataset"><code class="name flex">
<span>def <span class="ident">determine_transformations_dataset</span></span>(<span>fnms, DASK_client, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_transformations_dataset(fnms, DASK_client, **kwargs):
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    params_s4 = []
    for j, fnm in enumerate(fnms[:-1]):
        fname1 = fnms[j]
        fname2 = fnms[j+1]
        params_s4.append([fname1, fname2, kwargs])
    if use_DASK:
        print(&#39;Using DASK distributed&#39;)
        futures4 = DASK_client.map(determine_transformations_files, params_s4, retries = DASK_client_retries)
        #determine_transformations_files returns (transform_matrix, fnm_matches, kpts, iteration)
        results_s4 = DASK_client.gather(futures4)
    else:
        print(&#39;Using Local Computation&#39;)
        results_s4 = []
        for param_s4 in tqdm(params_s4, desc = &#39;Extracting Transformation Parameters: &#39;):
            results_s4.append(determine_transformations_files(param_s4))
    return results_s4</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.determine_transformations_files"><code class="name flex">
<span>def <span class="ident">determine_transformations_files</span></span>(<span>params_dsf)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine the transformation matrix from two sets of Key-Points and Descriptors.
©G.Shtengel, 09/2021. gleb.shtengel@gmail.com</p>
<p>This is a faster version of the procedure - it loads the keypoints and matches for each frame from files.
params_dsf = fnm_1, fnm_2, kwargs
where
fnm_1 - keypoints for the first image (source)
fnm_2 - keypoints for the first image (destination)
and kwargs must include:
TransformType - transformation type to be used (ShiftTransform, XScaleShiftTransform, ScaleShiftTransform, AffineTransform, RegularizedAffineTransform)
BF_Matcher -
if True - use BF matcher, otherwise use FLANN matcher for keypoint matching
solver - a string indicating which solver to use:
'LinReg' will use Linear Regression with iterative "Throwing out the Worst Residual" Heuristic
'RANSAC' will use RANSAC (Random Sample Consensus) algorithm.
Lowe_Ratio_Threshold - threshold for Lowe's Ratio Test
drmax - in the case of 'LinReg' - outlier threshold for iterative regression
- in the case of 'RANSAC' - Maximum distance for a data point to be classified as an inlier.
max_iter - max number of iterations
save_matches - if True - save the matched keypoints into a binary dump file</p>
<p>Returns:
transform_matrix, fnm_matches, kpts, error_abs_mean, iteration</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_transformations_files(params_dsf):
    &#39;&#39;&#39; Determine the transformation matrix from two sets of Key-Points and Descriptors.
    ©G.Shtengel, 09/2021. gleb.shtengel@gmail.com

    This is a faster version of the procedure - it loads the keypoints and matches for each frame from files.
    params_dsf = fnm_1, fnm_2, kwargs
    where
    fnm_1 - keypoints for the first image (source)
    fnm_2 - keypoints for the first image (destination)
    and kwargs must include:
    TransformType - transformation type to be used (ShiftTransform, XScaleShiftTransform, ScaleShiftTransform, AffineTransform, RegularizedAffineTransform)
    BF_Matcher -  if True - use BF matcher, otherwise use FLANN matcher for keypoint matching
    solver - a string indicating which solver to use:
    &#39;LinReg&#39; will use Linear Regression with iterative &#34;Throwing out the Worst Residual&#34; Heuristic
    &#39;RANSAC&#39; will use RANSAC (Random Sample Consensus) algorithm.
    Lowe_Ratio_Threshold - threshold for Lowe&#39;s Ratio Test
    drmax - in the case of &#39;LinReg&#39; - outlier threshold for iterative regression
           - in the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
    max_iter - max number of iterations
    save_matches - if True - save the matched keypoints into a binary dump file

    Returns:
    transform_matrix, fnm_matches, kpts, error_abs_mean, iteration
    &#39;&#39;&#39;
    fnm_1, fnm_2, kwargs = params_dsf

    ftype = kwargs.get(&#34;ftype&#34;, 0)
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)    # threshold for Lowe&#39;s Ratio Test

    if TransformType == RegularizedAffineTransform:

        def estimate(self, src, dst):
            self.params = determine_regularized_affine_transform(src, dst, l2_matrix, targ_vector)
        RegularizedAffineTransform.estimate = estimate

    kpp1s, des1 = pickle.load(open(fnm_1, &#39;rb&#39;))
    kpp2s, des2 = pickle.load(open(fnm_2, &#39;rb&#39;))

    kp1 = [list_to_kp(kpp1) for kpp1 in kpp1s]     # this converts a list of lists to a list of keypoint objects to be used by a matcher later
    kp2 = [list_to_kp(kpp2) for kpp2 in kpp2s]     # same for the second frame

    # establish matches
    if BFMatcher:    # if BFMatcher==True - use BF (Brute Force) matcher
        # This procedure uses BF (Brute-Force) Matcher.
        # BF matcher takes the descriptor of one feature in the first image and matches it with all other features
        # in second image using some distance calculation. The closest match in teh second image is returned.
        # For BF matcher, first we have to create the cv.DescriptorMatcher object with BFMatcher as type.
        # It takes two optional params:
        #
        # First parameter one is NormType. It specifies the distance measurement to be used. By default, it is L2.
        # It is good for SIFT, SURF, etc. (L1 is also there).
        # For binary string-based descriptors like ORB, BRIEF, BRISK, etc., Hamming should be used,
        # which uses Hamming distance as measurement. If ORB is using WTA_K of 3 or 4, Hamming2 should be used.
        #
        # Second parameter is boolean variable, CrossCheck which is false by default.
        # If it is true, Matcher returns only those matches with value (i,j)
        # such that i-th descriptor in set A has j-th descriptor in set B as the best match and vice-versa.
        # That is, the two features in both sets should match each other.
        # It provides consistant result, and is a good alternative to ratio test proposed by D.Lowe in SIFT paper.
        # http://amroamroamro.github.io/mexopencv/opencv_contrib/SURF_descriptor.html
        bf = cv2.BFMatcher()
        matches = bf.knnMatch(des1,des2,k=2)
    else:            # otherwise - use FLANN matcher
        # This procedure uses FLANN (Fast Library for Approximate Nearest Neighbors) Matcher (FlannBasedMatcher):
        # https://docs.opencv.org/3.4/d5/d6f/tutorial_feature_flann_matcher.html
        #
        # FLANN contains a collection of algorithms optimized for fast nearest neighbor search in large datasets
        # and for high dimensional features. It works faster than BFMatcher for large datasets.
        # For FlannBasedMatcher, it accepts two sets of options which specifies the algorithm to be used, its related parameters etc.
        # First one is Index. For various algorithms, the information to be passed is explained in FLANN docs.
        # http://amroamroamro.github.io/mexopencv/opencv_contrib/SURF_descriptor.html
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
        search_params = dict(checks = 50)
        flann = cv2.FlannBasedMatcher(index_params, search_params)
        matches = flann.knnMatch(des1, des2, k=2)

    # Lowe&#39;s Ratio test
    good = []
    for m, n in matches:
        if m.distance &lt; Lowe_Ratio_Threshold * n.distance:
            good.append(m)

    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1, 2)
    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1, 2)

    if solver == &#39;LinReg&#39;:
        # Determine the transformation matrix via iterative liear regression
        transform_matrix, kpts, error_abs_mean, iteration = determine_transformation_matrix(src_pts, dst_pts, TransformType, drmax = drmax, max_iter = max_iter)
        n_kpts = len(kpts[0])
    else:  # the other option is solver = &#39;RANSAC&#39;
        try:
            min_samples = len(src_pts)//20
            model, inliers = ransac((src_pts, dst_pts),
                TransformType, min_samples=min_samples,
                residual_threshold=drmax, max_trials=10000)
            n_inliers = np.sum(inliers)
            inlier_keypoints_left = [cv2.KeyPoint(point[0], point[1], 1) for point in src_pts[inliers]]
            inlier_keypoints_right = [cv2.KeyPoint(point[0], point[1], 1) for point in dst_pts[inliers]]
            placeholder_matches = [cv2.DMatch(idx, idx, 1) for idx in range(n_inliers)]
            src_pts_ransac = np.float32([ inlier_keypoints_left[m.queryIdx].pt for m in placeholder_matches ]).reshape(-1, 2)
            dst_pts_ransac = np.float32([ inlier_keypoints_right[m.trainIdx].pt for m in placeholder_matches ]).reshape(-1, 2)
            #non_nan_inds = ~np.isnan(src_pts_ransac) * ~np.isnan(dst_pts_ransac)
            #src_pts_ransac = src_pts_ransac[non_nan_inds]
            #dst_pts_ransac = dst_pts_ransac[non_nan_inds]
            kpts = [src_pts_ransac, dst_pts_ransac]
            # find shift parameters
            transform_matrix = model.params
            iteration = len(src_pts)- len(src_pts_ransac)
            error_abs_mean = np.mean(np.abs(estimate_kpts_transform_error(src_pts_ransac, dst_pts_ransac, transform_matrix)))
        except:
            transform_matrix = array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])
            kpts = [[], []]
            error_abs_mean = np.nan
            iteration = np.nan
    if save_matches:
        fnm_matches = fnm_2.replace(&#39;_kpdes.bin&#39;, &#39;_matches.bin&#39;)
        pickle.dump(kpts, open(fnm_matches, &#39;wb&#39;))
    else:
        fnm_matches = &#39;&#39;
    return transform_matrix, fnm_matches, kpts, error_abs_mean, iteration</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.dirichlet"><code class="name flex">
<span>def <span class="ident">dirichlet</span></span>(<span>alpha, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from the Dirichlet distribution.</p>
<p>Draw <code>size</code> samples of dimension k from a Dirichlet distribution. A
Dirichlet-distributed random variable can be seen as a multivariate
generalization of a Beta distribution. The Dirichlet distribution
is a conjugate prior of a multinomial distribution in Bayesian
inference.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.dirichlet</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>sequence</code> of <code>floats, length k</code></dt>
<dd>Parameter of the distribution (length <code>k</code> for sample of
length <code>k</code>).</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n)</code>, then
<code>m * n * k</code> samples are drawn.
Default is None, in which case a
vector of length <code>k</code> is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>samples</code></strong> :&ensp;<code>ndarray,</code></dt>
<dd>The drawn samples, of shape <code>(size, k)</code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If any value in <code>alpha</code> is less than or equal to zero</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.dirichlet</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The Dirichlet distribution is a distribution over vectors
:math:<code>x</code> that fulfil the conditions :math:<code>x_i&gt;0</code> and
:math:<code>\sum_{i=1}^k x_i = 1</code>.</p>
<p>The probability density function :math:<code>p</code> of a
Dirichlet-distributed random vector :math:<code>X</code> is
proportional to</p>
<p>[
]
where :math:<code>\alpha</code> is a vector containing the positive
concentration parameters.</p>
<p>The method uses the following property for computation: let :math:<code>Y</code>
be a random vector which has components that follow a standard gamma
distribution, then :math:<code>X = \frac{1}{\sum_{i=1}^k{Y_i}} Y</code>
is Dirichlet-distributed</p>
<h2 id="references">References</h2>
<p>.. [1] David McKay, "Information Theory, Inference and Learning
Algorithms," chapter 23,
<a href="http://www.inference.org.uk/mackay/itila/">http://www.inference.org.uk/mackay/itila/</a>
.. [2] Wikipedia, "Dirichlet distribution",
<a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">https://en.wikipedia.org/wiki/Dirichlet_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Taking an example cited in Wikipedia, this distribution can be used if
one wanted to cut strings (each of initial length 1.0) into K pieces
with different lengths, where each piece had, on average, a designated
average length, but allowing some variation in the relative sizes of
the pieces.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; s = np.random.dirichlet((10, 5, 3), 20).transpose()
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; plt.barh(range(20), s[0])
&gt;&gt;&gt; plt.barh(range(20), s[1], left=s[0], color='g')
&gt;&gt;&gt; plt.barh(range(20), s[2], left=s[0]+s[1], color='r')
&gt;&gt;&gt; plt.title(&quot;Lengths of Strings&quot;)
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.estimate_kpts_transform_error"><code class="name flex">
<span>def <span class="ident">estimate_kpts_transform_error</span></span>(<span>src_pts, dst_pts, transform_matrix)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate the transformation error for key-point pairs and known transformation matrix.
©G.Shtengel, 09/2021. gleb.shtengel@gmail.com</p>
<p>Image transformation matrix in a form:
A = [[a0
a1
a2]
[b0
b1
b2]
[0
0
1]]
Thransofrmation is supposed to be in a form:
Xnew = a0 * Xoriginal + a1 * Yoriginal + a2
Ynew = b0 * Xoriginal + b1 * Yoriginal + b2
source and destination points are pairs of coordinates (2xN array)
errors are estimated as norm(dest_pts - A*src_pts) so that least square regression can be performed</p>
<h2 id="returns">Returns</h2>
<p>np.linalg.norm(dst_pts - src_pts_transformed, ord=2, axis=1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate_kpts_transform_error(src_pts, dst_pts, transform_matrix):
    &#34;&#34;&#34;
    Estimate the transformation error for key-point pairs and known transformation matrix.
    ©G.Shtengel, 09/2021. gleb.shtengel@gmail.com

    Image transformation matrix in a form:
        A = [[a0  a1   a2]
             [b0   b1  b2]
             [0   0    1]]
     Thransofrmation is supposed to be in a form:
     Xnew = a0 * Xoriginal + a1 * Yoriginal + a2
     Ynew = b0 * Xoriginal + b1 * Yoriginal + b2
     source and destination points are pairs of coordinates (2xN array)
     errors are estimated as norm(dest_pts - A*src_pts) so that least square regression can be performed

    Returns:
        np.linalg.norm(dst_pts - src_pts_transformed, ord=2, axis=1)
    &#34;&#34;&#34;
    src_pts_transformed = src_pts @ transform_matrix[0:2, 0:2].T + transform_matrix[0:2, 2]
    return np.linalg.norm(dst_pts - src_pts_transformed, ord=2, axis=1)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.evaluate_FIBSEM_frame"><code class="name flex">
<span>def <span class="ident">evaluate_FIBSEM_frame</span></span>(<span>params)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates single FIB-SEM frame and extract parameters: data min/max, milling rate, FOV center.</p>
<ol>
<li>
<p>Calculates the data range of the EM data ©G.Shtengel 04/2022 gleb.shtengel@gmail.com
Calculates histogram of pixel intensities of of the loaded image
with number of bins determined by parameter nbins (default = 256)
and normalizes it to get the probability distribution function (PDF),
from which a cumulative distribution function (CDF) is calculated.
Then given the threshold_min, threshold_max parameters,
the minimum and maximum values for the image are found by finding
the intensities at which CDF= threshold_min and (1- threshold_max), respectively.</p>
</li>
<li>
<p>Extracts WD, MillingYVoltage, center_x, center_y data from the header</p>
</li>
</ol>
<h2 id="parameters">Parameters:</h2>
<p>params =
fl, kwargs
fl : str
The string containing a full path to the EM data file.
kwargs: dictioanry of kwargs:
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
image_name: string
the name of the image to perform this operations (defaulut is 'RawImageA')
threshold_min : float
CDF threshold for determining the minimum data value
threshold_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dmin, dmax</code></dt>
<dd>(float) minimum and maximum values of the data range.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_FIBSEM_frame(params):
    &#39;&#39;&#39;
    Evaluates single FIB-SEM frame and extract parameters: data min/max, milling rate, FOV center.

    1. Calculates the data range of the EM data ©G.Shtengel 04/2022 gleb.shtengel@gmail.com
    Calculates histogram of pixel intensities of of the loaded image
    with number of bins determined by parameter nbins (default = 256)
    and normalizes it to get the probability distribution function (PDF),
    from which a cumulative distribution function (CDF) is calculated.
    Then given the threshold_min, threshold_max parameters,
    the minimum and maximum values for the image are found by finding
    the intensities at which CDF= threshold_min and (1- threshold_max), respectively.

    2. Extracts WD, MillingYVoltage, center_x, center_y data from the header

    Parameters:
    ----------
    params =  fl, kwargs
        fl : str
            The string containing a full path to the EM data file.
        kwargs: dictioanry of kwargs:
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        image_name: string
            the name of the image to perform this operations (defaulut is &#39;RawImageA&#39;)
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF

    Returns:
        dmin, dmax: (float) minimum and maximum values of the data range.
    &#39;&#39;&#39;
    fl, kwargs = params
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
    thr_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    thr_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    frame = FIBSEM_frame(fl, ftype=ftype)
    if frame.EightBit ==1:
        dmin = uint8(0)
        dmax =  uint8(255)
    else:
        dmin, dmax = frame.get_image_min_max(image_name = &#39;RawImageA&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins)
    if ftype == 0:
        try:
            WD = frame.WD
            MillingYVoltage = frame.MillingYVoltage
        except:
            WD = 0
            MillingYVoltage = 0
        try:
            center_x = (frame.FirstPixelX + frame.XResolution/2.0)
            center_y = (frame.FirstPixelY + frame.YResolution/2.0)
        except:
            center_x = 0
            center_y = 0
    else:
        WD = 0
        MillingYVoltage = 0
        center_x = 0
        center_y = 0

    return dmin, dmax, WD, MillingYVoltage, center_x, center_y</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.evaluate_FIBSEM_frames_dataset"><code class="name flex">
<span>def <span class="ident">evaluate_FIBSEM_frames_dataset</span></span>(<span>fls, DASK_client, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates parameters of FIBSEM data set (Min/Max, Working Distance (WD), Milling Y Voltage (MV), FOV center positions).</p>
<p>Parameters:
use_DASK : boolean
perform remote DASK computations
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails</p>
<p>kwargs:
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
frame_inds : array
Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
data_dir : str
data directory (path) for saving the data
threshold_min : float
CDF threshold for determining the minimum data value
threshold_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF
sliding_minmax : boolean
if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
if False - same data_min_glob and data_max_glob will be used for all files
fit_params : list
Example: ['SG', 501, 3]
- perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
Other options are:
['LF'] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
['PF', 2]
- use polynomial fit (in this case of order 2)
Mill_Volt_Rate_um_per_V : float
Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.
FIBSEM_Data_xlsx : str
Filepath of the Excell file for the FIBSEM data set data to be saved (Data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)
disp_res : bolean
If True (default), intermediate messages and results will be displayed.</p>
<p>Returns:
list of 9 parameters: FIBSEM_Data_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding, mill_rate_WD, mill_rate_MV, center_x, center_y
FIBSEM_Data_xlsx : str
path to Excel file with the FIBSEM data
data_min_glob : float
min data value for I8 conversion (open CV SIFT requires I8)
data_man_glob : float
max data value for I8 conversion (open CV SIFT requires I8)
data_min_sliding : float array
min data values (one per file) for I8 conversion
data_max_sliding : float array
max data values (one per file) for I8 conversion</p>
<pre><code>mill_rate_WD : float array
    Milling rate calculated based on Working Distance (WD)
mill_rate_MV : float array
    Milling rate calculated based on Milling Y Voltage (MV)
center_x : float array
    FOV Center X-coordinate extrated from the header data
center_y : float array
    FOV Center Y-coordinate extrated from the header data
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_FIBSEM_frames_dataset(fls, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Evaluates parameters of FIBSEM data set (Min/Max, Working Distance (WD), Milling Y Voltage (MV), FOV center positions).

    Parameters:
    use_DASK : boolean
        perform remote DASK computations
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails

    kwargs:
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    frame_inds : array
        Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
    data_dir : str
        data directory (path) for saving the data
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    sliding_minmax : boolean
        if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
        if False - same data_min_glob and data_max_glob will be used for all files
    fit_params : list
        Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
        Other options are:
            [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
            [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
    Mill_Volt_Rate_um_per_V : float
        Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.
    FIBSEM_Data_xlsx : str
        Filepath of the Excell file for the FIBSEM data set data to be saved (Data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)
    disp_res : bolean
        If True (default), intermediate messages and results will be displayed.

    Returns:
    list of 9 parameters: FIBSEM_Data_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding, mill_rate_WD, mill_rate_MV, center_x, center_y
        FIBSEM_Data_xlsx : str
            path to Excel file with the FIBSEM data
        data_min_glob : float
            min data value for I8 conversion (open CV SIFT requires I8)
        data_man_glob : float
            max data value for I8 conversion (open CV SIFT requires I8)
        data_min_sliding : float array
            min data values (one per file) for I8 conversion
        data_max_sliding : float array
            max data values (one per file) for I8 conversion

        mill_rate_WD : float array
            Milling rate calculated based on Working Distance (WD)
        mill_rate_MV : float array
            Milling rate calculated based on Milling Y Voltage (MV)
        center_x : float array
            FOV Center X-coordinate extrated from the header data
        center_y : float array
            FOV Center Y-coordinate extrated from the header data
    &#39;&#39;&#39;

    nfrs = len(fls)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    frame_inds = kwargs.get(&#34;frame_inds&#34;, np.arange(len(fls)))
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, True)
    fit_params =  kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, 31.235258870176065)
    kwargs[&#39;Mill_Volt_Rate_um_per_V&#39;] = Mill_Volt_Rate_um_per_V

    FIBSEM_Data_xlsx = kwargs.get(&#39;FIBSEM_data_xlsx&#39;, &#39;FIBSEM_Data.xlsx&#39;)
    FIBSEM_Data_xlsx_path = os.path.join(data_dir, FIBSEM_Data_xlsx)
    disp_res = kwargs.get(&#34;disp_res&#34;, False)

    frame = FIBSEM_frame(fls[0], ftype=ftype)
    if frame.EightBit == 1 and ftype == 1:
        if disp_res:
            print(&#39;Original data is 8-bit, no need to find Min and Max for 8-bit conversion&#39;)
        data_min_glob = uint8(0)
        data_max_glob =  uint8(255)
        data_min_sliding = np.zeros(nfrs, dtype=uint8)
        data_max_sliding = np.zeros(nfrs, dtype=uint8)+ uint8(255)
        data_minmax_glob = np.zeros((nfrs, 2), dtype=uint8)
        data_minmax_glob[1, :] = uint8(255)
        mill_rate_WD = np.zeros(nfrs, dtype=float)
        mill_rate_MV = np.zeros(nfrs, dtype=float)
        center_x = np.zeros(nfrs, dtype=float)
        center_y = np.zeros(nfrs, dtype=float)

    else:
        params_s2 = [[fl, kwargs] for fl in fls]

        if use_DASK:
            if disp_res:
                print(&#39;Using DASK distributed&#39;)
            futures = DASK_client.map(evaluate_FIBSEM_frame, params_s2, retries = DASK_client_retries)
            results_s2 = np.array(DASK_client.gather(futures))
        else:
            if disp_res:
                print(&#39;Using Local Computation&#39;)
            results_s2 = np.zeros((nfrs, 6))
            for j, param_s2 in enumerate(tqdm(params_s2, desc=&#39;Evaluating FIB-SEM frames (data min/max, mill rate, FOV shifts): &#39;), display = disp_res):
                results_s2[j, :] = evaluate_FIBSEM_frame(param_s2)

        data_minmax_glob = results_s2[:, 0:2]
        data_min_glob, trash = get_min_max_thresholds(data_minmax_glob[:, 0], thr_min = threshold_min, thr_max = threshold_max, nbins = nbins, disp_res=False)
        trash, data_max_glob = get_min_max_thresholds(data_minmax_glob[:, 1], thr_min = threshold_min, thr_max = threshold_max, nbins = nbins, disp_res=False)
        data_min_sliding = savgol_filter(data_minmax_glob[:, 0].astype(double), min([fit_params[1], fit_params[1]]), fit_params[2])
        data_max_sliding = savgol_filter(data_minmax_glob[:, 1].astype(double), min([fit_params[1], fit_params[1]]), fit_params[2])
        mill_rate_WD = results_s2[:, 2]
        mill_rate_MV = results_s2[:, 3]
        center_x = results_s2[:, 4]
        center_y = results_s2[:, 5]

    if disp_res:
        print(&#39;Saving the FIBSEM dataset statistics (Min/Max, Mill Rate, FOV Shifts into the file: &#39;, FIBSEM_Data_xlsx_path)
        # Create a Pandas Excel writer using XlsxWriter as the engine.
    xlsx_writer = pd.ExcelWriter(FIBSEM_Data_xlsx_path, engine=&#39;xlsxwriter&#39;)
    columns=[&#39;Frame&#39;, &#39;Min&#39;, &#39;Max&#39;, &#39;Sliding Min&#39;, &#39;Sliding Max&#39;, &#39;Working Distance (mm)&#39;, &#39;Milling Y Voltage (V)&#39;, &#39;FOV X Center (Pix)&#39;, &#39;FOV Y Center (Pix)&#39;]
    minmax_df = pd.DataFrame(np.vstack((frame_inds.T,
        data_minmax_glob.T,
        data_min_sliding.T,
        data_max_sliding.T,
        mill_rate_WD.T,
        mill_rate_MV.T,
        center_x.T,
        center_y.T)).T, columns = columns, index = None)
    minmax_df.to_excel(xlsx_writer, index=None, sheet_name=&#39;FIBSEM Data&#39;)
    kwargs_info = pd.DataFrame([kwargs]).T   # prepare to be save in transposed format
    kwargs_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;kwargs Info&#39;)
    xlsx_writer.save()

    return FIBSEM_Data_xlsx_path, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding, mill_rate_WD, mill_rate_MV, center_x, center_y</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.evaluate_registration_two_frames"><code class="name flex">
<span>def <span class="ident">evaluate_registration_two_frames</span></span>(<span>params_mrc)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function used by DASK routine. Analyzes registration between two frames.
©G.Shtengel, 10/2020. gleb.shtengel@gmail.com</p>
<p>Parameters:
params_mrc : list of mrc_filename, fr, evals, save_frame_png, filename_frame_png
mrc_filename
: string
full path to mrc filename
fr : int
Index of the SECOND frame
evals :
list of image bounds to be used for evaluation exi_eval, xa_eval, yi_eval, ya_eval</p>
<p>Returns:
image_nsad, image_ncc, image_mi
: float, float, float</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_registration_two_frames(params_mrc):
    &#39;&#39;&#39;
    Helper function used by DASK routine. Analyzes registration between two frames.
    ©G.Shtengel, 10/2020. gleb.shtengel@gmail.com

    Parameters:
    params_mrc : list of mrc_filename, fr, evals, save_frame_png, filename_frame_png
    mrc_filename  : string
        full path to mrc filename
    fr : int
        Index of the SECOND frame
    evals :  list of image bounds to be used for evaluation exi_eval, xa_eval, yi_eval, ya_eval


    Returns:
    image_nsad, image_ncc, image_mi   : float, float, float

    &#39;&#39;&#39;
    mrc_filename, fr, invert_data, evals, save_frame_png, filename_frame_png = params_mrc
    mrc_obj = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;)
    header = mrc_obj.header
    &#39;&#39;&#39;
    mode 0 -&gt; uint8
    mode 1 -&gt; int16
    mode 2 -&gt; float32
    mode 4 -&gt; complex64
    mode 6 -&gt; uint16
    &#39;&#39;&#39;
    mrc_mode = mrc_obj.header.mode
    if mrc_mode==0:
        dt_mrc=uint8
    if mrc_mode==1:
        dt_mrc=int16
    if mrc_mode==2:
        dt_mrc=float32
    if mrc_mode==4:
        dt_mrc=complex64
    if mrc_mode==6:
        dt_mrc=uint16

    xi_eval, xa_eval, yi_eval, ya_eval = evals
    if invert_data:
        prev_frame = -1.0 * (((mrc_obj.data[fr-1, yi_eval:ya_eval, xi_eval:xa_eval]).astype(dt_mrc)).astype(float))
        curr_frame = -1.0 * (((mrc_obj.data[fr, yi_eval:ya_eval, xi_eval:xa_eval]).astype(dt_mrc)).astype(float))
    else:
        prev_frame = (mrc_obj.data[fr-1, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float)
        curr_frame = (mrc_obj.data[fr, yi_eval:ya_eval, xi_eval:xa_eval].astype(dt_mrc)).astype(float)
    fr_mean = np.abs(curr_frame/2.0 + prev_frame/2.0)
    image_nsad =  np.mean(np.abs(curr_frame-prev_frame))/(np.mean(fr_mean)-np.amin(fr_mean))
    #image_nsad =  np.mean(np.abs(curr_frame-prev_frame))/(np.mean(fr_mean))
    image_ncc = Two_Image_NCC_SNR(curr_frame, prev_frame)[0]
    image_mi = mutual_information_2d(prev_frame.ravel(), curr_frame.ravel(), sigma=1.0, bin=2048, normalized=True)

    if save_frame_png:
        fr_img = (mrc_obj.data[fr, :, :].astype(dt_mrc)).astype(float)
        yshape, xshape = fr_img.shape
        fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
        fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
        dmin, dmax = get_min_max_thresholds(fr_img[yi_eval:ya_eval, xi_eval:xa_eval])
        if invert_data:
            ax.imshow(fr_img, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
        else:
            ax.imshow(fr_img, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.text(0.06, 0.95, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(fr, image_nsad, image_ncc, image_mi), color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
        rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        ax.axis(&#39;off&#39;)
        fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
        plt.close(fig)

    mrc_obj.close()
    return image_nsad, image_ncc, image_mi</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.evaluate_registration_two_frames_tif"><code class="name flex">
<span>def <span class="ident">evaluate_registration_two_frames_tif</span></span>(<span>params_tif)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function used by DASK routine. Analyzes registration between two frames.
©G.Shtengel, 08/2022. gleb.shtengel@gmail.com</p>
<p>Parameters:
params_tif : list of tif_filename, fr, evals
tif_filename
: string
full path to tif filename
fr : int
Index of the SECOND frame
evals :
list of image bounds to be used for evaluation exi_eval, xa_eval, yi_eval, ya_eval, save_frame_png, filename_frame_png</p>
<p>Returns:
image_nsad, image_ncc, image_mi
: float, float, float</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_registration_two_frames_tif(params_tif):
    &#39;&#39;&#39;
    Helper function used by DASK routine. Analyzes registration between two frames.
    ©G.Shtengel, 08/2022. gleb.shtengel@gmail.com

    Parameters:
    params_tif : list of tif_filename, fr, evals
    tif_filename  : string
        full path to tif filename
    fr : int
        Index of the SECOND frame
    evals :  list of image bounds to be used for evaluation exi_eval, xa_eval, yi_eval, ya_eval, save_frame_png, filename_frame_png


    Returns:
    image_nsad, image_ncc, image_mi   : float, float, float

    &#39;&#39;&#39;
    tif_filename, fr, invert_data, evals, save_frame_png, filename_frame_png = params_tif
    xi_eval, xa_eval, yi_eval, ya_eval = evals

    frame0 = tiff.imread(tif_filename, key=int(fr-1)).astype(float)
    frame1 = tiff.imread(tif_filename, key=int(fr)).astype(float)

    if invert_data:
        prev_frame = -1.0 * frame0[yi_eval:ya_eval, xi_eval:xa_eval]
        curr_frame = -1.0 * frame1[yi_eval:ya_eval, xi_eval:xa_eval]
    else:
        prev_frame = frame0[yi_eval:ya_eval, xi_eval:xa_eval]
        curr_frame = frame1[yi_eval:ya_eval, xi_eval:xa_eval]
    fr_mean = np.abs(curr_frame/2.0 + prev_frame/2.0)
    #image_nsad =  np.mean(np.abs(curr_frame-prev_frame))/(np.mean(fr_mean)-np.amin(fr_mean))
    image_nsad =  np.mean(np.abs(curr_frame-prev_frame))/(np.mean(fr_mean)-np.amin(fr_mean))
    image_ncc = Two_Image_NCC_SNR(curr_frame, prev_frame)[0]
    image_mi = mutual_information_2d(prev_frame.ravel(), curr_frame.ravel(), sigma=1.0, bin=2048, normalized=True)
    if save_frame_png:
        yshape, xshape = frame0.shape
        fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
        fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
        dmin, dmax = get_min_max_thresholds(frame0[yi_eval:ya_eval, xi_eval:xa_eval])
        if invert_data:
            ax.imshow(frame0, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
        else:
            ax.imshow(frame0, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.text(0.06, 0.95, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(fr, image_nsad, image_ncc, image_mi), color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
        rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        ax.axis(&#39;off&#39;)
        fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
        plt.close(fig)

    return image_nsad, image_ncc, image_mi</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.exponential"><code class="name flex">
<span>def <span class="ident">exponential</span></span>(<span>scale=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from an exponential distribution.</p>
<p>Its probability density function is</p>
<p>[
]
for <code>x &gt; 0</code> and 0 elsewhere. :math:<code>\beta</code> is the scale parameter,
which is the inverse of the rate parameter :math:<code>\lambda = 1/\beta</code>.
The rate parameter is an alternative, widely used parameterization
of the exponential distribution [3]_.</p>
<p>The exponential distribution is a continuous analogue of the
geometric distribution.
It describes many common situations, such as
the size of raindrops measured over many rainstorms [1]<em>, or the time
between page requests to Wikipedia [2]</em>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.exponential</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>The scale parameter, :math:<code>\beta = 1/\lambda</code>. Must be
non-negative.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>scale</code> is a scalar.
Otherwise,
<code>np.array(scale).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized exponential distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.exponential</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="references">References</h2>
<p>.. [1] Peyton Z. Peebles Jr., "Probability, Random Variables and
Random Signal Principles", 4th ed, 2001, p. 57.
.. [2] Wikipedia, "Poisson process",
<a href="https://en.wikipedia.org/wiki/Poisson_process">https://en.wikipedia.org/wiki/Poisson_process</a>
.. [3] Wikipedia, "Exponential distribution",
<a href="https://en.wikipedia.org/wiki/Exponential_distribution">https://en.wikipedia.org/wiki/Exponential_distribution</a></p></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.extract_keypoints_dataset"><code class="name flex">
<span>def <span class="ident">extract_keypoints_dataset</span></span>(<span>fls, data_minmax, DASK_client, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts Key-Points and Descriptors for SIFT procedure for all images (files) in the dataset.
©G.Shtengel 10/2021 gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters:</h2>
<p>params = fl, data_minmax, kwargs</p>
<p>fls : str array
array of image filenames (full paths)
data_minmax : list of 5 parameters
minmax_xlsx : str
path to Excel file with Min/Max data
data_min_glob : float
min data value for I8 conversion (open CV SIFT requires I8)
data_min_sliding : float array
min data values (one per file) for I8 conversion
data_max_sliding : float array
max data values (one per file) for I8 conversion
data_minmax_glob : 2D float array
min and max data values without sliding averaging
DASK_client : DASK client object
DASK client (needs to be initialized and running by this time)</p>
<p>kwargs:
sliding_minmax : boolean
if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
if False - same data_min_glob and data_max_glob will be used for all files
use_DASK : boolean
use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
thr_min : float
CDF threshold for determining the minimum data value
thr_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF
kp_max_num : int
Max number of key-points to be matched.
Key-points in every frame are indexed (in descending order)
by the strength of the response. Only kp_max_num is kept for
further processing.
Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
SIFT_nfeatures : int
SIFT libary default is 0. The number of best features to retain.
The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
SIFT_nOctaveLayers : int
SIFT libary default
is 3. The number of layers in each octave.
3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
SIFT_contrastThreshold : double
SIFT libary default
is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
The larger the threshold, the less features are produced by the detector.
The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
When nOctaveLayers is set to default and if you want to use the value used in
D. Lowe paper (0.03), set this argument to 0.09.
SIFT_edgeThreshold : double
SIFT libary default
is 10. The threshold used to filter out edge-like features.
Note that the its meaning is different from the contrastThreshold,
i.e. the larger the edgeThreshold, the less features are filtered out
(more features are retained).
SIFT_sigma : double
SIFT library default is 1.6.
The sigma of the Gaussian applied to the input image at the octave #0.
If your image is captured with a weak camera with soft lenses, you might want to reduce the number.</p>
<p>Returns:
fnms : str array
array of paths to the files containing Key-Points and Descriptors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_keypoints_dataset(fls, data_minmax, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Extracts Key-Points and Descriptors for SIFT procedure for all images (files) in the dataset.
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

    Parameters:
    -----------
    params = fl, data_minmax, kwargs

    fls : str array
        array of image filenames (full paths)
    data_minmax : list of 5 parameters
        minmax_xlsx : str
            path to Excel file with Min/Max data
        data_min_glob : float
            min data value for I8 conversion (open CV SIFT requires I8)
        data_min_sliding : float array
            min data values (one per file) for I8 conversion
        data_max_sliding : float array
            max data values (one per file) for I8 conversion
        data_minmax_glob : 2D float array
            min and max data values without sliding averaging
    DASK_client : DASK client object
        DASK client (needs to be initialized and running by this time)

    kwargs:
    sliding_minmax : boolean
        if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
        if False - same data_min_glob and data_max_glob will be used for all files
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    thr_min : float
        CDF threshold for determining the minimum data value
    thr_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    kp_max_num : int
        Max number of key-points to be matched.
        Key-points in every frame are indexed (in descending order)
        by the strength of the response. Only kp_max_num is kept for
        further processing.
        Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
    SIFT_nfeatures : int
        SIFT libary default is 0. The number of best features to retain.
        The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
    SIFT_nOctaveLayers : int
        SIFT libary default  is 3. The number of layers in each octave.
        3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
    SIFT_contrastThreshold : double
        SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
        The larger the threshold, the less features are produced by the detector.
        The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
        When nOctaveLayers is set to default and if you want to use the value used in
        D. Lowe paper (0.03), set this argument to 0.09.
    SIFT_edgeThreshold : double
        SIFT libary default  is 10. The threshold used to filter out edge-like features.
        Note that the its meaning is different from the contrastThreshold,
        i.e. the larger the edgeThreshold, the less features are filtered out
        (more features are retained).
    SIFT_sigma : double
        SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
        If your image is captured with a weak camera with soft lenses, you might want to reduce the number.

    Returns:
    fnms : str array
        array of paths to the files containing Key-Points and Descriptors
    &#39;&#39;&#39;
    minmax_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding  = data_minmax
    sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, True)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    if sliding_minmax:
        params_s3 = [[dts3[0], dts3[1], dts3[2], kwargs] for dts3 in zip(fls, data_min_sliding, data_max_sliding)]
    else:
        params_s3 = [[fl, data_min_glob, data_max_glob, kwargs] for fl in fls]
    if use_DASK:
        print(&#39;Using DASK distributed&#39;)
        futures_s3 = DASK_client.map(extract_keypoints_descr_files, params_s3, retries = DASK_client_retries)
        fnms = DASK_client.gather(futures_s3)
    else:
        print(&#39;Using Local Computation&#39;)
        fnms = []
        for j, param_s3 in enumerate(tqdm(params_s3, desc=&#39;Extracting Key Points and Descriptors: &#39;)):
            fnms.append(extract_keypoints_descr_files(param_s3))
    return fnms</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.extract_keypoints_descr_files"><code class="name flex">
<span>def <span class="ident">extract_keypoints_descr_files</span></span>(<span>params)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts Key-Points and Descriptors (single image) for SIFT procedure.
©G.Shtengel 10/2021 gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters:</h2>
<p>params = fl, dmin, dmax, kwargs
fl : str
image filename (full path)
dmin : float
min data value for I8 conversion (open CV SIFT requires I8)
dmax : float
max data value for I8 conversion (open CV SIFT requires I8)
kwargs:
-------
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
thr_min : float
CDF threshold for determining the minimum data value
thr_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF
evaluation_box : list of 4 int
evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction.
kp_max_num : int
Max number of key-points to be matched.
Key-points in every frame are indexed (in descending order)
by the strength of the response. Only kp_max_num is kept for
further processing.
Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
SIFT_nfeatures : int
SIFT libary default is 0. The number of best features to retain.
The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
SIFT_nOctaveLayers : int
SIFT libary default
is 3. The number of layers in each octave.
3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
SIFT_contrastThreshold : double
SIFT libary default
is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
The larger the threshold, the less features are produced by the detector.
The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
When nOctaveLayers is set to default and if you want to use the value used in
D. Lowe paper (0.03), set this argument to 0.09.
SIFT_edgeThreshold : double
SIFT libary default
is 10. The threshold used to filter out edge-like features.
Note that the its meaning is different from the contrastThreshold,
i.e. the larger the edgeThreshold, the less features are filtered out
(more features are retained).
SIFT_sigma : double
SIFT library default is 1.6.
The sigma of the Gaussian applied to the input image at the octave #0.
If your image is captured with a weak camera with soft lenses, you might want to reduce the number.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>fnm </code></dt>
<dd>str
path to the file containing Key-Points and Descriptors</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_keypoints_descr_files(params):
    &#39;&#39;&#39;
    Extracts Key-Points and Descriptors (single image) for SIFT procedure.
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

    Parameters:
    -----------
    params = fl, dmin, dmax, kwargs
        fl : str
            image filename (full path)
        dmin : float
            min data value for I8 conversion (open CV SIFT requires I8)
        dmax : float
            max data value for I8 conversion (open CV SIFT requires I8)
        kwargs:
        -------
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        thr_min : float
            CDF threshold for determining the minimum data value
        thr_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction.
        kp_max_num : int
            Max number of key-points to be matched.
            Key-points in every frame are indexed (in descending order)
            by the strength of the response. Only kp_max_num is kept for
            further processing.
            Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
        SIFT_nfeatures : int
            SIFT libary default is 0. The number of best features to retain.
            The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
        SIFT_nOctaveLayers : int
            SIFT libary default  is 3. The number of layers in each octave.
            3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
        SIFT_contrastThreshold : double
            SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
            The larger the threshold, the less features are produced by the detector.
            The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
            When nOctaveLayers is set to default and if you want to use the value used in
            D. Lowe paper (0.03), set this argument to 0.09.
        SIFT_edgeThreshold : double
            SIFT libary default  is 10. The threshold used to filter out edge-like features.
            Note that the its meaning is different from the contrastThreshold,
            i.e. the larger the edgeThreshold, the less features are filtered out
            (more features are retained).
        SIFT_sigma : double
            SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
            If your image is captured with a weak camera with soft lenses, you might want to reduce the number.

    Returns:
        fnm : str
            path to the file containing Key-Points and Descriptors
    &#39;&#39;&#39;
    fl, dmin, dmax, kwargs = params
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    thr_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
    thr_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
    nbins = kwargs.get(&#34;nbins&#34;, 256)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, 10000)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])

    SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, 0)
    SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, 3)
    SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.04)
    SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, 10)
    SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, 1.6)

    #sift = cv2.xfeatures2d.SIFT_create(nfeatures=SIFT_nfeatures, nOctaveLayers=SIFT_nOctaveLayers, edgeThreshold=SIFT_edgeThreshold, contrastThreshold=SIFT_contrastThreshold, sigma=SIFT_sigma)
    sift = cv2.SIFT_create(nfeatures=SIFT_nfeatures, nOctaveLayers=SIFT_nOctaveLayers, edgeThreshold=SIFT_edgeThreshold, contrastThreshold=SIFT_contrastThreshold, sigma=SIFT_sigma)
    img, d1, d2 = FIBSEM_frame(fl, ftype=ftype).RawImageA_8bit_thresholds(thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = dmin, data_max = dmax, nbins=256)
    # extract keypoints and descriptors for both images

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = -1
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = -1

    kps, dess = sift.detectAndCompute(img[yi_eval:ya_eval, xi_eval:xa_eval], None)
    if kp_max_num != -1 and (len(kps) &gt; kp_max_num):
        kp_ind = np.argsort([-kp.response for kp in kps])[0:kp_max_num]
        kps = np.array(kps)[kp_ind]
        dess = np.array(dess)[kp_ind]
    if xi_eval &gt;0 or yi_eval&gt;0:   # add shifts to ke-pint coordinates to convert them to full image coordinated
        for kp in kps:
            kp.pt = kp.pt + np.array((xi_eval, yi_eval))
    #key_points = [KeyPoint(kp) for kp in kps]
    key_points = [kp_to_list(kp) for kp in kps]
    kpd = [key_points, dess]
    fnm = os.path.splitext(fl)[0] + &#39;_kpdes.bin&#39;
    pickle.dump(kpd, open(fnm, &#39;wb&#39;)) # converts array to binary and writes to output
    #pickle.dump(dess, open(fnm, &#39;wb&#39;)) # converts array to binary and writes to output
    return fnm</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.f"><code class="name flex">
<span>def <span class="ident">f</span></span>(<span>dfnum, dfden, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from an F distribution.</p>
<p>Samples are drawn from an F distribution with specified parameters,
<code>dfnum</code> (degrees of freedom in numerator) and <code>dfden</code> (degrees of
freedom in denominator), where both parameters must be greater than
zero.</p>
<p>The random variate of the F distribution (also known as the
Fisher distribution) is a continuous probability distribution
that arises in ANOVA tests, and is the ratio of two chi-square
variates.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.f</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dfnum</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Degrees of freedom in numerator, must be &gt; 0.</dd>
<dt><strong><code>dfden</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>float</code></dt>
<dd>Degrees of freedom in denominator, must be &gt; 0.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>dfnum</code> and <code>dfden</code> are both scalars.
Otherwise, <code>np.broadcast(dfnum, dfden).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized Fisher distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.f</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>random.Generator.f</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The F statistic is used to compare in-group variances to between-group
variances. Calculating the distribution depends on the sampling, and
so it is a function of the respective degrees of freedom in the
problem.
The variable <code>dfnum</code> is the number of samples minus one, the
between-groups degrees of freedom, while <code>dfden</code> is the within-groups
degrees of freedom, the sum of the number of samples in each group
minus the number of groups.</p>
<h2 id="references">References</h2>
<p>.. [1] Glantz, Stanton A. "Primer of Biostatistics.", McGraw-Hill,
Fifth Edition, 2002.
.. [2] Wikipedia, "F-distribution",
<a href="https://en.wikipedia.org/wiki/F-distribution">https://en.wikipedia.org/wiki/F-distribution</a></p>
<h2 id="examples">Examples</h2>
<p>An example from Glantz[1], pp 47-40:</p>
<p>Two groups, children of diabetics (25 people) and children from people
without diabetes (25 controls). Fasting blood glucose was measured,
case group had a mean value of 86.1, controls had a mean value of
82.2. Standard deviations were 2.09 and 2.49 respectively. Are these
data consistent with the null hypothesis that the parents diabetic
status does not affect their children's blood glucose levels?
Calculating the F statistic from the data gives a value of 36.01.</p>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dfnum = 1. # between group degrees of freedom
&gt;&gt;&gt; dfden = 48. # within groups degrees of freedom
&gt;&gt;&gt; s = np.random.f(dfnum, dfden, 1000)
</code></pre>
<p>The lower bound for the top 1% of the samples is :</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.sort(s)[-10]
7.61988120985 # random
</code></pre>
<p>So there is about a 1% chance that the F statistic will exceed 7.62,
the measured value is 36, so the null hypothesis is rejected at the 1%
level.</p></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.find_BW"><code class="name flex">
<span>def <span class="ident">find_BW</span></span>(<span>fr, FSC, SNRt)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_BW(fr, FSC, SNRt):
    npts = np.shape(FSC)[0]*0.75
    j = 15
    while (j&lt;npts-1) and FSC[j]&gt;SNRt:
        j = j+1
    BW = fr[j-1] + (fr[j]-fr[j-1])*(SNRt-FSC[j-1])/(FSC[j]-FSC[j-1])
    return BW</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.find_fit"><code class="name flex">
<span>def <span class="ident">find_fit</span></span>(<span>tr_matr_cum, fit_params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_fit(tr_matr_cum, fit_params):
    fit_method = fit_params[0]
    if fit_method == &#39;SG&#39;:  # perform Savitsky-Golay fitting with parameters
        ws, porder = fit_params[1:3]         # window size 701, polynomial order 3
        s00_fit = savgol_filter(tr_matr_cum[:, 0, 0].astype(double), ws, porder)
        s01_fit = savgol_filter(tr_matr_cum[:, 0, 1].astype(double), ws, porder)
        s10_fit = savgol_filter(tr_matr_cum[:, 1, 0].astype(double), ws, porder)
        s11_fit = savgol_filter(tr_matr_cum[:, 1, 1].astype(double), ws, porder)
    else:
        fr = np.arange(0, len(tr_matr_cum), dtype=np.double)
        if fit_method == &#39;PF&#39;:  # perform polynomial fitting with parameters
            porder = fit_params[1]         # polynomial order
            s00_coeffs = np.polyfit(fr, tr_matr_cum[:, 0, 0].astype(double), porder)
            s00_fit = np.polyval(s00_coeffs, fr)
            s01_coeffs = np.polyfit(fr, tr_matr_cum[:, 0, 1].astype(double), porder)
            s01_fit = np.polyval(s01_coeffs, fr)
            s10_coeffs = np.polyfit(fr, tr_matr_cum[:, 1, 0].astype(double), porder)
            s10_fit = np.polyval(s10_coeffs, fr)
            s11_coeffs = np.polyfit(fr, tr_matr_cum[:, 1, 1].astype(double), porder)
            s11_fit = np.polyval(s11_coeffs, fr)

        else:   # otherwise perform linear fit with origin point tied to 1 for Sxx and Syy and to 0 for Sxy and Syx
            slp00 = -1.0 * (np.sum(fr)-np.dot(tr_matr_cum[:, 0, 0],fr))/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
            s00_fit = 1.0 + slp00 * fr
            slp11 = -1.0 * (np.sum(fr)-np.dot(tr_matr_cum[:, 1, 1],fr))/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
            s11_fit = 1.0 + slp11 * fr
            slp01 = np.dot(tr_matr_cum[:, 0, 1],fr)/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
            s01_fit = slp01 * fr
            slp10 = np.dot(tr_matr_cum[:, 1, 0],fr)/np.dot(fr,fr) # find the slope of a linear fit with forced first scale=1
            s10_fit = slp10 * fr

    tr_matr_cum_new = tr_matr_cum.copy()
    tr_matr_cum_new[:, 0, 0] = tr_matr_cum[:, 0, 0].astype(double) + 1.0 - s00_fit
    tr_matr_cum_new[:, 0, 1] = tr_matr_cum[:, 0, 1].astype(double) - s01_fit
    tr_matr_cum_new[:, 1, 0] = tr_matr_cum[:, 1, 0].astype(double) - s10_fit
    tr_matr_cum_new[:, 1, 1] = tr_matr_cum[:, 1, 1].astype(double) + 1.0 - s11_fit
    s_fits = [s00_fit, s01_fit, s10_fit, s11_fit]
    return tr_matr_cum_new, s_fits</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.gamma"><code class="name flex">
<span>def <span class="ident">gamma</span></span>(<span>shape, scale=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Gamma distribution.</p>
<p>Samples are drawn from a Gamma distribution with specified parameters,
<code>shape</code> (sometimes designated "k") and <code>scale</code> (sometimes designated
"theta"), where both parameters are &gt; 0.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.gamma</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>The shape of the gamma distribution. Must be non-negative.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>The scale of the gamma distribution. Must be non-negative.
Default is equal to 1.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>shape</code> and <code>scale</code> are both scalars.
Otherwise, <code>np.broadcast(shape, scale).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized gamma distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.gamma</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>random.Generator.gamma</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the Gamma distribution is</p>
<p>[
]
where :math:<code>k</code> is the shape and :math:<code>\theta</code> the scale,
and :math:<code>\Gamma</code> is the Gamma function.</p>
<p>The Gamma distribution is often used to model the times to failure of
electronic components, and arises naturally in processes for which the
waiting times between Poisson distributed events are relevant.</p>
<h2 id="references">References</h2>
<p>.. [1] Weisstein, Eric W. "Gamma Distribution." From MathWorld&ndash;A
Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/GammaDistribution.html">http://mathworld.wolfram.com/GammaDistribution.html</a>
.. [2] Wikipedia, "Gamma distribution",
<a href="https://en.wikipedia.org/wiki/Gamma_distribution">https://en.wikipedia.org/wiki/Gamma_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; shape, scale = 2., 2.  # mean=4, std=2*sqrt(2)
&gt;&gt;&gt; s = np.random.gamma(shape, scale, 1000)
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; import scipy.special as sps  # doctest: +SKIP
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, 50, density=True)
&gt;&gt;&gt; y = bins**(shape-1)*(np.exp(-bins/scale) /  # doctest: +SKIP
...                      (sps.gamma(shape)*scale**shape))
&gt;&gt;&gt; plt.plot(bins, y, linewidth=2, color='r')  # doctest: +SKIP
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.generate_report_FOV_center_shift_xlsx"><code class="name flex">
<span>def <span class="ident">generate_report_FOV_center_shift_xlsx</span></span>(<span>Mill_Rate_Data_xlsx, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate Report Plot for FOV center shift from XLSX spreadsheet file. ©G.Shtengel 12/2022 gleb.shtengel@gmail.com</p>
<p>Parameters:
Mill_Rate_Data_xlsx : str
Path to the xlsx workbook containing the Working Distance (WD), Milling Y Voltage (MV), and FOV center shifts data.</p>
<p>kwargs:
Mill_Volt_Rate_um_per_V : float
Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.</p>
<p>Returns: trend_x, trend_y
Smoothed FOV shifts</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_report_FOV_center_shift_xlsx(Mill_Rate_Data_xlsx, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for FOV center shift from XLSX spreadsheet file. ©G.Shtengel 12/2022 gleb.shtengel@gmail.com

    Parameters:
    Mill_Rate_Data_xlsx : str
        Path to the xlsx workbook containing the Working Distance (WD), Milling Y Voltage (MV), and FOV center shifts data.

    kwargs:
    Mill_Volt_Rate_um_per_V : float
        Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.

    Returns: trend_x, trend_y
        Smoothed FOV shifts
    &#39;&#39;&#39;
    disp_res = kwargs.get(&#39;disp_res&#39;, False)
    if disp_res:
        print(&#39;Loading kwarg Data&#39;)
    saved_kwargs = read_kwargs_xlsx(Mill_Rate_Data_xlsx, &#39;kwargs Info&#39;, **kwargs)
    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)

    if disp_res:
        print(&#39;Loading FOV Center Location Data&#39;)
    try:
        int_results = pd.read_excel(Mill_Rate_Data_xlsx, sheet_name=&#39;FIBSEM Data&#39;)
    except:
        int_results = pd.read_excel(Mill_Rate_Data_xlsx, sheet_name=&#39;Milling Rate Data&#39;)
    fr = int_results[&#39;Frame&#39;]
    center_x = int_results[&#39;FOV X Center (Pix)&#39;]
    center_y = int_results[&#39;FOV Y Center (Pix)&#39;]
    apert = np.min((51, len(fr)-1))
    trend_x = savgol_filter(center_x*1.0, apert, 1) - center_x[0]
    trend_y = savgol_filter(center_y*1.0, apert, 1) - center_y[0]

    if disp_res:
        print(&#39;Generating Plot&#39;)
    fs = 12

    fig, axs = subplots(2,1, figsize = (6,7), sharex=True)
    fig.subplots_adjust(left=0.12, bottom=0.06, right=0.99, top=0.96, wspace=0.05, hspace=0.05)
    axs[0].plot(fr, center_x, label=&#39;FOV X center, Data&#39;, color=&#39;red&#39;)
    axs[0].plot(fr, center_y, label=&#39;FOV Y center, Data&#39;, color=&#39;blue&#39;)
    axs[0].grid(True)
    axs[0].set_ylabel(&#39;FOV Center (Pix)&#39;)
    #axs[0].set_xlim(xi, xa)
    axs[0].legend(fontsize=12)

    axs[1].plot(fr, trend_x, label=&#39;FOV X center shift, smoothed&#39;, color=&#39;red&#39;)
    axs[1].plot(fr, trend_y, label=&#39;FOV Y center shift, smoothed&#39;, color=&#39;blue&#39;)
    axs[1].grid(True)
    axs[1].set_ylabel(&#39;FOV Center Shift (Pix)&#39;)
    axs[1].legend(fontsize=12)
    axs[1].set_xlabel(&#39;Frame&#39;)
    ldm = 70
    data_dir_short = data_dir if len(data_dir)&lt;ldm else &#39;... &#39;+ data_dir[-ldm:]
    try:
        axs[0].text(-0.15, 1.05, Sample_ID + &#39;    &#39; +  data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    except:
        axs[0].text(-0.15, 1.05, data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    fig.savefig(os.path.join(data_dir, Mill_Rate_Data_xlsx.replace(&#39;.xlsx&#39;,&#39;_FOV_XYcenter.png&#39;)), dpi=300)
    return</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.generate_report_data_minmax_xlsx"><code class="name flex">
<span>def <span class="ident">generate_report_data_minmax_xlsx</span></span>(<span>minmax_xlsx_file, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate Report Plot for data Min-Max from XLSX spreadsheet file. ©G.Shtengel 10/2022 gleb.shtengel@gmail.com</p>
<p>Parameters:
minmax_xlsx_file : str
Path to the xlsx workbook containing Min-Max data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_report_data_minmax_xlsx(minmax_xlsx_file, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for data Min-Max from XLSX spreadsheet file. ©G.Shtengel 10/2022 gleb.shtengel@gmail.com

    Parameters:
    minmax_xlsx_file : str
        Path to the xlsx workbook containing Min-Max data
    &#39;&#39;&#39;
    disp_res = kwargs.get(&#39;disp_res&#39;, False)
    if disp_res:
        print(&#39;Loading kwarg Data&#39;)
    saved_kwargs = read_kwargs_xlsx(minmax_xlsx_file, &#39;kwargs Info&#39;, **kwargs)
    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = saved_kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    threshold_min = saved_kwargs.get(&#34;threshold_min&#34;, 0.0)
    threshold_max = saved_kwargs.get(&#34;threshold_min&#34;, 0.0)
    fit_params_saved = saved_kwargs.get(&#34;fit_params&#34;, [&#39;SG&#39;, 101, 3])
    fit_params = kwargs.get(&#34;fit_params&#34;, fit_params_saved)
    preserve_scales =  saved_kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below

    if disp_res:
        print(&#39;Loading MinMax Data&#39;)
    try:
        int_results = pd.read_excel(minmax_xlsx_file, sheet_name=&#39;FIBSEM Data&#39;)
    except:
        int_results = pd.read_excel(minmax_xlsx_file, sheet_name=&#39;MinMax Data&#39;)
    frames = int_results[&#39;Frame&#39;]
    frame_min = int_results[&#39;Min&#39;]
    frame_max = int_results[&#39;Max&#39;]
    data_min_glob  = np.min(frame_min)
    data_max_glob  = np.max(frame_max)
    &#39;&#39;&#39;
    sliding_min = int_results[&#39;Sliding Min&#39;]
    sliding_max = int_results[&#39;Sliding Max&#39;]
    &#39;&#39;&#39;
    sliding_min = savgol_filter(frame_min.astype(double), min([fit_params[1], fit_params[1]]), fit_params[2])
    sliding_max = savgol_filter(frame_max.astype(double), min([fit_params[1], fit_params[1]]), fit_params[2])

    if disp_res:
        print(&#39;Generating Plot&#39;)
    fs = 12
    fig0, ax0 = subplots(1,1,figsize=(6,4))
    fig0.subplots_adjust(left=0.14, bottom=0.11, right=0.99, top=0.94)
    ax0.plot(frame_min, &#39;b&#39;, linewidth=1, label=&#39;Frame Minima&#39;)
    ax0.plot(sliding_min, &#39;b&#39;, linewidth=2, linestyle = &#39;dotted&#39;, label=&#39;Sliding Minima&#39;)
    ax0.plot(frame_max, &#39;r&#39;, linewidth=1, label=&#39;Frame Maxima&#39;)
    ax0.plot(sliding_max, &#39;r&#39;, linewidth=2, linestyle = &#39;dotted&#39;, label=&#39;Sliding Maxima&#39;)
    ax0.legend()
    ax0.grid(True)
    ax0.set_xlabel(&#39;Frame&#39;)
    ax0.set_ylabel(&#39;Minima and Maxima Values&#39;)
    dxn = (data_max_glob - data_min_glob)*0.1
    ax0.set_ylim((data_min_glob - dxn, data_max_glob+dxn))
    # if needed, display the data in a narrower range
    #ax0.set_ylim((-4500, -1500))
    xminmax = [0, len(frame_min)]
    y_min = [data_min_glob, data_min_glob]
    y_max = [data_max_glob, data_max_glob]
    ax0.plot(xminmax, y_min, &#39;b&#39;, linestyle = &#39;--&#39;)
    ax0.plot(xminmax, y_max, &#39;r&#39;, linestyle = &#39;--&#39;)
    ax0.text(len(frame_min)/20.0, data_min_glob-dxn/1.75, &#39;data_min_glob={:.1f}&#39;.format(data_min_glob), fontsize = fs-2, c=&#39;b&#39;)
    ax0.text(len(frame_min)/20.0, data_max_glob+dxn/2.25, &#39;data_max_glob={:.1f}&#39;.format(data_max_glob), fontsize = fs-2, c=&#39;r&#39;)
    ax0.text(len(frame_min)/20.0, data_min_glob+dxn*4.5, &#39;threshold_min={:.1e}&#39;.format(threshold_min), fontsize = fs-2, c=&#39;b&#39;)
    ax0.text(len(frame_min)/20.0, data_min_glob+dxn*5.5, &#39;threshold_max={:.1e}&#39;.format(threshold_max), fontsize = fs-2, c=&#39;r&#39;)
    ldm = 70
    data_dir_short = data_dir if len(data_dir)&lt;ldm else &#39;... &#39;+ data_dir[-ldm:]

    try:
        ax0.text(-0.15, 1.05, Sample_ID + &#39;    &#39; +  data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    except:
        ax0.text(-0.15, 1.05, data_dir_short, fontsize = fs-2, transform=ax0.transAxes)
    &#39;&#39;&#39;
    try:
        fig0.suptitle(Sample_ID + &#39;    &#39; +  data_dir_short, fontsize = fs-2)
    except:
        fig0.suptitle(data_dir_short, fontsize = fs-2)
    &#39;&#39;&#39;
    fig0.savefig(os.path.join(data_dir, minmax_xlsx_file.replace(&#39;.xlsx&#39;,&#39;_Min_Max.png&#39;)), dpi=300)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.generate_report_from_xls_registration_summary"><code class="name flex">
<span>def <span class="ident">generate_report_from_xls_registration_summary</span></span>(<span>file_xlsx, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate Report Plot for FIB-SEM data set registration from xlxs workbook file. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com
XLS file should have pages (sheets):
- 'Registration Quality Statistics' - containing columns with the the evaluation box data and registration quality metrics data:
'Frame', 'xi_eval', 'xa_eval', 'yi_eval', 'ya_eval', 'Npts', 'Mean Abs Error', 'Image NSAD', 'Image NCC', 'Image MI'
- 'Stack Info' - containing the fields:
'Stack Filename' and 'data_dir'
- 'SIFT kwargs' (optional) - containg the kwargs with SIFT registration parameters.</p>
<p>Parameters:
xlsx_fname : str
full path to the XLSX workbook file</p>
<h2 id="kwargs">Kwargs</h2>
<p>sample_frame_files : list
List of paths to sample frame images
png_file : str
filename to save the results. Default is file_xlsx with extension '.xlsx' replaced with '.png'
invert_data : bolean
If True, the representative data frames will use inverse LUT.
dump_filename : str
Filename of a binary dump of the FIBSEM_dataset object.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_report_from_xls_registration_summary(file_xlsx, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for FIB-SEM data set registration from xlxs workbook file. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com
    XLS file should have pages (sheets):
        - &#39;Registration Quality Statistics&#39; - containing columns with the the evaluation box data and registration quality metrics data:
            &#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;, &#39;Npts&#39;, &#39;Mean Abs Error&#39;, &#39;Image NSAD&#39;, &#39;Image NCC&#39;, &#39;Image MI&#39;
        - &#39;Stack Info&#39; - containing the fields:
            &#39;Stack Filename&#39; and &#39;data_dir&#39;
        - &#39;SIFT kwargs&#39; (optional) - containg the kwargs with SIFT registration parameters.

    Parameters:
    xlsx_fname : str
        full path to the XLSX workbook file

    kwargs
    ---------
    sample_frame_files : list
        List of paths to sample frame images
    png_file : str
        filename to save the results. Default is file_xlsx with extension &#39;.xlsx&#39; replaced with &#39;.png&#39;
    invert_data : bolean
        If True, the representative data frames will use inverse LUT.
    dump_filename : str
        Filename of a binary dump of the FIBSEM_dataset object.

    &#39;&#39;&#39;
    xlsx_name = os.path.basename(os.path.abspath(file_xlsx))
    base_dir = os.path.dirname(os.path.abspath(file_xlsx))
    sample_frame_mask = xlsx_name.replace(&#39;_RegistrationQuality.xlsx&#39;, &#39;_sample_image_frame*.*&#39;)
    unsorted_sample_frame_files = glob.glob(os.path.join(base_dir, sample_frame_mask))
    try:
        unsorter_frames = [int(x.split(&#39;frame&#39;)[1].split(&#39;.png&#39;)[0]) for x in unsorted_sample_frame_files]
        sorted_inds = argsort(unsorter_frames)
        existing_sample_frame_files = [unsorted_sample_frame_files[i] for i in sorted_inds]
    except:
        existing_sample_frame_files = unsorted_sample_frame_files
    sample_frame_files = kwargs.get(&#39;sample_frame_files&#39;, existing_sample_frame_files)
    png_file_default = file_xlsx.replace(&#39;.xlsx&#39;,&#39;.png&#39;)
    png_file = kwargs.get(&#34;png_file&#34;, png_file_default)
    dump_filename = kwargs.get(&#34;dump_filename&#34;, &#39;&#39;)

    Regisration_data = pd.read_excel(file_xlsx, sheet_name=&#39;Registration Quality Statistics&#39;)
    # columns=[&#39;Frame&#39;, &#39;xi_eval&#39;, &#39;xa_eval&#39;, &#39;yi_eval&#39;, &#39;ya_eval&#39;, &#39;Npts&#39;, &#39;Mean Abs Error&#39;, &#39;Image NSAD&#39;, &#39;Image NCC&#39;, &#39;Image MI&#39;]
    frames = Regisration_data[&#39;Frame&#39;]
    xi_evals = Regisration_data[&#39;xi_eval&#39;]
    xa_evals = Regisration_data[&#39;xa_eval&#39;]
    yi_evals = Regisration_data[&#39;yi_eval&#39;]
    ya_evals = Regisration_data[&#39;ya_eval&#39;]

    &#39;&#39;&#39;
    image_nsad = Regisration_data[&#39;NSAD&#39;]
    image_ncc = Regisration_data[&#39;NCC&#39;]
    image_nmi = Regisration_data[&#39;NMI&#39;]
    nsads = [np.mean(image_nsad), np.median(image_nsad), np.std(image_nsad)]
    nccs = [np.mean(image_ncc), np.median(image_ncc), np.std(image_ncc)]
    nmis = [np.mean(image_nmi), np.median(image_nmi), np.std(image_nmi)]
    &#39;&#39;&#39;
    eval_metrics = Regisration_data.columns[5:]
    num_metrics = len(eval_metrics)

    num_frames = len(frames)

    stack_info_dict = read_kwargs_xlsx(file_xlsx, &#39;Stack Info&#39;, **kwargs)
    if &#39;dump_filename&#39; in stack_info_dict.keys():
        dump_filename = kwargs.get(&#34;dump_filename&#34;, stack_info_dict[&#39;dump_filename&#39;])
    else:
        dump_filename = kwargs.get(&#34;dump_filename&#34;, &#39;&#39;)
    try:
        if np.isnan(dump_filename):
            dump_filename = &#39;&#39;
    except:
        pass
    stack_info_dict[&#39;dump_filename&#39;] = dump_filename

    try:
        invert_data =  kwargs.get(&#34;invert_data&#34;, stack_info_dict[&#39;invert_data&#39;])
    except:
        invert_data =  kwargs.get(&#34;invert_data&#34;, False)

    default_stack_name = file_xlsx.replace(&#39;_RegistrationQuality.xlsx&#39;,&#39;.mrc&#39;)
    stack_filename = os.path.normpath(stack_info_dict.get(&#39;Stack Filename&#39;, default_stack_name))
    data_dir = stack_info_dict.get(&#39;data_dir&#39;, &#39;&#39;)
    ftype = stack_info_dict.get(&#34;ftype&#34;, 0)


    heights = [0.8]*3 + [1.5]*num_metrics
    gs_kw = dict(height_ratios=heights)
    fig, axs = subplots((num_metrics+3), 1, figsize=(6, 2*(num_metrics+2)), gridspec_kw=gs_kw)
    fig.subplots_adjust(left=0.14, bottom=0.04, right=0.99, top=0.98, wspace=0.18, hspace=0.04)
    for ax in axs[0:3]:
        ax.axis(&#39;off&#39;)

    fs=12
    lwl=1

    if len(sample_frame_files)&gt;0:
        sample_frame_images_available = True
        for jf, ax in enumerate(axs[0:3]):
            try:
                ax.imshow(mpimg.imread(sample_frame_files[jf]))
                ax.axis(False)
            except:
                pass
    else:
        sample_frame_images_available = False
        sample_data_available = True
        if stack_exists:
            print(&#39;Will use sample images from the registered stack&#39;)
            use_raw_data = False
            if Path(stack_filename).suffix == &#39;.mrc&#39;:
                mrc_obj = mrcfile.mmap(stack_filename, mode=&#39;r&#39;)
                header = mrc_obj.header
                mrc_mode = header.mode
                &#39;&#39;&#39;
                mode 0 -&gt; uint8
                mode 1 -&gt; int16
                mode 2 -&gt; float32
                mode 4 -&gt; complex64
                mode 6 -&gt; uint16
                &#39;&#39;&#39;
                if mrc_mode==0:
                    dt_mrc=uint8
                if mrc_mode==1:
                    dt_mrc=int16
                if mrc_mode==2:
                    dt_mrc=float32
                if mrc_mode==4:
                    dt_mrc=complex64
                if mrc_mode==6:
                    dt_mrc=uint16
        else:
            print(&#39;Will use sample images from the raw data&#39;)
            if os.path.exists(dump_filename):
                print(&#39;Trying to recall the data from &#39;, dump_filename)
            try:
                print(&#39;Looking for the raw data in the directory&#39;, data_dir)
                if ftype == 0:
                    fls = sorted(glob.glob(os.path.join(data_dir,&#39;*.dat&#39;)))
                    if len(fls) &lt; 1:
                        fls = sorted(glob.glob(os.path.join(data_dir,&#39;*/*.dat&#39;)))
                if ftype == 1:
                    fls = sorted(glob.glob(os.path.join(data_dir,&#39;*.tif&#39;)))
                    if len(fls) &lt; 1:
                        fls = sorted(glob.glob(os.path.join(data_dir,&#39;*/*.tif&#39;)))
                num_frames = len(fls)
                stack_info_dict[&#39;disp_res&#39;]=False
                raw_dataset = FIBSEM_dataset(fls, recall_parameters=os.path.exists(dump_filename), **stack_info_dict)
                XResolution = raw_dataset.XResolution
                YResolution = raw_dataset.YResolution
                if pad_edges and perfrom_transformation:
                    #shape = [test_frame.YResolution, test_frame.XResolution]
                    shape = [YResolution, XResolution]
                    xmn, xmx, ymn, ymx = determine_pad_offsets(shape, raw_dataset.tr_matr_cum_residual)
                    padx = int(xmx - xmn)
                    pady = int(ymx - ymn)
                    xi = int(np.max([xmx, 0]))
                    yi = int(np.max([ymx, 0]))
                    # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
                    # so that the transformed images are not clipped.
                    # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
                    # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
                    # those are calculated below base on the amount of padding calculated above
                    shift_matrix = np.array([[1.0, 0.0, xi],
                                             [0.0, 1.0, yi],
                                             [0.0, 0.0, 1.0]])
                    inv_shift_matrix = np.linalg.inv(shift_matrix)
                else:
                    padx = 0
                    pady = 0
                    xi = 0
                    yi = 0
                    shift_matrix = np.eye(3,3)
                    inv_shift_matrix = np.eye(3,3)
                xsz = XResolution + padx
                xa = xi + XResolution
                ysz = YResolution + pady
                ya = yi + YResolution
                use_raw_data = True
            except:
                sample_data_available = False
                use_raw_data = False
        if sample_data_available:
            print(&#39;Sample data is available&#39;)
        else:
            print(&#39;Sample data is NOT available&#39;)

        if num_frames//10*9 &gt; 0:
            ev_ind2 = num_frames//10*9
        else:
            ev_ind2 = num_frames-1
        eval_inds = [num_frames//10,  num_frames//2, ev_ind2]
        #print(eval_inds)

        for j, eval_ind in enumerate(eval_inds):
            ax = axs[j]
            if sample_data_available:
                if stack_exists:
                    if Path(stack_filename).suffix == &#39;.mrc&#39;:
                        frame_img = (mrc_obj.data[frames[eval_ind], :, :].astype(dt_mrc)).astype(float)
                    if Path(stack_filename).suffix == &#39;.tif&#39;:
                        frame_img = tiff.imread(stack_filename, key=eval_ind)
                else:
                    dtp=float
                    chunk_frames = np.arange(eval_ind, min(eval_ind+zbin_factor, len(fls)-2))
                    frame_filenames = np.array(raw_dataset.fls)[chunk_frames]
                    tr_matrices = np.array(raw_dataset.tr_matr_cum_residual)[chunk_frames]
                    frame_img = transform_chunk_of_frames(frame_filenames, xsz, ysz, ftype,
                            flatten_image, image_correction_file,
                            perfrom_transformation, tr_matrices, shift_matrix, inv_shift_matrix,
                            xi, xa, yi, ya,
                            ImgB_fraction=0.0,
                            invert_data=False,
                            int_order=1,
                            flipY = raw_dataset.flipY)
                #print(eval_ind, np.shape(frame_img), yi_evals[eval_ind], ya_evals[eval_ind], xi_evals[eval_ind], xa_evals[eval_ind])
                if use_raw_data:
                    eval_ind = eval_ind//zbin_factor
                dmin, dmax = get_min_max_thresholds(frame_img[yi_evals[eval_ind]:ya_evals[eval_ind], xi_evals[eval_ind]:xa_evals[eval_ind]])
                if invert_data:
                    ax.imshow(frame_img, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
                else:
                    ax.imshow(frame_img, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)

                ax.text(0.03, 1.01, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(frames[eval_ind], image_nsad[eval_ind], image_ncc[eval_ind], image_nmi[eval_ind]), color=&#39;red&#39;, transform=ax.transAxes)
                rect_patch = patches.Rectangle((xi_evals[eval_ind], yi_evals[eval_ind]),abs(xa_evals[eval_ind]-xi_evals[eval_ind])-2,abs(ya_evals[eval_ind]-yi_evals[eval_ind])-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
                ax.add_patch(rect_patch)
            ax.axis(&#39;off&#39;)

        if stack_exists:
            if Path(stack_filename).suffix == &#39;.mrc&#39;:
                mrc_obj.close()

    axes_names = {&#39;NSAD&#39; : &#39;Norm. Sum of Abs. Diff&#39;,
                 &#39;NCC&#39; : &#39;Norm. Cross-Corr.&#39;,
                  &#39;NMI&#39; : &#39;Norm. Mutual Inf.&#39;,
                 &#39;FSC&#39; : &#39;FSC BW (inv pix)&#39;}
    colors = [&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;magenta&#39;, &#39;lime&#39;]

    for j, metric in enumerate(eval_metrics):
        metric_data = Regisration_data[metric]
        nmis = []
        axs[j+3].plot(frames, Regisration_data[metric], linewidth=lwl, color = colors[j])
        try:
            axs[j+3].set_ylabel(axes_names[metric], fontsize=fs-2)
        except:
            axs[j+3].set_ylabel(metric, fontsize=fs-2)
        axs[j+3].text(0.02, 0.04, (metric+&#39; mean = {:.3f}   &#39; + metric + &#39; median = {:.3f}  &#39; + metric + &#39; STD = {:.3f}&#39;).format(np.mean(metric_data), np.median(metric_data), np.std(metric_data)), transform=axs[j+3].transAxes, fontsize = fs-4)

    axs[-1].set_xlabel(&#39;Binned Frame #&#39;)
    for ax in axs[2:]:
        ax.grid(True)

    axs[0].text(-0.15, 2.7,stack_filename, transform=axs[3].transAxes)
    fig.savefig(png_file, dpi=300)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.generate_report_mill_rate_xlsx"><code class="name flex">
<span>def <span class="ident">generate_report_mill_rate_xlsx</span></span>(<span>Mill_Rate_Data_xlsx, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate Report Plot for mill rate evaluation from XLSX spreadsheet file. ©G.Shtengel 12/2022 gleb.shtengel@gmail.com</p>
<p>Parameters:
Mill_Rate_Data_xlsx : str
Path to the xlsx workbook containing the Working Distance (WD), Milling Y Voltage (MV), and FOV center shifts data.</p>
<p>kwargs:
Mill_Volt_Rate_um_per_V : float
Milling Voltage to Z conversion (µm/V). Default is 31.235258870176065.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_report_mill_rate_xlsx(Mill_Rate_Data_xlsx, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for mill rate evaluation from XLSX spreadsheet file. ©G.Shtengel 12/2022 gleb.shtengel@gmail.com

    Parameters:
    Mill_Rate_Data_xlsx : str
        Path to the xlsx workbook containing the Working Distance (WD), Milling Y Voltage (MV), and FOV center shifts data.

    kwargs:
    Mill_Volt_Rate_um_per_V : float
        Milling Voltage to Z conversion (µm/V). Default is 31.235258870176065.

    &#39;&#39;&#39;
    disp_res = kwargs.get(&#39;disp_res&#39;, False)
    if disp_res:
        print(&#39;Loading kwarg Data&#39;)
    saved_kwargs = read_kwargs_xlsx(Mill_Rate_Data_xlsx, &#39;kwargs Info&#39;, **kwargs)
    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    Saved_Mill_Volt_Rate_um_per_V = saved_kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, 31.235258870176065)
    Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, Saved_Mill_Volt_Rate_um_per_V)

    if disp_res:
        print(&#39;Loading Working Distance and Milling Y Voltage Data&#39;)
    try:
        int_results = pd.read_excel(Mill_Rate_Data_xlsx, sheet_name=&#39;FIBSEM Data&#39;)
    except:
        int_results = pd.read_excel(Mill_Rate_Data_xlsx, sheet_name=&#39;Milling Rate Data&#39;)
    fr = int_results[&#39;Frame&#39;]
    WD = int_results[&#39;Working Distance (mm)&#39;]
    MillingYVoltage = int_results[&#39;Milling Y Voltage (V)&#39;]

    if disp_res:
        print(&#39;Generating Plot&#39;)
    fs = 12
    Mill_Volt_Rate_um_per_V = 31.235258870176065

    fig, axs = subplots(2,1, figsize = (6,7), sharex=True)
    fig.subplots_adjust(left=0.12, bottom=0.06, right=0.99, top=0.96, wspace=0.05, hspace=0.05)
    axs[0].plot(fr, WD, label=&#39;WD, Exp. Data&#39;, color=&#39;blue&#39;)
    axs[0].grid(True)
    axs[0].set_ylabel(&#39;Working Distance (mm)&#39;)
    #axs[0].set_xlim(xi, xa)
    WD_fit_coef = np.polyfit(fr, WD, 1)
    WD_fit=np.polyval(WD_fit_coef, fr)
    axs[0].plot(fr, WD_fit, label=&#39;Fit, slope = {:.2f} nm/line&#39;.format(WD_fit_coef[0]*1.0e6), color=&#39;red&#39;)
    axs[0].legend(fontsize=12)

    axs[1].plot(fr, MillingYVoltage, label=&#39;Mill. Y Volt. Exp. Data&#39;, color=&#39;green&#39;)
    axs[1].grid(True)
    axs[1].set_ylabel(&#39;Milling Y Voltage (V)&#39;)
    MV_fit_coef = np.polyfit(fr, MillingYVoltage, 1)
    MV_fit=np.polyval(MV_fit_coef, fr)
    axs[1].plot(fr, MV_fit, label=&#39;Fit, slope = {:.3f} nm/line&#39;.format(MV_fit_coef[0]*Mill_Volt_Rate_um_per_V*-1.0e3), color=&#39;orange&#39;)
    axs[1].legend(fontsize=12)
    axs[1].text(0.02, 0.05, &#39;Milling Voltage to Z conversion: {:.4f} µm/V&#39;.format(Mill_Volt_Rate_um_per_V), transform=axs[1].transAxes, fontsize=12)
    axs[1].set_xlabel(&#39;Frame&#39;)
    ldm = 70
    data_dir_short = data_dir if len(data_dir)&lt;ldm else &#39;... &#39;+ data_dir[-ldm:]
    try:
        axs[0].text(-0.15, 1.05, Sample_ID + &#39;    &#39; +  data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    except:
        axs[0].text(-0.15, 1.05, data_dir_short, fontsize = fs-2, transform=axs[0].transAxes)
    fig.savefig(os.path.join(data_dir, Mill_Rate_Data_xlsx.replace(&#39;.xlsx&#39;,&#39;_Mill_Rate.png&#39;)), dpi=300)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.generate_report_transf_matrix_details"><code class="name flex">
<span>def <span class="ident">generate_report_transf_matrix_details</span></span>(<span>transf_matrix_bin_file, *kwarrgs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate Report Plot for Transformation Matrix from binary dump file. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com
The binary dump file should contain list with these parameters (in this order):
[saved_kwargs, npts, error_abs_mean, transformation_matrix,
s00_cum_orig, s11_cum_orig, s00_fit, s11_fit, tr_matr_cum, s01_cum_orig, s10_cum_orig, s01_fit, s10_fit,
Xshift_cum_orig, Yshift_cum_orig, Xshift_cum, Yshift_cum, Xfit, Yfit]</p>
<p>Parameters:
transf_matrix_bin_file : str
Path to the binary dump file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_report_transf_matrix_details(transf_matrix_bin_file, *kwarrgs):
    &#39;&#39;&#39;
    Generate Report Plot for Transformation Matrix from binary dump file. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com
    The binary dump file should contain list with these parameters (in this order):
        [saved_kwargs, npts, error_abs_mean, transformation_matrix,
        s00_cum_orig, s11_cum_orig, s00_fit, s11_fit, tr_matr_cum, s01_cum_orig, s10_cum_orig, s01_fit, s10_fit,
        Xshift_cum_orig, Yshift_cum_orig, Xshift_cum, Yshift_cum, Xfit, Yfit]

    Parameters:
    transf_matrix_bin_file : str
        Path to the binary dump file

    &#39;&#39;&#39;
    with open(transf_matrix_bin_file, &#34;rb&#34;) as f:
        [saved_kwargs, npts, error_abs_mean,
         transformation_matrix, s00_cum_orig, s11_cum_orig, s00_fit, s11_fit,
         tr_matr_cum, s01_cum_orig, s10_cum_orig, s01_fit, s10_fit,
         Xshift_cum_orig, Yshift_cum_orig, Xshift_cum, Yshift_cum, Xfit, Yfit] = pickle.load(f)

    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = saved_kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    TransformType = saved_kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    SIFT_nfeatures = saved_kwargs.get(&#34;SIFT_nfeatures&#34;, 0)
    SIFT_nOctaveLayers = saved_kwargs.get(&#34;SIFT_nOctaveLayers&#34;, 3)
    SIFT_contrastThreshold = saved_kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.025)
    SIFT_edgeThreshold = saved_kwargs.get(&#34;SIFT_edgeThreshold&#34;, 10)
    SIFT_sigma = saved_kwargs.get(&#34;SIFT_sigma&#34;, 1.6)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = saved_kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = saved_kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = saved_kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = saved_kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = saved_kwargs.get(&#34;max_iter&#34;, 1000)
    BFMatcher = saved_kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = saved_kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    kp_max_num = saved_kwargs.get(&#34;kp_max_num&#34;, -1)
    save_res_png  = saved_kwargs.get(&#34;save_res_png&#34;, True)

    preserve_scales =  saved_kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
    fit_params =  saved_kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    subtract_linear_fit =  saved_kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # The linear slopes along X- and Y- directions (respectively) will be subtracted from the cumulative shifts.
    subtract_FOVtrend_from_fit = saved_kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])
    #print(&#34;subtract_linear_fit:&#34;, subtract_linear_fit)
    pad_edges =  saved_kwargs.get(&#34;pad_edges&#34;, True)

    fs = 14
    lwf = 2
    lwl = 1
    fig5, axs5 = subplots(4,3, figsize=(18, 16), sharex=True)
    fig5.subplots_adjust(left=0.07, bottom=0.03, right=0.99, top=0.95)
    # display the info
    axs5[0,0].axis(False)
    axs5[0,0].text(-0.1, 0.9, Sample_ID, fontsize = fs + 4)
    #axs5[0,0].text(-0.1, 0.73, &#39;Global Data Range:  Min={:.2f}, Max={:.2f}&#39;.format(data_min_glob, data_max_glob), transform=axs5[0,0].transAxes, fontsize = fs)

    if TransformType == RegularizedAffineTransform:
        tstr = [&#39;{:d}&#39;.format(x) for x in targ_vector]
        otext = &#39;Reg.Aff.Transf., λ= {:.1e}, t=[&#39;.format(l2_matrix[0,0]) + &#39; &#39;.join(tstr) + &#39;], w/&#39; + solver
    else:
        otext = TransformType.__name__ + &#39; with &#39; + solver + &#39; solver&#39;
    axs5[0,0].text(-0.1, 0.80, otext, transform=axs5[0,0].transAxes, fontsize = fs)

    SIFT1text = &#39;SIFT: nFeatures = {:d}, nOctaveLayers = {:d}, &#39;.format(SIFT_nfeatures, SIFT_nOctaveLayers)
    axs5[0,0].text(-0.1, 0.65, SIFT1text, transform=axs5[0,0].transAxes, fontsize = fs)

    SIFT2text = &#39;SIFT: contrThr = {:.3f}, edgeThr = {:.2f}, σ= {:.2f}&#39;.format(SIFT_contrastThreshold, SIFT_edgeThreshold, SIFT_sigma)
    axs5[0,0].text(-0.1, 0.50, SIFT2text, transform=axs5[0,0].transAxes, fontsize = fs)

    sbtrfit = (&#39;ON, &#39; if  subtract_linear_fit[0] else &#39;OFF, &#39;) + (&#39;ON&#39; if  subtract_linear_fit[1] else &#39;OFF&#39;)
    axs5[0,0].text(-0.1, 0.35, &#39;drmax={:.1f}, Max # of KeyPts={:d}, Max # of Iter.={:d}&#39;.format(drmax, kp_max_num, max_iter), transform=axs5[0,0].transAxes, fontsize = fs)
    padedges = &#39;ON&#39; if pad_edges else &#39;OFF&#39;
    if preserve_scales:
        fit_method = fit_params[0]
        if fit_method == &#39;LF&#39;:
            fit_str = &#39;, Meth: Linear Fit&#39;
            fm_string = &#39;linear&#39;
        else:
            if fit_method == &#39;SG&#39;:
                fit_str = &#39;, Meth: Sav.-Gol., &#39; + str(fit_params[1:])
                fm_string = &#39;Sav.-Gol.&#39;
            else:
                fit_str = &#39;, Meth: Pol.Fit, ord.={:d}&#39;.format(fit_params[1])
                fm_string = &#39;polyn.&#39;
        preserve_scales_string = &#39;Pres. Scls: ON&#39; + fit_str
    else:
        preserve_scales_string = &#39;Preserve Scales: OFF&#39;
    axs5[0,0].text(-0.1, 0.20, preserve_scales_string, transform=axs5[0,0].transAxes, fontsize = fs)
    axs5[0,0].text(-0.1, 0.05, &#39;Subtract Shift Fit: &#39; + sbtrfit + &#39;, Pad Edges: &#39; + padedges, transform=axs5[0,0].transAxes, fontsize = fs)
    # plot number of keypoints
    axs5[0, 1].plot(npts, &#39;g&#39;, linewidth = lwl, label = &#39;# of key-points per frame&#39;)
    axs5[0, 1].set_title(&#39;# of key-points per frame&#39;)
    axs5[0, 1].text(0.03, 0.2, &#39;Mean # of kpts= {:.0f}   Median # of kpts= {:.0f}&#39;.format(np.mean(npts), np.median(npts)), transform=axs5[0, 1].transAxes, fontsize = fs-1)
    # plot Standard deviations
    axs5[0, 2].plot(error_abs_mean, &#39;magenta&#39;, linewidth = lwl, label = &#39;Mean Abs Error over keyponts per frame&#39;)
    axs5[0, 2].set_title(&#39;Mean Abs Error keyponts per frame&#39;)
    axs5[0, 2].text(0.03, 0.2, &#39;Mean Abs Error= {:.3f}   Median Abs Error= {:.3f}&#39;.format(np.mean(error_abs_mean), np.median(error_abs_mean)), transform=axs5[0, 2].transAxes, fontsize = fs-1)

    # plot scales terms
    axs5[1, 0].plot(transformation_matrix[:, 0, 0], &#39;r&#39;, linewidth = lwl, label = &#39;Sxx frame-to-frame&#39;)
    axs5[1, 0].plot(transformation_matrix[:, 1, 1], &#39;b&#39;, linewidth = lwl, label = &#39;Syy frame-to-frame&#39;)
    axs5[1, 0].set_title(&#39;Frame-to-Frame Scale Change&#39;, fontsize = fs)
    axs5[2, 0].plot(s00_cum_orig, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Sxx cum.&#39;)
    axs5[2, 0].plot(s11_cum_orig, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Syy cum.&#39;)
    if preserve_scales:
        axs5[2, 0].plot(s00_fit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Sxx cum. - &#39;+fm_string+&#39; fit&#39;)
        axs5[2, 0].plot(s11_fit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Syy cum. - &#39;+fm_string+&#39; fit&#39;)
    axs5[2, 0].set_title(&#39;Cumulative Scale&#39;, fontsize = fs)
    yi10,ya10 = axs5[1, 0].get_ylim()
    dy0 = (ya10-yi10)/2.0
    yi20,ya20 = axs5[2, 0].get_ylim()
    if (ya20-yi20)&lt;0.01*dy0:
        axs5[2, 0].set_ylim((yi20-dy0, ya20+dy0))
    axs5[3, 0].plot(tr_matr_cum[:, 0, 0], &#39;r&#39;, linewidth = lwl, label = &#39;Sxx cum. - residual&#39;)
    axs5[3, 0].plot(tr_matr_cum[:, 1, 1], &#39;b&#39;, linewidth = lwl, label = &#39;Syy cum. - residual&#39;)
    axs5[3, 0].set_title(&#39;Residual Cumulative Scale&#39;, fontsize = fs)
    axs5[3, 0].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)
    yi30,ya30 = axs5[3, 0].get_ylim()
    if (ya30-yi30)&lt;0.01*dy0:
        axs5[3, 0].set_ylim((yi30-dy0, ya30+dy0))

    # plot shear terms
    axs5[1, 1].plot(transformation_matrix[:, 0, 1], &#39;r&#39;, linewidth = lwl, label = &#39;Sxy frame-to-frame&#39;)
    axs5[1, 1].plot(transformation_matrix[:, 1, 0], &#39;b&#39;, linewidth = lwl, label = &#39;Syx frame-to-frame&#39;)
    axs5[1, 1].set_title(&#39;Frame-to-Frame Shear Change&#39;, fontsize = fs)
    axs5[2, 1].plot(s01_cum_orig, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Sxy cum.&#39;)
    axs5[2, 1].plot(s10_cum_orig, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Syx cum.&#39;)
    if preserve_scales:
        axs5[2, 1].plot(s01_fit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Sxy cum. - &#39;+fm_string+&#39; fit&#39;)
        axs5[2, 1].plot(s10_fit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Syx cum. - &#39;+fm_string+&#39; fit&#39;)
    axs5[2, 1].set_title(&#39;Cumulative Shear&#39;, fontsize = fs)
    yi11,ya11 = axs5[1, 1].get_ylim()
    dy1 = (ya11-yi11)/2.0
    yi21,ya21 = axs5[2, 1].get_ylim()
    if (ya21-yi21)&lt;0.01*dy1:
        axs5[2, 1].set_ylim((yi21-dy1, ya21+dy1))
    axs5[3, 1].plot(tr_matr_cum[:, 0, 1], &#39;r&#39;, linewidth = lwl, label = &#39;Sxy cum. - residual&#39;)
    axs5[3, 1].plot(tr_matr_cum[:, 1, 0], &#39;b&#39;, linewidth = lwl, label = &#39;Syx cum. - residual&#39;)
    axs5[3, 1].set_title(&#39;Residual Cumulative Shear&#39;, fontsize = fs)
    axs5[3, 1].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)
    yi31,ya31 = axs5[3, 1].get_ylim()
    if (ya31-yi21)&lt;0.01*dy1:
        axs5[3, 1].set_ylim((yi31-dy1, ya31+dy1))

    # plot shifts
    axs5[1, 2].plot(transformation_matrix[:, 0, 2], &#39;r&#39;, linewidth = lwl, label = &#39;Tx fr.-to-fr.&#39;)
    axs5[1, 2].plot(transformation_matrix[:, 1, 2], &#39;b&#39;, linewidth = lwl, label = &#39;Ty fr.-to-fr.&#39;)
    axs5[1, 2].set_title(&#39;Frame-to-Frame Shift&#39;, fontsize = fs)
    if preserve_scales:
        axs5[2, 2].plot(Xshift_cum_orig, &#39;r&#39;, linewidth = lwl, label = &#39;Tx cum. - orig.&#39;)
        axs5[2, 2].plot(Yshift_cum_orig, &#39;b&#39;, linewidth = lwl, label = &#39;Ty cum. - orig.&#39;)
        axs5[2, 2].plot(Xshift_cum, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Tx cum. - pres. scales&#39;)
        axs5[2, 2].plot(Yshift_cum, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Ty cum. - pres. scales&#39;)
    else:
        axs5[2, 2].plot(Xshift_cum, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Tx cum.&#39;)
        axs5[2, 2].plot(Yshift_cum, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Ty cum.&#39;)
    if subtract_linear_fit[0]:
        axs5[2, 2].plot(Xfit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Tx cum. - lin. fit&#39;)
    if subtract_linear_fit[1]:
        axs5[2, 2].plot(Yfit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Ty cum. - lin. fit&#39;)
    axs5[2, 2].set_title(&#39;Cumulative Shift&#39;, fontsize = fs)
    axs5[3, 2].plot(tr_matr_cum[:, 0, 2], &#39;r&#39;, linewidth = lwl, label = &#39;Tx cum. - residual&#39;)
    axs5[3, 2].plot(tr_matr_cum[:, 1, 2], &#39;b&#39;, linewidth = lwl, label = &#39;Ty cum. - residual&#39;)
    axs5[3, 2].set_title(&#39;Residual Cumulative Shift&#39;, fontsize = fs)
    axs5[3, 2].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)

    for ax in axs5.ravel()[1:]:
        ax.grid(True)
        ax.legend(fontsize = fs-1)
    fn = os.path.join(data_dir, fnm_reg)
    fig5.suptitle(fn, fontsize = fs)
    if save_res_png :
        fig5.savefig(fn.replace(&#39;.mrc&#39;, &#39;_Transform_Summary.png&#39;), dpi=300)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.generate_report_transf_matrix_from_xlsx"><code class="name flex">
<span>def <span class="ident">generate_report_transf_matrix_from_xlsx</span></span>(<span>transf_matrix_xlsx_file, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate Report Plot for Transformation Matrix from XLSX spreadsheet file. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com</p>
<p>Parameters:
transf_matrix_xlsx_file : str
Path to the xlsx workbook containing Transformation Matrix data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_report_transf_matrix_from_xlsx(transf_matrix_xlsx_file, **kwargs):
    &#39;&#39;&#39;
    Generate Report Plot for Transformation Matrix from XLSX spreadsheet file. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters:
    transf_matrix_xlsx_file : str
        Path to the xlsx workbook containing Transformation Matrix data

    &#39;&#39;&#39;
    disp_res = kwargs.get(&#39;disp_res&#39;, False)
    if disp_res:
        print(&#39;Loading kwarg Data&#39;)
    saved_kwargs = read_kwargs_xlsx(transf_matrix_xlsx_file, &#39;kwargs Info&#39;, **kwargs)
    data_dir = saved_kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = saved_kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    TransformType = saved_kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    Sample_ID = saved_kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    SIFT_nfeatures = saved_kwargs.get(&#34;SIFT_nfeatures&#34;, 0)
    SIFT_nOctaveLayers = saved_kwargs.get(&#34;SIFT_nOctaveLayers&#34;, 3)
    SIFT_contrastThreshold = saved_kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.025)
    SIFT_edgeThreshold = saved_kwargs.get(&#34;SIFT_edgeThreshold&#34;, 10)
    SIFT_sigma = saved_kwargs.get(&#34;SIFT_sigma&#34;, 1.6)
    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = saved_kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = saved_kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = saved_kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = saved_kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = saved_kwargs.get(&#34;max_iter&#34;, 1000)
    BFMatcher = saved_kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = saved_kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    kp_max_num = saved_kwargs.get(&#34;kp_max_num&#34;, -1)
    save_res_png  = saved_kwargs.get(&#34;save_res_png&#34;, True)

    preserve_scales = saved_kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
    fit_params = saved_kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    subtract_linear_fit = saved_kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # The linear slopes along X- and Y- directions (respectively) will be subtracted from the cumulative shifts.
    subtract_FOVtrend_from_fit = saved_kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])
    pad_edges =  saved_kwargs.get(&#34;pad_edges&#34;, True)

    if disp_res:
        print(&#39;Loading Original Transformation Data&#39;)
    orig_transf_matrix = pd.read_excel(transf_matrix_xlsx_file, sheet_name=&#39;Orig. Transformation Matrix&#39;)
    transformation_matrix = np.vstack((orig_transf_matrix[&#39;T00 (Sxx)&#39;],
                         orig_transf_matrix[&#39;T01 (Sxy)&#39;],
                         orig_transf_matrix[&#39;T02 (Tx)&#39;],
                         orig_transf_matrix[&#39;T10 (Syx)&#39;],
                         orig_transf_matrix[&#39;T11 (Syy)&#39;],
                         orig_transf_matrix[&#39;T12 (Ty)&#39;],
                         orig_transf_matrix[&#39;T20 (0.0)&#39;],
                         orig_transf_matrix[&#39;T21 (0.0)&#39;],
                         orig_transf_matrix[&#39;T22 (1.0)&#39;])).T.reshape((len(orig_transf_matrix[&#39;T00 (Sxx)&#39;]), 3, 3))

    if disp_res:
        print(&#39;Loading Cumulative Transformation Data&#39;)
    cum_transf_matrix = pd.read_excel(transf_matrix_xlsx_file, sheet_name=&#39;Cum. Transformation Matrix&#39;)
    tr_matr_cum = np.vstack((cum_transf_matrix[&#39;T00 (Sxx)&#39;],
                         cum_transf_matrix[&#39;T01 (Sxy)&#39;],
                         cum_transf_matrix[&#39;T02 (Tx)&#39;],
                         cum_transf_matrix[&#39;T10 (Syx)&#39;],
                         cum_transf_matrix[&#39;T11 (Syy)&#39;],
                         cum_transf_matrix[&#39;T12 (Ty)&#39;],
                         cum_transf_matrix[&#39;T20 (0.0)&#39;],
                         cum_transf_matrix[&#39;T21 (0.0)&#39;],
                         cum_transf_matrix[&#39;T22 (1.0)&#39;])).T.reshape((len(cum_transf_matrix[&#39;T00 (Sxx)&#39;]), 3, 3))

    if disp_res:
        print(&#39;Loading Intermediate Data&#39;)
    int_results = pd.read_excel(transf_matrix_xlsx_file, sheet_name=&#39;Intermediate Results&#39;)
    s00_cum_orig = int_results[&#39;s00_cum_orig&#39;]
    s11_cum_orig = int_results[&#39;s11_cum_orig&#39;]
    s00_fit = int_results[&#39;s00_fit&#39;]
    s11_fit = int_results[&#39;s11_fit&#39;]
    s01_cum_orig = int_results[&#39;s01_cum_orig&#39;]
    s10_cum_orig = int_results[&#39;s10_cum_orig&#39;]
    s01_fit = int_results[&#39;s01_fit&#39;]
    s10_fit = int_results[&#39;s10_fit&#39;]
    Xshift_cum_orig = int_results[&#39;Xshift_cum_orig&#39;]
    Yshift_cum_orig = int_results[&#39;Yshift_cum_orig&#39;]
    Xshift_cum = int_results[&#39;Xshift_cum&#39;]
    Yshift_cum = int_results[&#39;Yshift_cum&#39;]
    Xfit = int_results[&#39;Xfit&#39;]
    Yfit = int_results[&#39;Yfit&#39;]

    if disp_res:
        print(&#39;Loading Statistics&#39;)
    stat_results = pd.read_excel(transf_matrix_xlsx_file, sheet_name=&#39;Reg. Stat. Info&#39;)
    npts = stat_results[&#39;Npts&#39;]
    error_abs_mean = stat_results[&#39;Mean Abs Error&#39;]

    fs = 14
    lwf = 2
    lwl = 1
    fig5, axs5 = subplots(4,3, figsize=(18, 16), sharex=True)
    fig5.subplots_adjust(left=0.07, bottom=0.03, right=0.99, top=0.95)
    # display the info
    axs5[0,0].axis(False)
    axs5[0,0].text(-0.1, 0.9, Sample_ID, fontsize = fs + 4)
    #axs5[0,0].text(-0.1, 0.73, &#39;Global Data Range:  Min={:.2f}, Max={:.2f}&#39;.format(data_min_glob, data_max_glob), transform=axs5[0,0].transAxes, fontsize = fs)

    if TransformType == RegularizedAffineTransform:
        tstr = [&#39;{:d}&#39;.format(x) for x in targ_vector]
        otext = &#39;Reg.Aff.Transf., λ= {:.1e}, t=[&#39;.format(l2_matrix[0,0]) + &#39; &#39;.join(tstr) + &#39;], w/&#39; + solver
    else:
        otext = TransformType.__name__ + &#39; with &#39; + solver + &#39; solver&#39;
    axs5[0,0].text(-0.1, 0.80, otext, transform=axs5[0,0].transAxes, fontsize = fs)

    SIFT1text = &#39;SIFT: nFeatures = {:d}, nOctaveLayers = {:d}, &#39;.format(SIFT_nfeatures, SIFT_nOctaveLayers)
    axs5[0,0].text(-0.1, 0.65, SIFT1text, transform=axs5[0,0].transAxes, fontsize = fs)

    SIFT2text = &#39;SIFT: contrThr = {:.3f}, edgeThr = {:.2f}, σ= {:.2f}&#39;.format(SIFT_contrastThreshold, SIFT_edgeThreshold, SIFT_sigma)
    axs5[0,0].text(-0.1, 0.50, SIFT2text, transform=axs5[0,0].transAxes, fontsize = fs)

    sbtrfit = (&#39;ON, &#39; if  subtract_linear_fit[0] else &#39;OFF, &#39;) + (&#39;ON&#39; if  subtract_linear_fit[1] else &#39;OFF&#39;) + (&#39;(ON, &#39; if  subtract_FOVtrend_from_fit[0] else &#39;(OFF, &#39;) + (&#39;ON)&#39; if  subtract_FOVtrend_from_fit[1] else &#39;OFF)&#39;)
    axs5[0,0].text(-0.1, 0.35, &#39;drmax={:.1f}, Max # of KeyPts={:d}, Max # of Iter.={:d}&#39;.format(drmax, kp_max_num, max_iter), transform=axs5[0,0].transAxes, fontsize = fs)
    padedges = &#39;ON&#39; if pad_edges else &#39;OFF&#39;
    if preserve_scales:
        fit_method = fit_params[0]
        if fit_method == &#39;LF&#39;:
            fit_str = &#39;, Meth: Linear Fit&#39;
            fm_string = &#39;linear&#39;
        else:
            if fit_method == &#39;SG&#39;:
                fit_str = &#39;, Meth: Sav.-Gol., &#39; + str(fit_params[1:])
                fm_string = &#39;Sav.-Gol.&#39;
            else:
                fit_str = &#39;, Meth: Pol.Fit, ord.={:d}&#39;.format(fit_params[1])
                fm_string = &#39;polyn.&#39;
        preserve_scales_string = &#39;Pres. Scls: ON&#39; + fit_str
    else:
        preserve_scales_string = &#39;Preserve Scales: OFF&#39;
    axs5[0,0].text(-0.1, 0.20, preserve_scales_string, transform=axs5[0,0].transAxes, fontsize = fs)
    axs5[0,0].text(-0.1, 0.05, &#39;Subtract Shift Fit: &#39; + sbtrfit + &#39;, Pad Edges: &#39; + padedges, transform=axs5[0,0].transAxes, fontsize = fs)
    # plot number of keypoints
    axs5[0, 1].plot(npts, &#39;g&#39;, linewidth = lwl, label = &#39;# of key-points per frame&#39;)
    axs5[0, 1].set_title(&#39;# of key-points per frame&#39;)
    axs5[0, 1].text(0.03, 0.2, &#39;Mean # of kpts= {:.0f}   Median # of kpts= {:.0f}&#39;.format(np.mean(npts), np.median(npts)), transform=axs5[0, 1].transAxes, fontsize = fs-1)
    # plot Standard deviations
    axs5[0, 2].plot(error_abs_mean, &#39;magenta&#39;, linewidth = lwl, label = &#39;Mean Abs Error over keyponts per frame&#39;)
    axs5[0, 2].set_title(&#39;Mean Abs Error keyponts per frame&#39;)
    axs5[0, 2].text(0.03, 0.2, &#39;Mean Abs Error= {:.3f}   Median Abs Error= {:.3f}&#39;.format(np.mean(error_abs_mean), np.median(error_abs_mean)), transform=axs5[0, 2].transAxes, fontsize = fs-1)

    # plot scales terms
    axs5[1, 0].plot(transformation_matrix[:, 0, 0], &#39;r&#39;, linewidth = lwl, label = &#39;Sxx frame-to-frame&#39;)
    axs5[1, 0].plot(transformation_matrix[:, 1, 1], &#39;b&#39;, linewidth = lwl, label = &#39;Syy frame-to-frame&#39;)
    axs5[1, 0].set_title(&#39;Frame-to-Frame Scale Change&#39;, fontsize = fs)
    axs5[2, 0].plot(s00_cum_orig, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Sxx cum.&#39;)
    axs5[2, 0].plot(s11_cum_orig, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Syy cum.&#39;)
    if preserve_scales:
        axs5[2, 0].plot(s00_fit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Sxx cum. - &#39;+fm_string+&#39; fit&#39;)
        axs5[2, 0].plot(s11_fit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Syy cum. - &#39;+fm_string+&#39; fit&#39;)
    axs5[2, 0].set_title(&#39;Cumulative Scale&#39;, fontsize = fs)
    yi10,ya10 = axs5[1, 0].get_ylim()
    dy0 = (ya10-yi10)/2.0
    yi20,ya20 = axs5[2, 0].get_ylim()
    if (ya20-yi20)&lt;0.01*dy0:
        axs5[2, 0].set_ylim((yi20-dy0, ya20+dy0))
    axs5[3, 0].plot(tr_matr_cum[:, 0, 0], &#39;r&#39;, linewidth = lwl, label = &#39;Sxx cum. - residual&#39;)
    axs5[3, 0].plot(tr_matr_cum[:, 1, 1], &#39;b&#39;, linewidth = lwl, label = &#39;Syy cum. - residual&#39;)
    axs5[3, 0].set_title(&#39;Residual Cumulative Scale&#39;, fontsize = fs)
    axs5[3, 0].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)
    yi30,ya30 = axs5[3, 0].get_ylim()
    if (ya30-yi30)&lt;0.01*dy0:
        axs5[3, 0].set_ylim((yi30-dy0, ya30+dy0))

    # plot shear terms
    axs5[1, 1].plot(transformation_matrix[:, 0, 1], &#39;r&#39;, linewidth = lwl, label = &#39;Sxy frame-to-frame&#39;)
    axs5[1, 1].plot(transformation_matrix[:, 1, 0], &#39;b&#39;, linewidth = lwl, label = &#39;Syx frame-to-frame&#39;)
    axs5[1, 1].set_title(&#39;Frame-to-Frame Shear Change&#39;, fontsize = fs)
    axs5[2, 1].plot(s01_cum_orig, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Sxy cum.&#39;)
    axs5[2, 1].plot(s10_cum_orig, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Syx cum.&#39;)
    if preserve_scales:
        axs5[2, 1].plot(s01_fit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Sxy cum. - &#39;+fm_string+&#39; fit&#39;)
        axs5[2, 1].plot(s10_fit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Syx cum. - &#39;+fm_string+&#39; fit&#39;)
    axs5[2, 1].set_title(&#39;Cumulative Shear&#39;, fontsize = fs)
    yi11,ya11 = axs5[1, 1].get_ylim()
    dy1 = (ya11-yi11)/2.0
    yi21,ya21 = axs5[2, 1].get_ylim()
    if (ya21-yi21)&lt;0.01*dy1:
        axs5[2, 1].set_ylim((yi21-dy1, ya21+dy1))
    axs5[3, 1].plot(tr_matr_cum[:, 0, 1], &#39;r&#39;, linewidth = lwl, label = &#39;Sxy cum. - residual&#39;)
    axs5[3, 1].plot(tr_matr_cum[:, 1, 0], &#39;b&#39;, linewidth = lwl, label = &#39;Syx cum. - residual&#39;)
    axs5[3, 1].set_title(&#39;Residual Cumulative Shear&#39;, fontsize = fs)
    axs5[3, 1].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)
    yi31,ya31 = axs5[3, 1].get_ylim()
    if (ya31-yi21)&lt;0.01*dy1:
        axs5[3, 1].set_ylim((yi31-dy1, ya31+dy1))

    # plot shifts
    axs5[1, 2].plot(transformation_matrix[:, 0, 2], &#39;r&#39;, linewidth = lwl, label = &#39;Tx fr.-to-fr.&#39;)
    axs5[1, 2].plot(transformation_matrix[:, 1, 2], &#39;b&#39;, linewidth = lwl, label = &#39;Ty fr.-to-fr.&#39;)
    axs5[1, 2].set_title(&#39;Frame-to-Frame Shift&#39;, fontsize = fs)
    if preserve_scales:
        axs5[2, 2].plot(Xshift_cum_orig, &#39;r&#39;, linewidth = lwl, label = &#39;Tx cum. - orig.&#39;)
        axs5[2, 2].plot(Yshift_cum_orig, &#39;b&#39;, linewidth = lwl, label = &#39;Ty cum. - orig.&#39;)
        axs5[2, 2].plot(Xshift_cum, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Tx cum. - pres. scales&#39;)
        axs5[2, 2].plot(Yshift_cum, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Ty cum. - pres. scales&#39;)
    else:
        axs5[2, 2].plot(Xshift_cum, &#39;r&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Tx cum.&#39;)
        axs5[2, 2].plot(Yshift_cum, &#39;b&#39;, linewidth = lwl, linestyle=&#39;dotted&#39;, label = &#39;Ty cum.&#39;)
    if subtract_linear_fit[0]:
        axs5[2, 2].plot(Xfit, &#39;orange&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Tx cum. - lin. fit&#39;)
    if subtract_linear_fit[1]:
        axs5[2, 2].plot(Yfit, &#39;cyan&#39;, linewidth = lwf, linestyle=&#39;dashed&#39;, label = &#39;Ty cum. - lin. fit&#39;)
    axs5[2, 2].set_title(&#39;Cumulative Shift&#39;, fontsize = fs)
    axs5[3, 2].plot(tr_matr_cum[:, 0, 2], &#39;r&#39;, linewidth = lwl, label = &#39;Tx cum. - residual&#39;)
    axs5[3, 2].plot(tr_matr_cum[:, 1, 2], &#39;b&#39;, linewidth = lwl, label = &#39;Ty cum. - residual&#39;)
    axs5[3, 2].set_title(&#39;Residual Cumulative Shift&#39;, fontsize = fs)
    axs5[3, 2].set_xlabel(&#39;Frame&#39;, fontsize = fs+1)

    for ax in axs5.ravel()[1:]:
        ax.grid(True)
        ax.legend(fontsize = fs-1)
    fig5.suptitle(transf_matrix_xlsx_file, fontsize = fs)
    if save_res_png :
        fig5.savefig(transf_matrix_xlsx_file.replace(&#39;.xlsx&#39;, &#39;.png&#39;), dpi=300)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.geometric"><code class="name flex">
<span>def <span class="ident">geometric</span></span>(<span>p, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from the geometric distribution.</p>
<p>Bernoulli trials are experiments with one of two outcomes:
success or failure (an example of such an experiment is flipping
a coin).
The geometric distribution models the number of trials
that must be run in order to achieve success.
It is therefore
supported on the positive integers, <code>k = 1, 2, ...</code>.</p>
<p>The probability mass function of the geometric distribution is</p>
<p>[
]
where <code>p</code> is the probability of success of an individual trial.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.geometric</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>The probability of success of an individual trial.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>p</code> is a scalar.
Otherwise,
<code>np.array(p).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized geometric distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.geometric</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Draw ten thousand values from the geometric distribution,
with the probability of an individual success equal to 0.35:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; z = np.random.geometric(p=0.35, size=10000)
</code></pre>
<p>How many trials succeeded after a single run?</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; (z == 1).sum() / 10000.
0.34889999999999999 #random
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.get_min_max_thresholds"><code class="name flex">
<span>def <span class="ident">get_min_max_thresholds</span></span>(<span>image, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the data range (min and max) with given fractional thresholds for cumulative distribution.
©G.Shtengel 11/2022 gleb.shtengel@gmail.com</p>
<p>Calculates the histogram of pixel intensities of the image with number of bins determined by parameter nbins (default = 256)
and normalizes it to get the probability distribution function (PDF), from which a cumulative distribution function (CDF) is calculated.
Then given the thr_min, thr_max parameters, the minimum and maximum values of the data range are found
by determining the intensities at which CDF= thr_min and (1- thr_max), respectively</p>
<h2 id="parameters">Parameters:</h2>
<p>image : 2D array
Image to be analyzed</p>
<p>kwargs:</p>
<hr>
<p>thr_min : float
lower CDF threshold for determining the minimum data value. Default is 1.0e-3
thr_max : float
upper CDF threshold for determining the maximum data value. Default is 1.0e-3
nbins : int
number of histogram bins for building the PDF and CDF
log
: bolean
If True, the histogram will have log scale. Default is false
disp_res : bolean
If True display the results. Default is True.
save_res : boolean
If True the image will be saved. Default is False.
dpi : int
Default is 300
save_filename : string
the name of the image to perform this operations (defaulut is 'min_max_thresholds.png').</p>
<p>Returns
(dmin, dmax) : float array</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_min_max_thresholds(image, **kwargs):
    &#39;&#39;&#39;
    Determines the data range (min and max) with given fractional thresholds for cumulative distribution.
    ©G.Shtengel 11/2022 gleb.shtengel@gmail.com

    Calculates the histogram of pixel intensities of the image with number of bins determined by parameter nbins (default = 256)
    and normalizes it to get the probability distribution function (PDF), from which a cumulative distribution function (CDF) is calculated.
    Then given the thr_min, thr_max parameters, the minimum and maximum values of the data range are found
    by determining the intensities at which CDF= thr_min and (1- thr_max), respectively

    Parameters:
    ----------
    image : 2D array
        Image to be analyzed

    kwargs:
     ----------
    thr_min : float
        lower CDF threshold for determining the minimum data value. Default is 1.0e-3
    thr_max : float
        upper CDF threshold for determining the maximum data value. Default is 1.0e-3
    nbins : int
        number of histogram bins for building the PDF and CDF
    log  : bolean
        If True, the histogram will have log scale. Default is false
    disp_res : bolean
        If True display the results. Default is True.
    save_res : boolean
        If True the image will be saved. Default is False.
    dpi : int
        Default is 300
    save_filename : string
        the name of the image to perform this operations (defaulut is &#39;min_max_thresholds.png&#39;).

    Returns
    (dmin, dmax) : float array
    &#39;&#39;&#39;
    thr_min = kwargs.get(&#39;thr_min&#39;, 1.0e-3)
    thr_max = kwargs.get(&#39;thr_max&#39;, 1.0e-3)
    nbins = kwargs.get(&#39;nbins&#39;, 256)
    disp_res = kwargs.get(&#39;disp_res&#39;, True)
    log = kwargs.get(&#39;log&#39;, False)
    save_res = kwargs.get(&#39;save_res&#39;, False)
    dpi = kwargs.get(&#39;dpi&#39;, 300)
    save_filename = kwargs.get(&#39;save_filename&#39;, &#39;min_max_thresholds.png&#39;)

    if disp_res:
        fsz=11
        fig, axs = subplots(2,1, figsize = (6,8))
        hist, bins, patches = axs[0].hist(image.ravel(), bins=nbins, log=log)
    else:
        hist, bins = np.histogram(image.ravel(), bins=nbins)
    pdf = hist / np.prod(image.shape)
    cdf = np.cumsum(pdf)
    data_max = bins[argmin(abs(cdf-(1.0-thr_max)))]
    data_min = bins[argmin(abs(cdf-thr_min))]

    if disp_res:
        xCDF = bins[0:-1]+(bins[1]-bins[0])/2.0
        xthr = [xCDF[0], xCDF[-1]]
        ythr_min = [thr_min, thr_min]
        ythr_max = [1-thr_max, 1-thr_max]
        axs[1].plot(xCDF, cdf, label=&#39;CDF&#39;)
        axs[1].plot(xthr, ythr_min, &#39;r&#39;, label=&#39;thr_min={:.5f}&#39;.format(thr_min))
        axs[1].plot(xthr, ythr_max, &#39;g&#39;, label=&#39;1.0 - thr_max = {:.5f}&#39;.format(1-thr_max))
        axs[1].set_xlabel(&#39;Intensity Level&#39;, fontsize = fsz)
        axs[0].set_ylabel(&#39;PDF&#39;, fontsize = fsz)
        axs[1].set_ylabel(&#39;CDF&#39;, fontsize = fsz)
        xi = data_min - (np.abs(data_max-data_min)/2)
        xa = data_max + (np.abs(data_max-data_min)/2)
        rys = [[0, np.max(hist)], [0, 1]]
        for ax, ry in zip(axs, rys):
            ax.plot([data_min, data_min], ry, &#39;r&#39;, linestyle = &#39;--&#39;, label = &#39;data_min={:.1f}&#39;.format(data_min))
            ax.plot([data_max, data_max], ry, &#39;g&#39;, linestyle = &#39;--&#39;, label = &#39;data_max={:.1f}&#39;.format(data_max))
            ax.set_xlim(xi, xa)
            ax.grid(True)
        axs[1].legend(loc=&#39;center&#39;, fontsize=fsz)
        axs[1].set_title(&#39;Data Min and max with thr_min={:.0e},  thr_max={:.0e}&#39;.format(thr_min, thr_max), fontsize = fsz)
        if save_res:
            fig.savefig(save_filename, dpi=dpi)
    return np.array((data_min, data_max))</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.get_spread"><code class="name flex">
<span>def <span class="ident">get_spread</span></span>(<span>data, window=501, porder=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates spread - standard deviation of the (signal - Sav-Gol smoothed signal).
©G.Shtengel 10/2021 gleb.shtengel@gmail.com</p>
<p>Parameters:
data : 1D array
window : int
aperture (number of points) for Sav-Gol filter)
porder : int
polynomial order for Sav-Gol filter</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>data_spread </code></dt>
<dd>float</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_spread(data, window=501, porder=3):
    &#39;&#39;&#39;
    Calculates spread - standard deviation of the (signal - Sav-Gol smoothed signal).
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

    Parameters:
    data : 1D array
    window : int
        aperture (number of points) for Sav-Gol filter)
    porder : int
        polynomial order for Sav-Gol filter

    Returns:
        data_spread : float

    &#39;&#39;&#39;

    try:
        #sm_data = savgol_filter(data.astype(double), window, porder)
        sm_data = savgol_filter(data.astype(double), window, porder, mode=&#39;mirror&#39;)
        data_spread = np.std(data-sm_data)
    except :
        print(&#39;spread error&#39;)
        data_spread = 0.0
    return data_spread</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.get_state"><code class="name flex">
<span>def <span class="ident">get_state</span></span>(<span>legacy=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a tuple representing the internal state of the generator.</p>
<p>For more details, see <code><a title="SIFT_gs.FIBSEM_SIFT_gs.set_state" href="#SIFT_gs.FIBSEM_SIFT_gs.set_state">RandomState.set_state()</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>legacy</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag indicating to return a legacy tuple state when the BitGenerator
is MT19937, instead of a dict. Raises ValueError if the underlying
bit generator is not an instance of MT19937.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>{tuple(str, ndarray</code> of <code>624 uints, int, int, float), dict}</code></dt>
<dd>
<p>If legacy is True, the returned tuple has the following items:</p>
<ol>
<li>the string 'MT19937'.</li>
<li>a 1-D array of 624 unsigned integer keys.</li>
<li>an integer <code>pos</code>.</li>
<li>an integer <code>has_gauss</code>.</li>
<li>a float <code>cached_gaussian</code>.</li>
</ol>
<p>If <code>legacy</code> is False, or the BitGenerator is not MT19937, then
state is returned as a dictionary.</p>
</dd>
</dl>
<h2 id="see-also">See Also</h2>
<p><code><a title="SIFT_gs.FIBSEM_SIFT_gs.set_state" href="#SIFT_gs.FIBSEM_SIFT_gs.set_state">RandomState.set_state()</a></code></p>
<h2 id="notes">Notes</h2>
<p><code><a title="SIFT_gs.FIBSEM_SIFT_gs.set_state" href="#SIFT_gs.FIBSEM_SIFT_gs.set_state">RandomState.set_state()</a></code> and <code><a title="SIFT_gs.FIBSEM_SIFT_gs.get_state" href="#SIFT_gs.FIBSEM_SIFT_gs.get_state">RandomState.get_state()</a></code> are not needed to work with any of the
random distributions in NumPy. If the internal state is manually altered,
the user should know exactly what he/she is doing.</p></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.gumbel"><code class="name flex">
<span>def <span class="ident">gumbel</span></span>(<span>loc=0.0, scale=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Gumbel distribution.</p>
<p>Draw samples from a Gumbel distribution with specified location and
scale.
For more information on the Gumbel distribution, see
Notes and References below.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.gumbel</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loc</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>The location of the mode of the distribution. Default is 0.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>The scale parameter of the distribution. Default is 1. Must be non-
negative.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>loc</code> and <code>scale</code> are both scalars.
Otherwise, <code>np.broadcast(loc, scale).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized Gumbel distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<p><code>scipy.stats.gumbel_l</code>
<code>scipy.stats.gumbel_r</code>
<code>scipy.stats.genextreme</code>
<code><a title="SIFT_gs.FIBSEM_SIFT_gs.weibull" href="#SIFT_gs.FIBSEM_SIFT_gs.weibull">RandomState.weibull()</a></code>
<code>random.Generator.gumbel: which should be used for new code.</code></p>
<h2 id="notes">Notes</h2>
<p>The Gumbel (or Smallest Extreme Value (SEV) or the Smallest Extreme
Value Type I) distribution is one of a class of Generalized Extreme
Value (GEV) distributions used in modeling extreme value problems.
The Gumbel is a special case of the Extreme Value Type I distribution
for maximums from distributions with "exponential-like" tails.</p>
<p>The probability density for the Gumbel distribution is</p>
<p>[ \beta}}, ]
where :math:<code>\mu</code> is the mode, a location parameter, and
:math:<code>\beta</code> is the scale parameter.</p>
<p>The Gumbel (named for German mathematician Emil Julius Gumbel) was used
very early in the hydrology literature, for modeling the occurrence of
flood events. It is also used for modeling maximum wind speed and
rainfall rates.
It is a "fat-tailed" distribution - the probability of
an event in the tail of the distribution is larger than if one used a
Gaussian, hence the surprisingly frequent occurrence of 100-year
floods. Floods were initially modeled as a Gaussian process, which
underestimated the frequency of extreme events.</p>
<p>It is one of a class of extreme value distributions, the Generalized
Extreme Value (GEV) distributions, which also includes the Weibull and
Frechet.</p>
<p>The function has a mean of :math:<code>\mu + 0.57721\beta</code> and a variance
of :math:<code>\frac{\pi^2}{6}\beta^2</code>.</p>
<h2 id="references">References</h2>
<p>.. [1] Gumbel, E. J., "Statistics of Extremes,"
New York: Columbia University Press, 1958.
.. [2] Reiss, R.-D. and Thomas, M., "Statistical Analysis of Extreme
Values from Insurance, Finance, Hydrology and Other Fields,"
Basel: Birkhauser Verlag, 2001.</p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; mu, beta = 0, 0.1 # location and scale
&gt;&gt;&gt; s = np.random.gumbel(mu, beta, 1000)
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, 30, density=True)
&gt;&gt;&gt; plt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)
...          * np.exp( -np.exp( -(bins - mu) /beta) ),
...          linewidth=2, color='r')
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Show how an extreme value distribution can arise from a Gaussian process
and compare to a Gaussian:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; means = []
&gt;&gt;&gt; maxima = []
&gt;&gt;&gt; for i in range(0,1000) :
...    a = np.random.normal(mu, beta, 1000)
...    means.append(a.mean())
...    maxima.append(a.max())
&gt;&gt;&gt; count, bins, ignored = plt.hist(maxima, 30, density=True)
&gt;&gt;&gt; beta = np.std(maxima) * np.sqrt(6) / np.pi
&gt;&gt;&gt; mu = np.mean(maxima) - 0.57721*beta
&gt;&gt;&gt; plt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)
...          * np.exp(-np.exp(-(bins - mu)/beta)),
...          linewidth=2, color='r')
&gt;&gt;&gt; plt.plot(bins, 1/(beta * np.sqrt(2 * np.pi))
...          * np.exp(-(bins - mu)**2 / (2 * beta**2)),
...          linewidth=2, color='g')
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.hypergeometric"><code class="name flex">
<span>def <span class="ident">hypergeometric</span></span>(<span>ngood, nbad, nsample, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Hypergeometric distribution.</p>
<p>Samples are drawn from a hypergeometric distribution with specified
parameters, <code>ngood</code> (ways to make a good selection), <code>nbad</code> (ways to make
a bad selection), and <code>nsample</code> (number of items sampled, which is less
than or equal to the sum <code>ngood + nbad</code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.hypergeometric</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ngood</code></strong> :&ensp;<code>int</code> or <code>array_like</code> of <code>ints</code></dt>
<dd>Number of ways to make a good selection.
Must be nonnegative.</dd>
<dt><strong><code>nbad</code></strong> :&ensp;<code>int</code> or <code>array_like</code> of <code>ints</code></dt>
<dd>Number of ways to make a bad selection.
Must be nonnegative.</dd>
<dt><strong><code>nsample</code></strong> :&ensp;<code>int</code> or <code>array_like</code> of <code>ints</code></dt>
<dd>Number of items sampled.
Must be at least 1 and at most
<code>ngood + nbad</code>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>ngood</code>, <code>nbad</code>, and <code>nsample</code>
are all scalars.
Otherwise, <code>np.broadcast(ngood, nbad, nsample).size</code>
samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized hypergeometric distribution. Each
sample is the number of good items within a randomly selected subset of
size <code>nsample</code> taken from a set of <code>ngood</code> good items and <code>nbad</code> bad items.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.hypergeom</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>random.Generator.hypergeometric</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the Hypergeometric distribution is</p>
<p>[
]
where :math:<code>0 \le x \le n</code> and :math:<code>n-b \le x \le g</code></p>
<p>for P(x) the probability of <code>x</code> good results in the drawn sample,
g = <code>ngood</code>, b = <code>nbad</code>, and n = <code>nsample</code>.</p>
<p>Consider an urn with black and white marbles in it, <code>ngood</code> of them
are black and <code>nbad</code> are white. If you draw <code>nsample</code> balls without
replacement, then the hypergeometric distribution describes the
distribution of black balls in the drawn sample.</p>
<p>Note that this distribution is very similar to the binomial
distribution, except that in this case, samples are drawn without
replacement, whereas in the Binomial case samples are drawn with
replacement (or the sample space is infinite). As the sample space
becomes large, this distribution approaches the binomial.</p>
<h2 id="references">References</h2>
<p>.. [1] Lentner, Marvin, "Elementary Applied Statistics", Bogden
and Quigley, 1972.
.. [2] Weisstein, Eric W. "Hypergeometric Distribution." From
MathWorld&ndash;A Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/HypergeometricDistribution.html">http://mathworld.wolfram.com/HypergeometricDistribution.html</a>
.. [3] Wikipedia, "Hypergeometric distribution",
<a href="https://en.wikipedia.org/wiki/Hypergeometric_distribution">https://en.wikipedia.org/wiki/Hypergeometric_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; ngood, nbad, nsamp = 100, 2, 10
# number of good, number of bad, and number of samples
&gt;&gt;&gt; s = np.random.hypergeometric(ngood, nbad, nsamp, 1000)
&gt;&gt;&gt; from matplotlib.pyplot import hist
&gt;&gt;&gt; hist(s)
#   note that it is very unlikely to grab both bad items
</code></pre>
<p>Suppose you have an urn with 15 white and 15 black marbles.
If you pull 15 marbles at random, how likely is it that
12 or more of them are one color?</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; s = np.random.hypergeometric(15, 15, 15, 100000)
&gt;&gt;&gt; sum(s&gt;=12)/100000. + sum(s&lt;=3)/100000.
#   answer = 0.003 ... pretty unlikely!
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.kp_to_list"><code class="name flex">
<span>def <span class="ident">kp_to_list</span></span>(<span>kp)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a keypont object to a list (so that it can be "pickled").</p>
<p>Returns
pt, angle, size, response, class_id, octave
(all extracted from corresponding cv2.KeyPoint() object attributes)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kp_to_list(kp):
    &#39;&#39;&#39;
    Convert a keypont object to a list (so that it can be &#34;pickled&#34;).

    Returns
    pt, angle, size, response, class_id, octave
    (all extracted from corresponding cv2.KeyPoint() object attributes)
    &#39;&#39;&#39;
    x, y = kp.pt
    pt = float(x), float(y)
    angle = float(kp.angle) if kp.angle is not None else None
    size = float(kp.size) if kp.size is not None else None
    response = float(kp.response) if kp.response is not None else None
    class_id = int(kp.class_id) if kp.class_id is not None else None
    octave = int(kp.octave) if kp.octave is not None else None
    return pt, angle, size, response, class_id, octave</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.laplace"><code class="name flex">
<span>def <span class="ident">laplace</span></span>(<span>loc=0.0, scale=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from the Laplace or double exponential distribution with
specified location (or mean) and scale (decay).</p>
<p>The Laplace distribution is similar to the Gaussian/normal distribution,
but is sharper at the peak and has fatter tails. It represents the
difference between two independent, identically distributed exponential
random variables.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.laplace</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loc</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>The position, :math:<code>\mu</code>, of the distribution peak. Default is 0.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>:math:<code>\lambda</code>, the exponential decay. Default is 1. Must be non-
negative.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>loc</code> and <code>scale</code> are both scalars.
Otherwise, <code>np.broadcast(loc, scale).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized Laplace distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.laplace</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>It has the probability density function</p>
<p>[ \exp\left(-\frac{|x - \mu|}{\lambda}\right). ]
The first law of Laplace, from 1774, states that the frequency
of an error can be expressed as an exponential function of the
absolute magnitude of the error, which leads to the Laplace
distribution. For many problems in economics and health
sciences, this distribution seems to model the data better
than the standard Gaussian distribution.</p>
<h2 id="references">References</h2>
<p>.. [1] Abramowitz, M. and Stegun, I. A. (Eds.). "Handbook of
Mathematical Functions with Formulas, Graphs, and Mathematical
Tables, 9th printing," New York: Dover, 1972.
.. [2] Kotz, Samuel, et. al. "The Laplace Distribution and
Generalizations, " Birkhauser, 2001.
.. [3] Weisstein, Eric W. "Laplace Distribution."
From MathWorld&ndash;A Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/LaplaceDistribution.html">http://mathworld.wolfram.com/LaplaceDistribution.html</a>
.. [4] Wikipedia, "Laplace distribution",
<a href="https://en.wikipedia.org/wiki/Laplace_distribution">https://en.wikipedia.org/wiki/Laplace_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; loc, scale = 0., 1.
&gt;&gt;&gt; s = np.random.laplace(loc, scale, 1000)
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, 30, density=True)
&gt;&gt;&gt; x = np.arange(-8., 8., .01)
&gt;&gt;&gt; pdf = np.exp(-abs(x-loc)/scale)/(2.*scale)
&gt;&gt;&gt; plt.plot(x, pdf)
</code></pre>
<p>Plot Gaussian for comparison:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; g = (1/(scale * np.sqrt(2 * np.pi)) *
...      np.exp(-(x - loc)**2 / (2 * scale**2)))
&gt;&gt;&gt; plt.plot(x,g)
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.list_to_kp"><code class="name flex">
<span>def <span class="ident">list_to_kp</span></span>(<span>inp_list)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a list to a keypont object</p>
<p>Parameters:
inp_list : list
List of Key-Point properties to initialize the following cv2.KeyPoint() object attributes:
[pt, angle, size, response, class_id, octave]</p>
<p>Returns:
kp : Instance of cv2.KeyPoint() object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_to_kp(inp_list):
    &#39;&#39;&#39;
    Convert a list to a keypont object

    Parameters:
    inp_list : list
        List of Key-Point properties to initialize the following cv2.KeyPoint() object attributes:
        [pt, angle, size, response, class_id, octave]

    Returns:
    kp : Instance of cv2.KeyPoint() object
    &#39;&#39;&#39;
    kp = cv2.KeyPoint()
    kp.pt = inp_list[0]
    kp.angle = inp_list[1]
    kp.size = inp_list[2]
    kp.response = inp_list[3]
    kp.octave = inp_list[4]
    kp.class_id = inp_list[5]
    return kp</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.logistic"><code class="name flex">
<span>def <span class="ident">logistic</span></span>(<span>loc=0.0, scale=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a logistic distribution.</p>
<p>Samples are drawn from a logistic distribution with specified
parameters, loc (location or mean, also median), and scale (&gt;0).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.logistic</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loc</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>Parameter of the distribution. Default is 0.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>Parameter of the distribution. Must be non-negative.
Default is 1.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>loc</code> and <code>scale</code> are both scalars.
Otherwise, <code>np.broadcast(loc, scale).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized logistic distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.logistic</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>random.Generator.logistic</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the Logistic distribution is</p>
<p>[
]
where :math:<code>\mu</code> = location and :math:<code>s</code> = scale.</p>
<p>The Logistic distribution is used in Extreme Value problems where it
can act as a mixture of Gumbel distributions, in Epidemiology, and by
the World Chess Federation (FIDE) where it is used in the Elo ranking
system, assuming the performance of each player is a logistically
distributed random variable.</p>
<h2 id="references">References</h2>
<p>.. [1] Reiss, R.-D. and Thomas M. (2001), "Statistical Analysis of
Extreme Values, from Insurance, Finance, Hydrology and Other
Fields," Birkhauser Verlag, Basel, pp 132-133.
.. [2] Weisstein, Eric W. "Logistic Distribution." From
MathWorld&ndash;A Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/LogisticDistribution.html">http://mathworld.wolfram.com/LogisticDistribution.html</a>
.. [3] Wikipedia, "Logistic-distribution",
<a href="https://en.wikipedia.org/wiki/Logistic_distribution">https://en.wikipedia.org/wiki/Logistic_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; loc, scale = 10, 1
&gt;&gt;&gt; s = np.random.logistic(loc, scale, 10000)
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, bins=50)
</code></pre>
<h1 id="plot-against-distribution">plot against distribution</h1>
<pre><code class="language-python-repl">&gt;&gt;&gt; def logist(x, loc, scale):
...     return np.exp((loc-x)/scale)/(scale*(1+np.exp((loc-x)/scale))**2)
&gt;&gt;&gt; lgst_val = logist(bins, loc, scale)
&gt;&gt;&gt; plt.plot(bins, lgst_val * count.max() / lgst_val.max())
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.lognormal"><code class="name flex">
<span>def <span class="ident">lognormal</span></span>(<span>mean=0.0, sigma=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a log-normal distribution.</p>
<p>Draw samples from a log-normal distribution with specified mean,
standard deviation, and array shape.
Note that the mean and standard
deviation are not the values for the distribution itself, but of the
underlying normal distribution it is derived from.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.lognormal</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>Mean value of the underlying normal distribution. Default is 0.</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>Standard deviation of the underlying normal distribution. Must be
non-negative. Default is 1.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>mean</code> and <code>sigma</code> are both scalars.
Otherwise, <code>np.broadcast(mean, sigma).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized log-normal distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.lognorm</code></dt>
<dd>probability density function, distribution, cumulative density function, etc.</dd>
<dt><code>random.Generator.lognormal</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>A variable <code>x</code> has a log-normal distribution if <code>log(x)</code> is normally
distributed.
The probability density function for the log-normal
distribution is:</p>
<p>[ e^{(-\frac{(ln(x)-\mu)^2}{2\sigma^2})} ]
where :math:<code>\mu</code> is the mean and :math:<code>\sigma</code> is the standard
deviation of the normally distributed logarithm of the variable.
A log-normal distribution results if a random variable is the <em>product</em>
of a large number of independent, identically-distributed variables in
the same way that a normal distribution results if the variable is the
<em>sum</em> of a large number of independent, identically-distributed
variables.</p>
<h2 id="references">References</h2>
<p>.. [1] Limpert, E., Stahel, W. A., and Abbt, M., "Log-normal
Distributions across the Sciences: Keys and Clues,"
BioScience, Vol. 51, No. 5, May, 2001.
<a href="https://stat.ethz.ch/~stahel/lognormal/bioscience.pdf">https://stat.ethz.ch/~stahel/lognormal/bioscience.pdf</a>
.. [2] Reiss, R.D. and Thomas, M., "Statistical Analysis of Extreme
Values," Basel: Birkhauser Verlag, 2001, pp. 31-32.</p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; mu, sigma = 3., 1. # mean and standard deviation
&gt;&gt;&gt; s = np.random.lognormal(mu, sigma, 1000)
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, 100, density=True, align='mid')
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; x = np.linspace(min(bins), max(bins), 10000)
&gt;&gt;&gt; pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))
...        / (x * sigma * np.sqrt(2 * np.pi)))
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; plt.plot(x, pdf, linewidth=2, color='r')
&gt;&gt;&gt; plt.axis('tight')
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Demonstrate that taking the products of random samples from a uniform
distribution can be fit well by a log-normal probability density
function.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; # Generate a thousand samples: each is the product of 100 random
&gt;&gt;&gt; # values, drawn from a normal distribution.
&gt;&gt;&gt; b = []
&gt;&gt;&gt; for i in range(1000):
...    a = 10. + np.random.standard_normal(100)
...    b.append(np.product(a))
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; b = np.array(b) / np.min(b) # scale values to be positive
&gt;&gt;&gt; count, bins, ignored = plt.hist(b, 100, density=True, align='mid')
&gt;&gt;&gt; sigma = np.std(np.log(b))
&gt;&gt;&gt; mu = np.mean(np.log(b))
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; x = np.linspace(min(bins), max(bins), 10000)
&gt;&gt;&gt; pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))
...        / (x * sigma * np.sqrt(2 * np.pi)))
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; plt.plot(x, pdf, color='r', linewidth=2)
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.logseries"><code class="name flex">
<span>def <span class="ident">logseries</span></span>(<span>p, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a logarithmic series distribution.</p>
<p>Samples are drawn from a log series distribution with specified
shape parameter, 0 &lt;= <code>p</code> &lt; 1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.logseries</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Shape parameter for the distribution.
Must be in the range [0, 1).</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>p</code> is a scalar.
Otherwise,
<code>np.array(p).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized logarithmic series distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.logser</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>random.Generator.logseries</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the Log Series distribution is</p>
<p>[
]
where p = probability.</p>
<p>The log series distribution is frequently used to represent species
richness and occurrence, first proposed by Fisher, Corbet, and
Williams in 1943 [2].
It may also be used to model the numbers of
occupants seen in cars [3].</p>
<h2 id="references">References</h2>
<p>.. [1] Buzas, Martin A.; Culver, Stephen J.,
Understanding regional
species diversity through the log series distribution of
occurrences: BIODIVERSITY RESEARCH Diversity &amp; Distributions,
Volume 5, Number 5, September 1999 , pp. 187-195(9).
.. [2] Fisher, R.A,, A.S. Corbet, and C.B. Williams. 1943. The
relation between the number of species and the number of
individuals in a random sample of an animal population.
Journal of Animal Ecology, 12:42-58.
.. [3] D. J. Hand, F. Daly, D. Lunn, E. Ostrowski, A Handbook of Small
Data Sets, CRC Press, 1994.
.. [4] Wikipedia, "Logarithmic distribution",
<a href="https://en.wikipedia.org/wiki/Logarithmic_distribution">https://en.wikipedia.org/wiki/Logarithmic_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; a = .6
&gt;&gt;&gt; s = np.random.logseries(a, 10000)
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s)
</code></pre>
<h1 id="plot-against-distribution">plot against distribution</h1>
<pre><code class="language-python-repl">&gt;&gt;&gt; def logseries(k, p):
...     return -p**k/(k*np.log(1-p))
&gt;&gt;&gt; plt.plot(bins, logseries(bins, a)*count.max()/
...          logseries(bins, a).max(), 'r')
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.multinomial"><code class="name flex">
<span>def <span class="ident">multinomial</span></span>(<span>n, pvals, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a multinomial distribution.</p>
<p>The multinomial distribution is a multivariate generalization of the
binomial distribution.
Take an experiment with one of <code>p</code>
possible outcomes.
An example of such an experiment is throwing a dice,
where the outcome can be 1 through 6.
Each sample drawn from the
distribution represents <code>n</code> such experiments.
Its values,
<code>X_i = [X_0, X_1, ..., X_p]</code>, represent the number of times the
outcome was <code>i</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.multinomial</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of experiments.</dd>
<dt><strong><code>pvals</code></strong> :&ensp;<code>sequence</code> of <code>floats, length p</code></dt>
<dd>Probabilities of each of the <code>p</code> different outcomes.
These
must sum to 1 (however, the last element is always assumed to
account for the remaining probability, as long as
<code>sum(pvals[:-1]) &lt;= 1)</code>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
Default is None, in which case a
single value is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>
<p>The drawn samples, of shape <em>size</em>, if that was provided.
If not,
the shape is <code>(N,)</code>.</p>
<p>In other words, each entry <code>out[i,j,...,:]</code> is an N-dimensional
value drawn from the distribution.</p>
</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.multinomial</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Throw a dice 20 times:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.multinomial(20, [1/6.]*6, size=1)
array([[4, 1, 7, 5, 2, 1]]) # random
</code></pre>
<p>It landed 4 times on 1, once on 2, etc.</p>
<p>Now, throw the dice 20 times, and 20 times again:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.multinomial(20, [1/6.]*6, size=2)
array([[3, 4, 3, 3, 4, 3], # random
       [2, 4, 3, 4, 0, 7]])
</code></pre>
<p>For the first run, we threw 3 times 1, 4 times 2, etc.
For the second,
we threw 2 times 1, 4 times 2, etc.</p>
<p>A loaded die is more likely to land on number 6:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.multinomial(100, [1/7.]*5 + [2/7.])
array([11, 16, 14, 17, 16, 26]) # random
</code></pre>
<p>The probability inputs should be normalized. As an implementation
detail, the value of the last entry is ignored and assumed to take
up any leftover probability mass, but this should not be relied on.
A biased coin which has twice as much weight on one side as on the
other should be sampled like so:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.multinomial(100, [1.0 / 3, 2.0 / 3])  # RIGHT
array([38, 62]) # random
</code></pre>
<p>not like:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.multinomial(100, [1.0, 2.0])  # WRONG
Traceback (most recent call last):
ValueError: pvals &lt; 0, pvals &gt; 1 or pvals contains NaNs
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.multivariate_normal"><code class="name flex">
<span>def <span class="ident">multivariate_normal</span></span>(<span>mean, cov, size=None, check_valid='warn', tol=1e-08)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw random samples from a multivariate normal distribution.</p>
<p>The multivariate normal, multinormal or Gaussian distribution is a
generalization of the one-dimensional normal distribution to higher
dimensions.
Such a distribution is specified by its mean and
covariance matrix.
These parameters are analogous to the mean
(average or "center") and variance (standard deviation, or "width,"
squared) of the one-dimensional normal distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.multivariate_normal</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>1-D array_like,</code> of <code>length N</code></dt>
<dd>Mean of the N-dimensional distribution.</dd>
<dt><strong><code>cov</code></strong> :&ensp;<code>2-D array_like,</code> of <code>shape (N, N)</code></dt>
<dd>Covariance matrix of the distribution. It must be symmetric and
positive-semidefinite for proper sampling.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Given a shape of, for example, <code>(m,n,k)</code>, <code>m*n*k</code> samples are
generated, and packed in an <code>m</code>-by-<code>n</code>-by-<code>k</code> arrangement.
Because
each sample is <code>N</code>-dimensional, the output shape is <code>(m,n,k,N)</code>.
If no shape is specified, a single (<code>N</code>-D) sample is returned.</dd>
<dt><strong><code>check_valid</code></strong> :&ensp;<code>{ 'warn', 'raise', 'ignore' }</code>, optional</dt>
<dd>Behavior when the covariance matrix is not positive semidefinite.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Tolerance when checking the singular values in covariance matrix.
cov is cast to double before the check.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>
<p>The drawn samples, of shape <em>size</em>, if that was provided.
If not,
the shape is <code>(N,)</code>.</p>
<p>In other words, each entry <code>out[i,j,...,:]</code> is an N-dimensional
value drawn from the distribution.</p>
</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.multivariate_normal</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The mean is a coordinate in N-dimensional space, which represents the
location where samples are most likely to be generated.
This is
analogous to the peak of the bell curve for the one-dimensional or
univariate normal distribution.</p>
<p>Covariance indicates the level to which two variables vary together.
From the multivariate normal distribution, we draw N-dimensional
samples, :math:<code>X = [x_1, x_2, ... x_N]</code>.
The covariance matrix
element :math:<code>C_{ij}</code> is the covariance of :math:<code>x_i</code> and :math:<code>x_j</code>.
The element :math:<code>C_{ii}</code> is the variance of :math:<code>x_i</code> (i.e. its
"spread").</p>
<p>Instead of specifying the full covariance matrix, popular
approximations include:</p>
<ul>
<li>Spherical covariance (<code>cov</code> is a multiple of the identity matrix)</li>
<li>Diagonal covariance (<code>cov</code> has non-negative elements, and only on
the diagonal)</li>
</ul>
<p>This geometrical property can be seen in two dimensions by plotting
generated data-points:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; mean = [0, 0]
&gt;&gt;&gt; cov = [[1, 0], [0, 100]]  # diagonal covariance
</code></pre>
<p>Diagonal covariance means that points are oriented along x or y-axis:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; x, y = np.random.multivariate_normal(mean, cov, 5000).T
&gt;&gt;&gt; plt.plot(x, y, 'x')
&gt;&gt;&gt; plt.axis('equal')
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Note that the covariance matrix must be positive semidefinite (a.k.a.
nonnegative-definite). Otherwise, the behavior of this method is
undefined and backwards compatibility is not guaranteed.</p>
<h2 id="references">References</h2>
<p>.. [1] Papoulis, A., "Probability, Random Variables, and Stochastic
Processes," 3rd ed., New York: McGraw-Hill, 1991.
.. [2] Duda, R. O., Hart, P. E., and Stork, D. G., "Pattern
Classification," 2nd ed., New York: Wiley, 2001.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; mean = (1, 2)
&gt;&gt;&gt; cov = [[1, 0], [0, 1]]
&gt;&gt;&gt; x = np.random.multivariate_normal(mean, cov, (3, 3))
&gt;&gt;&gt; x.shape
(3, 3, 2)
</code></pre>
<p>Here we generate 800 samples from the bivariate normal distribution
with mean [0, 0] and covariance matrix [[6, -3], [-3, 3.5]].
The
expected variances of the first and second components of the sample
are 6 and 3.5, respectively, and the expected correlation
coefficient is -3/sqrt(6*3.5) ≈ -0.65465.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; cov = np.array([[6, -3], [-3, 3.5]])
&gt;&gt;&gt; pts = np.random.multivariate_normal([0, 0], cov, size=800)
</code></pre>
<p>Check that the mean, covariance, and correlation coefficient of the
sample are close to the expected values:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; pts.mean(axis=0)
array([ 0.0326911 , -0.01280782])  # may vary
&gt;&gt;&gt; np.cov(pts.T)
array([[ 5.96202397, -2.85602287],
       [-2.85602287,  3.47613949]])  # may vary
&gt;&gt;&gt; np.corrcoef(pts.T)[0, 1]
-0.6273591314603949  # may vary
</code></pre>
<p>We can visualize this data with a scatter plot.
The orientation
of the point cloud illustrates the negative correlation of the
components of this sample.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; plt.plot(pts[:, 0], pts[:, 1], '.', alpha=0.5)
&gt;&gt;&gt; plt.axis('equal')
&gt;&gt;&gt; plt.grid()
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.mutual_information_2d"><code class="name flex">
<span>def <span class="ident">mutual_information_2d</span></span>(<span>x, y, sigma=1, bin=256, normalized=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes (normalized) mutual information between two 1D variate from a
joint histogram.
Parameters</p>
<hr>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>1D array</code></dt>
<dd>first variable</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>1D array</code></dt>
<dd>second variable</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>sigma for Gaussian smoothing of the joint histogram</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mi</code></strong> :&ensp;<code>float</code></dt>
<dd>the computed similarity measure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mutual_information_2d(x, y, sigma=1, bin=256, normalized=False):
    &#34;&#34;&#34;
    Computes (normalized) mutual information between two 1D variate from a
    joint histogram.
    Parameters
    ----------
    x : 1D array
        first variable
    y : 1D array
        second variable
    sigma: float
        sigma for Gaussian smoothing of the joint histogram
    Returns
    -------
    mi: float
        the computed similarity measure
    &#34;&#34;&#34;
    bins = (bin, bin)

    jh = np.histogram2d(x, y, bins=bins)[0]

    # smooth the jh with a gaussian filter of given sigma
    #ndimage.gaussian_filter(jh, sigma=sigma, mode=&#39;constant&#39;,
    #                             output=jh)
    # compute marginal histograms
    jh = jh + EPS
    sh = np.sum(jh)
    jh = jh / sh
    s1 = np.sum(jh, axis=0).reshape((-1, jh.shape[0]))
    s2 = np.sum(jh, axis=1).reshape((jh.shape[1], -1))

    # Normalised Mutual Information of:
    # Studholme,  jhill &amp; jhawkes (1998).
    # &#34;A normalized entropy measure of 3-D medical image alignment&#34;.
    # in Proc. Medical Imaging 1998, vol. 3338, San Diego, CA, pp. 132-143.
    if normalized:
        mi = ((np.sum(s1 * np.log(s1)) + np.sum(s2 * np.log(s2)))
                / np.sum(jh * np.log(jh))) - 1
    else:
        mi = ( np.sum(jh * np.log(jh)) - np.sum(s1 * np.log(s1))
               - np.sum(s2 * np.log(s2)))
    return mi</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.mutual_information_2d_cp"><code class="name flex">
<span>def <span class="ident">mutual_information_2d_cp</span></span>(<span>x, y, sigma=1, bin=256, normalized=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes (normalized) mutual information between two 1D variate from a
joint histogram using CUPY package.
Parameters</p>
<hr>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>1D array</code></dt>
<dd>first variable</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>1D array</code></dt>
<dd>second variable</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>sigma for Gaussian smoothing of the joint histogram</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mi</code></strong> :&ensp;<code>float</code></dt>
<dd>the computed similarity measure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mutual_information_2d_cp(x, y, sigma=1, bin=256, normalized=False):
    &#34;&#34;&#34;
    Computes (normalized) mutual information between two 1D variate from a
    joint histogram using CUPY package.
    Parameters
    ----------
    x : 1D array
        first variable
    y : 1D array
        second variable
    sigma: float
        sigma for Gaussian smoothing of the joint histogram
    Returns
    -------
    mi: float
        the computed similarity measure
    &#34;&#34;&#34;
    bins = (bin, bin)

    jhf = cp.histogram2d(x, y, bins=bins)

    # smooth the jh with a gaussian filter of given sigma
    #ndimage.gaussian_filter(jh, sigma=sigma, mode=&#39;constant&#39;,
    #                             output=jh)
    # compute marginal histograms
    jh = jhf[0] + EPS
    sh = cp.sum(jh)
    jh = jh / sh
    s1 = cp.sum(jh, axis=0).reshape((-1, jh.shape[0]))
    s2 = cp.sum(jh, axis=1).reshape((jh.shape[1], -1))

    # Normalised Mutual Information of:
    # Studholme,  jhill &amp; jhawkes (1998).
    # &#34;A normalized entropy measure of 3-D medical image alignment&#34;.
    # in Proc. Medical Imaging 1998, vol. 3338, San Diego, CA, pp. 132-143.
    if normalized:
        mi = ((cp.sum(s1 * cp.log(s1)) + cp.sum(s2 * cp.log(s2)))
                / cp.sum(jh * cp.log(jh))) - 1
    else:
        mi = ( cp.sum(jh * cp.log(jh)) - np.sum(s1 * cp.log(s1))
               - cp.sum(s2 * cp.log(s2)))
    return mi</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.negative_binomial"><code class="name flex">
<span>def <span class="ident">negative_binomial</span></span>(<span>n, p, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a negative binomial distribution.</p>
<p>Samples are drawn from a negative binomial distribution with specified
parameters, <code>n</code> successes and <code>p</code> probability of success where <code>n</code>
is &gt; 0 and <code>p</code> is in the interval [0, 1].</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.negative_binomial</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Parameter of the distribution, &gt; 0.</dd>
<dt><strong><code>p</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Parameter of the distribution, &gt;= 0 and &lt;=1.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>n</code> and <code>p</code> are both scalars.
Otherwise, <code>np.broadcast(n, p).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized negative binomial distribution,
where each sample is equal to N, the number of failures that
occurred before a total of n successes was reached.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.negative_binomial</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability mass function of the negative binomial distribution is</p>
<p>[
]
where :math:<code>n</code> is the number of successes, :math:<code>p</code> is the
probability of success, :math:<code>N+n</code> is the number of trials, and
:math:<code>\Gamma</code> is the gamma function. When :math:<code>n</code> is an integer,
:math:<code>\frac{\Gamma(N+n)}{N!\Gamma(n)} = \binom{N+n-1}{N}</code>, which is
the more common form of this term in the pmf. The negative
binomial distribution gives the probability of N failures given n
successes, with a success on the last trial.</p>
<p>If one throws a die repeatedly until the third time a "1" appears,
then the probability distribution of the number of non-"1"s that
appear before the third "1" is a negative binomial distribution.</p>
<h2 id="references">References</h2>
<p>.. [1] Weisstein, Eric W. "Negative Binomial Distribution." From
MathWorld&ndash;A Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/NegativeBinomialDistribution.html">http://mathworld.wolfram.com/NegativeBinomialDistribution.html</a>
.. [2] Wikipedia, "Negative binomial distribution",
<a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">https://en.wikipedia.org/wiki/Negative_binomial_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<p>A real world example. A company drills wild-cat oil
exploration wells, each with an estimated probability of
success of 0.1.
What is the probability of having one success
for each successive well, that is what is the probability of a
single success after drilling 5 wells, after 6 wells, etc.?</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; s = np.random.negative_binomial(1, 0.1, 100000)
&gt;&gt;&gt; for i in range(1, 11): # doctest: +SKIP
...    probability = sum(s&lt;i) / 100000.
...    print(i, &quot;wells drilled, probability of one success =&quot;, probability)
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.noncentral_chisquare"><code class="name flex">
<span>def <span class="ident">noncentral_chisquare</span></span>(<span>df, nonc, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a noncentral chi-square distribution.</p>
<p>The noncentral :math:<code>\chi^2</code> distribution is a generalization of
the :math:<code>\chi^2</code> distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.noncentral_chisquare</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>
<p>Degrees of freedom, must be &gt; 0.</p>
<div class="admonition versionchanged">
<p class="admonition-title">Changed in version:&ensp;1.10.0</p>
<p>Earlier NumPy versions required dfnum &gt; 1.</p>
</div>
</dd>
<dt><strong><code>nonc</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Non-centrality, must be non-negative.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>df</code> and <code>nonc</code> are both scalars.
Otherwise, <code>np.broadcast(df, nonc).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized noncentral chi-square distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.noncentral_chisquare</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density function for the noncentral Chi-square
distribution is</p>
<p>[ \frac{e^{-nonc/2}(nonc/2)^{i}}{i!}
P_{Y_{df+2i}}(x), ]
where :math:<code>Y_{q}</code> is the Chi-square with q degrees of freedom.</p>
<h2 id="references">References</h2>
<p>.. [1] Wikipedia, "Noncentral chi-squared distribution"
<a href="https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution">https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw values from the distribution and plot the histogram</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; values = plt.hist(np.random.noncentral_chisquare(3, 20, 100000),
...                   bins=200, density=True)
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Draw values from a noncentral chisquare with very small noncentrality,
and compare to a chisquare.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; plt.figure()
&gt;&gt;&gt; values = plt.hist(np.random.noncentral_chisquare(3, .0000001, 100000),
...                   bins=np.arange(0., 25, .1), density=True)
&gt;&gt;&gt; values2 = plt.hist(np.random.chisquare(3, 100000),
...                    bins=np.arange(0., 25, .1), density=True)
&gt;&gt;&gt; plt.plot(values[1][0:-1], values[0]-values2[0], 'ob')
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Demonstrate how large values of non-centrality lead to a more symmetric
distribution.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; plt.figure()
&gt;&gt;&gt; values = plt.hist(np.random.noncentral_chisquare(3, 20, 100000),
...                   bins=200, density=True)
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.noncentral_f"><code class="name flex">
<span>def <span class="ident">noncentral_f</span></span>(<span>dfnum, dfden, nonc, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from the noncentral F distribution.</p>
<p>Samples are drawn from an F distribution with specified parameters,
<code>dfnum</code> (degrees of freedom in numerator) and <code>dfden</code> (degrees of
freedom in denominator), where both parameters &gt; 1.
<code>nonc</code> is the non-centrality parameter.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.noncentral_f</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dfnum</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>
<p>Numerator degrees of freedom, must be &gt; 0.</p>
<div class="admonition versionchanged">
<p class="admonition-title">Changed in version:&ensp;1.14.0</p>
<p>Earlier NumPy versions required dfnum &gt; 1.</p>
</div>
</dd>
<dt><strong><code>dfden</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Denominator degrees of freedom, must be &gt; 0.</dd>
<dt><strong><code>nonc</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Non-centrality parameter, the sum of the squares of the numerator
means, must be &gt;= 0.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>dfnum</code>, <code>dfden</code>, and <code>nonc</code>
are all scalars.
Otherwise, <code>np.broadcast(dfnum, dfden, nonc).size</code>
samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized noncentral Fisher distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.noncentral_f</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>When calculating the power of an experiment (power = probability of
rejecting the null hypothesis when a specific alternative is true) the
non-central F statistic becomes important.
When the null hypothesis is
true, the F statistic follows a central F distribution. When the null
hypothesis is not true, then it follows a non-central F statistic.</p>
<h2 id="references">References</h2>
<p>.. [1] Weisstein, Eric W. "Noncentral F-Distribution."
From MathWorld&ndash;A Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/NoncentralF-Distribution.html">http://mathworld.wolfram.com/NoncentralF-Distribution.html</a>
.. [2] Wikipedia, "Noncentral F-distribution",
<a href="https://en.wikipedia.org/wiki/Noncentral_F-distribution">https://en.wikipedia.org/wiki/Noncentral_F-distribution</a></p>
<h2 id="examples">Examples</h2>
<p>In a study, testing for a specific alternative to the null hypothesis
requires use of the Noncentral F distribution. We need to calculate the
area in the tail of the distribution that exceeds the value of the F
distribution for the null hypothesis.
We'll plot the two probability
distributions for comparison.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dfnum = 3 # between group deg of freedom
&gt;&gt;&gt; dfden = 20 # within groups degrees of freedom
&gt;&gt;&gt; nonc = 3.0
&gt;&gt;&gt; nc_vals = np.random.noncentral_f(dfnum, dfden, nonc, 1000000)
&gt;&gt;&gt; NF = np.histogram(nc_vals, bins=50, density=True)
&gt;&gt;&gt; c_vals = np.random.f(dfnum, dfden, 1000000)
&gt;&gt;&gt; F = np.histogram(c_vals, bins=50, density=True)
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; plt.plot(F[1][1:], F[0])
&gt;&gt;&gt; plt.plot(NF[1][1:], NF[0])
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.normal"><code class="name flex">
<span>def <span class="ident">normal</span></span>(<span>loc=0.0, scale=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw random samples from a normal (Gaussian) distribution.</p>
<p>The probability density function of the normal distribution, first
derived by De Moivre and 200 years later by both Gauss and Laplace
independently [2]_, is often called the bell curve because of
its characteristic shape (see the example below).</p>
<p>The normal distributions occurs often in nature.
For example, it
describes the commonly occurring distribution of samples influenced
by a large number of tiny, random disturbances, each with its own
unique distribution [2]_.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.normal</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loc</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Mean ("centre") of the distribution.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Standard deviation (spread or "width") of the distribution. Must be
non-negative.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>loc</code> and <code>scale</code> are both scalars.
Otherwise, <code>np.broadcast(loc, scale).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized normal distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.norm</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>random.Generator.normal</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the Gaussian distribution is</p>
<p>[ e^{ - \frac{ (x - \mu)^2 } {2 \sigma^2} }, ]
where :math:<code>\mu</code> is the mean and :math:<code>\sigma</code> the standard
deviation. The square of the standard deviation, :math:<code>\sigma^2</code>,
is called the variance.</p>
<p>The function has its peak at the mean, and its "spread" increases with
the standard deviation (the function reaches 0.607 times its maximum at
:math:<code>x + \sigma</code> and :math:<code>x - \sigma</code> [2]_).
This implies that
normal is more likely to return samples lying close to the mean, rather
than those far away.</p>
<h2 id="references">References</h2>
<p>.. [1] Wikipedia, "Normal distribution",
<a href="https://en.wikipedia.org/wiki/Normal_distribution">https://en.wikipedia.org/wiki/Normal_distribution</a>
.. [2] P. R. Peebles Jr., "Central Limit Theorem" in "Probability,
Random Variables and Random Signal Principles", 4th ed., 2001,
pp. 51, 51, 125.</p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; mu, sigma = 0, 0.1 # mean and standard deviation
&gt;&gt;&gt; s = np.random.normal(mu, sigma, 1000)
</code></pre>
<p>Verify the mean and the variance:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; abs(mu - np.mean(s))
0.0  # may vary
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; abs(sigma - np.std(s, ddof=1))
0.1  # may vary
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, 30, density=True)
&gt;&gt;&gt; plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
...          linewidth=2, color='r')
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Two-by-four array of samples from the normal distribution with
mean 3 and standard deviation 2.5:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.normal(3, 2.5, size=(2, 4))
array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random
       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.pareto"><code class="name flex">
<span>def <span class="ident">pareto</span></span>(<span>a, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Pareto II or Lomax distribution with
specified shape.</p>
<p>The Lomax or Pareto II distribution is a shifted Pareto
distribution. The classical Pareto distribution can be
obtained from the Lomax distribution by adding 1 and
multiplying by the scale parameter <code>m</code> (see Notes).
The
smallest value of the Lomax distribution is zero while for the
classical Pareto distribution it is <code>mu</code>, where the standard
Pareto distribution has location <code>mu = 1</code>.
Lomax can also
be considered as a simplified version of the Generalized
Pareto distribution (available in SciPy), with the scale set
to one and the location set to zero.</p>
<p>The Pareto distribution must be greater than zero, and is
unbounded above.
It is also known as the "80-20 rule".
In
this distribution, 80 percent of the weights are in the lowest
20 percent of the range, while the other 20 percent fill the
remaining 80 percent of the range.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.pareto</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Shape of the distribution. Must be positive.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>a</code> is a scalar.
Otherwise,
<code>np.array(a).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized Pareto distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.lomax</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>scipy.stats.genpareto</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>random.Generator.pareto</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the Pareto distribution is</p>
<p>[
]
where :math:<code>a</code> is the shape and :math:<code>m</code> the scale.</p>
<p>The Pareto distribution, named after the Italian economist
Vilfredo Pareto, is a power law probability distribution
useful in many real world problems.
Outside the field of
economics it is generally referred to as the Bradford
distribution. Pareto developed the distribution to describe
the distribution of wealth in an economy.
It has also found
use in insurance, web page access statistics, oil field sizes,
and many other problems, including the download frequency for
projects in Sourceforge [1]_.
It is one of the so-called
"fat-tailed" distributions.</p>
<h2 id="references">References</h2>
<p>.. [1] Francis Hunt and Paul Johnson, On the Pareto Distribution of
Sourceforge projects.
.. [2] Pareto, V. (1896). Course of Political Economy. Lausanne.
.. [3] Reiss, R.D., Thomas, M.(2001), Statistical Analysis of Extreme
Values, Birkhauser Verlag, Basel, pp 23-30.
.. [4] Wikipedia, "Pareto distribution",
<a href="https://en.wikipedia.org/wiki/Pareto_distribution">https://en.wikipedia.org/wiki/Pareto_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; a, m = 3., 2.  # shape and mode
&gt;&gt;&gt; s = (np.random.pareto(a, 1000) + 1) * m
</code></pre>
<p>Display the histogram of the samples, along with the probability
density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, _ = plt.hist(s, 100, density=True)
&gt;&gt;&gt; fit = a*m**a / bins**(a+1)
&gt;&gt;&gt; plt.plot(bins, max(count)*fit/max(fit), linewidth=2, color='r')
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.permutation"><code class="name flex">
<span>def <span class="ident">permutation</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Randomly permute a sequence, or return a permuted range.</p>
<p>If <code>x</code> is a multi-dimensional array, it is only shuffled along its
first index.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.permutation</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>int</code> or <code>array_like</code></dt>
<dd>If <code>x</code> is an integer, randomly permute <code>np.arange(x)</code>.
If <code>x</code> is an array, make a copy and shuffle the elements
randomly.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Permuted sequence or array range.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.permutation</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.permutation(10)
array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6]) # random
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.permutation([1, 4, 9, 12, 15])
array([15,  1,  9,  4, 12]) # random
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; arr = np.arange(9).reshape((3, 3))
&gt;&gt;&gt; np.random.permutation(arr)
array([[6, 7, 8], # random
       [0, 1, 2],
       [3, 4, 5]])
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.plot_registrtion_quality_xlsx"><code class="name flex">
<span>def <span class="ident">plot_registrtion_quality_xlsx</span></span>(<span>data_files, labels, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Read and plot together multiple registration quality summaries.
©G.Shtengel, 04/2021. gleb.shtengel@gmail.com</p>
<p>Parameters:
data_files : array of str
Filenames (full paths) of the registration summaries (*.xlsx files)
labels : array of str
Labels (for each registration)</p>
<p>kwargs:
frame_inds : array or list of int
Array or list oif frame indecis to use to azalyze the data.
save_res_png : boolean
If True, the PNG's of summary plots as well as summary Excel notebook are saved
save_filename : str
Filename (full path) to save the results (default is data_dir +'Regstration_Summary.png')
nsad_bounds : list of floats
Bounds for NSAD plot (default is determined by get_min_max_thresholds with thresholds of 1e-4)
ncc_bounds : list of floats
Bounds for NCC plot (default is determined by get_min_max_thresholds with thresholds of 1e-4)
nmi_bounds : list of floats
Bounds for NMI plot (default is determined by get_min_max_thresholds with thresholds of 1e-4)
colors : array or list of colors
Optional colors for each plot/file. If not provided, will be auto-generated.
linewidths : array of float
linewidths for individual files. If not provided, all linewidts are set to 0.5</p>
<p>Returns
xlsx_fname : str
Filename of the summary Excel notebook</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_registrtion_quality_xlsx(data_files, labels, **kwargs):
    &#39;&#39;&#39;
    Read and plot together multiple registration quality summaries.
    ©G.Shtengel, 04/2021. gleb.shtengel@gmail.com

    Parameters:
    data_files : array of str
        Filenames (full paths) of the registration summaries (*.xlsx files)
    labels : array of str
        Labels (for each registration)

    kwargs:
    frame_inds : array or list of int
        Array or list oif frame indecis to use to azalyze the data.
    save_res_png : boolean
        If True, the PNG&#39;s of summary plots as well as summary Excel notebook are saved
    save_filename : str
        Filename (full path) to save the results (default is data_dir +&#39;Regstration_Summary.png&#39;)
    nsad_bounds : list of floats
        Bounds for NSAD plot (default is determined by get_min_max_thresholds with thresholds of 1e-4)
    ncc_bounds : list of floats
        Bounds for NCC plot (default is determined by get_min_max_thresholds with thresholds of 1e-4)
    nmi_bounds : list of floats
        Bounds for NMI plot (default is determined by get_min_max_thresholds with thresholds of 1e-4)
    colors : array or list of colors
        Optional colors for each plot/file. If not provided, will be auto-generated.
    linewidths : array of float
        linewidths for individual files. If not provided, all linewidts are set to 0.5

    Returns
    xlsx_fname : str
        Filename of the summary Excel notebook
    &#39;&#39;&#39;
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    linewidths = kwargs.get(&#34;linewidths&#34;, np.ones(len(data_files))*0.5)
    data_dir = os.path.split(data_files[0])[0]
    default_save_filename = os.path.join(data_dir, &#39;Regstration_Summary.png&#39;)
    save_filename = kwargs.get(&#34;save_filename&#34;, default_save_filename)
    nsad_bounds = kwargs.get(&#34;nsad_bounds&#34;, [0.0, 0.0])
    ncc_bounds = kwargs.get(&#34;ncc_bounds&#34;, [0.0, 0.0])
    nmi_bounds = kwargs.get(&#34;nmi_bounds&#34;, [0.0, 0.0])
    frame_inds = kwargs.get(&#34;frame_inds&#34;, [])

    nfls = len(data_files)
    reg_datas = []
    for data_file in data_files:
        # fl = os.path.join(data_dir, df)
        # data = pd.read_csv(fl)
        data = pd.read_excel(data_file, sheet_name=&#39;Registration Quality Statistics&#39;)
        reg_datas.append(data)

    lw0 = 0.5
    lw1 = 1

    fs=12
    fs2=10
    fig1, axs1 = subplots(3,1, figsize=(7, 11), sharex=True)
    fig1.subplots_adjust(left=0.1, bottom=0.05, right=0.99, top=0.96, wspace=0.2, hspace=0.1)

    ax_nsad = axs1[0]
    ax_ncc = axs1[1]
    ax_nmi = axs1[2]
    ax_nsad.set_ylabel(&#39;Normalized Sum of Abs. Differences&#39;, fontsize=fs)
    ax_ncc.set_ylabel(&#39;Normalized Cross-Correlation&#39;, fontsize=fs)
    ax_nmi.set_ylabel(&#39;Normalized Mutual Information&#39;, fontsize=fs)
    ax_nmi.set_xlabel(&#39;Frame&#39;, fontsize=fs)

    spreads=[]
    my_cols = [get_cmap(&#34;gist_rainbow_r&#34;)((nfls-j)/(nfls)) for j in np.arange(nfls)]
    my_cols[0] = &#39;grey&#39;
    my_cols[-1] = &#39;red&#39;
    my_cols = kwargs.get(&#34;colors&#34;, my_cols)

    means = []
    image_nsads= []
    image_nccs= []
    image_snrs= []
    image_nmis = []
    frame_inds_glob = []
    for j, reg_data in enumerate(tqdm(reg_datas, desc=&#39;generating the registration quality summary plots&#39;)):
        #my_col = get_cmap(&#34;gist_rainbow_r&#34;)((nfls-j)/(nfls))
        #my_cols.append(my_col)
        my_col = my_cols[j]
        pf = labels[j]
        lw0 = linewidths[j]
        if len(frame_inds)&gt;0:
            try:
                image_nsad = np.array(reg_data[&#39;Image NSAD&#39;])[frame_inds]
                image_ncc = np.array(reg_data[&#39;Image NCC&#39;])[frame_inds]
                image_nmi = np.array(reg_data[&#39;Image MI&#39;])[frame_inds]
            except:
                image_nsad = np.array(reg_data[&#39;NSAD&#39;])[frame_inds]
                image_ncc = np.array(reg_data[&#39;NCC&#39;])[frame_inds]
                image_nmi = np.array(reg_data[&#39;NMI&#39;])[frame_inds]
            frame_inds_loc = frame_inds.copy()
        else:
            try:
                image_nsad = np.array(reg_data[&#39;Image NSAD&#39;])
                image_ncc = np.array(reg_data[&#39;Image NCC&#39;])
                image_nmi = np.array(reg_data[&#39;Image MI&#39;])
            except:
                image_nsad = np.array(reg_data[&#39;NSAD&#39;])
                image_ncc = np.array(reg_data[&#39;NCC&#39;])
                image_nmi = np.array(reg_data[&#39;NMI&#39;])
            frame_inds_loc = np.arange(len(image_ncc))
        fr_i = min(frame_inds_loc) - (max(frame_inds_loc) - min(frame_inds_loc))*0.05
        fr_a = max(frame_inds_loc) + (max(frame_inds_loc) - min(frame_inds_loc))*0.05
        image_nsads.append(image_nsad)
        image_nccs.append(image_ncc)
        image_snr = image_ncc/(1.0-image_ncc)
        image_snrs.append(image_snr)
        image_nmis.append(image_nmi)
        frame_inds_glob.append(frame_inds_loc)

        eval_metrics = [image_nsad, image_ncc, image_snr, image_nmi]
        spreads.append([get_spread(metr) for metr in eval_metrics])
        means.append([np.mean(metr) for metr in eval_metrics])

        ax_nsad.plot(frame_inds_loc, image_nsad, c=my_col, linewidth=lw0)
        ax_nsad.plot(image_nsad[0], c=my_col, linewidth=lw1, label=pf)
        ax_ncc.plot(frame_inds_loc, image_ncc, c=my_col, linewidth=lw0)
        ax_ncc.plot(image_ncc[0], c=my_col, linewidth=lw1, label=pf)
        ax_nmi.plot(frame_inds_loc, image_nmi, c=my_col, linewidth=lw0)
        ax_nmi.plot(image_nmi[0], c=my_col, linewidth=lw1, label=pf)

    for ax in axs1.ravel():
        ax.grid(True)
        ax.legend(fontsize=fs2)

    if nsad_bounds[0]==nsad_bounds[1]:
        nsad_min, nsad_max = get_min_max_thresholds(np.concatenate(image_nsads),
                                                    thr_min=1e-4, thr_max=1e-4,
                                                    nbins=256, disp_res=False)
    else:
        nsad_min, nsad_max = nsad_bounds
    ax_nsad.set_ylim(nsad_min, nsad_max)

    if ncc_bounds[0]==ncc_bounds[1]:
        ncc_min, ncc_max = get_min_max_thresholds(np.concatenate(image_nccs),
                                                    thr_min=1e-4, thr_max=1e-4,
                                                    nbins=256, disp_res=False)
    else:
        ncc_min, ncc_max = ncc_bounds
    ax_ncc.set_ylim(ncc_min, ncc_max)

    if nmi_bounds[0]==nmi_bounds[1]:
        nmi_min, nmi_max = get_min_max_thresholds(np.concatenate(image_nmis),
                                                    thr_min=1e-4, thr_max=1e-4,
                                                    nbins=256, disp_res=False)
    else:
        nmi_min, nmi_max = nmi_bounds
    ax_nmi.set_ylim(nmi_min, nmi_max)
    ax_nmi.set_xlim(fr_i, fr_a)


    ax_nsad.text(-0.05, 1.05, data_dir, transform=ax_nsad.transAxes, fontsize=10)
    if save_res_png:
        fig1.savefig(save_filename, dpi=300)

    # Generate the Cell Text
    cell_text = []
    fig2_data = []
    limits = []
    rows = labels
    fst=9

    for j, (mean, spread) in enumerate(zip(means, spreads)):
        cell_text.append([&#39;{:.4f}&#39;.format(mean[0]), &#39;{:.4f}&#39;.format(spread[0]),
                          &#39;{:.4f}&#39;.format(mean[1]), &#39;{:.4f}&#39;.format(spread[1]), &#39;{:.4f}&#39;.format(mean[2]),
                          &#39;{:.4f}&#39;.format(mean[3]), &#39;{:.4f}&#39;.format(spread[3])])
        fig2_data.append([mean[0], spread[0], mean[1], spread[1], mean[2], mean[3], spread[3]])

    # Generate the table
    fig2, ax = subplots(1, 1, figsize=(9.5,1.3))
    fig2.subplots_adjust(left=0.32, bottom=0.01, right=0.98, top=0.86, wspace=0.05, hspace=0.05)
    ax.axis(False)
    ax.text(-0.30, 1.07, &#39;SIFT Registration Comparisons:  &#39; + data_dir, fontsize=fst)
    llw1=0.3
    clw = [llw1, llw1]

    columns = [&#39;NSAD Mean&#39;, &#39;NSAD Spread&#39;, &#39;NCC Mean&#39;, &#39;NCC Spread&#39;, &#39;Mean SNR&#39;, &#39;NMI Mean&#39;, &#39;NMI Spread&#39;]

    n_cols = len(columns)
    n_rows = len(rows)

    tbl = ax.table(cellText = cell_text,
                   rowLabels = rows,
                   colLabels = columns,
                   cellLoc = &#39;center&#39;,
                   colLoc = &#39;center&#39;,
                   bbox = [0.01, 0, 0.995, 1.0],
                  fontsize=16)
    tbl.auto_set_column_width(col=3)

    table_props = tbl.properties()
    try:
        table_cells = table_props[&#39;child_artists&#39;]
    except:
        table_cells = table_props[&#39;children&#39;]

    tbl.auto_set_font_size(False)
    for j, cell in enumerate(table_cells[0:n_cols*n_rows]):
        cell.get_text().set_color(my_cols[j//n_cols])
        cell.get_text().set_fontsize(fst)
    for j, cell in enumerate(table_cells[n_cols*(n_rows+1):]):
        cell.get_text().set_color(my_cols[j])
    for cell in table_cells[n_cols*n_rows:]:
    #    cell.get_text().set_fontweight(&#39;bold&#39;)
        cell.get_text().set_fontsize(fst)
    save_filename2 = save_filename.replace(&#39;.png&#39;, &#39;_table.png&#39;)
    if save_res_png:
        fig2.savefig(save_filename2, dpi=300)

    ysize_fig = 4
    ysize_tbl = 0.25 * nfls
    fst3 = 8
    fig3, axs3 = subplots(2, 1, figsize=(7, ysize_fig+ysize_tbl),  gridspec_kw={&#34;height_ratios&#34; : [ysize_tbl, ysize_fig]})
    fig3.subplots_adjust(left=0.10, bottom=0.10, right=0.98, top=0.96, wspace=0.05, hspace=0.05)

    for j, reg_data in enumerate(reg_datas):
        my_col = my_cols[j]
        pf = labels[j]
        lw0 = linewidths[j]
        if len(frame_inds)&gt;0:
            try:
                image_ncc = np.array(reg_data[&#39;Image NCC&#39;])[frame_inds]
            except:
                image_ncc = np.array(reg_data[&#39;NCC&#39;])[frame_inds]
            frame_inds_loc = frame_inds.copy()
        else:
            try:
                image_ncc = np.array(reg_data[&#39;Image NCC&#39;])
            except:
                image_ncc = np.array(reg_data[&#39;NCC&#39;])
            frame_inds_loc = np.arange(len(image_ncc))

        axs3[1].plot(frame_inds_loc, image_ncc, c=my_col, linewidth=lw0)
        axs3[1].plot(image_ncc[0], c=my_col, linewidth=lw1, label=pf)
    axs3[1].grid(True)
    axs3[1].legend(fontsize=fs2)
    axs3[1].set_ylabel(&#39;Normalized Cross-Correlation&#39;, fontsize=fs)
    axs3[1].set_xlabel(&#39;Frame&#39;, fontsize=fs)
    axs3[1].set_ylim(ncc_min, ncc_max)
    axs3[1].set_xlim(fr_i, fr_a)
    axs3[0].axis(False)
    axs3[0].text(-0.1, 1.07, &#39;SIFT Registration Comparisons:  &#39; + data_dir, fontsize=fst3)
    llw1=0.3
    clw = [llw1, llw1]

    columns2 = [&#39;NCC Mean&#39;, &#39;NCC Spread&#39;, &#39;SNR Mean&#39;, &#39;SNR Spread&#39;]
    cell2_text = []
    fig3_data = []
    limits = []
    rows = labels

    for j, (mean, spread) in enumerate(zip(means, spreads)):
        cell2_text.append([&#39;{:.4f}&#39;.format(mean[1]), &#39;{:.4f}&#39;.format(spread[1]),
                          &#39;{:.4f}&#39;.format(mean[2]), &#39;{:.4f}&#39;.format(spread[2])])
    n_cols = len(columns2)
    n_rows = len(rows)

    tbl2 = axs3[0].table(cellText = cell2_text,
                   rowLabels = rows,
                   colLabels = columns2,
                   cellLoc = &#39;center&#39;,
                   colLoc = &#39;center&#39;,
                   bbox = [0.38, 0, 0.55, 1.0],  # (left, bottom, width, height)
                  fontsize=16)
    rl = max([len(pf) for pf in labels])
    #tbl2.auto_set_column_width(col=[rl+5, len(columns2[0]), len(columns2[1]), len(columns2[2]), len(columns2[3])])
    tbl2.auto_set_column_width(col=list(range(len(columns2)+1)))
    tbl2.auto_set_font_size(False)

    table2_props = tbl2.properties()
    try:
        table2_cells = table2_props[&#39;child_artists&#39;]
    except:
        table2_cells = table2_props[&#39;children&#39;]

    tbl.auto_set_font_size(False)
    for j, cell in enumerate(table2_cells[0:n_cols*n_rows]):
        cell.get_text().set_color(my_cols[j//n_cols])
        cell.get_text().set_fontsize(fst3)
    for j, cell in enumerate(table2_cells[n_cols*(n_rows+1):]):
        cell.get_text().set_color(my_cols[j])
    for cell in table2_cells[n_cols*n_rows:]:
    #    cell.get_text().set_fontweight(&#39;bold&#39;)
        cell.get_text().set_fontsize(fst3)
    save_filename3 = save_filename.replace(&#39;.png&#39;, &#39;_fig_and_table.png&#39;)
    if save_res_png:
        fig3.savefig(save_filename3, dpi=300)

    if save_res_png:
        # Generate a single multi-page CSV file
        xlsx_fname = save_filename.replace(&#39;.png&#39;, &#39;.xlsx&#39;)
        # Create a Pandas Excel writer using XlsxWriter as the engine.
        writer = pd.ExcelWriter(xlsx_fname, engine=&#39;xlsxwriter&#39;)
        fig2_df = pd.DataFrame(fig2_data, columns=columns)
        fig2_df.insert(0, &#39;&#39;, labels)
        fig2_df.insert(1, &#39;Path&#39;, data_files)
        fig2_df.to_excel(writer, index=None, sheet_name=&#39;Summary&#39;)
        for reg_data, label in zip(tqdm(reg_datas, desc=&#39;saving the data into xlsx file&#39;), labels):
            data_fn = label[0:31]
            reg_data.to_excel(writer, sheet_name=data_fn)
        writer.save()
    return xlsx_fname</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.poisson"><code class="name flex">
<span>def <span class="ident">poisson</span></span>(<span>lam=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Poisson distribution.</p>
<p>The Poisson distribution is the limit of the binomial distribution
for large N.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.poisson</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lam</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Expected number of events occurring in a fixed-time interval,
must be &gt;= 0. A sequence must be broadcastable over the requested
size.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>lam</code> is a scalar. Otherwise,
<code>np.array(lam).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized Poisson distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.poisson</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The Poisson distribution</p>
<p>[
]
For events with an expected separation :math:<code>\lambda</code> the Poisson
distribution :math:<code>f(k; \lambda)</code> describes the probability of
:math:<code>k</code> events occurring within the observed
interval :math:<code>\lambda</code>.</p>
<p>Because the output is limited to the range of the C int64 type, a
ValueError is raised when <code>lam</code> is within 10 sigma of the maximum
representable value.</p>
<h2 id="references">References</h2>
<p>.. [1] Weisstein, Eric W. "Poisson Distribution."
From MathWorld&ndash;A Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/PoissonDistribution.html">http://mathworld.wolfram.com/PoissonDistribution.html</a>
.. [2] Wikipedia, "Poisson distribution",
<a href="https://en.wikipedia.org/wiki/Poisson_distribution">https://en.wikipedia.org/wiki/Poisson_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; s = np.random.poisson(5, 10000)
</code></pre>
<p>Display histogram of the sample:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, 14, density=True)
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Draw each 100 values for lambda 100 and 500:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; s = np.random.poisson(lam=(100., 500.), size=(100, 2))
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.power"><code class="name flex">
<span>def <span class="ident">power</span></span>(<span>a, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draws samples in [0, 1] from a power distribution with positive
exponent a - 1.</p>
<p>Also known as the power function distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.power</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Parameter of the distribution. Must be non-negative.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>a</code> is a scalar.
Otherwise,
<code>np.array(a).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized power distribution.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If a &lt;= 0.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.power</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density function is</p>
<p>[
]
The power function distribution is just the inverse of the Pareto
distribution. It may also be seen as a special case of the Beta
distribution.</p>
<p>It is used, for example, in modeling the over-reporting of insurance
claims.</p>
<h2 id="references">References</h2>
<p>.. [1] Christian Kleiber, Samuel Kotz, "Statistical size distributions
in economics and actuarial sciences", Wiley, 2003.
.. [2] Heckert, N. A. and Filliben, James J. "NIST Handbook 148:
Dataplot Reference Manual, Volume 2: Let Subcommands and Library
Functions", National Institute of Standards and Technology
Handbook Series, June 2003.
<a href="https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/powpdf.pdf">https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/powpdf.pdf</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; a = 5. # shape
&gt;&gt;&gt; samples = 1000
&gt;&gt;&gt; s = np.random.power(a, samples)
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, bins=30)
&gt;&gt;&gt; x = np.linspace(0, 1, 100)
&gt;&gt;&gt; y = a*x**(a-1.)
&gt;&gt;&gt; normed_y = samples*np.diff(bins)[0]*y
&gt;&gt;&gt; plt.plot(x, normed_y)
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Compare the power function distribution to the inverse of the Pareto.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; from scipy import stats # doctest: +SKIP
&gt;&gt;&gt; rvs = np.random.power(5, 1000000)
&gt;&gt;&gt; rvsp = np.random.pareto(5, 1000000)
&gt;&gt;&gt; xx = np.linspace(0,1,100)
&gt;&gt;&gt; powpdf = stats.powerlaw.pdf(xx,5)  # doctest: +SKIP
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; plt.figure()
&gt;&gt;&gt; plt.hist(rvs, bins=50, density=True)
&gt;&gt;&gt; plt.plot(xx,powpdf,'r-')  # doctest: +SKIP
&gt;&gt;&gt; plt.title('np.random.power(5)')
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; plt.figure()
&gt;&gt;&gt; plt.hist(1./(1.+rvsp), bins=50, density=True)
&gt;&gt;&gt; plt.plot(xx,powpdf,'r-')  # doctest: +SKIP
&gt;&gt;&gt; plt.title('inverse of 1 + np.random.pareto(5)')
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; plt.figure()
&gt;&gt;&gt; plt.hist(1./(1.+rvsp), bins=50, density=True)
&gt;&gt;&gt; plt.plot(xx,powpdf,'r-')  # doctest: +SKIP
&gt;&gt;&gt; plt.title('inverse of stats.pareto(5)')
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.process_transf_matrix"><code class="name flex">
<span>def <span class="ident">process_transf_matrix</span></span>(<span>transformation_matrix, FOVtrend_x, FOVtrend_y, fnms_matches, npts, error_abs_mean, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_transf_matrix(transformation_matrix, FOVtrend_x, FOVtrend_y, fnms_matches, npts, error_abs_mean, **kwargs):
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)

    l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
    l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
    l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
    l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
    solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
    drmax = kwargs.get(&#34;drmax&#34;, 2.0)
    max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)

    preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, True)  # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
    fit_params =  kwargs.get(&#34;fit_params&#34;, False)           # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                            # window size 701, polynomial order 3
    subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # The linear slopes along X- and Y- directions (respectively) will be subtracted from the cumulative shifts.
    subtract_FOVtrend_from_fit = kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])

    #print(&#34;subtract_linear_fit:&#34;, subtract_linear_fit)
    pad_edges =  kwargs.get(&#34;pad_edges&#34;, True)

    tr_matr_cum = transformation_matrix.copy()
    prev_mt = np.eye(3,3)
    for j, cur_mt in enumerate(tqdm(transformation_matrix, desc=&#39;Calculating Cummilative Transformation Matrix&#39;)):
        if any(np.isnan(cur_mt)):
            print(&#39;Frame: {:d} has ill-defined transformation matrix, will use identity transformation instead:&#39;.format(j))
            print(cur_mt)
        else:
            prev_mt = np.matmul(cur_mt, prev_mt)
        tr_matr_cum[j] = prev_mt
    # Now insert identity matrix for the zero frame which does not need to be trasformed

    tr_matr_cum = np.insert(tr_matr_cum, 0, np.eye(3,3), axis=0)

    fr = np.arange(0, len(tr_matr_cum), dtype=np.double)
    s00_cum_orig = tr_matr_cum[:, 0, 0].astype(np.double)
    s01_cum_orig = tr_matr_cum[:, 0, 1].astype(np.double)
    s10_cum_orig = tr_matr_cum[:, 1, 0].astype(np.double)
    s11_cum_orig = tr_matr_cum[:, 1, 1].astype(np.double)
    Xshift_cum_orig = tr_matr_cum[:, 0, 2].astype(np.double)
    Yshift_cum_orig = tr_matr_cum[:, 1, 2].astype(np.double)

    if preserve_scales:  # in case of ScaleShift Transform WITH scale perservation
        #print(&#39;Recalculating the transformation matrix for preserved scales&#39;)
        tr_matr_cum, s_fits = find_fit(tr_matr_cum, fit_params)
        s00_fit, s01_fit, s10_fit, s11_fit = s_fits
        txs = np.zeros(len(tr_matr_cum), dtype=float)
        tys = np.zeros(len(tr_matr_cum), dtype=float)

        for j, fnm_matches in enumerate(tqdm(fnms_matches, desc=&#39;Recalculating the shifts for preserved scales: &#39;)):
            try:
                src_pts, dst_pts = pickle.load(open(fnm_matches, &#39;rb&#39;))

                txs[j+1] = np.mean(tr_matr_cum[j, 0, 0] * dst_pts[:, 0] + tr_matr_cum[j, 0, 1] * dst_pts[:, 1]
                                   - tr_matr_cum[j+1, 0, 0] * src_pts[:, 0] - tr_matr_cum[j+1, 0, 1] * src_pts[:, 1])
                tys[j+1] = np.mean(tr_matr_cum[j, 1, 1] * dst_pts[:, 1] + tr_matr_cum[j, 1, 0] * dst_pts[:, 0]
                                   - tr_matr_cum[j+1, 1, 1] * src_pts[:, 1] - tr_matr_cum[j+1, 1, 0] * src_pts[:, 0])
            except:
                txs[j+1] = 0.0
                tys[j+1] = 0.0
        txs_cum = np.cumsum(txs)
        tys_cum = np.cumsum(tys)
        tr_matr_cum[:, 0, 2] = txs_cum
        tr_matr_cum[:, 1, 2] = tys_cum

    Xshift_cum = tr_matr_cum[:, 0, 2].copy()
    Yshift_cum = tr_matr_cum[:, 1, 2].copy()

    # Subtract linear trends from offsets
    if subtract_linear_fit[0]:
        fr = np.arange(0, len(Xshift_cum))
        if subtract_FOVtrend_from_fit[0]:
            pX = np.polyfit(fr, Xshift_cum+FOVtrend_x, 1)
        else:
            pX = np.polyfit(fr, Xshift_cum, 1)
        Xfit = np.polyval(pX, fr)
        Xshift_residual = Xshift_cum - Xfit
        #Xshift_residual0 = -np.polyval(pX, 0.0)
    else:
        Xshift_residual = Xshift_cum.copy()
        Xfit = np.zeros(len(Xshift_cum))

    if subtract_linear_fit[1]:
        fr = np.arange(0, len(Yshift_cum))
        if subtract_FOVtrend_from_fit[1]:
            pY = np.polyfit(fr, Yshift_cum+FOVtrend_y, 1)
        else:
            pY = np.polyfit(fr, Yshift_cum, 1)
        Yfit = np.polyval(pY, fr)
        Yshift_residual = Yshift_cum - Yfit
        #Yshift_residual0 = -np.polyval(pY, 0.0)
    else:
        Yshift_residual = Yshift_cum.copy()
        Yfit = np.zeros(len(Yshift_cum))

    # define new cumulative transformation matrix where the offests may have linear slopes subtracted
    tr_matr_cum[:, 0, 2] = Xshift_residual
    tr_matr_cum[:, 1, 2] = Yshift_residual

    # save the data
    default_bin_file = os.path.join(data_dir, fnm_reg.replace(&#39;.mrc&#39;, &#39;_transf_matrix.bin&#39;))
    transf_matrix_bin_file = kwargs.get(&#34;dump_filename&#34;, default_bin_file)
    transf_matrix_xlsx_file = default_bin_file.replace(&#39;.bin&#39;, &#39;.xlsx&#39;)

    xlsx_writer = pd.ExcelWriter(transf_matrix_xlsx_file, engine=&#39;xlsxwriter&#39;)
    columns=[&#39;T00 (Sxx)&#39;, &#39;T01 (Sxy)&#39;, &#39;T02 (Tx)&#39;,
                 &#39;T10 (Syx)&#39;, &#39;T11 (Syy)&#39;, &#39;T12 (Ty)&#39;,
                 &#39;T20 (0.0)&#39;, &#39;T21 (0.0)&#39;, &#39;T22 (1.0)&#39;]
    tr_mx_dt = pd.DataFrame(transformation_matrix.reshape((len(transformation_matrix), 9)), columns = columns, index = None)
    tr_mx_dt.to_excel(xlsx_writer, index=None, sheet_name=&#39;Orig. Transformation Matrix&#39;)

    tr_mx_cum_dt = pd.DataFrame(tr_matr_cum.reshape((len(tr_matr_cum), 9)), columns = columns, index = None)
    tr_mx_cum_dt.to_excel(xlsx_writer, index=None, sheet_name=&#39;Cum. Transformation Matrix&#39;)

    columns_shifts=[&#39;s00_cum_orig&#39;, &#39;s00_fit&#39;, &#39;s11_cum_orig&#39;, &#39;s11_fit&#39;, &#39;s01_cum_orig&#39;, &#39;s01_fit&#39;, &#39;s10_cum_orig&#39;, &#39;s10_fit&#39;, &#39;Xshift_cum_orig&#39;, &#39;Yshift_cum_orig&#39;, &#39;Xshift_cum&#39;, &#39;Yshift_cum&#39;, &#39;Xfit&#39;, &#39;Yfit&#39;]
    shifts_dt = pd.DataFrame(np.vstack((s00_cum_orig, s00_fit, s11_cum_orig, s11_fit, s01_cum_orig, s01_fit, s10_cum_orig, s10_fit, Xshift_cum_orig, Yshift_cum_orig, Xshift_cum, Yshift_cum, Xfit, Yfit)).T, columns = columns_shifts, index = None)
    shifts_dt.to_excel(xlsx_writer, index=None, sheet_name=&#39;Intermediate Results&#39;)

    columns_reg_stat = [&#39;Npts&#39;, &#39;Mean Abs Error&#39;]
    reg_stat_dt = pd.DataFrame(np.vstack((npts, error_abs_mean)).T, columns = columns_reg_stat, index = None)
    reg_stat_dt.to_excel(xlsx_writer, index=None, sheet_name=&#39;Reg. Stat. Info&#39;)

    kwargs_info = pd.DataFrame([kwargs]).T   # prepare to be save in transposed format
    kwargs_info.to_excel(xlsx_writer, header=False, sheet_name=&#39;kwargs Info&#39;)

    xlsx_writer.save()

    DumpObject = [kwargs, npts, error_abs_mean,
              transformation_matrix, s00_cum_orig, s11_cum_orig, s00_fit, s11_fit,
              tr_matr_cum, s01_cum_orig, s10_cum_orig, s01_fit, s10_fit,
              Xshift_cum_orig, Yshift_cum_orig, Xshift_cum, Yshift_cum, Yshift_cum, Xfit, Yfit]
    with open(transf_matrix_bin_file,&#34;wb&#34;) as f:
        pickle.dump(DumpObject, f)

    return tr_matr_cum, transf_matrix_xlsx_file</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.radial_profile"><code class="name flex">
<span>def <span class="ident">radial_profile</span></span>(<span>data, center)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates radially average profile of the 2D array (used for FRC and auto-correlation)
©G.Shtengel 08/2020 gleb.shtengel@gmail.com</p>
<p>Parameters:
data : 2D array</p>
<p>center : list of two ints
[xcenter, ycenter]</p>
<p>Returns
radialprofile : float array
limited to x-size//2 of the input data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def radial_profile(data, center):
    &#39;&#39;&#39;
    Calculates radially average profile of the 2D array (used for FRC and auto-correlation)
    ©G.Shtengel 08/2020 gleb.shtengel@gmail.com

    Parameters:
    data : 2D array

    center : list of two ints
        [xcenter, ycenter]

    Returns
        radialprofile : float array
            limited to x-size//2 of the input data
    &#39;&#39;&#39;
    ysz, xsz = data.shape
    y, x = np.indices((data.shape))
    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)
    r = r.astype(int)
    tbin = np.bincount(r.ravel(), data.ravel())
    nr = np.bincount(r.ravel())
    radialprofile = (np.array(tbin) / nr)[0:(xsz//2+1)]
    return radialprofile</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.radial_profile_select_angles"><code class="name flex">
<span>def <span class="ident">radial_profile_select_angles</span></span>(<span>data, center, astart=89, astop=91, symm=4)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates radially average profile of the 2D array (used for FRC) within a select range of angles.
©G.Shtengel 08/2020 gleb.shtengel@gmail.com</p>
<p>Parameters:
data : 2D array
center : list of two ints
[xcenter, ycenter]</p>
<p>astart : float
Start angle for radial averaging. Default is 0
astop : float
Stop angle for radial averaging. Default is 90
symm : int
Symmetry factor (how many times Start and stop angle intervalks are repeated within 360 deg). Default is 4.</p>
<p>Returns
radialprofile : float array
limited to x-size//2 of the input data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def radial_profile_select_angles(data, center, astart = 89, astop = 91, symm=4):
    &#39;&#39;&#39;
    Calculates radially average profile of the 2D array (used for FRC) within a select range of angles.
    ©G.Shtengel 08/2020 gleb.shtengel@gmail.com

    Parameters:
    data : 2D array
    center : list of two ints
        [xcenter, ycenter]

    astart : float
        Start angle for radial averaging. Default is 0
    astop : float
        Stop angle for radial averaging. Default is 90
    symm : int
        Symmetry factor (how many times Start and stop angle intervalks are repeated within 360 deg). Default is 4.


    Returns
        radialprofile : float array
            limited to x-size//2 of the input data
    &#39;&#39;&#39;
    y, x = np.indices((data.shape))
    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)
    r_ang = (np.angle(x - center[0]+1j*(y - center[1]), deg=True)).ravel()
    r = r.astype(np.int)

    if symm&gt;0:
        ind = np.squeeze(np.array(np.where((r_ang &gt; astart) &amp; (r_ang &lt; astart))))
        for i in np.arange(symm):
            ai = astart +360.0/symm*i -180.0
            aa = astop +360.0/symm*i -180.0
            ind = np.concatenate((ind, np.squeeze((np.array(np.where((r_ang &gt; ai) &amp; (r_ang &lt; aa)))))), axis=0)
    else:
        ind = np.squeeze(np.where((r_ang &gt; astart) &amp; (r_ang &lt; astop)))

    rr = np.ravel(r)[ind]
    dd = np.ravel(data)[ind]

    tbin = np.bincount(rr, dd)
    nr = np.bincount(rr)
    radialprofile = tbin / nr
    return radialprofile</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.rand"><code class="name flex">
<span>def <span class="ident">rand</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>rand(d0, d1, &hellip;, dn)</p>
<p>Random values in a given shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a convenience function for users porting code from Matlab,
and wraps <code><a title="SIFT_gs.FIBSEM_SIFT_gs.random_sample" href="#SIFT_gs.FIBSEM_SIFT_gs.random_sample">RandomState.random_sample()</a></code>. That function takes a
tuple to specify the size of the output, which is consistent with
other NumPy functions like <code>numpy.zeros</code> and <code>numpy.ones</code>.</p>
</div>
<p>Create an array of the given shape and populate it with
random samples from a uniform distribution
over <code>[0, 1)</code>.</p>
<h2 id="parameters">Parameters</h2>
<p>d0, d1, &hellip;, dn : int, optional
The dimensions of the returned array, must be non-negative.
If no argument is given a single Python float is returned.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;`ndarray, shape ``(d0, d1, &hellip;, dn)```</dt>
<dd>Random values.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<p><code><a title="SIFT_gs.FIBSEM_SIFT_gs.random" href="#SIFT_gs.FIBSEM_SIFT_gs.random">RandomState.random()</a></code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.rand(3,2)
array([[ 0.14022471,  0.96360618],  #random
       [ 0.37601032,  0.25528411],  #random
       [ 0.49313049,  0.94909878]]) #random
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.randint"><code class="name flex">
<span>def <span class="ident">randint</span></span>(<span>low, high=None, size=None, dtype=builtins.int)</span>
</code></dt>
<dd>
<div class="desc"><p>Return random integers from <code>low</code> (inclusive) to <code>high</code> (exclusive).</p>
<p>Return random integers from the "discrete uniform" distribution of
the specified dtype in the "half-open" interval [<code>low</code>, <code>high</code>). If
<code>high</code> is None (the default), then results are from [0, <code>low</code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.randint</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>low</code></strong> :&ensp;<code>int</code> or <code>array-like</code> of <code>ints</code></dt>
<dd>Lowest (signed) integers to be drawn from the distribution (unless
<code>high=None</code>, in which case this parameter is one above the
<em>highest</em> such integer).</dd>
<dt><strong><code>high</code></strong> :&ensp;<code>int</code> or <code>array-like</code> of <code>ints</code>, optional</dt>
<dd>If provided, one above the largest (signed) integer to be drawn
from the distribution (see above for behavior if <code>high=None</code>).
If array-like, must contain integer values</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
Default is None, in which case a
single value is returned.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>dtype</code>, optional</dt>
<dd>
<p>Desired dtype of the result. Byteorder must be native.
The default value is int.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.11.0</p>
</div>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>int</code> or <code>ndarray</code> of <code>ints</code></dt>
<dd><code>size</code>-shaped array of random integers from the appropriate
distribution, or a single such random int if <code>size</code> not provided.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.random_integers" href="#SIFT_gs.FIBSEM_SIFT_gs.random_integers">RandomState.random_integers()</a></code></dt>
<dd>similar to <code><a title="SIFT_gs.FIBSEM_SIFT_gs.randint" href="#SIFT_gs.FIBSEM_SIFT_gs.randint">RandomState.randint()</a></code>, only for the closed interval [<code>low</code>, <code>high</code>], and 1 is the lowest value if <code>high</code> is omitted.</dd>
<dt><code>random.Generator.integers</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.randint(2, size=10)
array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random
&gt;&gt;&gt; np.random.randint(1, size=10)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
</code></pre>
<p>Generate a 2 x 4 array of ints between 0 and 4, inclusive:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.randint(5, size=(2, 4))
array([[4, 0, 2, 1], # random
       [3, 2, 2, 0]])
</code></pre>
<p>Generate a 1 x 3 array with 3 different upper bounds</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.randint(1, [3, 5, 10])
array([2, 2, 9]) # random
</code></pre>
<p>Generate a 1 by 3 array with 3 different lower bounds</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.randint([1, 5, 7], 10)
array([9, 8, 7]) # random
</code></pre>
<p>Generate a 2 by 4 array using broadcasting with dtype of uint8</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)
array([[ 8,  6,  9,  7], # random
       [ 1, 16,  9, 12]], dtype=uint8)
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.randn"><code class="name flex">
<span>def <span class="ident">randn</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>randn(d0, d1, &hellip;, dn)</p>
<p>Return a sample (or samples) from the "standard normal" distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a convenience function for users porting code from Matlab,
and wraps <code><a title="SIFT_gs.FIBSEM_SIFT_gs.standard_normal" href="#SIFT_gs.FIBSEM_SIFT_gs.standard_normal">RandomState.standard_normal()</a></code>. That function takes a
tuple to specify the size of the output, which is consistent with
other NumPy functions like <code>numpy.zeros</code> and <code>numpy.ones</code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.standard_normal</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<p>If positive int_like arguments are provided, <code><a title="SIFT_gs.FIBSEM_SIFT_gs.randn" href="#SIFT_gs.FIBSEM_SIFT_gs.randn">RandomState.randn()</a></code> generates an array
of shape <code>(d0, d1, &hellip;, dn)</code>, filled
with random floats sampled from a univariate "normal" (Gaussian)
distribution of mean 0 and variance 1. A single float randomly sampled
from the distribution is returned if no argument is provided.</p>
<h2 id="parameters">Parameters</h2>
<p>d0, d1, &hellip;, dn : int, optional
The dimensions of the returned array, must be non-negative.
If no argument is given a single Python float is returned.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Z</code></strong> :&ensp;<code>ndarray</code> or <code>float</code></dt>
<dd>A <code>(d0, d1, &hellip;, dn)</code>-shaped array of floating-point samples from
the standard normal distribution, or a single such float if
no parameters were supplied.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.standard_normal" href="#SIFT_gs.FIBSEM_SIFT_gs.standard_normal">RandomState.standard_normal()</a></code></dt>
<dd>Similar, but takes a tuple as its argument.</dd>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.normal" href="#SIFT_gs.FIBSEM_SIFT_gs.normal">RandomState.normal()</a></code></dt>
<dd>Also accepts mu and sigma arguments.</dd>
<dt><code>random.Generator.standard_normal</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For random samples from the normal distribution with mean <code>mu</code> and
standard deviation <code>sigma</code>, use::</p>
<pre><code>sigma * np.random.randn(...) + mu
</code></pre>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.randn()
2.1923875335537315  # random
</code></pre>
<p>Two-by-four array of samples from the normal distribution with
mean 3 and standard deviation 2.5:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; 3 + 2.5 * np.random.randn(2, 4)
array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random
       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.random"><code class="name flex">
<span>def <span class="ident">random</span></span>(<span>size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Return random floats in the half-open interval [0.0, 1.0). Alias for
<code>random_sample</code> to ease forward-porting to the new random API.</p></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.random_integers"><code class="name flex">
<span>def <span class="ident">random_integers</span></span>(<span>low, high=None, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Random integers of type <code>np.int_</code> between <code>low</code> and <code>high</code>, inclusive.</p>
<p>Return random integers of type <code>np.int_</code> from the "discrete uniform"
distribution in the closed interval [<code>low</code>, <code>high</code>].
If <code>high</code> is
None (the default), then results are from [1, <code>low</code>]. The <code>np.int_</code>
type translates to the C long integer type and its precision
is platform dependent.</p>
<p>This function has been deprecated. Use randint instead.</p>
<div class="admonition deprecated">
<p class="admonition-title">Deprecated since version:&ensp;1.11.0</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>low</code></strong> :&ensp;<code>int</code></dt>
<dd>Lowest (signed) integer to be drawn from the distribution (unless
<code>high=None</code>, in which case this parameter is the <em>highest</em> such
integer).</dd>
<dt><strong><code>high</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>If provided, the largest (signed) integer to be drawn from the
distribution (see above for behavior if <code>high=None</code>).</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
Default is None, in which case a
single value is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>int</code> or <code>ndarray</code> of <code>ints</code></dt>
<dd><code>size</code>-shaped array of random integers from the appropriate
distribution, or a single such random int if <code>size</code> not provided.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.randint" href="#SIFT_gs.FIBSEM_SIFT_gs.randint">RandomState.randint()</a></code></dt>
<dd>Similar to <code><a title="SIFT_gs.FIBSEM_SIFT_gs.random_integers" href="#SIFT_gs.FIBSEM_SIFT_gs.random_integers">RandomState.random_integers()</a></code>, only for the half-open interval [<code>low</code>, <code>high</code>), and 0 is the lowest value if <code>high</code> is omitted.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>To sample from N evenly spaced floating-point numbers between a and b,
use::</p>
<p>a + (b - a) * (np.random.random_integers(N) - 1) / (N - 1.)</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.random_integers(5)
4 # random
&gt;&gt;&gt; type(np.random.random_integers(5))
&lt;class 'numpy.int64'&gt;
&gt;&gt;&gt; np.random.random_integers(5, size=(3,2))
array([[5, 4], # random
       [3, 3],
       [4, 5]])
</code></pre>
<p>Choose five random numbers from the set of five evenly-spaced
numbers between 0 and 2.5, inclusive (<em>i.e.</em>, from the set
:math:<code>{0, 5/8, 10/8, 15/8, 20/8}</code>):</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; 2.5 * (np.random.random_integers(5, size=(5,)) - 1) / 4.
array([ 0.625,  1.25 ,  0.625,  0.625,  2.5  ]) # random
</code></pre>
<p>Roll two six sided dice 1000 times and sum the results:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; d1 = np.random.random_integers(1, 6, 1000)
&gt;&gt;&gt; d2 = np.random.random_integers(1, 6, 1000)
&gt;&gt;&gt; dsums = d1 + d2
</code></pre>
<p>Display results as a histogram:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(dsums, 11, density=True)
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.random_sample"><code class="name flex">
<span>def <span class="ident">random_sample</span></span>(<span>size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Return random floats in the half-open interval [0.0, 1.0).</p>
<p>Results are from the "continuous uniform" distribution over the
stated interval.
To sample :math:<code>Unif[a, b), b &gt; a</code> multiply
the output of <code><a title="SIFT_gs.FIBSEM_SIFT_gs.random_sample" href="#SIFT_gs.FIBSEM_SIFT_gs.random_sample">RandomState.random_sample()</a></code> by <code>(b-a)</code> and add <code>a</code>::</p>
<p>(b - a) * random_sample() + a</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.random</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
Default is None, in which case a
single value is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>float</code> or <code>ndarray</code> of <code>floats</code></dt>
<dd>Array of random floats of shape <code>size</code> (unless <code>size=None</code>, in which
case a single float is returned).</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.random</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.random_sample()
0.47108547995356098 # random
&gt;&gt;&gt; type(np.random.random_sample())
&lt;class 'float'&gt;
&gt;&gt;&gt; np.random.random_sample((5,))
array([ 0.30220482,  0.86820401,  0.1654503 ,  0.11659149,  0.54323428]) # random
</code></pre>
<p>Three-by-two array of random numbers from [-5, 0):</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; 5 * np.random.random_sample((3, 2)) - 5
array([[-3.99149989, -0.52338984], # random
       [-2.99091858, -0.79479508],
       [-1.23204345, -1.75224494]])
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.rayleigh"><code class="name flex">
<span>def <span class="ident">rayleigh</span></span>(<span>scale=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Rayleigh distribution.</p>
<p>The :math:<code>\chi</code> and Weibull distributions are generalizations of the
Rayleigh.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.rayleigh</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>Scale, also equals the mode. Must be non-negative. Default is 1.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>scale</code> is a scalar.
Otherwise,
<code>np.array(scale).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized Rayleigh distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.rayleigh</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density function for the Rayleigh distribution is</p>
<p>[
]
The Rayleigh distribution would arise, for example, if the East
and North components of the wind velocity had identical zero-mean
Gaussian distributions.
Then the wind speed would have a Rayleigh
distribution.</p>
<h2 id="references">References</h2>
<p>.. [1] Brighton Webs Ltd., "Rayleigh Distribution,"
<a href="https://web.archive.org/web/20090514091424/http://brighton-webs.co.uk:80/distributions/rayleigh.asp">https://web.archive.org/web/20090514091424/http://brighton-webs.co.uk:80/distributions/rayleigh.asp</a>
.. [2] Wikipedia, "Rayleigh distribution"
<a href="https://en.wikipedia.org/wiki/Rayleigh_distribution">https://en.wikipedia.org/wiki/Rayleigh_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw values from the distribution and plot the histogram</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; from matplotlib.pyplot import hist
&gt;&gt;&gt; values = hist(np.random.rayleigh(3, 100000), bins=200, density=True)
</code></pre>
<p>Wave heights tend to follow a Rayleigh distribution. If the mean wave
height is 1 meter, what fraction of waves are likely to be larger than 3
meters?</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; meanvalue = 1
&gt;&gt;&gt; modevalue = np.sqrt(2 / np.pi) * meanvalue
&gt;&gt;&gt; s = np.random.rayleigh(modevalue, 1000000)
</code></pre>
<p>The percentage of waves larger than 3 meters is:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; 100.*sum(s&gt;3)/1000000.
0.087300000000000003 # random
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.read_kwargs_xlsx"><code class="name flex">
<span>def <span class="ident">read_kwargs_xlsx</span></span>(<span>file_xlsx, kwargs_sheet_name, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads (SIFT processing) kwargs from XLSX file and returns them as dictionary. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com</p>
<p>Parameters:
file_xlsx : str
Full path to XLSX file containing a worksheet with SIFt parameters saves as two columns: (name, value)
kwargs_sheet_name : str
Name of the worksheet containing SIFT parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_kwargs_xlsx(file_xlsx, kwargs_sheet_name, **kwargs):
    &#39;&#39;&#39;
    Reads (SIFT processing) kwargs from XLSX file and returns them as dictionary. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters:
    file_xlsx : str
        Full path to XLSX file containing a worksheet with SIFt parameters saves as two columns: (name, value)
    kwargs_sheet_name : str
        Name of the worksheet containing SIFT parameters
    &#39;&#39;&#39;
    disp_res_local = kwargs.get(&#39;disp_res&#39;, False)

    kwargs_dict_initial = {}
    try:
        stack_info = pd.read_excel(file_xlsx, header=None, sheet_name=kwargs_sheet_name).T                #read from transposed
        if len(stack_info.keys())&gt;0:
            if len(stack_info.keys())&gt;0:
                for key in stack_info.keys():
                    kwargs_dict_initial[stack_info[key][0]] = stack_info[key][1]
            else:
                kwargs_dict_initial[&#39;Stack Filename&#39;] = stack_info.index[1]
    except:
        if disp_res_local:
            print(&#39;No stack info record present, using defaults&#39;)
    kwargs_dict = {}
    for key in kwargs_dict_initial:
        if &#39;TransformType&#39; in key:
            exec(&#39;kwargs_dict[&#34;TransformType&#34;] = &#39; + kwargs_dict_initial[key].split(&#39;.&#39;)[-1].split(&#34;&#39;&#34;)[0])
        elif &#39;targ_vector&#39; in key:
            exec(&#39;kwargs_dict[&#34;targ_vector&#34;] = np.array(&#39; + kwargs_dict_initial[key].replace(&#39; &#39;, &#39;,&#39;)+ &#39;)&#39;)
        elif &#39;l2_matrix&#39; in key:
            exec(&#39;kwargs_dict[&#34;l2_matrix&#34;] = np.array(&#39; + kwargs_dict_initial[key].replace(&#39; &#39;, &#39;,&#39;) + &#39;)&#39;)
        elif &#39;fit_params&#39; in key:
            exec(&#39;kwargs_dict[&#34;fit_params&#34;] = &#39; + kwargs_dict_initial[key])
        elif &#39;subtract_linear_fit&#39; in key:
            exec(&#39;kwargs_dict[&#34;subtract_linear_fit&#34;] = np.array(&#39; + kwargs_dict_initial[key]+&#39;)&#39;)
        elif &#39;Stack Filename&#39; in key:
            exec(&#39;kwargs_dict[&#34;Stack Filename&#34;] = str(kwargs_dict_initial[key])&#39;)
        else:
            try:
                exec(&#39;kwargs_dict[&#34;&#39;+str(key)+&#39;&#34;] = &#39;+ str(kwargs_dict_initial[key]))
            except:
                exec(&#39;kwargs_dict[&#34;&#39;+str(key)+&#39;&#34;] = &#34;&#39; + kwargs_dict_initial[key].replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\n&#39;, &#39;,&#39;) + &#39;&#34;&#39;)
    if &#39;dump_filename&#39; in kwargs.keys():
        kwargs_dict[&#39;dump_filename&#39;] = kwargs[&#39;dump_filename&#39;]
    #correct for pandas mixed read failures
    try:
        if kwargs_dict[&#39;mrc_mode&#39;]:
            kwargs_dict[&#39;mrc_mode&#39;]=1
    except:
        pass
    try:
        if kwargs_dict[&#39;int_order&#39;]:
            kwargs_dict[&#39;int_order&#39;]=1
    except:
        pass
    try:
        if kwargs_dict[&#39;flipY&#39;] == 1:
            kwargs_dict[&#39;flipY&#39;] = True
        else:
            kwargs_dict[&#39;flipY&#39;] = False
    except:
        pass
    try:
        if kwargs_dict[&#39;BFMatcher&#39;] == 1:
            kwargs_dict[&#39;BFMatcher&#39;] = True
        else:
            kwargs_dict[&#39;BFMatcher&#39;] = False
    except:
        pass
    try:
        if kwargs_dict[&#39;invert_data&#39;] == 1:
            kwargs_dict[&#39;invert_data&#39;] = True
        else:
            kwargs_dict[&#39;invert_data&#39;] = False
    except:
        pass
    try:
        if kwargs_dict[&#39;sliding_evaluation_box&#39;] == 1:
            kwargs_dict[&#39;sliding_evaluation_box&#39;] = True
        else:
            kwargs_dict[&#39;sliding_evaluation_box&#39;] = False
    except:
        pass

    return kwargs_dict</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.read_transformation_matrix_from_xf_file"><code class="name flex">
<span>def <span class="ident">read_transformation_matrix_from_xf_file</span></span>(<span>xf_filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads transformation matrix created by FiJi-based workflow from *.xf file. ©G.Shtengel 10/2022 gleb.shtengel@gmail.com</p>
<p>Parameters:
xf_filename : str
Full path to *.xf file containing the transformation matrix data</p>
<p>Returns:
transformation_matrix : array</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_transformation_matrix_from_xf_file(xf_filename):
    &#39;&#39;&#39;
    Reads transformation matrix created by FiJi-based workflow from *.xf file. ©G.Shtengel 10/2022 gleb.shtengel@gmail.com

    Parameters:
    xf_filename : str
        Full path to *.xf file containing the transformation matrix data

    Returns:
    transformation_matrix : array
    &#39;&#39;&#39;
    npdt_recalled = pd.read_csv(xf_filename, sep = &#39;  &#39;, header = None)
    tr = npdt_recalled.to_numpy()
    transformation_matrix = np.zeros((len(tr), 3, 3))
    transformation_matrix[:, 0, 0:2] = tr[:,0:2]
    transformation_matrix[:, 0, 2] = tr[:,6]
    transformation_matrix[:, 1, 0:2] = tr[:,2:4]
    transformation_matrix[:, 1, 2] = tr[:,9]
    transformation_matrix[:, 2, 2] = np.ones((len(tr)))
    return transformation_matrix</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.save_data_stack"><code class="name flex">
<span>def <span class="ident">save_data_stack</span></span>(<span>FIBSEMstack, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the dataset into a file.</p>
<p>Parameters
FIBSEMstack : 3D array (may be DASK array)
Data set to be saved</p>
<h2 id="kwargs">Kwargs</h2>
<pre><code>data_dir : str
    data directory for saving the data
fnm_reg : str
    filename for the final registed dataset
fnm_types : list of strings
    File type(s) for output data. Options are: ['h5', 'mrc'].
    Defauls is 'mrc'. 'h5' is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
voxel_size : rec array of 3 elemets
    voxel size in nm
dtp  : dtype
    Python data type for saving. Deafult is int16, the other option currently is uint8.
disp_res : bolean
    Display messages and intermediate results
</code></pre>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>fnms_saved </code></dt>
<dd>list of strings
Paths to the files where the data set was saved.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_data_stack(FIBSEMstack, **kwargs):
    &#39;&#39;&#39;
    Saves the dataset into a file.

    Parameters
        FIBSEMstack : 3D array (may be DASK array)
            Data set to be saved

    kwargs
    ---------
        data_dir : str
            data directory for saving the data
        fnm_reg : str
            filename for the final registed dataset
        fnm_types : list of strings
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is &#39;mrc&#39;. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        voxel_size : rec array of 3 elemets
            voxel size in nm
        dtp  : dtype
            Python data type for saving. Deafult is int16, the other option currently is uint8.
        disp_res : bolean
            Display messages and intermediate results

    Returns:
        fnms_saved : list of strings
            Paths to the files where the data set was saved.

    &#39;&#39;&#39;
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registered_set.mrc&#39;)
    fpath_reg = os.path.join(data_dir, fnm_reg)
    fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
    voxel_size_default = np.rec.array((8.0, 8.0, 8.0), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
    voxel_size = kwargs.get(&#34;voxel_size&#34;, voxel_size_default)
    dtp = kwargs.get(&#34;dtp&#34;, int16)
    disp_res  = kwargs.get(&#34;disp_res&#34;, False )
    nz, ny, nx = FIBSEMstack.shape
    if disp_res:
        print(&#39;The resulting stack shape will be  nx={:d}, ny={:d}, nz={:d},  data type:&#39;.format(nx, ny, nz), dtp)
        print(&#39;Voxel Size (nm): {:2f} x {:2f} x {:2f}&#39;.format(voxel_size.x, voxel_size.y, voxel_size.z))

    fnms_saved = []
    if len(fnm_types)&gt;0:
        for fnm_type in fnm_types:
            # save dataset at HDF5 file
            if fnm_type == &#39;h5&#39;:
                fpath_reg_h5 = fpath_reg.replace(&#39;.mrc&#39;, &#39;.h5&#39;)
                try:
                    os.remove(fpath_reg_h5)
                except:
                    pass
                fnms_saved.append(fpath_reg_h5)
                if disp_res:
                    print(&#39;Saving dataset into Big Data Viewer HDF5 file: &#39;, fpath_reg_h5)
                bdv_writer = npy2bdv.BdvWriter(fpath_reg_h5, nchannels=1, blockdim=((1, 256, 256),))
                bdv_writer.append_view(stack=FIBSEMstack,
                       virtual_stack_dim=(nz,ny,nx),
                       time=0, channel=0,
                       voxel_size_xyz=(voxel_size.x, voxel_size.y, voxel_size.z),
                       voxel_units=&#39;nm&#39;)
                bdv_writer.write_xml()
                bdv_writer.close()
            if fnm_type == &#39;mrc&#39;:
                if disp_res:
                    print(&#39;Saving dataset into MRC file: &#39;, fpath_reg)
                fnms_saved.append(fpath_reg)
                &#39;&#39;&#39;
                mode 0 -&gt; uint8
                mode 1 -&gt; int16
                &#39;&#39;&#39;
                if dtp==int16:
                    mrc_mode = 1
                else:
                    mrc_mode = 0
                # Make a new, empty memory-mapped MRC file
                mrc = mrcfile.new_mmap(fpath_reg, shape=(nz, ny, nx), mrc_mode=mrc_mode, overwrite=True)
                voxel_size_angstr = voxel_size.copy()
                voxel_size_angstr.x = voxel_size_angstr.x * 10.0
                voxel_size_angstr.y = voxel_size_angstr.y * 10.0
                voxel_size_angstr.z = voxel_size_angstr.z * 10.0
                #mrc.header.cella = voxel_size_angstr
                mrc.voxel_size = voxel_size_angstr
                for j, FIBSEMframe in enumerate(tqdm(FIBSEMstack, desc = &#39;Saving Frames into MRC File: &#39;, display = disp_res)):
                    mrc.data[j,:,:] = FIBSEMframe.astype(dtp)
                mrc.close()
    else:
        print(&#39;Registered data set is NOT saved into a file&#39;)
    return fnms_saved</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.save_inlens_data"><code class="name flex">
<span>def <span class="ident">save_inlens_data</span></span>(<span>fname)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_inlens_data(fname):
    tfr = FIBSEM_frame(fname)
    tfr.save_images_tif(&#39;A&#39;)
    return fname</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.set_state"><code class="name flex">
<span>def <span class="ident">set_state</span></span>(<span>state)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the internal state of the generator from a tuple.</p>
<p>For use if one has reason to manually (re-)set the internal state of
the bit generator used by the RandomState instance. By default,
RandomState uses the "Mersenne Twister"[1]_ pseudo-random number
generating algorithm.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>{tuple(str, ndarray</code> of <code>624 uints, int, int, float), dict}</code></dt>
<dd>
<p>The <code>state</code> tuple has the following items:</p>
<ol>
<li>the string 'MT19937', specifying the Mersenne Twister algorithm.</li>
<li>a 1-D array of 624 unsigned integers <code>keys</code>.</li>
<li>an integer <code>pos</code>.</li>
<li>an integer <code>has_gauss</code>.</li>
<li>a float <code>cached_gaussian</code>.</li>
</ol>
<p>If state is a dictionary, it is directly set using the BitGenerators
<code>state</code> property.</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>None</code></dt>
<dd>Returns 'None' on success.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<p><code><a title="SIFT_gs.FIBSEM_SIFT_gs.get_state" href="#SIFT_gs.FIBSEM_SIFT_gs.get_state">RandomState.get_state()</a></code></p>
<h2 id="notes">Notes</h2>
<p><code><a title="SIFT_gs.FIBSEM_SIFT_gs.set_state" href="#SIFT_gs.FIBSEM_SIFT_gs.set_state">RandomState.set_state()</a></code> and <code><a title="SIFT_gs.FIBSEM_SIFT_gs.get_state" href="#SIFT_gs.FIBSEM_SIFT_gs.get_state">RandomState.get_state()</a></code> are not needed to work with any of the
random distributions in NumPy. If the internal state is manually altered,
the user should know exactly what he/she is doing.</p>
<p>For backwards compatibility, the form (str, array of 624 uints, int) is
also accepted although it is missing some information about the cached
Gaussian value: <code>state = ('MT19937', keys, pos)</code>.</p>
<h2 id="references">References</h2>
<p>.. [1] M. Matsumoto and T. Nishimura, "Mersenne Twister: A
623-dimensionally equidistributed uniform pseudorandom number
generator," <em>ACM Trans. on Modeling and Computer Simulation</em>,
Vol. 8, No. 1, pp. 3-30, Jan. 1998.</p></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.show_eval_box_mrc_stack"><code class="name flex">
<span>def <span class="ident">show_eval_box_mrc_stack</span></span>(<span>mrc_filename, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Read MRC stack and display the eval box for each frame from the list.
©G.Shtengel, 04/2021. gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mrc_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>File name (full path) of the mrc stack to be analyzed</dd>
<dt>kwargs:</dt>
<dt><strong><code>evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.</dd>
<dt>save_res_png
: boolean</dt>
<dt>Save PNG images of the intermediate processing statistics and final registration quality check</dt>
<dt><strong><code>ax</code></strong> :&ensp;<code>matplotlib ax artist</code></dt>
<dd>if provided, the data is exported to external ax object.</dd>
<dt><strong><code>frame_inds</code></strong> :&ensp;<code>array</code></dt>
<dd>List of frame indices to display the evaluation box. If not provided, three frames will be used:
[nz//10,
nz//2, nz//10*9] where nz is number of frames in mrc stack</dd>
<dt><strong><code>evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.</dd>
<dt><strong><code>sliding_evaluation_box</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box</dd>
<dt><strong><code>start_evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>see above</dd>
<dt><strong><code>stop_evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>see above</dd>
<dt><strong><code>invert_data</code></strong> :&ensp;<code>Boolean</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_eval_box_mrc_stack(mrc_filename, **kwargs):
    &#39;&#39;&#39;
    Read MRC stack and display the eval box for each frame from the list.
    ©G.Shtengel, 04/2021. gleb.shtengel@gmail.com

    Parameters
    ---------
    mrc_filename : str
        File name (full path) of the mrc stack to be analyzed

    kwargs:
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    ax : matplotlib ax artist
        if provided, the data is exported to external ax object.
    frame_inds : array
        List of frame indices to display the evaluation box. If not provided, three frames will be used:
        [nz//10,  nz//2, nz//10*9] where nz is number of frames in mrc stack
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    invert_data : Boolean
    &#39;&#39;&#39;
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    save_filename = kwargs.get(&#34;save_filename&#34;, mrc_filename )
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    ax = kwargs.get(&#34;ax&#34;, &#39;&#39;)
    plot_internal = (ax == &#39;&#39;)

    mrc = mrcfile.mmap(mrc_filename, mode=&#39;r&#39;)
    header = mrc.header
    nx, ny, nz = int32(header[&#39;nx&#39;]), int32(header[&#39;ny&#39;]), int32(header[&#39;nz&#39;])
    &#39;&#39;&#39;
        mode 0 -&gt; uint8
        mode 1 -&gt; int16
        mode 2 -&gt; float32
        mode 4 -&gt; complex64
        mode 6 -&gt; uint16
    &#39;&#39;&#39;
    mrc_mode = header.mode
    if mrc_mode==0:
        dt_mrc=uint8
    if mrc_mode==1:
        dt_mrc=int16
    if mrc_mode==2:
        dt_mrc=float32
    if mrc_mode==4:
        dt_mrc=complex64
    if mrc_mode==6:
        dt_mrc=uint16

    frame_inds = kwargs.get(&#34;frame_inds&#34;, [nz//10,  nz//2, nz//10*9] )

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = nx
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ny

    if sliding_evaluation_box:
        dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
        dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
    else:
        dx_eval = 0
        dy_eval = 0

    for fr_ind in frame_inds:
        eval_frame = (mrc.data[fr_ind, :, :].astype(dt_mrc)).astype(float)

        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*fr_ind//nz
            yi_eval = start_evaluation_box[0] + dy_eval*fr_ind//nz
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny

        if plot_internal:
            fig, ax = subplots(1,1, figsize = (10.0, 11.0*ny/nx))
        dmin, dmax = get_min_max_thresholds(eval_frame[yi_eval:ya_eval, xi_eval:xa_eval], disp_res=False)
        if invert_data:
            ax.imshow(eval_frame, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
        else:
            ax.imshow(eval_frame, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.grid(True, color = &#34;cyan&#34;)
        ax.set_title(Sample_ID + &#39; &#39;+mrc_filename +&#39;,  frame={:d}&#39;.format(fr_ind))
        rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        if save_res_png  and plot_internal:
            fname = os.path.splitext(save_filename)[0] + &#39;_frame_{:d}_evaluation_box.png&#39;.format(fr_ind)
            fig.savefig(fname, dpi=300)

    mrc.close()</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.show_eval_box_tif_stack"><code class="name flex">
<span>def <span class="ident">show_eval_box_tif_stack</span></span>(<span>tif_filename, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Read tif stack and display the eval box for each frame from the list.
©G.Shtengel, 08/2022. gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tif_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>File name (full path) of the tif stack to be analyzed</dd>
<dt>kwargs:</dt>
<dt><strong><code>evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.</dd>
<dt>save_res_png
: boolean</dt>
<dt>Save PNG images of the intermediate processing statistics and final registration quality check</dt>
<dt><strong><code>ax</code></strong> :&ensp;<code>matplotlib ax artist</code></dt>
<dd>if provided, the data is exported to external ax object.</dd>
<dt><strong><code>frame_inds</code></strong> :&ensp;<code>array</code></dt>
<dd>List of frame indices to display the evaluation box. If not provided, three frames will be used:
[nz//10,
nz//2, nz//10*9] where nz is number of frames in tif stack</dd>
<dt><strong><code>evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.</dd>
<dt><strong><code>sliding_evaluation_box</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box</dd>
<dt><strong><code>start_evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>see above</dd>
<dt><strong><code>stop_evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>see above</dd>
<dt><strong><code>invert_data</code></strong> :&ensp;<code>Boolean</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_eval_box_tif_stack(tif_filename, **kwargs):
    &#39;&#39;&#39;
    Read tif stack and display the eval box for each frame from the list.
    ©G.Shtengel, 08/2022. gleb.shtengel@gmail.com

    Parameters
    ---------
    tif_filename : str
        File name (full path) of the tif stack to be analyzed

    kwargs:
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    ax : matplotlib ax artist
        if provided, the data is exported to external ax object.
    frame_inds : array
        List of frame indices to display the evaluation box. If not provided, three frames will be used:
        [nz//10,  nz//2, nz//10*9] where nz is number of frames in tif stack
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    invert_data : Boolean
    &#39;&#39;&#39;
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, True )
    save_filename = kwargs.get(&#34;save_filename&#34;, tif_filename )
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    ax = kwargs.get(&#34;ax&#34;, &#39;&#39;)
    plot_internal = (ax == &#39;&#39;)

    with tiff.TiffFile(tif_filename) as tif:
        tif_tags = {}
        for tag in tif.pages[0].tags.values():
            name, value = tag.name, tag.value
            tif_tags[name] = value
    try:
        shape = eval(tif_tags[&#39;ImageDescription&#39;])
        nz, ny, nx = shape[&#39;shape&#39;]
    except:
        try:
            shape = eval(tif_tags[&#39;image_description&#39;])
            nz, ny, nx = shape[&#39;shape&#39;]
        except:
            fr0 = tiff.imread(tif_filename, key=0)
            ny, nx = np.shape(fr0)
            nz = eval(tif_tags[&#39;nimages&#39;])

    frame_inds = kwargs.get(&#34;frame_inds&#34;, [nz//10,  nz//2, nz//10*9] )

    xi_eval = evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = nx
    yi_eval = evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ny

    if sliding_evaluation_box:
        dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
        dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
    else:
        dx_eval = 0
        dy_eval = 0

    for fr_ind in frame_inds:
        #eval_frame = (tif.data[fr_ind, :, :].astype(dt)).astype(float)
        eval_frame = tiff.imread(tif_filename, key=fr_ind).astype(float)

        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*fr_ind//nz
            yi_eval = start_evaluation_box[0] + dy_eval*fr_ind//nz
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny

        if plot_internal:
            fig, ax = subplots(1,1, figsize = (10.0, 11.0*ny/nx))
        dmin, dmax = get_min_max_thresholds(eval_frame[yi_eval:ya_eval, xi_eval:xa_eval], disp_res=False)
        if invert_data:
            ax.imshow(eval_frame, cmap=&#39;Greys_r&#39;, vmin=dmin, vmax=dmax)
        else:
            ax.imshow(eval_frame, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.grid(True, color = &#34;cyan&#34;)
        ax.set_title(Sample_ID + &#39; &#39;+tif_filename +&#39;,  frame={:d}&#39;.format(fr_ind))
        rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        if save_res_png  and plot_internal:
            fname = os.path.splitext(save_filename)[0] + &#39;_frame_{:d}_evaluation_box.png&#39;.format(fr_ind)
            fig.savefig(fname, dpi=300)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.shuffle"><code class="name flex">
<span>def <span class="ident">shuffle</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Modify a sequence in-place by shuffling its contents.</p>
<p>This function only shuffles the array along the first axis of a
multi-dimensional array. The order of sub-arrays is changed but
their contents remains the same.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.shuffle</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code> or <code>MutableSequence</code></dt>
<dd>The array, list or mutable sequence to be shuffled.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.shuffle</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; arr = np.arange(10)
&gt;&gt;&gt; np.random.shuffle(arr)
&gt;&gt;&gt; arr
[1 7 5 2 9 4 3 6 0 8] # random
</code></pre>
<p>Multi-dimensional arrays are only shuffled along the first axis:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; arr = np.arange(9).reshape((3, 3))
&gt;&gt;&gt; np.random.shuffle(arr)
&gt;&gt;&gt; arr
array([[3, 4, 5], # random
       [6, 7, 8],
       [0, 1, 2]])
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.smooth"><code class="name flex">
<span>def <span class="ident">smooth</span></span>(<span>x, window_len=11, window='hanning')</span>
</code></dt>
<dd>
<div class="desc"><p>smooth the data using a window with requested size.</p>
<p>This method is based on the convolution of a scaled window with the signal.
The signal is prepared by introducing reflected copies of the signal
(with the window size) in both ends so that transient parts are minimized
in the begining and end part of the output signal.</p>
<p>input:
x: the input signal
window_len: the dimension of the smoothing window; should be an odd integer
window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'
flat window will produce a moving average smoothing.</p>
<p>output:
the smoothed signal</p>
<p>example:</p>
<p>t=linspace(-2,2,0.1)
x=sin(t)+randn(len(t))*0.1
y=smooth(x)</p>
<p>see also:</p>
<p>numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve
scipy.signal.lfilter</p>
<p>TODO: the window parameter could be the window itself if an array instead of a string
NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def smooth(x, window_len=11, window=&#39;hanning&#39;):
    &#34;&#34;&#34;smooth the data using a window with requested size.

    This method is based on the convolution of a scaled window with the signal.
    The signal is prepared by introducing reflected copies of the signal
    (with the window size) in both ends so that transient parts are minimized
    in the begining and end part of the output signal.

    input:
        x: the input signal
        window_len: the dimension of the smoothing window; should be an odd integer
        window: the type of window from &#39;flat&#39;, &#39;hanning&#39;, &#39;hamming&#39;, &#39;bartlett&#39;, &#39;blackman&#39;
            flat window will produce a moving average smoothing.

    output:
        the smoothed signal

    example:

    t=linspace(-2,2,0.1)
    x=sin(t)+randn(len(t))*0.1
    y=smooth(x)

    see also:

    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve
    scipy.signal.lfilter

    TODO: the window parameter could be the window itself if an array instead of a string
    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.
    &#34;&#34;&#34;

    if x.ndim != 1:
        raise ValueError(&#34;smooth only accepts 1 dimension arrays.&#34;)

    if x.size &lt; window_len:
        raise ValueError(&#34;Input vector needs to be bigger than window size.&#34;)


    if window_len&lt;3:
        return x


    if not window in [&#39;flat&#39;, &#39;hanning&#39;, &#39;hamming&#39;, &#39;bartlett&#39;, &#39;blackman&#39;]:
        raise ValueError(&#34;Window is on of &#39;flat&#39;, &#39;hanning&#39;, &#39;hamming&#39;, &#39;bartlett&#39;, &#39;blackman&#39;&#34;)

    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]

    if window == &#39;flat&#39;: #moving average
        w=np.ones(window_len,&#39;d&#39;)
    else:
        w=eval(&#39;np.&#39;+window+&#39;(window_len)&#39;)

    y=np.convolve(w/w.sum(),s,mode=&#39;valid&#39;)
    return y[(window_len//2-1):-(window_len//2)]</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.standard_cauchy"><code class="name flex">
<span>def <span class="ident">standard_cauchy</span></span>(<span>size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a standard Cauchy distribution with mode = 0.</p>
<p>Also known as the Lorentz distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.standard_cauchy</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
Default is None, in which case a
single value is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>samples</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>The drawn samples.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.standard_cauchy</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density function for the full Cauchy distribution is</p>
<p>[ (\frac{x-x_0}{\gamma})^2 \bigr] } ]
and the Standard Cauchy distribution just sets :math:<code>x_0=0</code> and
:math:<code>\gamma=1</code></p>
<p>The Cauchy distribution arises in the solution to the driven harmonic
oscillator problem, and also describes spectral line broadening. It
also describes the distribution of values at which a line tilted at
a random angle will cut the x axis.</p>
<p>When studying hypothesis tests that assume normality, seeing how the
tests perform on data from a Cauchy distribution is a good indicator of
their sensitivity to a heavy-tailed distribution, since the Cauchy looks
very much like a Gaussian distribution, but with heavier tails.</p>
<h2 id="references">References</h2>
<p>.. [1] NIST/SEMATECH e-Handbook of Statistical Methods, "Cauchy
Distribution",
<a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda3663.htm">https://www.itl.nist.gov/div898/handbook/eda/section3/eda3663.htm</a>
.. [2] Weisstein, Eric W. "Cauchy Distribution." From MathWorld&ndash;A
Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/CauchyDistribution.html">http://mathworld.wolfram.com/CauchyDistribution.html</a>
.. [3] Wikipedia, "Cauchy distribution"
<a href="https://en.wikipedia.org/wiki/Cauchy_distribution">https://en.wikipedia.org/wiki/Cauchy_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples and plot the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; s = np.random.standard_cauchy(1000000)
&gt;&gt;&gt; s = s[(s&gt;-25) &amp; (s&lt;25)]  # truncate distribution so it plots well
&gt;&gt;&gt; plt.hist(s, bins=100)
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.standard_exponential"><code class="name flex">
<span>def <span class="ident">standard_exponential</span></span>(<span>size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from the standard exponential distribution.</p>
<p><code><a title="SIFT_gs.FIBSEM_SIFT_gs.standard_exponential" href="#SIFT_gs.FIBSEM_SIFT_gs.standard_exponential">RandomState.standard_exponential()</a></code> is identical to the exponential distribution
with a scale parameter of 1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.standard_exponential</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
Default is None, in which case a
single value is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>float</code> or <code>ndarray</code></dt>
<dd>Drawn samples.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.standard_exponential</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Output a 3x8000 array:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; n = np.random.standard_exponential((3, 8000))
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.standard_gamma"><code class="name flex">
<span>def <span class="ident">standard_gamma</span></span>(<span>shape, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a standard Gamma distribution.</p>
<p>Samples are drawn from a Gamma distribution with specified parameters,
shape (sometimes designated "k") and scale=1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.standard_gamma</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Parameter, must be non-negative.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>shape</code> is a scalar.
Otherwise,
<code>np.array(shape).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized standard gamma distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.gamma</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>random.Generator.standard_gamma</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the Gamma distribution is</p>
<p>[
]
where :math:<code>k</code> is the shape and :math:<code>\theta</code> the scale,
and :math:<code>\Gamma</code> is the Gamma function.</p>
<p>The Gamma distribution is often used to model the times to failure of
electronic components, and arises naturally in processes for which the
waiting times between Poisson distributed events are relevant.</p>
<h2 id="references">References</h2>
<p>.. [1] Weisstein, Eric W. "Gamma Distribution." From MathWorld&ndash;A
Wolfram Web Resource.
<a href="http://mathworld.wolfram.com/GammaDistribution.html">http://mathworld.wolfram.com/GammaDistribution.html</a>
.. [2] Wikipedia, "Gamma distribution",
<a href="https://en.wikipedia.org/wiki/Gamma_distribution">https://en.wikipedia.org/wiki/Gamma_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; shape, scale = 2., 1. # mean and width
&gt;&gt;&gt; s = np.random.standard_gamma(shape, 1000000)
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; import scipy.special as sps  # doctest: +SKIP
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, 50, density=True)
&gt;&gt;&gt; y = bins**(shape-1) * ((np.exp(-bins/scale))/  # doctest: +SKIP
...                       (sps.gamma(shape) * scale**shape))
&gt;&gt;&gt; plt.plot(bins, y, linewidth=2, color='r')  # doctest: +SKIP
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.standard_normal"><code class="name flex">
<span>def <span class="ident">standard_normal</span></span>(<span>size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a standard Normal distribution (mean=0, stdev=1).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the
<code>~numpy.random.Generator.standard_normal</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
Default is None, in which case a
single value is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>float</code> or <code>ndarray</code></dt>
<dd>A floating-point array of shape <code>size</code> of drawn samples, or a
single sample if <code>size</code> was not specified.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.normal" href="#SIFT_gs.FIBSEM_SIFT_gs.normal">RandomState.normal()</a></code></dt>
<dd>Equivalent function with additional <code>loc</code> and <code>scale</code> arguments for setting the mean and standard deviation.</dd>
<dt><code>random.Generator.standard_normal</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For random samples from the normal distribution with mean <code>mu</code> and
standard deviation <code>sigma</code>, use one of::</p>
<pre><code>mu + sigma * np.random.standard_normal(size=...)
np.random.normal(mu, sigma, size=...)
</code></pre>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.standard_normal()
2.1923875335537315 #random
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; s = np.random.standard_normal(8000)
&gt;&gt;&gt; s
array([ 0.6888893 ,  0.78096262, -0.89086505, ...,  0.49876311,  # random
       -0.38672696, -0.4685006 ])                                # random
&gt;&gt;&gt; s.shape
(8000,)
&gt;&gt;&gt; s = np.random.standard_normal(size=(3, 4, 2))
&gt;&gt;&gt; s.shape
(3, 4, 2)
</code></pre>
<p>Two-by-four array of samples from the normal distribution with
mean 3 and standard deviation 2.5:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; 3 + 2.5 * np.random.standard_normal(size=(2, 4))
array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random
       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.standard_t"><code class="name flex">
<span>def <span class="ident">standard_t</span></span>(<span>df, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a standard Student's t distribution with <code>df</code> degrees
of freedom.</p>
<p>A special case of the hyperbolic distribution.
As <code>df</code> gets
large, the result resembles that of the standard normal
distribution (<code><a title="SIFT_gs.FIBSEM_SIFT_gs.standard_normal" href="#SIFT_gs.FIBSEM_SIFT_gs.standard_normal">RandomState.standard_normal()</a></code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.standard_t</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Degrees of freedom, must be &gt; 0.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>df</code> is a scalar.
Otherwise,
<code>np.array(df).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized standard Student's t distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.standard_t</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density function for the t distribution is</p>
<p>[ \Gamma(\frac{df}{2})}\Bigl( 1+\frac{x^2}{df} \Bigr)^{-(df+1)/2} ]
The t test is based on an assumption that the data come from a
Normal distribution. The t test provides a way to test whether
the sample mean (that is the mean calculated from the data) is
a good estimate of the true mean.</p>
<p>The derivation of the t-distribution was first published in
1908 by William Gosset while working for the Guinness Brewery
in Dublin. Due to proprietary issues, he had to publish under
a pseudonym, and so he used the name Student.</p>
<h2 id="references">References</h2>
<p>.. [1] Dalgaard, Peter, "Introductory Statistics With R",
Springer, 2002.
.. [2] Wikipedia, "Student's t-distribution"
<a href="https://en.wikipedia.org/wiki/Student's_t-distribution">https://en.wikipedia.org/wiki/Student's_t-distribution</a></p>
<h2 id="examples">Examples</h2>
<p>From Dalgaard page 83 [1]_, suppose the daily energy intake for 11
women in kilojoules (kJ) is:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; intake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \
...                    7515, 8230, 8770])
</code></pre>
<p>Does their energy intake deviate systematically from the recommended
value of 7725 kJ? Our null hypothesis will be the absence of deviation,
and the alternate hypothesis will be the presence of an effect that could be
either positive or negative, hence making our test 2-tailed. </p>
<p>Because we are estimating the mean and we have N=11 values in our sample,
we have N-1=10 degrees of freedom. We set our significance level to 95% and
compute the t statistic using the empirical mean and empirical standard
deviation of our intake. We use a ddof of 1 to base the computation of our
empirical standard deviation on an unbiased estimate of the variance (note:
the final estimate is not unbiased due to the concave nature of the square
root).</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.mean(intake)
6753.636363636364
&gt;&gt;&gt; intake.std(ddof=1)
1142.1232221373727
&gt;&gt;&gt; t = (np.mean(intake)-7725)/(intake.std(ddof=1)/np.sqrt(len(intake)))
&gt;&gt;&gt; t
-2.8207540608310198
</code></pre>
<p>We draw 1000000 samples from Student's t distribution with the adequate
degrees of freedom.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; s = np.random.standard_t(10, size=1000000)
&gt;&gt;&gt; h = plt.hist(s, bins=100, density=True)
</code></pre>
<p>Does our t statistic land in one of the two critical regions found at
both tails of the distribution?</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.sum(np.abs(t) &lt; np.abs(s)) / float(len(s))
0.018318  #random &lt; 0.05, statistic is in critical region
</code></pre>
<p>The probability value for this 2-tailed test is about 1.83%, which is
lower than the 5% pre-determined significance threshold. </p>
<p>Therefore, the probability of observing values as extreme as our intake
conditionally on the null hypothesis being true is too low, and we reject
the null hypothesis of no deviation.</p></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.transform_and_save_chunk_of_frames"><code class="name flex">
<span>def <span class="ident">transform_and_save_chunk_of_frames</span></span>(<span>chunk_of_frame_parametrs)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform Chunk of Frames and save into a single transformed frame. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com</p>
<p>Parameters
chunk_of_frame_parametrs : list of following parameters
[save_filename, frame_filenames, tr_matrices, tr_args]</p>
<p>save_filename : path
Filename for saving the transformed frame
frame_filenames : list of strings
Filenames (Full paths) of FIB-SEM frame files for every frame in frame_inds
tr_matrices : list of 2D (or 3d array)
Transformation matrix for every frame in frame_inds.</p>
<p>tr_args : list of lowwowing parameters:
tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, flatten_image, image_correction_file, perfrom_transformation, shift_matrix, inv_shift_matrix, ftype, dtp]</p>
<p>ImgB_fraction : float
Fractional weight of Image B for fused images, default is 0
xsz
:
int
X-size (pixels)
ysz
:
int
Y-size (pixels)
xi : int
Low X-axis bound for placing the transformed frame into the image before transformation
xa : int
High X-axis bound for placing the transformed frame into the image before transformation
yi : int
Low Y-axis bound for placing the transformed frame into the image before transformation
ya : int
High Y-axis bound for placing the transformed frame into the image before transformation
int_order : int
Default is 1. Interpolation order (0: Nearest-neighbor, 1: Bi-linear (default), 2: Bi-quadratic, 3: Bi-cubic, 4: Bi-quartic, 5: Bi-quintic)
invert_data : boolean
Invert data, default is False.
flipY : boolean
Flip output along Y-axis, default is False.
flatten_image : bolean
perform image flattening
image_correction_file : str
full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
perfrom_transformation
: boolean
perform transformation
shift_matrix : 2d array
shift matrix
inv_shift_matrix : 2d array
inverse shift matrix.
ftype : int
File Type. 0 for Shan's .dat files, 1 for tif files
dtp : data type
Python data type for saving. Deafult is int16, the other option currently is uint8.</p>
<p>Returns</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_and_save_chunk_of_frames(chunk_of_frame_parametrs):
    &#39;&#39;&#39;
    Transform Chunk of Frames and save into a single transformed frame. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters
    chunk_of_frame_parametrs : list of following parameters
        [save_filename, frame_filenames, tr_matrices, tr_args]

    save_filename : path
        Filename for saving the transformed frame
    frame_filenames : list of strings
        Filenames (Full paths) of FIB-SEM frame files for every frame in frame_inds
    tr_matrices : list of 2D (or 3d array)
        Transformation matrix for every frame in frame_inds.

    tr_args : list of lowwowing parameters:
        tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, flatten_image, image_correction_file, perfrom_transformation, shift_matrix, inv_shift_matrix, ftype, dtp]

    ImgB_fraction : float
        Fractional weight of Image B for fused images, default is 0
    xsz  :  int
        X-size (pixels)
    ysz  :  int
        Y-size (pixels)
    xi : int
        Low X-axis bound for placing the transformed frame into the image before transformation
    xa : int
        High X-axis bound for placing the transformed frame into the image before transformation
    yi : int
        Low Y-axis bound for placing the transformed frame into the image before transformation
    ya : int
        High Y-axis bound for placing the transformed frame into the image before transformation
    int_order : int
        Default is 1. Interpolation order (0: Nearest-neighbor, 1: Bi-linear (default), 2: Bi-quadratic, 3: Bi-cubic, 4: Bi-quartic, 5: Bi-quintic)
    invert_data : boolean
        Invert data, default is False.
    flipY : boolean
        Flip output along Y-axis, default is False.
    flatten_image : bolean
        perform image flattening
    image_correction_file : str
        full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
    perfrom_transformation  : boolean
        perform transformation
    shift_matrix : 2d array
        shift matrix
    inv_shift_matrix : 2d array
        inverse shift matrix.
    ftype : int
        File Type. 0 for Shan&#39;s .dat files, 1 for tif files
    dtp : data type
        Python data type for saving. Deafult is int16, the other option currently is uint8.

    Returns
    &#39;&#39;&#39;
    save_filename, frame_filenames, tr_matrices, tr_args = chunk_of_frame_parametrs
    ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, flatten_image, image_correction_file, perfrom_transformation, shift_matrix, inv_shift_matrix, ftype, dtp = tr_args
    num_frames = len(frame_filenames)
    transformed_img = np.zeros((ysz, xsz), dtype=float)
    frame_img = np.zeros((ysz, xsz), dtype=float)

    for frame_filename, tr_matrix in zip(frame_filenames, tr_matrices):
        frame = FIBSEM_frame(frame_filename, ftype=ftype)

        if ImgB_fraction &lt; 1e-5:
            #image = frame.RawImageA.astype(float)
            if flatten_image:
                image = (frame.flatten_image(image_correction_file = image_correction_file)[0]).astype(float)
            else:
                image = frame.RawImageA.astype(float)
        else:
            if flatten_image:
                flattened_images = frame.flatten_image(image_correction_file = image_correction_file)
                flattened_RawImageA = flattened_images[0].astype(float)
                if len(flattened_images)&gt;1:
                    flattened_RawImageB = flattened_images[1].astype(float)
                else:
                    flattened_RawImageB = frame.RawImageB.astype(float)
                image = flattened_RawImageA* (1.0 - ImgB_fraction) + flattened_RawImageB * ImgB_fraction
            else:
                image = frame.RawImageA.astype(float) * (1.0 - ImgB_fraction) + frame.RawImageB.astype(float) * ImgB_fraction

        if invert_data:
            frame_img[yi:ya, xi:xa] = np.negative(image)
            &#39;&#39;&#39;
            if frame.EightBit==0:
                frame_img[yi:ya, xi:xa] = np.negative(image)
            else:
                frame_img[yi:ya, xi:xa]  =  uint8(255) - image
            &#39;&#39;&#39;
        else:
            frame_img[yi:ya, xi:xa]  = image

        if perfrom_transformation:
            transf = ProjectiveTransform(matrix = shift_matrix @ (tr_matrix @ inv_shift_matrix))
            frame_img_reg = warp(frame_img, transf, order = int_order,  preserve_range=True)
        else:
            frame_img_reg = frame_img.copy()

        transformed_img = transformed_img + frame_img_reg

    if num_frames &gt; 1:
        transformed_img = transformed_img/num_frames

    if flipY:
        transformed_img = np.flip(transformed_img, axis=0)

    tiff.imsave(save_filename, transformed_img.astype(dtp))

    return save_filename</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.transform_and_save_frames"><code class="name flex">
<span>def <span class="ident">transform_and_save_frames</span></span>(<span>DASK_client, frame_inds, fls, tr_matr_cum_residual, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>frank power supply
Transform and save FIB-SEM data set. A new vesion, with variable zbin_factor option. ©G.Shtengel 01/2023 gleb.shtengel@gmail.com</p>
<p>Parameters
DASK_client : DASK client
frame_inds : int array
Array of frame indecis. If not set or set to np.array((-1)), all frames will be transformed
fls : array of strings
full array of filenames
tr_matr_cum_residual : array
transformation matrix</p>
<h2 id="kwargs">Kwargs</h2>
<p>use_DASK : boolean
perform remote DASK computations
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
data_dir : str
data directory (path)
ImgB_fraction : float
fractional ratio of Image B to be used for constructing the fuksed image:
ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction
pad_edges : boolean
If True, the data will be padded before transformation to avoid clipping.
flipY : boolean
If True, the data will be flipped along Y-axis. Default is False.
zbin_factor : int
binning factor along Z-axis
perfrom_transformation : boolean
If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed.
int_order : int
The order of interpolation. 1: Bi-linear
flatten_image : bolean
perform image flattening
image_correction_file : str
full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
invert_data : boolean
If True - the data is inverted.
dtp
: dtype
Python data type for saving. Deafult is int16, the other option currently is uint8.
disp_res : bolean
Default is False</p>
<p>Returns:
registered_filenames : list of filenames (one for each transformed / z-binned frame)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_and_save_frames(DASK_client, frame_inds, fls, tr_matr_cum_residual, **kwargs):
    &#39;&#39;&#39;frank power supply
    Transform and save FIB-SEM data set. A new vesion, with variable zbin_factor option. ©G.Shtengel 01/2023 gleb.shtengel@gmail.com

    Parameters
    DASK_client : DASK client
    frame_inds : int array
        Array of frame indecis. If not set or set to np.array((-1)), all frames will be transformed
    fls : array of strings
        full array of filenames
    tr_matr_cum_residual : array
        transformation matrix

    kwargs
    ---------
    use_DASK : boolean
        perform remote DASK computations
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    data_dir : str
        data directory (path)
    ImgB_fraction : float
        fractional ratio of Image B to be used for constructing the fuksed image:
        ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.
    flipY : boolean
        If True, the data will be flipped along Y-axis. Default is False.
    zbin_factor : int
        binning factor along Z-axis
    perfrom_transformation : boolean
        If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed.
    int_order : int
        The order of interpolation. 1: Bi-linear
    flatten_image : bolean
        perform image flattening
    image_correction_file : str
        full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
    invert_data : boolean
        If True - the data is inverted.
    dtp  : dtype
        Python data type for saving. Deafult is int16, the other option currently is uint8.
    disp_res : bolean
        Default is False

    Returns:
    registered_filenames : list of filenames (one for each transformed / z-binned frame)

    &#39;&#39;&#39;
    ftype = kwargs.get(&#34;ftype&#34;, 0)
    test_frame = FIBSEM_frame(fls[0], ftype=ftype)

    save_transformed_dataset = kwargs.get(&#34;save_transformed_dataset&#34;, True)
    use_DASK = kwargs.get(&#34;use_DASK&#34;, False)  # do not use DASK the data is to be saved
    DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    data_dir = kwargs.get(&#34;data_dir&#34;, &#39;&#39;)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, &#39;Registration_file.mrc&#39;)
    dump_filename = kwargs.get(&#39;dump_filename&#39;, &#39;&#39;)
    ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, 0.0)         # fusion fraction. In case if Img B is present, the fused image
                                                            # for each frame will be constructed ImgF = (1.0-ImgB_fraction)*ImgA + ImgB_fraction*ImgB
    if test_frame.DetB == &#39;None&#39;:
        ImgB_fraction=0.0

    XResolution = kwargs.get(&#34;XResolution&#34;, test_frame.XResolution)
    YResolution = kwargs.get(&#34;YResolution&#34;, test_frame.YResolution)
    pad_edges =  kwargs.get(&#34;pad_edges&#34;, True)
    flipY = kwargs.get(&#34;flipY&#34;, False)
    zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, 1)
    perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True)
    int_order = kwargs.get(&#34;int_order&#34;, 1)                  # The order of interpolation. 1: Bi-linear
    flatten_image = kwargs.get(&#34;flatten_image&#34;, False)
    image_correction_file = kwargs.get(&#34;image_correction_file&#34;, &#39;&#39;)
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    dtp = kwargs.get(&#34;dtp&#34;, int16)  # Python data type for saving. Deafult is int16, the other option currently is uint8.
    disp_res = kwargs.get(&#34;disp_res&#34;, False)
    nfrs = len(frame_inds)                                                   # number of source images(frames) before z-binning
    end_frame = ((frame_inds[0]+len(frame_inds)-1)//zbin_factor+1)*zbin_factor
    st_frames = np.arange(frame_inds[0], end_frame, zbin_factor)             # starting frame for each z-bin
    nfrs_zbinned = len(st_frames)                                            # number of frames after z-ninning

    frames_new = np.arange(nfrs_zbinned-1)

    if pad_edges and perfrom_transformation:
        shape = [YResolution, XResolution]
        xmn, xmx, ymn, ymx = determine_pad_offsets(shape, tr_matr_cum_residual)
        padx = int(xmx - xmn)
        pady = int(ymx - ymn)
        xi = int(np.max([xmx, 0]))
        yi = int(np.max([ymx, 0]))
        # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
        # so that the transformed images are not clipped.
        # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
        # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
        # those are calculated below base on the amount of padding calculated above
        shift_matrix = np.array([[1.0, 0.0, xi],
                                 [0.0, 1.0, yi],
                                 [0.0, 0.0, 1.0]])
        inv_shift_matrix = np.linalg.inv(shift_matrix)
    else:
        padx = 0
        pady = 0
        xi = 0
        yi = 0
        shift_matrix = np.eye(3,3)
        inv_shift_matrix = np.eye(3,3)

    fpath_reg = os.path.join(data_dir, fnm_reg)
    xsz = XResolution + padx
    xa = xi + XResolution
    ysz = YResolution + pady
    ya = yi + YResolution

    &#39;&#39;&#39;
    transform_and_save_chunk_of_frames(save_filename, frame_filenames, tr_matrices, tr_args):
    chunk_of_frame_parametrs = save_filename, frame_filenames, tr_matrices_cum_residual, tr_args
    tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, flatten_image, image_correction_file, perfrom_transformation, shift_matrix, inv_shift_matrix, ftype, dtp]
    process_frames = np.arange(st_frame, min(st_frame+zbin_factor, (frame_inds[-1]+1)))
    chunk_of_frame_parametrs_dataset.append([save_filename, process_frames, np.array(tr_matr_cum_residual)[process_frames], tr_args])

    &#39;&#39;&#39;
    tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, flatten_image, image_correction_file, perfrom_transformation, shift_matrix, inv_shift_matrix, ftype, dtp]
    chunk_of_frame_parametrs_dataset = []
    for j, st_frame in enumerate(tqdm(st_frames, desc=&#39;Setting up DASK parameter sets&#39;, display=disp_res)):
        save_filename = os.path.join(os.path.split(fls[st_frame])[0],&#39;Registered_Frame_{:d}.tif&#39;.format(j))
        process_frames = np.arange(st_frame, min(st_frame+zbin_factor, (frame_inds[-1]+1)))
        chunk_of_frame_parametrs_dataset.append([save_filename, np.array(fls)[process_frames], np.array(tr_matr_cum_residual)[process_frames], tr_args])

    if use_DASK:
        if disp_res:
            print(&#39;Starting DASK jobs&#39;)
        futures_td = DASK_client.map(transform_and_save_chunk_of_frames, chunk_of_frame_parametrs_dataset, retries = DASK_client_retries)
        registered_filenames = np.array(DASK_client.gather(futures_td))
        if disp_res:
            print(&#39;Finished DASK jobs&#39;)
    else:   # if DASK is not used - perform local computations
        if disp_res:
            print(&#39;Will perform local computations&#39;)
        registered_filenames = []
        for chunk_of_frame_parametrs in tqdm(chunk_of_frame_parametrs_dataset, desc = &#39;Transforming and saving frame chunks&#39;, display = disp_res):
            registered_filenames.append(transform_and_save_chunk_of_frames(chunk_of_frame_parametrs))

    return registered_filenames</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.transform_chunk_of_frames"><code class="name flex">
<span>def <span class="ident">transform_chunk_of_frames</span></span>(<span>frame_filenames, xsz, ysz, ftype, flatten_image, image_correction_file, perfrom_transformation, tr_matrices, shift_matrix, inv_shift_matrix, xi, xa, yi, ya, ImgB_fraction=0.0, invert_data=False, int_order=1, flipY=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform Chunk of Frames and average into a single transformed frames. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com</p>
<p>Parameters
frame_filenames : list of strings
Filenames (Full paths) of FIB-SEM frame files for every frame in frame_inds
xsz
:
int
X-size (pixels)
ysz
:
int
Y-size (pixels)
ftype : int
File Type. 0 for Shan's .dat files, 1 for tif files
flatten_image : bolean
perform image flattening
image_correction_file : str
full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
xi : int
Low X-axis bound for placing the transformed frame into the image before transformation
xa : int
High X-axis bound for placing the transformed frame into the image before transformation
yi : int
Low Y-axis bound for placing the transformed frame into the image before transformation
ya : int
High Y-axis bound for placing the transformed frame into the image before transformation
perfrom_transformation
: boolean
perform transformation
tr_matrices : list of 2D (or 3d array)
Transformation matrix for every frame in frame_inds.
shift_matrix : 2d array
shift matrix
inv_shift_matrix : 2d array
inverse shift matrix.
ImgB_fraction : float
Fractional weight of Image B for fused images, default is 0
invert_data : boolean
Invert data, default is False.
int_order : int
Default is 1. Interpolation order (0: Nearest-neighbor, 1: Bi-linear (default), 2: Bi-quadratic, 3: Bi-cubic, 4: Bi-quartic, 5: Bi-quintic)
flipY : boolean
Flip output along Y-axis, default is False.</p>
<p>Returns</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_chunk_of_frames(frame_filenames, xsz, ysz, ftype,
                        flatten_image, image_correction_file,
                        perfrom_transformation, tr_matrices, shift_matrix, inv_shift_matrix,
                        xi, xa, yi, ya,
                        ImgB_fraction=0.0,
                        invert_data=False,
                        int_order=1,
                        flipY=False):
    &#39;&#39;&#39;
    Transform Chunk of Frames and average into a single transformed frames. ©G.Shtengel 09/2022 gleb.shtengel@gmail.com

    Parameters
    frame_filenames : list of strings
        Filenames (Full paths) of FIB-SEM frame files for every frame in frame_inds
    xsz  :  int
        X-size (pixels)
    ysz  :  int
        Y-size (pixels)
    ftype : int
        File Type. 0 for Shan&#39;s .dat files, 1 for tif files
    flatten_image : bolean
        perform image flattening
    image_correction_file : str
        full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
    xi : int
        Low X-axis bound for placing the transformed frame into the image before transformation
    xa : int
        High X-axis bound for placing the transformed frame into the image before transformation
    yi : int
        Low Y-axis bound for placing the transformed frame into the image before transformation
    ya : int
        High Y-axis bound for placing the transformed frame into the image before transformation
    perfrom_transformation  : boolean
        perform transformation
    tr_matrices : list of 2D (or 3d array)
        Transformation matrix for every frame in frame_inds.
    shift_matrix : 2d array
        shift matrix
    inv_shift_matrix : 2d array
        inverse shift matrix.
    ImgB_fraction : float
        Fractional weight of Image B for fused images, default is 0
    invert_data : boolean
        Invert data, default is False.
    int_order : int
        Default is 1. Interpolation order (0: Nearest-neighbor, 1: Bi-linear (default), 2: Bi-quadratic, 3: Bi-cubic, 4: Bi-quartic, 5: Bi-quintic)
    flipY : boolean
        Flip output along Y-axis, default is False.

    Returns
    &#39;&#39;&#39;
    transformed_img = np.zeros((ysz, xsz), dtype=float)
    frame_img = np.zeros((ysz, xsz), dtype=float)
    num_frames = len(frame_filenames)
    for frame_filename, tr_matrix in zip(frame_filenames, tr_matrices):
        frame = FIBSEM_frame(frame_filename, ftype=ftype)

        if ImgB_fraction &lt; 1e-5:
            #image = frame.RawImageA.astype(float)
            if flatten_image:
                image = (frame.flatten_image(image_correction_file = image_correction_file)[0]).astype(float)
            else:
                image = frame.RawImageA.astype(float)
        else:
            if flatten_image:
                flattened_images = frame.flatten_image(image_correction_file = image_correction_file)
                flattened_RawImageA = flattened_images[0].astype(float)
                if len(flattened_images)&gt;1:
                    flattened_RawImageB = flattened_images[1].astype(float)
                else:
                    flattened_RawImageB = frame.RawImageB.astype(float)
                image = flattened_RawImageA* (1.0 - ImgB_fraction) + flattened_RawImageB * ImgB_fraction
            else:
                image = frame.RawImageA.astype(float) * (1.0 - ImgB_fraction) + frame.RawImageB.astype(float) * ImgB_fraction

        if invert_data:
            frame_img[yi:ya, xi:xa] = np.negative(image)
            &#39;&#39;&#39;
            if frame.EightBit==0:
                frame_img[yi:ya, xi:xa] = np.negative(image)
            else:
                frame_img[yi:ya, xi:xa]  =  uint8(255) - image
            &#39;&#39;&#39;
        else:
            frame_img[yi:ya, xi:xa]  = image

        if perfrom_transformation:
            transf = ProjectiveTransform(matrix = shift_matrix @ (tr_matrix @ inv_shift_matrix))
            frame_img_reg = warp(frame_img, transf, order = int_order,  preserve_range=True)
        else:
            frame_img_reg = frame_img.copy()

        transformed_img = transformed_img + frame_img_reg

    if num_frames &gt; 1:
        transformed_img = transformed_img/num_frames

    if flipY:
        transformed_img = np.flip(transformed_img, axis=0)
    &#39;&#39;&#39;
    if frame.EightBit==1:
        transformed_img = np.clip(np.round(transformed_img) , 0, 255)
    &#39;&#39;&#39;

    return transformed_img</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.transform_two_chunks"><code class="name flex">
<span>def <span class="ident">transform_two_chunks</span></span>(<span>params)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms two chunks of EM frames and evaluates registration</p>
<p>Parameters:
params: list
params = [chunk0_frames, chunk1_frames, fls, tr_matr_cum_residual, shift_matrix, inv_shift_matrix, xi_eval, xa_eval, yi_eval, ya_eval, save_frame_png, frame_number, filename_frame_png, tr_args]
chunk0_frames : array of int
indecis of the frames in the first chunk
chunk1_frames : array of int
indecis of the frames in the second chunk
fls : array of str
Filenames (full paths) of the ioriginal data frames
tr_matr_cum_residual : 3d array
Transformation matrix
shift_matrix : 2d array
shift matrix
inv_shift_matrix : 2d array
inverse shift matrix
tr_args : list
tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, zbin_factor, flatten_image, image_correction_file, perfrom_transformation, ftype]</p>
<p>Returns:
registration_stat : list
registration</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_two_chunks(params):
    &#39;&#39;&#39;
    Transforms two chunks of EM frames and evaluates registration

    Parameters:
    params: list
    params = [chunk0_frames, chunk1_frames, fls, tr_matr_cum_residual, shift_matrix, inv_shift_matrix, xi_eval, xa_eval, yi_eval, ya_eval, save_frame_png, frame_number, filename_frame_png, tr_args]
        chunk0_frames : array of int
            indecis of the frames in the first chunk
        chunk1_frames : array of int
            indecis of the frames in the second chunk
        fls : array of str
            Filenames (full paths) of the ioriginal data frames
        tr_matr_cum_residual : 3d array
            Transformation matrix
        shift_matrix : 2d array
            shift matrix
        inv_shift_matrix : 2d array
            inverse shift matrix
    tr_args : list
        tr_args = [ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, zbin_factor, flatten_image, image_correction_file, perfrom_transformation, ftype]

    Returns:
    registration_stat : list
        registration
    &#39;&#39;&#39;
    # first, unpack the parameters
    chunk0_filenames, chunk1_filenames, chunk0_tr_matrices, chunk1_tr_matrices, shift_matrix, inv_shift_matrix, xi_eval, xa_eval, yi_eval, ya_eval, save_frame_png, frame_number, filename_frame_png, tr_args = params
    ImgB_fraction, xsz, ysz, xi, xa, yi, ya, int_order, invert_data, flipY, zbin_factor, flatten_image, image_correction_file, perfrom_transformation, ftype = tr_args
    binned_fr_img0 = transform_chunk_of_frames(chunk0_filenames, xsz, ysz, ftype,
                                flatten_image, image_correction_file,
                                perfrom_transformation, chunk0_tr_matrices, shift_matrix, inv_shift_matrix,
                                xi, xa, yi, ya,
                                ImgB_fraction = ImgB_fraction,
                                invert_data = invert_data,
                                int_order = int_order,
                                flipY=flipY)
    binned_fr_img1 = transform_chunk_of_frames(chunk1_filenames, xsz, ysz, ftype,
                                flatten_image, image_correction_file,
                                perfrom_transformation, chunk1_tr_matrices, shift_matrix, inv_shift_matrix,
                                xi, xa, yi, ya,
                                ImgB_fraction = ImgB_fraction,
                                invert_data = invert_data,
                                int_order = int_order,
                                flipY=flipY)

    I1 = binned_fr_img0[yi_eval:ya_eval, xi_eval:xa_eval]
    I2 = binned_fr_img1[yi_eval:ya_eval, xi_eval:xa_eval]
    fr_mean = np.abs(I1/2.0 + I2/2.0)
    image_nsad =  np.mean(np.abs(I1-I2))/(np.mean(fr_mean)-np.amin(fr_mean))
    image_ncc = Two_Image_NCC_SNR(I1, I2)[0]
    image_mi = mutual_information_2d(I1.ravel(), I2.ravel(), sigma=1.0, bin=2048, normalized=True)
    if save_frame_png:
        yshape, xshape = binned_fr_img0.shape
        fig, ax = subplots(1,1, figsize=(3.0*xshape/yshape, 3))
        fig.subplots_adjust(left=0.0, bottom=0.00, right=1.0, top=1.0)
        dmin, dmax = get_min_max_thresholds(I1)
        ax.imshow(binned_fr_img0, cmap=&#39;Greys&#39;, vmin=dmin, vmax=dmax)
        ax.text(0.06, 0.95, &#39;Frame={:d},  NSAD={:.3f},  NCC={:.3f},  NMI={:.3f}&#39;.format(frame_number, image_nsad, image_ncc, image_mi), color=&#39;red&#39;, transform=ax.transAxes, fontsize=12)
        rect_patch = patches.Rectangle((xi_eval, yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=1.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        ax.axis(&#39;off&#39;)
        fig.savefig(filename_frame_png, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True, pad_inches=0)   # save the figure to file
        plt.close(fig)
    return [image_nsad, image_ncc, image_mi]</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.triangular"><code class="name flex">
<span>def <span class="ident">triangular</span></span>(<span>left, mode, right, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from the triangular distribution over the
interval <code>[left, right]</code>.</p>
<p>The triangular distribution is a continuous probability
distribution with lower limit left, peak at mode, and upper
limit right. Unlike the other distributions, these parameters
directly define the shape of the pdf.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.triangular</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>left</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Lower limit.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>The value where the peak of the distribution occurs.
The value must fulfill the condition <code>left &lt;= mode &lt;= right</code>.</dd>
<dt><strong><code>right</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Upper limit, must be larger than <code>left</code>.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>left</code>, <code>mode</code>, and <code>right</code>
are all scalars.
Otherwise, <code>np.broadcast(left, mode, right).size</code>
samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized triangular distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.triangular</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density function for the triangular distribution is</p>
<p>[ \frac{2(x-l)}{(r-l)(m-l)}&amp; \text{for $l \leq x \leq m$},\
\frac{2(r-x)}{(r-l)(r-m)}&amp; \text{for $m \leq x \leq r$},\
0&amp; \text{otherwise}.
\end{cases} ]
The triangular distribution is often used in ill-defined
problems where the underlying distribution is not known, but
some knowledge of the limits and mode exists. Often it is used
in simulations.</p>
<h2 id="references">References</h2>
<p>.. [1] Wikipedia, "Triangular distribution"
<a href="https://en.wikipedia.org/wiki/Triangular_distribution">https://en.wikipedia.org/wiki/Triangular_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw values from the distribution and plot the histogram:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; h = plt.hist(np.random.triangular(-3, 0, 8, 100000), bins=200,
...              density=True)
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.uniform"><code class="name flex">
<span>def <span class="ident">uniform</span></span>(<span>low=0.0, high=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a uniform distribution.</p>
<p>Samples are uniformly distributed over the half-open interval
<code>[low, high)</code> (includes low, but excludes high).
In other words,
any value within the given interval is equally likely to be drawn
by <code><a title="SIFT_gs.FIBSEM_SIFT_gs.uniform" href="#SIFT_gs.FIBSEM_SIFT_gs.uniform">RandomState.uniform()</a></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.uniform</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>low</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code>, optional</dt>
<dd>Lower boundary of the output interval.
All values generated will be
greater than or equal to low.
The default value is 0.</dd>
<dt><strong><code>high</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Upper boundary of the output interval.
All values generated will be
less than or equal to high.
The high limit may be included in the
returned array of floats due to floating-point rounding in the
equation <code>low + (high-low) * random_sample()</code>.
The default value
is 1.0.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>low</code> and <code>high</code> are both scalars.
Otherwise, <code>np.broadcast(low, high).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized uniform distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.randint" href="#SIFT_gs.FIBSEM_SIFT_gs.randint">RandomState.randint()</a></code></dt>
<dd>Discrete uniform distribution, yielding integers.</dd>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.random_integers" href="#SIFT_gs.FIBSEM_SIFT_gs.random_integers">RandomState.random_integers()</a></code></dt>
<dd>Discrete uniform distribution over the closed interval <code>[low, high]</code>.</dd>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.random_sample" href="#SIFT_gs.FIBSEM_SIFT_gs.random_sample">RandomState.random_sample()</a></code></dt>
<dd>Floats uniformly distributed over <code>[0, 1)</code>.</dd>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.random" href="#SIFT_gs.FIBSEM_SIFT_gs.random">RandomState.random()</a></code></dt>
<dd>Alias for <code><a title="SIFT_gs.FIBSEM_SIFT_gs.random_sample" href="#SIFT_gs.FIBSEM_SIFT_gs.random_sample">RandomState.random_sample()</a></code>.</dd>
<dt><code><a title="SIFT_gs.FIBSEM_SIFT_gs.rand" href="#SIFT_gs.FIBSEM_SIFT_gs.rand">RandomState.rand()</a></code></dt>
<dd>Convenience function that accepts dimensions as input, e.g., <code><a title="SIFT_gs.FIBSEM_SIFT_gs.rand" href="#SIFT_gs.FIBSEM_SIFT_gs.rand">RandomState.rand()</a>(2,2)</code> would generate a 2-by-2 array of floats, uniformly distributed over <code>[0, 1)</code>.</dd>
<dt><code>random.Generator.uniform</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density function of the uniform distribution is</p>
<p>[
]
anywhere within the interval <code>[a, b)</code>, and zero elsewhere.</p>
<p>When <code>high</code> == <code>low</code>, values of <code>low</code> will be returned.
If <code>high</code> &lt; <code>low</code>, the results are officially undefined
and may eventually raise an error, i.e. do not rely on this
function to behave when passed arguments satisfying that
inequality condition. The <code>high</code> limit may be included in the
returned array of floats due to floating-point rounding in the
equation <code>low + (high-low) * random_sample()</code>. For example:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; x = np.float32(5*0.99999999)
&gt;&gt;&gt; x
5.0
</code></pre>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; s = np.random.uniform(-1,0,1000)
</code></pre>
<p>All values are within the given interval:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.all(s &gt;= -1)
True
&gt;&gt;&gt; np.all(s &lt; 0)
True
</code></pre>
<p>Display the histogram of the samples, along with the
probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, 15, density=True)
&gt;&gt;&gt; plt.plot(bins, np.ones_like(bins), linewidth=2, color='r')
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.vonmises"><code class="name flex">
<span>def <span class="ident">vonmises</span></span>(<span>mu, kappa, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a von Mises distribution.</p>
<p>Samples are drawn from a von Mises distribution with specified mode
(mu) and dispersion (kappa), on the interval [-pi, pi].</p>
<p>The von Mises distribution (also known as the circular normal
distribution) is a continuous probability distribution on the unit
circle.
It may be thought of as the circular analogue of the normal
distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.vonmises</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mu</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Mode ("center") of the distribution.</dd>
<dt><strong><code>kappa</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Dispersion of the distribution, has to be &gt;=0.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>mu</code> and <code>kappa</code> are both scalars.
Otherwise, <code>np.broadcast(mu, kappa).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized von Mises distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.vonmises</code></dt>
<dd>probability density function, distribution, or cumulative density function, etc.</dd>
<dt><code>random.Generator.vonmises</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the von Mises distribution is</p>
<p>[
]
where :math:<code>\mu</code> is the mode and :math:<code>\kappa</code> the dispersion,
and :math:<code>I_0(\kappa)</code> is the modified Bessel function of order 0.</p>
<p>The von Mises is named for Richard Edler von Mises, who was born in
Austria-Hungary, in what is now the Ukraine.
He fled to the United
States in 1939 and became a professor at Harvard.
He worked in
probability theory, aerodynamics, fluid mechanics, and philosophy of
science.</p>
<h2 id="references">References</h2>
<p>.. [1] Abramowitz, M. and Stegun, I. A. (Eds.). "Handbook of
Mathematical Functions with Formulas, Graphs, and Mathematical
Tables, 9th printing," New York: Dover, 1972.
.. [2] von Mises, R., "Mathematical Theory of Probability
and Statistics", New York: Academic Press, 1964.</p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; mu, kappa = 0.0, 4.0 # mean and dispersion
&gt;&gt;&gt; s = np.random.vonmises(mu, kappa, 1000)
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; from scipy.special import i0  # doctest: +SKIP
&gt;&gt;&gt; plt.hist(s, 50, density=True)
&gt;&gt;&gt; x = np.linspace(-np.pi, np.pi, num=51)
&gt;&gt;&gt; y = np.exp(kappa*np.cos(x-mu))/(2*np.pi*i0(kappa))  # doctest: +SKIP
&gt;&gt;&gt; plt.plot(x, y, linewidth=2, color='r')  # doctest: +SKIP
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.wald"><code class="name flex">
<span>def <span class="ident">wald</span></span>(<span>mean, scale, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Wald, or inverse Gaussian, distribution.</p>
<p>As the scale approaches infinity, the distribution becomes more like a
Gaussian. Some references claim that the Wald is an inverse Gaussian
with mean equal to 1, but this is by no means universal.</p>
<p>The inverse Gaussian distribution was first studied in relationship to
Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian
because there is an inverse relationship between the time to cover a
unit distance and distance covered in unit time.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.wald</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Distribution mean, must be &gt; 0.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Scale parameter, must be &gt; 0.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>mean</code> and <code>scale</code> are both scalars.
Otherwise, <code>np.broadcast(mean, scale).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized Wald distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.wald</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density function for the Wald distribution is</p>
<p>[ \frac{-scale(x-mean)^2}{2\cdotp mean^2x} ]
As noted above the inverse Gaussian distribution first arise
from attempts to model Brownian motion. It is also a
competitor to the Weibull for use in reliability modeling and
modeling stock returns and interest rate processes.</p>
<h2 id="references">References</h2>
<p>.. [1] Brighton Webs Ltd., Wald Distribution,
<a href="https://web.archive.org/web/20090423014010/http://www.brighton-webs.co.uk:80/distributions/wald.asp">https://web.archive.org/web/20090423014010/http://www.brighton-webs.co.uk:80/distributions/wald.asp</a>
.. [2] Chhikara, Raj S., and Folks, J. Leroy, "The Inverse Gaussian
Distribution: Theory : Methodology, and Applications", CRC Press,
1988.
.. [3] Wikipedia, "Inverse Gaussian distribution"
<a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution">https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw values from the distribution and plot the histogram:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; h = plt.hist(np.random.wald(3, 2, 100000), bins=200, density=True)
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.weibull"><code class="name flex">
<span>def <span class="ident">weibull</span></span>(<span>a, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Weibull distribution.</p>
<p>Draw samples from a 1-parameter Weibull distribution with the given
shape parameter <code>a</code>.</p>
<p>[
]
Here, U is drawn from the uniform distribution over (0,1].</p>
<p>The more common 2-parameter Weibull, including a scale parameter
:math:<code>\lambda</code> is just :math:<code>X = \lambda(-ln(U))^{1/a}</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.weibull</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Shape parameter of the distribution.
Must be nonnegative.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>a</code> is a scalar.
Otherwise,
<code>np.array(a).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized Weibull distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<p><code>scipy.stats.weibull_max</code>
<code>scipy.stats.weibull_min</code>
<code>scipy.stats.genextreme</code>
<code><a title="SIFT_gs.FIBSEM_SIFT_gs.gumbel" href="#SIFT_gs.FIBSEM_SIFT_gs.gumbel">RandomState.gumbel()</a></code>
<code>random.Generator.weibull: which should be used for new code.</code></p>
<h2 id="notes">Notes</h2>
<p>The Weibull (or Type III asymptotic extreme value distribution
for smallest values, SEV Type III, or Rosin-Rammler
distribution) is one of a class of Generalized Extreme Value
(GEV) distributions used in modeling extreme value problems.
This class includes the Gumbel and Frechet distributions.</p>
<p>The probability density for the Weibull distribution is</p>
<p>[ {\lambda}(\frac{x}{\lambda})^{a-1}e^{-(x/\lambda)^a}, ]
where :math:<code>a</code> is the shape and :math:<code>\lambda</code> the scale.</p>
<p>The function has its peak (the mode) at
:math:<code>\lambda(\frac{a-1}{a})^{1/a}</code>.</p>
<p>When <code>a = 1</code>, the Weibull distribution reduces to the exponential
distribution.</p>
<h2 id="references">References</h2>
<p>.. [1] Waloddi Weibull, Royal Technical University, Stockholm,
1939 "A Statistical Theory Of The Strength Of Materials",
Ingeniorsvetenskapsakademiens Handlingar Nr 151, 1939,
Generalstabens Litografiska Anstalts Forlag, Stockholm.
.. [2] Waloddi Weibull, "A Statistical Distribution Function of
Wide Applicability", Journal Of Applied Mechanics ASME Paper
1951.
.. [3] Wikipedia, "Weibull distribution",
<a href="https://en.wikipedia.org/wiki/Weibull_distribution">https://en.wikipedia.org/wiki/Weibull_distribution</a></p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; a = 5. # shape
&gt;&gt;&gt; s = np.random.weibull(a, 1000)
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; x = np.arange(1,100.)/50.
&gt;&gt;&gt; def weib(x,n,a):
...     return (a / n) * (x / n)**(a - 1) * np.exp(-(x / n)**a)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; count, bins, ignored = plt.hist(np.random.weibull(5.,1000))
&gt;&gt;&gt; x = np.arange(1,100.)/50.
&gt;&gt;&gt; scale = count.max()/weib(x, 1., 5.).max()
&gt;&gt;&gt; plt.plot(x, weib(x, 1., 5.)*scale)
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.zipf"><code class="name flex">
<span>def <span class="ident">zipf</span></span>(<span>a, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw samples from a Zipf distribution.</p>
<p>Samples are drawn from a Zipf distribution with specified parameter
<code>a</code> &gt; 1.</p>
<p>The Zipf distribution (also known as the zeta distribution) is a
discrete probability distribution that satisfies Zipf's law: the
frequency of an item is inversely proportional to its rank in a
frequency table.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>~numpy.random.Generator.zipf</code>
method of a <code>~numpy.random.Generator</code> instance instead;
please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Distribution parameter. Must be greater than 1.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>a</code> is a scalar. Otherwise,
<code>np.array(a).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized Zipf distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.zipf</code></dt>
<dd>probability density function, distribution, or cumulative density function, etc.</dd>
<dt><code>random.Generator.zipf</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the Zipf distribution is</p>
<p>[
]
for integers :math:<code>k \geq 1</code>, where :math:<code>\zeta</code> is the Riemann Zeta
function.</p>
<p>It is named for the American linguist George Kingsley Zipf, who noted
that the frequency of any word in a sample of a language is inversely
proportional to its rank in the frequency table.</p>
<h2 id="references">References</h2>
<p>.. [1] Zipf, G. K., "Selected Studies of the Principle of Relative
Frequency in Language," Cambridge, MA: Harvard Univ. Press,
1932.</p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; a = 4.0
&gt;&gt;&gt; n = 20000
&gt;&gt;&gt; s = np.random.zipf(a, n)
</code></pre>
<p>Display the histogram of the samples, along with
the expected histogram based on the probability
density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; from scipy.special import zeta  # doctest: +SKIP
</code></pre>
<p><code>bincount</code> provides a fast histogram for small integers.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; count = np.bincount(s)
&gt;&gt;&gt; k = np.arange(1, s.max() + 1)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; plt.bar(k, count[1:], alpha=0.5, label='sample count')
&gt;&gt;&gt; plt.plot(k, n*(k**-a)/zeta(a), 'k.-', alpha=0.5,
...          label='expected count')   # doctest: +SKIP
&gt;&gt;&gt; plt.semilogy()
&gt;&gt;&gt; plt.grid(alpha=0.4)
&gt;&gt;&gt; plt.legend()
&gt;&gt;&gt; plt.title(f'Zipf sample, a={a}, size={n}')
&gt;&gt;&gt; plt.show()
</code></pre></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset"><code class="flex name class">
<span>class <span class="ident">FIBSEM_dataset</span></span>
<span>(</span><span>fls, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A class representing a FIB-SEM data set
©G.Shtengel 10/2021 gleb.shtengel@gmail.com
Contains the info/settings on the FIB-SEM dataset and the procedures that can be performed on it.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>fls</code></strong> :&ensp;<code>array</code> of <code>str</code></dt>
<dd>filenames for the individual data frames in the set</dd>
<dt><strong><code>data_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>data directory (path)</dd>
<dt><strong><code>Sample_ID</code></strong> :&ensp;<code>str</code></dt>
<dd>Sample ID</dd>
<dt><strong><code>ftype</code></strong> :&ensp;<code>int</code></dt>
<dd>file type (0 - Shan Xu's .dat, 1 - tif)</dd>
<dt><strong><code>PixelSize</code></strong> :&ensp;<code>float</code></dt>
<dd>pixel size in nm. This is inherited from FIBSEM_frame object. Default is 8.0</dd>
<dt><strong><code>voxel_size</code></strong> :&ensp;<code>rec.array(( float,
float,
float), dtype=[('x', '&lt;f4'), ('y', '&lt;f4'), ('z', '&lt;f4')])</code></dt>
<dd>voxel size in nm. Default is isotropic (PixelSize, PixelSize, PixelSize)</dd>
<dt><strong><code>Scaling</code></strong> :&ensp;<code>2D array</code> of <code>floats</code></dt>
<dd>scaling parameters allowing to convert I16 data into actual electron counts</dd>
<dt><strong><code>fnm_reg</code></strong> :&ensp;<code>str</code></dt>
<dd>filename for the final registed dataset</dd>
<dt><strong><code>use_DASK</code></strong> :&ensp;<code>boolean</code></dt>
<dd>use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).</dd>
<dt><strong><code>threshold_min</code></strong> :&ensp;<code>float</code></dt>
<dd>CDF threshold for determining the minimum data value</dd>
<dt><strong><code>threshold_max</code></strong> :&ensp;<code>float</code></dt>
<dd>CDF threshold for determining the maximum data value</dd>
<dt><strong><code>nbins</code></strong> :&ensp;<code>int</code></dt>
<dd>number of histogram bins for building the PDF and CDF</dd>
<dt><strong><code>sliding_minmax</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
if False - same data_min_glob and data_max_glob will be used for all files</dd>
<dt><strong><code>TransformType</code></strong> :&ensp;<code>object reference</code></dt>
<dd>Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
Choose from the following options:
ShiftTransform - only x-shift and y-shift
XScaleShiftTransform
-
x-scale, x-shift, y-shift
ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
AffineTransform -
full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform</dd>
<dt><strong><code>l2_matrix</code></strong> :&ensp;<code>2D float array</code></dt>
<dd>matrix of regularization (shrinkage) parameters</dd>
<dt>targ_vector = 1D float array</dt>
<dt>target vector for regularization</dt>
<dt><strong><code>solver</code></strong> :&ensp;<code>str</code></dt>
<dd>Solver used for SIFT ('RANSAC' or 'LinReg')</dd>
<dt><strong><code>drmax</code></strong> :&ensp;<code>float</code></dt>
<dd>In the case of 'RANSAC' - Maximum distance for a data point to be classified as an inlier.
In the case of 'LinReg' - outlier threshold for iterative regression</dd>
<dt><strong><code>max_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>Max number of iterations in the iterative procedure above (RANSAC or LinReg)</dd>
<dt><strong><code>BFMatcher</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used</dd>
<dt><strong><code>save_matches</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, matches will be saved into individual files</dd>
<dt><strong><code>kp_max_num</code></strong> :&ensp;<code>int</code></dt>
<dd>Max number of key-points to be matched.
Key-points in every frame are indexed (in descending order) by the strength of the response.
Only kp_max_num is kept for further processing.
Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)</dd>
<dt><strong><code>SIFT_nfeatures</code></strong> :&ensp;<code>int</code></dt>
<dd>SIFT libary default is 0. The number of best features to retain.
The features are ranked by their scores (measured in SIFT algorithm as the local contrast)</dd>
<dt><strong><code>SIFT_nOctaveLayers</code></strong> :&ensp;<code>int</code></dt>
<dd>SIFT libary default
is 3. The number of layers in each octave.
3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.</dd>
<dt><strong><code>SIFT_contrastThreshold</code></strong> :&ensp;<code>double</code></dt>
<dd>SIFT libary default
is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
The larger the threshold, the less features are produced by the detector.
The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
When nOctaveLayers is set to default and if you want to use the value used in
D. Lowe paper (0.03), set this argument to 0.09.</dd>
<dt><strong><code>SIFT_edgeThreshold</code></strong> :&ensp;<code>double</code></dt>
<dd>SIFT libary default
is 10. The threshold used to filter out edge-like features.
Note that the its meaning is different from the contrastThreshold,
i.e. the larger the edgeThreshold, the less features are filtered out
(more features are retained).</dd>
<dt><strong><code>SIFT_sigma</code></strong> :&ensp;<code>double</code></dt>
<dd>SIFT library default is 1.6.
The sigma of the Gaussian applied to the input image at the octave #0.
If your image is captured with a weak camera with soft lenses, you might want to reduce the number.</dd>
<dt>save_res_png
: boolean</dt>
<dt>Save PNG images of the intermediate processing statistics and final registration quality check</dt>
<dt><strong><code>dtp</code></strong> :&ensp;<code>Data Type</code></dt>
<dd>Python data type for saving. Deafult is int16, the other option currently is uint8.</dd>
<dt><strong><code>zbin_factor</code></strong> :&ensp;<code>int</code></dt>
<dd>binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.</dd>
<dt><strong><code>flipY</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, the data will be flipped along Y-axis. Default is False.</dd>
<dt><strong><code>preserve_scales</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.</dd>
<dt><strong><code>fit_params</code></strong> :&ensp;<code>list</code></dt>
<dd>Example: ['SG', 501, 3]
- perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
Other options are:
['LF'] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
['PF', 2]
- use polynomial fit (in this case of order 2)</dd>
<dt><strong><code>int_order</code></strong> :&ensp;<code>int</code></dt>
<dd>The order of interpolation (when transforming the data).
The order has to be in the range 0-5:
0: Nearest-neighbor
1: Bi-linear (default)
2: Bi-quadratic
3: Bi-cubic
4: Bi-quartic
5: Bi-quintic</dd>
<dt><strong><code>subtract_linear_fit</code></strong> :&ensp;<code>[boolean, boolean]</code></dt>
<dd>List of two Boolean values for two directions: X- and Y-.
If True, the linear slopes along X- and Y- directions (respectively)
will be subtracted from the cumulative shifts.
This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.</dd>
<dt><strong><code>pad_edges</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, the data will be padded before transformation to avoid clipping.</dd>
<dt><strong><code>ImgB_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>fractional ratio of Image B to be used for constructing the fuksed image:
ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction</dd>
<dt><strong><code>evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>SIFT_evaluation(eval_fls = [], **kwargs):
Evaluate SIFT settings and perfromance of few test frames (eval_fls).</p>
<p>convert_raw_data_to_tif_files(DASK_client = '', **kwargs):
Convert binary ".dat" files into ".tif" files</p>
<p>evaluate_FIBSEM_statistics(self, DASK_client, **kwargs):
Evaluates parameters of FIBSEM data set (data Min/Max, Working Distance, Milling Y Voltage, FOV center positions).</p>
<p>extract_keypoints(DASK_client, **kwargs):
Extract Key-Points and Descriptors</p>
<p>determine_transformations(DASK_client, **kwargs):
Determine transformation matrices for sequential frame pairs</p>
<p>process_transformation_matrix(**kwargs):
Calculate cumulative transformation matrix</p>
<p>save_parameters(**kwargs):
Save transformation attributes and parameters (including transformation matrices)</p>
<p>check_for_nomatch_frames(thr_npt, **kwargs):
Check for frames with low number of Key-Point matches,m exclude them and re-calculate the cumulative transformation matrix</p>
<p>transform_and_save(DASK_client, **kwargs):
Transform the frames using the cumulative transformation matrix and save the data set into .mrc file</p>
<p>show_eval_box(**kwargs):
Show the box used for evaluating the registration quality</p>
<p>estimate_SNRs(**kwargs):
Estimate SNRs in Image A and Image B based on single-image SNR calculation.</p>
<p>evaluate_ImgB_fractions(ImgB_fractions, frame_inds, **kwargs):
Calculate NCC and SNR vs Image B fraction over a set of frames.</p>
<p>Initializes an instance of
FIBSEM_dataset object. ©G.Shtengel 10/2021 gleb.shtengel@gmail.com</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>fls</code></strong> :&ensp;<code>array</code> of <code>str</code></dt>
<dd>filenames for the individual data frames in the set</dd>
<dt><strong><code>data_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>data directory (path)</dd>
</dl>
<h2 id="kwargs">Kwargs</h2>
<p>ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
use_DASK : boolean
use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails
Sample_ID : str
Sample ID
PixelSize : float
pixel size in nm. Default is 8.0
Scaling : 2D array of floats
scaling parameters allowing to convert I16 data into actual electron counts
threshold_min : float
CDF threshold for determining the minimum data value
threshold_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF
sliding_minmax : boolean
if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
if False - same data_min_glob and data_max_glob will be used for all files
TransformType : object reference
Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
Choose from the following options:
ShiftTransform - only x-shift and y-shift
XScaleShiftTransform
-
x-scale, x-shift, y-shift
ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
AffineTransform -
full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
l2_matrix : 2D float array
matrix of regularization (shrinkage) parameters
targ_vector = 1D float array
target vector for regularization
solver : str
Solver used for SIFT ('RANSAC' or 'LinReg')
drmax : float
In the case of 'RANSAC' - Maximum distance for a data point to be classified as an inlier.
In the case of 'LinReg' - outlier threshold for iterative regression
max_iter : int
Max number of iterations in the iterative procedure above (RANSAC or LinReg)
BFMatcher : boolean
If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
save_matches : boolean
If True, matches will be saved into individual files
kp_max_num : int
Max number of key-points to be matched.
Key-points in every frame are indexed (in descending order) by the strength of the response.
Only kp_max_num is kept for further processing.
Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
SIFT_nfeatures : int
SIFT libary default is 0. The number of best features to retain.
The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
SIFT_nOctaveLayers : int
SIFT libary default
is 3. The number of layers in each octave.
3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
SIFT_contrastThreshold : double
SIFT libary default
is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
The larger the threshold, the less features are produced by the detector.
The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
When nOctaveLayers is set to default and if you want to use the value used in
D. Lowe paper (0.03), set this argument to 0.09.
SIFT_edgeThreshold : double
SIFT libary default
is 10. The threshold used to filter out edge-like features.
Note that the its meaning is different from the contrastThreshold,
i.e. the larger the edgeThreshold, the less features are filtered out
(more features are retained).
SIFT_sigma : double
SIFT library default is 1.6.
The sigma of the Gaussian applied to the input image at the octave #0.
If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check
dtp : Data Type
Python data type for saving. Deafult is int16, the other option currently is uint8.
zbin_factor : int
binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
preserve_scales : boolean
If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
fit_params : list
Example: ['SG', 501, 3]
- perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
Other options are:
['LF'] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
['PF', 2]
- use polynomial fit (in this case of order 2)
int_order : int
The order of interpolation (when transforming the data).
The order has to be in the range 0-5:
0: Nearest-neighbor
1: Bi-linear (default)
2: Bi-quadratic
3: Bi-cubic
4: Bi-quartic
5: Bi-quintic
subtract_linear_fit : [boolean, boolean]
List of two Boolean values for two directions: X- and Y-.
If True, the linear slopes along X- and Y- directions (respectively)
will be subtracted from the cumulative shifts.
This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
pad_edges : boolean
If True, the data will be padded before transformation to avoid clipping.
disp_res : boolean
If False, the intermediate printouts will be suppressed</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FIBSEM_dataset:
    &#34;&#34;&#34;
    A class representing a FIB-SEM data set
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com
    Contains the info/settings on the FIB-SEM dataset and the procedures that can be performed on it.

    Attributes
    ----------
    fls : array of str
        filenames for the individual data frames in the set
    data_dir : str
        data directory (path)
    Sample_ID : str
            Sample ID
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    PixelSize : float
        pixel size in nm. This is inherited from FIBSEM_frame object. Default is 8.0
    voxel_size : rec.array(( float,  float,  float), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
        voxel size in nm. Default is isotropic (PixelSize, PixelSize, PixelSize)
    Scaling : 2D array of floats
        scaling parameters allowing to convert I16 data into actual electron counts
    fnm_reg : str
        filename for the final registed dataset
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    sliding_minmax : boolean
        if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
        if False - same data_min_glob and data_max_glob will be used for all files
    TransformType : object reference
        Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
        Choose from the following options:
            ShiftTransform - only x-shift and y-shift
            XScaleShiftTransform  -  x-scale, x-shift, y-shift
            ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
            AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
            RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
    l2_matrix : 2D float array
        matrix of regularization (shrinkage) parameters
    targ_vector = 1D float array
        target vector for regularization
    solver : str
        Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
    drmax : float
        In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
        In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
    max_iter : int
        Max number of iterations in the iterative procedure above (RANSAC or LinReg)
    BFMatcher : boolean
        If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches : boolean
        If True, matches will be saved into individual files
    kp_max_num : int
        Max number of key-points to be matched.
        Key-points in every frame are indexed (in descending order) by the strength of the response.
        Only kp_max_num is kept for further processing.
        Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
    SIFT_nfeatures : int
        SIFT libary default is 0. The number of best features to retain.
        The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
    SIFT_nOctaveLayers : int
        SIFT libary default  is 3. The number of layers in each octave.
        3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
    SIFT_contrastThreshold : double
        SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
        The larger the threshold, the less features are produced by the detector.
        The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
        When nOctaveLayers is set to default and if you want to use the value used in
        D. Lowe paper (0.03), set this argument to 0.09.
    SIFT_edgeThreshold : double
        SIFT libary default  is 10. The threshold used to filter out edge-like features.
        Note that the its meaning is different from the contrastThreshold,
        i.e. the larger the edgeThreshold, the less features are filtered out
        (more features are retained).
    SIFT_sigma : double
        SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
        If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    dtp : Data Type
        Python data type for saving. Deafult is int16, the other option currently is uint8.
    zbin_factor : int
        binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
    flipY : boolean
        If True, the data will be flipped along Y-axis. Default is False.
    preserve_scales : boolean
        If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
    fit_params : list
        Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
        Other options are:
            [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
            [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
    int_order : int
        The order of interpolation (when transforming the data).
            The order has to be in the range 0-5:
                0: Nearest-neighbor
                1: Bi-linear (default)
                2: Bi-quadratic
                3: Bi-cubic
                4: Bi-quartic
                5: Bi-quintic
    subtract_linear_fit : [boolean, boolean]
        List of two Boolean values for two directions: X- and Y-.
        If True, the linear slopes along X- and Y- directions (respectively)
        will be subtracted from the cumulative shifts.
        This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.
    ImgB_fraction : float
            fractional ratio of Image B to be used for constructing the fuksed image:
            ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction
    evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.

    Methods
    -------
    SIFT_evaluation(eval_fls = [], **kwargs):
        Evaluate SIFT settings and perfromance of few test frames (eval_fls).

    convert_raw_data_to_tif_files(DASK_client = &#39;&#39;, **kwargs):
        Convert binary &#34;.dat&#34; files into &#34;.tif&#34; files

    evaluate_FIBSEM_statistics(self, DASK_client, **kwargs):
        Evaluates parameters of FIBSEM data set (data Min/Max, Working Distance, Milling Y Voltage, FOV center positions).

    extract_keypoints(DASK_client, **kwargs):
        Extract Key-Points and Descriptors

    determine_transformations(DASK_client, **kwargs):
        Determine transformation matrices for sequential frame pairs

    process_transformation_matrix(**kwargs):
        Calculate cumulative transformation matrix

    save_parameters(**kwargs):
        Save transformation attributes and parameters (including transformation matrices)

    check_for_nomatch_frames(thr_npt, **kwargs):
        Check for frames with low number of Key-Point matches,m exclude them and re-calculate the cumulative transformation matrix

    transform_and_save(DASK_client, **kwargs):
        Transform the frames using the cumulative transformation matrix and save the data set into .mrc file

    show_eval_box(**kwargs):
        Show the box used for evaluating the registration quality

    estimate_SNRs(**kwargs):
        Estimate SNRs in Image A and Image B based on single-image SNR calculation.

    evaluate_ImgB_fractions(ImgB_fractions, frame_inds, **kwargs):
        Calculate NCC and SNR vs Image B fraction over a set of frames.
    &#34;&#34;&#34;

    def __init__(self, fls, **kwargs):
        &#34;&#34;&#34;
        Initializes an instance of  FIBSEM_dataset object. ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

        Parameters
        ----------
        fls : array of str
            filenames for the individual data frames in the set
        data_dir : str
            data directory (path)

        kwargs
        ---------
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        Sample_ID : str
                Sample ID
        PixelSize : float
            pixel size in nm. Default is 8.0
        Scaling : 2D array of floats
            scaling parameters allowing to convert I16 data into actual electron counts
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        sliding_minmax : boolean
            if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
            if False - same data_min_glob and data_max_glob will be used for all files
        TransformType : object reference
            Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
            Choose from the following options:
                ShiftTransform - only x-shift and y-shift
                XScaleShiftTransform  -  x-scale, x-shift, y-shift
                ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        kp_max_num : int
            Max number of key-points to be matched.
            Key-points in every frame are indexed (in descending order) by the strength of the response.
            Only kp_max_num is kept for further processing.
            Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
        SIFT_nfeatures : int
            SIFT libary default is 0. The number of best features to retain.
            The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
        SIFT_nOctaveLayers : int
            SIFT libary default  is 3. The number of layers in each octave.
            3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
        SIFT_contrastThreshold : double
            SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
            The larger the threshold, the less features are produced by the detector.
            The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
            When nOctaveLayers is set to default and if you want to use the value used in
            D. Lowe paper (0.03), set this argument to 0.09.
        SIFT_edgeThreshold : double
            SIFT libary default  is 10. The threshold used to filter out edge-like features.
            Note that the its meaning is different from the contrastThreshold,
            i.e. the larger the edgeThreshold, the less features are filtered out
            (more features are retained).
        SIFT_sigma : double
            SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
            If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        dtp : Data Type
            Python data type for saving. Deafult is int16, the other option currently is uint8.
        zbin_factor : int
            binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
        preserve_scales : boolean
            If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
        fit_params : list
            Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
            Other options are:
                [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
                [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
        int_order : int
            The order of interpolation (when transforming the data).
                The order has to be in the range 0-5:
                    0: Nearest-neighbor
                    1: Bi-linear (default)
                    2: Bi-quadratic
                    3: Bi-cubic
                    4: Bi-quartic
                    5: Bi-quintic
        subtract_linear_fit : [boolean, boolean]
            List of two Boolean values for two directions: X- and Y-.
            If True, the linear slopes along X- and Y- directions (respectively)
            will be subtracted from the cumulative shifts.
            This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.
        disp_res : boolean
            If False, the intermediate printouts will be suppressed
        &#34;&#34;&#34;

        disp_res = kwargs.get(&#39;disp_res&#39;, True)
        self.fls = fls
        self.fnms = [os.path.splitext(fl)[0] + &#39;_kpdes.bin&#39; for fl in fls]
        self.nfrs = len(fls)
        if disp_res:
            print(&#39;Total Number of frames: &#39;, self.nfrs)
        self.data_dir = kwargs.get(&#39;data_dir&#39;, os.getcwd())
        self.ftype = kwargs.get(&#34;ftype&#34;, 0) # ftype=0 - Shan Xu&#39;s binary format  ftype=1 - tif files
        mid_frame = FIBSEM_frame(fls[self.nfrs//2], ftype = self.ftype)
        self.XResolution = kwargs.get(&#34;XResolution&#34;, mid_frame.XResolution)
        self.YResolution = kwargs.get(&#34;YResolution&#34;, mid_frame.YResolution)
        self.Scaling = kwargs.get(&#34;Scaling&#34;, mid_frame.Scaling)
        if hasattr(mid_frame, &#39;PixelSize&#39;):
            self.PixelSize = kwargs.get(&#34;PixelSize&#34;, mid_frame.PixelSize)
        else:
            self.PixelSize = kwargs.get(&#34;PixelSize&#34;, 8.0)
        self.voxel_size = np.rec.array((self.PixelSize,  self.PixelSize,  self.PixelSize), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
        if hasattr(self, &#39;YResolution&#39;):
            YResolution_default = self.YResolution
        else:
            YResolution_default = FIBSEM_frame(self.fls[len(self.fls)//2]).YResolution
        YResolution = kwargs.get(&#34;YResolution&#34;, YResolution_default)

        test_frame = FIBSEM_frame(fls[0], ftype=self.ftype)
        self.DetA = test_frame.DetA
        self.DetB = test_frame.DetB
        self.ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, 0.0)
        if self.DetB == &#39;None&#39;:
            ImgB_fraction = 0.0
        self.Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
        self.EightBit = kwargs.get(&#34;EightBit&#34;, 1)
        self.use_DASK = kwargs.get(&#34;use_DASK&#34;, True)
        self.DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        self.threshold_min = kwargs.get(&#34;threshold_min&#34;, 1e-3)
        self.threshold_max = kwargs.get(&#34;threshold_max&#34;, 1e-3)
        self.nbins = kwargs.get(&#34;nbins&#34;, 256)
        self.sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, True)
        self.TransformType = kwargs.get(&#34;TransformType&#34;, RegularizedAffineTransform)
        self.tr_matr_cum_residual = [np.eye(3,3) for i in np.arange(self.nfrs)]  # placeholder - identity transformation matrix
        l2_param_default = 1e-5                                  # regularization strength (shrinkage parameter)
        l2_matrix_default = np.eye(6)*l2_param_default                   # initially set equal shrinkage on all coefficients
        l2_matrix_default[2,2] = 0                                 # turn OFF the regularization on shifts
        l2_matrix_default[5,5] = 0                                 # turn OFF the regularization on shifts
        self.l2_matrix = kwargs.get(&#34;l2_matrix&#34;, l2_matrix_default)
        self.targ_vector = kwargs.get(&#34;targ_vector&#34;, np.array([1, 0, 0, 0, 1, 0]))   # target transformation is shift only: Sxx=Syy=1, Sxy=Syx=0
        self.solver = kwargs.get(&#34;solver&#34;, &#39;RANSAC&#39;)
        self.drmax = kwargs.get(&#34;drmax&#34;, 2.0)
        self.max_iter = kwargs.get(&#34;max_iter&#34;, 1000)
        self.BFMatcher = kwargs.get(&#34;BFMatcher&#34;, False)           # If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        self.save_matches = kwargs.get(&#34;save_matches&#34;, True)      # If True, matches will be saved into individual files
        self.kp_max_num = kwargs.get(&#34;kp_max_num&#34;, -1)
        self.SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, 0)
        self.SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, 3)
        self.SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, 0.04)
        self.SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, 10)
        self.SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, 1.6)
        self.save_res_png  = kwargs.get(&#34;save_res_png&#34;, True)
        self.zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, 1)         # binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
        self.eval_metrics = kwargs.get(&#39;eval_metrics&#39;, [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;])
        self.fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
        self.flipY = kwargs.get(&#34;flipY&#34;, False)                     # If True, the registered data will be flipped along Y axis
        self.preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, True) # If True, the transformation matrix will be adjusted using teh settings defined by fit_params below
        self.fit_params =  kwargs.get(&#34;fit_params&#34;, False)          # perform the above adjustment using  Savitzky-Golay (SG) fith with parameters
                                                                    # window size 701, polynomial order 3

        self.int_order = kwargs.get(&#34;int_order&#34;, False)             #     The order of interpolation. The order has to be in the range 0-5:
                                                                    #    - 0: Nearest-neighbor
                                                                    #    - 1: Bi-linear (default)
                                                                    #    - 2: Bi-quadratic
                                                                    #    - 3: Bi-cubic
                                                                    #    - 4: Bi-quartic
                                                                    #    - 5: Bi-quintic
        self.subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, [True, True])   # If True, the linear slope will be subtracted from the cumulative shifts.
        self.subtract_FOVtrend_from_fit = kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, [True, True])
        self.FOVtrend_x = np.zeros(len(fls))
        self.FOVtrend_y = np.zeros(len(fls))
        self.pad_edges =  kwargs.get(&#34;pad_edges&#34;, True)
        build_fnm_reg, build_dtp = build_filename(fls[0], **kwargs)
        self.fnm_reg = kwargs.get(&#34;fnm_reg&#34;, build_fnm_reg)
        self.dtp = kwargs.get(&#34;dtp&#34;, build_dtp)
        if disp_res:
            print(&#39;Registered data will be saved into: &#39;, self.fnm_reg)


        kwargs.update({&#39;data_dir&#39; : self.data_dir, &#39;fnm_reg&#39; : self.fnm_reg, &#39;dtp&#39; : self.dtp})

        if kwargs.get(&#34;recall_parameters&#34;, False):
            dump_filename = kwargs.get(&#34;dump_filename&#34;, &#39;&#39;)
            try:
                dump_data = pickle.load(open(dump_filename, &#39;rb&#39;))
                dump_loaded = True
            except Exception as ex1:
                dump_loaded = False
                if disp_res:
                    print(&#39;Failed to open Parameter dump filename: &#39;, dump_filename)
                    print(ex1.message)

            if dump_loaded:
                try:
                    for key in tqdm(dump_data, desc=&#39;Recalling the data set parameters&#39;):
                        setattr(self, key, dump_data[key])
                except Exception as ex2:
                    if disp_res:
                        print(&#39;Parameter dump filename: &#39;, dump_filename)
                        print(&#39;Failed to restore the object parameters&#39;)
                        print(ex2.message)



    def SIFT_evaluation(self, eval_fls = [], **kwargs):
        &#39;&#39;&#39;
        Evaluate SIFT settings and perfromance of few test frames (eval_fls). ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

        Parameters:
        eval_fls : array of str
            filenames for the data frames to be used for SIFT evaluation

        kwargs
        ---------
        data_dir : str
            data directory (path)
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        fnm_reg : str
            filename for the final registed dataset
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        TransformType : object reference
            Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
            Choose from the following options:
                ShiftTransform - only x-shift and y-shift
                XScaleShiftTransform  -  x-scale, x-shift, y-shift
                ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        kp_max_num : int
            Max number of key-points to be matched.
            Key-points in every frame are indexed (in descending order) by the strength of the response.
            Only kp_max_num is kept for further processing.
            Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
        SIFT_nfeatures : int
            SIFT libary default is 0. The number of best features to retain.
            The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
        SIFT_nOctaveLayers : int
            SIFT libary default  is 3. The number of layers in each octave.
            3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
        SIFT_contrastThreshold : double
            SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
            The larger the threshold, the less features are produced by the detector.
            The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
            When nOctaveLayers is set to default and if you want to use the value used in
            D. Lowe paper (0.03), set this argument to 0.09.
        SIFT_edgeThreshold : double
            SIFT libary default  is 10. The threshold used to filter out edge-like features.
            Note that the its meaning is different from the contrastThreshold,
            i.e. the larger the edgeThreshold, the less features are filtered out
            (more features are retained).
        SIFT_sigma : double
            SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
            If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check

        Returns:
        dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts
        &#39;&#39;&#39;
        if len(eval_fls) == 0:
            eval_fls = [self.fls[self.nfrs//2], self.fls[self.nfrs//2+1]]
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        threshold_min = kwargs.get(&#34;threshold_min&#34;, self.threshold_min)
        threshold_max = kwargs.get(&#34;threshold_max&#34;, self.threshold_max)
        nbins = kwargs.get(&#34;nbins&#34;, self.nbins)
        TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
        l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
        targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
        solver = kwargs.get(&#34;solver&#34;, self.solver)
        drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
        max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
        kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
        SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, self.SIFT_nfeatures)
        SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, self.SIFT_nOctaveLayers)
        SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, self.SIFT_contrastThreshold)
        SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, self.SIFT_edgeThreshold)
        SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, self.SIFT_sigma)
        Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)
        BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
        save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])

        SIFT_evaluation_kwargs = {&#39;ftype&#39; : ftype,
                                &#39;Sample_ID&#39; : Sample_ID,
                                &#39;data_dir&#39; : data_dir,
                                &#39;fnm_reg&#39; : fnm_reg,
                                &#39;threshold_min&#39; : threshold_min,
                                &#39;threshold_max&#39; : threshold_max,
                                &#39;nbins&#39; : nbins,
                                &#39;evaluation_box&#39; : evaluation_box,
                                &#39;TransformType&#39; : TransformType,
                                &#39;l2_matrix&#39; : l2_matrix,
                                &#39;targ_vector&#39; : targ_vector,
                                &#39;solver&#39; : solver,
                                &#39;drmax&#39; : drmax,
                                &#39;max_iter&#39; : max_iter,
                                &#39;kp_max_num&#39; : kp_max_num,
                                &#39;SIFT_Transform&#39; : TransformType,
                                &#39;SIFT_nfeatures&#39; : SIFT_nfeatures,
                                &#39;SIFT_nOctaveLayers&#39; : SIFT_nOctaveLayers,
                                &#39;SIFT_contrastThreshold&#39; : SIFT_contrastThreshold,
                                &#39;SIFT_edgeThreshold&#39; : SIFT_edgeThreshold,
                                &#39;SIFT_sigma&#39; : SIFT_sigma,
                                &#39;Lowe_Ratio_Threshold&#39; : Lowe_Ratio_Threshold,
                                &#39;BFMatcher&#39; : BFMatcher,
                                &#39;save_matches&#39; : save_matches,
                                &#39;save_res_png&#39;  : save_res_png}

        dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts = SIFT_evaluation_dataset(eval_fls, **SIFT_evaluation_kwargs)
        src_pts_filtered, dst_pts_filtered = kpts
        print(&#39;Transformation Matrix determined using &#39;+ TransformType.__name__ +&#39; using &#39; + solver + &#39; solver&#39;)
        print(transform_matrix)
        print(&#39;{:d} keypoint matches were detected with {:.1f} pixel outlier threshold&#39;.format(n_matches, drmax))
        print(&#39;Number of iterations: {:d}&#39;.format(iteration))
        return dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts


    def convert_raw_data_to_tif_files(self, DASK_client = &#39;&#39;, **kwargs):
        &#39;&#39;&#39;
        Convert binary &#34;.dat&#34; files into &#34;.tif&#34; files.

        Parameters:
        DASK_client : instance of the DASK client object

        kwargs
        ---------
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        &#39;&#39;&#39;
        if hasattr(self, &#34;use_DASK&#34;):
            use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
        else:
            use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
        if hasattr(self, &#34;DASK_client_retries&#34;):
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
        else:
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        if self.ftype ==0 :
            print(&#39;Step 2a: Creating &#34;*InLens.tif&#34; files using DASK distributed&#39;)
            t00 = time.time()
            if use_DASK:
                try:
                    futures = DASK_client.map(save_inlens_data, self.fls, retries = DASK_client_retries)
                    fls_new = np.array(DASK_client.gather(futures))
                except:
                    fls_new = []
                    for fl in tqdm(self.fls, desc = &#39;Converting .dat data files into .tif format&#39;):
                            fls_new.append(save_inlens_data(fl))
            else:
                fls_new = []
                for fl in tqdm(self.fls, desc = &#39;Converting .dat data files into .tif format&#39;):
                    fls_new.append(save_inlens_data(fl))

            t01 = time.time()
            print(&#39;Step 2a: Elapsed time: {:.2f} seconds&#39;.format(t01 - t00))
            print(&#39;Step 2a: Quick check if all files were converted: &#39;, np.array_equal(self.fls, fls_new))
        else:
            print(&#39;Step 2a: data is already in TIF format&#39;)


    def evaluate_FIBSEM_statistics(self, DASK_client, **kwargs):
        &#39;&#39;&#39;
        Evaluates parameters of FIBSEM data set (Min/Max, Working Distance (WD), Milling Y Voltage (MV), FOV center positions).

        Parameters:
        use_DASK : boolean
            perform remote DASK computations
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails

        kwargs:
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        frame_inds : array
            Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
        data_dir : str
            data directory (path)  for saving the data
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        sliding_minmax : boolean
            if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
            if False - same data_min_glob and data_max_glob will be used for all files
        fit_params : list
            Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
            Other options are:
                [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
                [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
        Mill_Volt_Rate_um_per_V : float
            Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.
        FIBSEM_Data_xlsx : str
            Filepath of the Excell file for the FIBSEM data set data to be saved (Data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)
        disp_res : bolean
            If True (default), intermediate messages and results will be displayed.

        Returns:
        list of 9 parameters: FIBSEM_Data_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding, mill_rate_WD, mill_rate_MV, center_x, center_y
            FIBSEM_Data_xlsx : str
                path to Excel file with the FIBSEM data
            data_min_glob : float
                min data value for I8 conversion (open CV SIFT requires I8)
            data_man_glob : float
                max data value for I8 conversion (open CV SIFT requires I8)
            data_min_sliding : float array
                min data values (one per file) for I8 conversion
            data_max_sliding : float array
                max data values (one per file) for I8 conversion

            mill_rate_WD : float array
                Milling rate calculated based on Working Distance (WD)
            mill_rate_MV : float array
                Milling rate calculated based on Milling Y Voltage (MV)
            center_x : float array
                FOV Center X-coordinate extrated from the header data
            center_y : float array
                FOV Center Y-coordinate extrated from the header data
        &#39;&#39;&#39;
        if hasattr(self, &#34;use_DASK&#34;):
            use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
        else:
            use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
        if hasattr(self, &#34;DASK_client_retries&#34;):
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
        else:
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        frame_inds = kwargs.get(&#34;frame_inds&#34;, np.arange(len(self.fls)))
        data_dir = self.data_dir
        threshold_min = kwargs.get(&#34;threshold_min&#34;, self.threshold_min)
        threshold_max = kwargs.get(&#34;threshold_max&#34;, self.threshold_max)
        nbins = kwargs.get(&#34;nbins&#34;, self.nbins)
        sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, self.sliding_minmax)
        fit_params = kwargs.get(&#34;fit_params&#34;, self.fit_params)

        if hasattr(self, &#39;Mill_Volt_Rate_um_per_V&#39;):
            Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, self.Mill_Volt_Rate_um_per_V)
        else:
            Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, 31.235258870176065)
        FIBSEM_Data_xlsx = kwargs.get(&#39;FIBSEM_Data_xlsx&#39;, &#39;FIBSEM_Data_xlsx.xlsx&#39;)
        disp_res = kwargs.get(&#39;disp_res&#39;, True)

        local_kwargs = {&#39;use_DASK&#39; : use_DASK,
                        &#39;DASK_client_retries&#39; : DASK_client_retries,
                        &#39;ftype&#39; : ftype,
                        &#39;frame_inds&#39; : frame_inds,
                        &#39;data_dir&#39; : data_dir,
                        &#39;threshold_min&#39; : threshold_min,
                        &#39;threshold_max&#39; : threshold_max,
                        &#39;nbins&#39; : nbins,
                        &#39;sliding_minmax&#39; : sliding_minmax,
                        &#39;fit_params&#39; : fit_params,
                        &#39;Mill_Volt_Rate_um_per_V&#39; : Mill_Volt_Rate_um_per_V,
                        &#39;FIBSEM_Data_xlsx&#39; : FIBSEM_Data_xlsx,
                        &#39;disp_res&#39; : disp_res}

        if disp_res:
            print(&#39;Evaluating the parameters of FIBSEM data set (data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)&#39;)
        self.FIBSEM_Data = evaluate_FIBSEM_frames_dataset(self.fls, DASK_client, **local_kwargs)
        self.data_minmax = self.FIBSEM_Data[0:5]
        WD = self.FIBSEM_Data[5]
        MillingYVoltage = self.FIBSEM_Data[6]

        apert = np.min((51, len(self.FIBSEM_Data[7])-1))
        self.FOVtrend_x = savgol_filter(self.FIBSEM_Data[7]*1.0, apert, 1) - self.FIBSEM_Data[7][0]
        self.FOVtrend_y = savgol_filter(self.FIBSEM_Data[8]*1.0, apert, 1) - self.FIBSEM_Data[8][0]

        WD_fit_coef = np.polyfit(frame_inds, WD, 1)
        rate_WD = WD_fit_coef[0]*1.0e6

        MV_fit_coef = np.polyfit(frame_inds, MillingYVoltage, 1)
        rate_MV = MV_fit_coef[0]*Mill_Volt_Rate_um_per_V*-1.0e3

        Z_pixel_size_WD = rate_WD
        Z_pixel_size_MV = rate_MV

        if ftype == 0:
            if disp_res:
                if self.zbin_factor &gt; 1:
                    print(&#39;Z pixel (after {:d}-x Z-binning) = {:.2f} nm - based on WD data&#39;.format(self.zbin_factor, Z_pixel_size_WD*self.zbin_factor))
                    print(&#39;Z pixel (after {:d}-x Z-binning) = {:.2f} nm - based on Milling Voltage data&#39;.format(self.zbin_factor, Z_pixel_size_MV*self.zbin_factor))
                else:
                    print(&#39;Z pixel = {:.2f} nm  - based on WD data&#39;.format(Z_pixel_size_WD))
                    print(&#39;Z pixel = {:.2f} nm  - based on Milling Voltage data&#39;.format(Z_pixel_size_MV))

            self.voxel_size = np.rec.array((self.PixelSize,  self.PixelSize,  Z_pixel_size_WD), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
        else:
            if disp_res:
                print(&#39;No milling rate data is available, isotropic voxel size is set to {:.2f} nm&#39;.format(self.PixelSize))
            self.voxel_size = np.rec.array((self.PixelSize,  self.PixelSize,  Z_pixel_size_WD), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])

        return self.FIBSEM_Data


    def extract_keypoints(self, DASK_client, **kwargs):
        &#39;&#39;&#39;
        Extract Key-Points and Descriptors

        Parameters:
        DASK_client : instance of the DASK client object

        kwargs
        ---------
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        EightBit : int
            0 - 16-bit data, 1: 8-bit data
        fnm_reg : str
            filename for the final registed dataset
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        sliding_minmax : boolean
            if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
            if False - same data_min_glob and data_max_glob will be used for all files
        data_minmax : list of 5 parameters
            minmax_xlsx : str
                path to Excel file with Min/Max data
            data_min_glob : float
                min data value for I8 conversion (open CV SIFT requires I8)
            data_min_sliding : float array
                min data values (one per file) for I8 conversion
            data_max_sliding : float array
                max data values (one per file) for I8 conversion
            data_minmax_glob : 2D float array
                min and max data values without sliding averaging
        kp_max_num : int
            Max number of key-points to be matched.
            Key-points in every frame are indexed (in descending order) by the strength of the response.
            Only kp_max_num is kept for further processing.
            Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)

        Returns:
        fnms : array of str
            filenames for binary files kontaining Key-Point and Descriptors for each frame
        &#39;&#39;&#39;
        if len(self.fls) == 0:
            print(&#39;Data set not defined, perform initialization first&#39;)
            fnms = []
        else:
            if hasattr(self, &#34;use_DASK&#34;):
                use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
            else:
                use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
            if hasattr(self, &#34;DASK_client_retries&#34;):
                DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
            else:
                DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
            ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
            data_dir = self.data_dir
            fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
            threshold_min = kwargs.get(&#34;threshold_min&#34;, self.threshold_min)
            threshold_max = kwargs.get(&#34;threshold_max&#34;, self.threshold_max)
            nbins = kwargs.get(&#34;nbins&#34;, self.nbins)
            sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, self.sliding_minmax)
            data_minmax = kwargs.get(&#34;data_minmax&#34;, self.data_minmax)
            kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)

            SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, self.SIFT_nfeatures)
            SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, self.SIFT_nOctaveLayers)
            SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, self.SIFT_contrastThreshold)
            SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, self.SIFT_edgeThreshold)
            SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, self.SIFT_sigma)

            minmax_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding = data_minmax
            kpt_kwargs = {&#39;ftype&#39; : ftype,
                        &#39;threshold_min&#39; : threshold_min,
                        &#39;threshold_max&#39; : threshold_max,
                        &#39;nbins&#39; : nbins,
                        &#39;kp_max_num&#39; : kp_max_num,
                        &#39;SIFT_nfeatures&#39; : SIFT_nfeatures,
                        &#39;SIFT_nOctaveLayers&#39; : SIFT_nOctaveLayers,
                        &#39;SIFT_contrastThreshold&#39; : SIFT_contrastThreshold,
                        &#39;SIFT_edgeThreshold&#39; : SIFT_edgeThreshold,
                        &#39;SIFT_sigma&#39; : SIFT_sigma}

            if sliding_minmax:
                params_s3 = [[dts3[0], dts3[1], dts3[2], kpt_kwargs] for dts3 in zip(self.fls, data_min_sliding, data_max_sliding)]
            else:
                params_s3 = [[fl, data_min_glob, data_max_glob, kpt_kwargs] for fl in self.fls]
            if use_DASK:
                print(&#39;Using DASK distributed&#39;)
                futures_s3 = DASK_client.map(extract_keypoints_descr_files, params_s3, retries = DASK_client_retries)
                fnms = DASK_client.gather(futures_s3)
            else:
                print(&#39;Using Local Computation&#39;)
                fnms = []
                for j, param_s3 in enumerate(tqdm(params_s3, desc=&#39;Extracting Key Points and Descriptors: &#39;)):
                    fnms.append(extract_keypoints_descr_files(param_s3))

            self.fnms = fnms
        return fnms


    def determine_transformations(self, DASK_client, **kwargs):
        &#39;&#39;&#39;
        Determine transformation matrices for sequential frame pairs

        Parameters:
        DASK_client : instance of the DASK client object

        kwargs
        ---------
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        TransformType : object reference
                Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
                Choose from the following options:
                    ShiftTransform - only x-shift and y-shift
                    XScaleShiftTransform  -  x-scale, x-shift, y-shift
                    ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                    AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                    RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        Lowe_Ratio_Threshold : float
            threshold for Lowe&#39;s Ratio Test
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check


        Returns:
        results_s4 : array of lists containing the reults:
            results_s4 = [transformation_matrix, fnm_matches, npt, error_abs_mean]
            transformation_matrix : 2D float array
                transformation matrix for each sequential frame pair
            fnm_matches : str
                filename containing the matches used to determin the transformation for the par of frames
            npts : int
                number of matches
            error_abs_mean : float
                mean abs error of registration for all matched Key-Points
        &#39;&#39;&#39;
        if len(self.fnms) == 0:
            print(&#39;No data on individual key-point data files, peform key-point search&#39;)
            results_s4 = []
        else:
            if hasattr(self, &#34;use_DASK&#34;):
                use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
            else:
                use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
            if hasattr(self, &#34;DASK_client_retries&#34;):
                DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
            else:
                DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
            ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
            TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
            l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
            targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
            solver = kwargs.get(&#34;solver&#34;, self.solver)
            drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
            max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
            kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
            Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)   # threshold for Lowe&#39;s Ratio Test
            BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
            save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
            save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
            dt_kwargs = {&#39;ftype&#39; : ftype,
                            &#39;TransformType&#39; : TransformType,
                            &#39;l2_matrix&#39; : l2_matrix,
                            &#39;targ_vector&#39;: targ_vector,
                            &#39;solver&#39; : solver,
                            &#39;drmax&#39; : drmax,
                            &#39;max_iter&#39; : max_iter,
                            &#39;BFMatcher&#39; : BFMatcher,
                            &#39;save_matches&#39; : save_matches,
                            &#39;kp_max_num&#39; : kp_max_num,
                            &#39;Lowe_Ratio_Threshold&#39; : Lowe_Ratio_Threshold}

            params_s4 = []
            for j, fnm in enumerate(self.fnms[:-1]):
                fname1 = self.fnms[j]
                fname2 = self.fnms[j+1]
                params_s4.append([fname1, fname2, dt_kwargs])
            if use_DASK:
                print(&#39;Using DASK distributed&#39;)
                futures4 = DASK_client.map(determine_transformations_files, params_s4, retries = DASK_client_retries)
                #determine_transformations_files returns (transform_matrix, fnm_matches, kpts, iteration)
                results_s4 = DASK_client.gather(futures4)
            else:
                print(&#39;Using Local Computation&#39;)
                results_s4 = []
                for param_s4 in tqdm(params_s4, desc = &#39;Extracting Transformation Parameters: &#39;):
                    results_s4.append(determine_transformations_files(param_s4))
            #determine_transformations_files returns (transform_matrix, fnm_matches, kpts, errors, iteration)
            self.transformation_matrix = np.nan_to_num(np.array([result[0] for result in results_s4]))
            self.fnms_matches = [result[1] for result in results_s4]
            self.error_abs_mean = np.nan_to_num(np.array([result[3] for result in results_s4]))
            self.npts = np.nan_to_num(np.array([len(result[2][0])  for result in results_s4]))
            print(&#39;Mean Number of Keypoints :&#39;, np.mean(self.npts).astype(np.int16))
        return results_s4


    def process_transformation_matrix(self, **kwargs):
        &#39;&#39;&#39;
        Calculate cumulative transformation matrix

        kwargs
        ---------
        data_dir : str
            data directory (path)
        fnm_reg : str
            filename for the final registed dataset
        Sample_ID : str
            Sample ID
        TransformType : object reference
                Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
                Choose from the following options:
                    ShiftTransform - only x-shift and y-shift
                    XScaleShiftTransform  -  x-scale, x-shift, y-shift
                    ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                    AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                    RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        preserve_scales : boolean
            If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
        fit_params : list
            Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
            Other options are:
                [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
                [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
        subtract_linear_fit : [boolean, boolean]
            List of two Boolean values for two directions: X- and Y-.
            If True, the linear slopes along X- and Y- directions (respectively)
            will be subtracted from the cumulative shifts.
            This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.

        Returns:
        tr_matr_cum_residual, tr_matr_cum_xlsx_file : list of 2D arrays of float and the filename of the XLSX file with the transf matrix results
            Cumulative transformation matrices
        &#39;&#39;&#39;
        if len(self.transformation_matrix) == 0:
            print(&#39;No data on individual key-point matches, peform key-point search / matching first&#39;)
            self.tr_matr_cum_residual = []
        else:
            data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
            fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
            TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
            SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, self.SIFT_nfeatures)
            SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, self.SIFT_nOctaveLayers)
            SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, self.SIFT_contrastThreshold)
            SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, self.SIFT_edgeThreshold)
            SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, self.SIFT_sigma)
            Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
            l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
            targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
            solver = kwargs.get(&#34;solver&#34;, self.solver)
            drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
            max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
            BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
            save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
            kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
            save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
            preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, self.preserve_scales)
            fit_params =  kwargs.get(&#34;fit_params&#34;, self.fit_params)
            subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, self.subtract_linear_fit)
            subtract_FOVtrend_from_fit =  kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, self.subtract_FOVtrend_from_fit)
            pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)

            TM_kwargs = {&#39;fnm_reg&#39; : fnm_reg,
                            &#39;data_dir&#39; : data_dir,
                            &#39;TransformType&#39; : TransformType,
                            &#39;SIFT_nfeatures&#39; : SIFT_nfeatures,
                            &#39;SIFT_nOctaveLayers&#39; : SIFT_nOctaveLayers,
                            &#39;SIFT_contrastThreshold&#39; : SIFT_contrastThreshold,
                            &#39;SIFT_edgeThreshold&#39; : SIFT_edgeThreshold,
                            &#39;SIFT_sigma&#39; : SIFT_sigma,
                            &#39;Sample_ID&#39; : Sample_ID,
                            &#39;l2_matrix&#39; : l2_matrix,
                            &#39;targ_vector&#39;: targ_vector,
                            &#39;solver&#39; : solver,
                            &#39;drmax&#39; : drmax,
                            &#39;max_iter&#39; : max_iter,
                            &#39;BFMatcher&#39; : BFMatcher,
                            &#39;save_matches&#39; : save_matches,
                            &#39;kp_max_num&#39; : kp_max_num,
                            &#39;save_res_png &#39; : save_res_png ,
                            &#39;preserve_scales&#39; : preserve_scales,
                            &#39;fit_params&#39; : fit_params,
                            &#39;subtract_linear_fit&#39; : subtract_linear_fit,
                            &#39;subtract_FOVtrend_from_fit&#39; : subtract_FOVtrend_from_fit,
                            &#39;pad_edges&#39; : pad_edges}
            self.tr_matr_cum_residual, self.transf_matrix_xlsx_file = process_transf_matrix(self.transformation_matrix,
                                             self.FOVtrend_x,
                                             self.FOVtrend_y,
                                             self.fnms_matches,
                                             self.npts,
                                             self.error_abs_mean,
                                             **TM_kwargs)
        return self.tr_matr_cum_residual, self.transf_matrix_xlsx_file

    def save_parameters(self, **kwargs):
        &#39;&#39;&#39;
        Save transformation attributes and parameters (including transformation matrices).

        kwargs:
        -------
        dump_filename : string
            String containing the name of the binary dump for saving all attributes of the current istance of the FIBSEM_dataset object.


        Returns:
        dump_filename : string
        &#39;&#39;&#39;
        default_dump_filename = os.path.join(self.data_dir, self.fnm_reg.replace(&#39;.mrc&#39;, &#39;_params.bin&#39;))
        dump_filename = kwargs.get(&#34;dump_filename&#34;, default_dump_filename)

        pickle.dump(self.__dict__, open(dump_filename, &#39;wb&#39;))

        npts_fnm = dump_filename.replace(&#39;_params.bin&#39;, &#39;_Npts_Errs_data.csv&#39;)
        Tr_matrix_xls_fnm = dump_filename.replace(&#39;_params.bin&#39;, &#39;_Transform_Matrix_data.csv&#39;)

        # Save the keypoint statistics into a CSV file
        columns=[&#39;Npts&#39;, &#39;Mean Abs Error&#39;]
        npdt = pd.DataFrame(np.vstack((self.npts, self.error_abs_mean)).T, columns = columns, index = None)
        npdt.to_csv(npts_fnm, index = None)

        # Save the X-Y shift data and keypoint statistics into a CSV file
        columns=[&#39;T00 (Sxx)&#39;, &#39;T01 (Sxy)&#39;, &#39;T02 (Tx)&#39;,
                 &#39;T10 (Syx)&#39;, &#39;T11 (Syy)&#39;, &#39;T12 (Ty)&#39;,
                 &#39;T20 (0.0)&#39;, &#39;T21 (0.0)&#39;, &#39;T22 (1.0)&#39;]
        tr_mx_dt = pd.DataFrame(self.transformation_matrix.reshape((len(self.transformation_matrix), 9)), columns = columns, index = None)
        tr_mx_dt.to_csv(Tr_matrix_xls_fnm, index = None)
        return dump_filename

    def check_for_nomatch_frames(self, thr_npt, **kwargs):
        &#39;&#39;&#39;
        Calculate cumulative transformation matrix

        Parameters:
        -----------
        thr_npt : int
            minimum number of matches. If the pair has less than this - it is reported as &#34;suspicious&#34; and is excluded.

        kwargs
        ---------
        data_dir : str
            data directory (path)
        fnm_reg : str
            filename for the final registed dataset
        Sample_ID : str
            Sample ID
        TransformType : object reference
                Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
                Choose from the following options:
                    ShiftTransform - only x-shift and y-shift
                    XScaleShiftTransform  -  x-scale, x-shift, y-shift
                    ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                    AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                    RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
        l2_matrix : 2D float array
            matrix of regularization (shrinkage) parameters
        targ_vector = 1D float array
            target vector for regularization
        solver : str
            Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
        drmax : float
            In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
            In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
        max_iter : int
            Max number of iterations in the iterative procedure above (RANSAC or LinReg)
        BFMatcher : boolean
            If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
        save_matches : boolean
            If True, matches will be saved into individual files
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        preserve_scales : boolean
            If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
        fit_params : list
            Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
            Other options are:
                [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
                [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
        subtract_linear_fit : [boolean, boolean]
            List of two Boolean values for two directions: X- and Y-.
            If True, the linear slopes along X- and Y- directions (respectively)
            will be subtracted from the cumulative shifts.
            This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.

        Returns:
        tr_matr_cum_residual : list of 2D arrays of float
            Cumulative transformation matrices
        &#39;&#39;&#39;
        self.thr_npt = thr_npt
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
        targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
        solver = kwargs.get(&#34;solver&#34;, self.solver)
        drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
        max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
        BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
        save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
        kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, self.preserve_scales)
        fit_params =  kwargs.get(&#34;fit_params&#34;, self.fit_params)
        subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, self.subtract_linear_fit)
        subtract_FOVtrend_from_fit =  kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, self.subtract_FOVtrend_from_fit)
        pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)

        res_nomatch_check = check_for_nomatch_frames_dataset(self.fls, self.fnms, self.fnms_matches,
                                     self.transformation_matrix, self.error_abs_mean, self.npts,
                                     thr_npt,
                                     data_dir = self.data_dir, fnm_reg = self.fnm_reg)
        frames_to_remove, self.fls, self.fnms, self.fnms_matches, self.error_abs_mean, self.npts, self.transformation_matrix = res_nomatch_check

        if len(frames_to_remove) &gt; 0:
            TM_kwargs = {&#39;fnm_reg&#39; : fnm_reg,
                            &#39;data_dir&#39; : data_dir,
                            &#39;TransformType&#39; : TransformType,
                            &#39;Sample_ID&#39; : Sample_ID,
                            &#39;l2_matrix&#39; : l2_matrix,
                            &#39;targ_vector&#39;: targ_vector,
                            &#39;solver&#39; : solver,
                            &#39;drmax&#39; : drmax,
                            &#39;max_iter&#39; : max_iter,
                            &#39;BFMatcher&#39; : BFMatcher,
                            &#39;save_matches&#39; : save_matches,
                            &#39;kp_max_num&#39; : kp_max_num,
                            &#39;save_res_png &#39; : save_res_png ,
                            &#39;preserve_scales&#39; : preserve_scales,
                            &#39;fit_params&#39; : fit_params,
                            &#39;subtract_linear_fit&#39; : subtract_linear_fit,
                            &#39;subtract_FOVtrend_from_fit&#39; : subtract_FOVtrend_from_fit,
                            &#39;pad_edges&#39; : pad_edges}
            self.tr_matr_cum_residual, self.transf_matrix_xlsx_file = process_transf_matrix(self.transformation_matrix,
                                             self.FOVtrend_x,
                                             self.FOVtrend_y,
                                             self.fnms_matches,
                                             self.npts,
                                             self.error_abs_mean,
                                             **TM_kwargs)
        return self.tr_matr_cum_residual, self.transf_matrix_xlsx_file


    def transform_and_save(self, DASK_client, save_transformed_dataset=True, save_registration_summary=True, frame_inds=np.array((-1)), **kwargs):
        &#39;&#39;&#39;
        Transform the frames using the cumulative transformation matrix and save the data set into .mrc and/or .h5 file

        Parameters
        DASK_client : instance of the DASK client object
        save_transformed_dataset : boolean
            If true, the transformed data set will be saved into MRC file
        save_registration_summary : bolean
            If True, the registration analysis data will be saved into XLSX file
        frame_inds : int array (or list)
            Array of frame indecis. If not set or set to np.array((-1)), all frames will be transformed

        kwargs
        ---------
        use_DASK : boolean
            use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
        DASK_client_retries : int (default to 0)
            Number of allowed automatic retries if a task fails
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        data_dir : str
            data directory (path)
        fnm_types : list of strings
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is &#39;mrc&#39;. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        fnm_reg : str
            filename for the final registed dataset
        ImgB_fraction : float
            fractional ratio of Image B to be used for constructing the fuksed image:
            ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction
        add_offset : boolean
            If True - the Dark Oount offset will be added before saving to make values positive (set True if saving into BigDataViewer HDF5 - it uses UI16 data format)
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        perfrom_transformation : boolean
            If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed.
        invert_data : boolean
            If True - the data is inverted.
        flatten_image : bolean
            perform image flattening
        image_correction_file : str
            full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
        flipY : boolean
            If True, the data will be flipped along Y-axis. Default is False.
        zbin_factor : int
            binning factor along Z-axis
        eval_metrics : list of str
            list of evaluation metrics to use. default is [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;]
        fnm_types : list of strings
            File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
            Defauls is &#39;mrc&#39;. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        sliding_evaluation_box : boolean
            if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
        start_evaluation_box : list of 4 int
            see above
        stop_evaluation_box : list of 4 int
            see above
        save_sample_frames_png : bolean
            If True, sample frames with superimposed eval box and registration analysis data will be saved into png files
        dtp  : dtype
            Python data type for saving. Deafult is int16, the other option currently is uint8.
        disp_res : bolean
            If True (default), intermediate messages and results will be displayed.

        Returns:
        reg_summary, reg_summary_xlsx
            reg_summary : pandas DataFrame
            reg_summary = pd.DataFrame(np.vstack((npts, error_abs_mean, image_nsad, image_ncc, image_mi)
            reg_summary_xlsx : name of the XLSX workbook containing the data
        &#39;&#39;&#39;
        if (frame_inds == np.array((-1))).all():
            frame_inds = np.arange(len(self.fls))

        if hasattr(self, &#34;use_DASK&#34;):
            use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
        else:
            use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
        if hasattr(self, &#34;DASK_client_retries&#34;):
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
        else:
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        if hasattr(self, &#39;XResolution&#39;):
            XResolution_default = self.XResolution
        else:
            XResolution_default = FIBSEM_frame(self.fls[len(self.fls)//2]).XResolution
        XResolution = kwargs.get(&#34;XResolution&#34;, XResolution_default)
        if hasattr(self, &#39;YResolution&#39;):
            YResolution_default = self.YResolution
        else:
            YResolution_default = FIBSEM_frame(self.fls[len(self.fls)//2]).YResolution
        YResolution = kwargs.get(&#34;YResolution&#34;, YResolution_default)

        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        if hasattr(self, &#39;fnm_types&#39;):
            fnm_types = kwargs.get(&#34;fnm_types&#34;, self.fnm_types)
        else:
            fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
        ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, self.ImgB_fraction)
        if self.DetB == &#39;None&#39;:
            ImgB_fraction = 0.0
        if hasattr(self, &#39;add_offset&#39;):
            add_offset = kwargs.get(&#34;add_offset&#34;, self.add_offset)
        else:
            add_offset = kwargs.get(&#34;add_offset&#34;, False)
        save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, True)
        pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        if hasattr(self, &#39;eval_metrics&#39;):
            eval_metrics =  kwargs.get(&#34;eval_metrics&#34;, self.eval_metrics)
        else:
            eval_metrics = kwargs.get(&#39;eval_metrics&#39;, [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;])
        if hasattr(self, &#39;zbin_factor&#39;):
            zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, self.zbin_factor)         # binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
        else:
            zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, 1)
        if hasattr(self, &#39;voxel_size&#39;):
            voxel_size = kwargs.get(&#34;voxel_size&#34;, self.voxel_size)
        else:
            voxel_size_default = np.rec.array((self.PixelSize,  self.PixelSize,  self.PixelSize), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
            voxel_size = kwargs.get(&#34;voxel_size&#34;, voxel_size_default)
        voxel_size_zbinned = voxel_size.copy()
        voxel_size_zbinned.z = voxel_size.z * zbin_factor
        if hasattr(self, &#39;flipY&#39;):
            flipY = kwargs.get(&#34;flipY&#34;, self.flipY)
        else:
            flipY = kwargs.get(&#34;flipY&#34;, False)
        if hasattr(self, &#39;dump_filename&#39;):
            dump_filename = kwargs.get(&#34;dump_filename&#34;, self.dump_filename)
        else:
            dump_filename = kwargs.get(&#34;dump_filename&#34;, &#39;&#39;)
        int_order = kwargs.get(&#34;int_order&#34;, self.int_order)
        preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, self.preserve_scales)
        if hasattr(self, &#39;flatten_image&#39;):
            flatten_image = kwargs.get(&#34;flatten_image&#34;, self.flatten_image)
        else:
            flatten_image = kwargs.get(&#34;flatten_image&#34;, False)
        if hasattr(self, &#39;image_correction_file&#39;):
            image_correction_file = kwargs.get(&#34;image_correction_file&#34;, self.image_correction_file)
        else:
            image_correction_file = kwargs.get(&#34;image_correction_file&#34;, &#39;&#39;)
        perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True)  and hasattr(self, &#39;tr_matr_cum_residual&#39;)
        if hasattr(self, &#39;invert_data&#39;):
            invert_data = kwargs.get(&#34;invert_data&#34;, self.invert_data)
        else:
            invert_data = kwargs.get(&#34;invert_data&#34;, False)
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
        start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
        stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
        disp_res  = kwargs.get(&#34;disp_res&#34;, True )
        dtp = kwargs.get(&#34;dtp&#34;, int16)  # Python data type for saving. Deafult is int16, the other option currently is uint8.

        save_kwargs = {&#39;fnm_types&#39; : fnm_types,
                        &#39;fnm_reg&#39; : fnm_reg,
                        &#39;use_DASK&#39; : use_DASK,
                        &#39;DASK_client_retries&#39; : DASK_client_retries,
                        &#39;ftype&#39; : ftype,
                        &#39;XResolution&#39; : XResolution,
                        &#39;YResolution&#39; : YResolution,
                        &#39;data_dir&#39; : data_dir,
                        &#39;voxel_size&#39; : voxel_size_zbinned,
                        &#39;pad_edges&#39; : pad_edges,
                        &#39;ImgB_fraction&#39; : ImgB_fraction,
                        &#39;save_res_png &#39; : save_res_png ,
                        &#39;dump_filename&#39; : dump_filename,
                        &#39;dtp&#39; : dtp,
                        &#39;zbin_factor&#39; : zbin_factor,
                        &#39;eval_metrics&#39; : eval_metrics,
                        &#39;flipY&#39; : flipY,
                        &#39;int_order&#39; : int_order,
                        &#39;preserve_scales&#39; : preserve_scales,
                        &#39;flatten_image&#39; : flatten_image,
                        &#39;image_correction_file&#39; : image_correction_file,
                        &#39;perfrom_transformation&#39; : perfrom_transformation,
                        &#39;invert_data&#39; : invert_data,
                        &#39;evaluation_box&#39; : evaluation_box,
                        &#39;sliding_evaluation_box&#39; : sliding_evaluation_box,
                        &#39;start_evaluation_box&#39; : start_evaluation_box,
                        &#39;stop_evaluation_box&#39; : stop_evaluation_box,
                        &#39;save_sample_frames_png&#39; : save_sample_frames_png,
                        &#39;save_registration_summary&#39; : save_registration_summary,
                        &#39;disp_res&#39; : disp_res}

        # first, transform, bin and save frame chunks into individual tif files
        if disp_res:
            print(&#39;Transforming and Saving Intermediate Registered Frames&#39;)
        registered_filenames = transform_and_save_frames(DASK_client, frame_inds, self.fls, self.tr_matr_cum_residual, **save_kwargs)

        frame0 = tiff.imread(registered_filenames[0])
        ny, nx = np.shape(frame0)
        if disp_res:
            print(&#39;Analyzing Registration Quality&#39;)
        if pad_edges and perfrom_transformation:
            xmn, xmx, ymn, ymx = determine_pad_offsets([ny, nx], self.tr_matr_cum_residual)
            padx = int(xmx - xmn)
            pady = int(ymx - ymn)
            xi = int(np.max([xmx, 0]))
            yi = int(np.max([ymx, 0]))
        else:
            padx = 0
            pady = 0
            xi = 0
            yi = 0
        if sliding_evaluation_box:
            dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
            dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
        else:
            dx_eval = 0
            dy_eval = 0

        eval_bounds = []
        for j, registered_filename in enumerate(tqdm(registered_filenames, desc=&#39;Setting up evaluation bounds&#39;, display = disp_res)):
            if sliding_evaluation_box:
                xi_eval = xi + start_evaluation_box[2] + dx_eval*j//nz
                yi_eval = yi + start_evaluation_box[0] + dy_eval*j//nz
                if start_evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + start_evaluation_box[3]
                else:
                    xa_eval = nx
                if start_evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + start_evaluation_box[1]
                else:
                    ya_eval = ny
            else:
                xi_eval = xi + evaluation_box[2]
                if evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + evaluation_box[3]
                else:
                    xa_eval = nx
                yi_eval = yi + evaluation_box[0]
                if evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + evaluation_box[1]
                else:
                    ya_eval = ny
            eval_bounds.append([xi_eval, xa_eval, yi_eval, ya_eval])

        save_kwargs[&#39;eval_bounds&#39;] = eval_bounds

        reg_summary, reg_summary_xlsx = analyze_registration_frames(DASK_client, registered_filenames, **save_kwargs)

        if save_transformed_dataset:
            if add_offset:
                offset = self.Scaling[1, 0] * (1.0-ImgB_fraction) + self.Scaling[1, 1] * ImgB_fraction
            if disp_res:
                print(&#34;Creating Dask Array Stack&#34;)
            # now build dask array of the transformed dataset
            # read the first file to get the shape and dtype (ASSUMING THAT ALL FILES SHARE THE SAME SHAPE/TYPE)
            lazy_imread = dask.delayed(tiff.imread)  # lazy reader
            lazy_arrays = [lazy_imread(fn) for fn in registered_filenames]
            dask_arrays = [ da.from_delayed(delayed_reader, shape=frame0.shape, dtype=frame0.dtype)   for delayed_reader in lazy_arrays]
            # Stack infividual frames into one large dask.array
            if add_offset:
                FIBSEMstack = da.stack(dask_arrays, axis=0) - offset
            else:
                FIBSEMstack = da.stack(dask_arrays, axis=0)
            #nz, ny, nx = FIBSEMstack.shape
            fnms_saved = save_data_stack(FIBSEMstack, **save_kwargs)
        else:
            if disp_res:
                print(&#39;Registered data set is NOT saved into a file&#39;)

        # Remove Intermediate Registered Frame Files
        for registered_filename in tqdm(registered_filenames, desc=&#39;Removing Intermediate Registered Frame Files: &#39;, display = disp_res):
            try:
                os.remove(registered_filename)
            except:
                pass

        return reg_summary, reg_summary_xlsx


    def show_eval_box(self, **kwargs):
        &#39;&#39;&#39;
        Show the box used for evaluating the registration quality

        kwargs
        ---------
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        sliding_evaluation_box : boolean
            if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
        start_evaluation_box : list of 4 int
            see above
        stop_evaluation_box : list of 4 int
            see above
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        data_dir : str
            data directory (path)
        fnm_reg : str
            filename for the final registed dataset
        Sample_ID : str
            Sample ID
        int_order : int
            The order of interpolation (when transforming the data).
                The order has to be in the range 0-5:
                    0: Nearest-neighbor
                    1: Bi-linear (default)
                    2: Bi-quadratic
                    3: Bi-cubic
                    4: Bi-quartic
                    5: Bi-quintic
        perfrom_transformation : boolean
            If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed
        invert_data : boolean
            If True - the data is inverted
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.
        flipY : boolean
            If True, the data will be flipped along Y-axis. Default is False.
        frame_inds : array or list of int
            Array or list oif frame indecis to use to display the evaluation box.
            Default are [nfrs//10, nfrs//2, nfrs//10*9]
        &#39;&#39;&#39;
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
        start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
        stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        int_order = kwargs.get(&#34;int_order&#34;, self.int_order)
        perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True) and hasattr(self, &#39;tr_matr_cum_residual&#39;)
        invert_data =  kwargs.get(&#34;invert_data&#34;, False)
        flipY = kwargs.get(&#34;flipY&#34;, False)
        pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        fls = self.fls
        nfrs = len(fls)
        default_indecis = [nfrs//10, nfrs//2, nfrs//10*9]
        frame_inds = kwargs.get(&#34;frame_inds&#34;, default_indecis)

        for j in frame_inds:
            frame = FIBSEM_frame(fls[j], ftype=ftype)
            if pad_edges and perfrom_transformation:
                shape = [frame.YResolution, frame.XResolution]
                xmn, xmx, ymn, ymx = determine_pad_offsets(shape, self.tr_matr_cum_residual)
                padx = np.int16(xmx - xmn)
                pady = np.int16(ymx - ymn)
                xi = np.int16(np.max([xmx, 0]))
                yi = np.int16(np.max([ymx, 0]))
                # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
                # so that the transformed images are not clipped.
                # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
                # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
                # those are calculated below base on the amount of padding calculated above
                shift_matrix = np.array([[1.0, 0.0, xi],
                                         [0.0, 1.0, yi],
                                         [0.0, 0.0, 1.0]])
                inv_shift_matrix = np.linalg.inv(shift_matrix)
            else:
                padx = 0
                pady = 0
                xi = 0
                yi = 0
                shift_matrix = np.eye(3,3)
                inv_shift_matrix = np.eye(3,3)

            xsz = frame.XResolution + padx
            xa = xi + frame.XResolution
            ysz = frame.YResolution + pady
            ya = yi + frame.YResolution

            xi_eval = xi + evaluation_box[2]
            if evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + evaluation_box[3]
            else:
                xa_eval = xa
            yi_eval = yi + evaluation_box[0]
            if evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + evaluation_box[1]
            else:
                ya_eval = ya

            if sliding_evaluation_box:
                dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
                dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
            else:
                dx_eval = 0
                dy_eval = 0

            frame_img = np.zeros((ysz, xsz))

            if invert_data:
                frame_img[yi:ya, xi:xa] = np.negative(frame.RawImageA.astype(float))
                &#39;&#39;&#39;
                if frame.EightBit==0:
                    frame_img[yi:ya, xi:xa] = np.negative(frame.RawImageA)
                else:
                    frame_img[yi:ya, xi:xa]  =  uint8(255) - frame.RawImageA
                &#39;&#39;&#39;
            else:
                frame_img[yi:ya, xi:xa]  = frame.RawImageA.astype(float)

            if perfrom_transformation:
                transf = ProjectiveTransform(matrix = shift_matrix @ (self.tr_matr_cum_residual[j] @ inv_shift_matrix))
                frame_img_reg = warp(frame_img, transf, order = int_order, preserve_range=True)
            else:
                frame_img_reg = frame_img.copy()

            if flipY:
                frame_img_reg = np.flip(frame_img_reg, axis=0)

            if sliding_evaluation_box:
                xi_eval = start_evaluation_box[2] + dx_eval*j//nfrs
                yi_eval = start_evaluation_box[0] + dy_eval*j//nfrs
                if start_evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + start_evaluation_box[3]
                else:
                    xa_eval = xsz
                if start_evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + start_evaluation_box[1]
                else:
                    ya_eval = ysz

            vmin, vmax = get_min_max_thresholds(frame_img_reg[yi_eval:ya_eval, xi_eval:xa_eval], disp_res=False)
            fig, ax = subplots(1,1, figsize=(10.0, 11.0*ysz/xsz))
            ax.imshow(frame_img_reg, cmap=&#39;Greys&#39;, vmin=vmin, vmax=vmax)
            ax.grid(True, color = &#34;cyan&#34;)
            ax.set_title(fls[j])
            rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=2.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
            ax.add_patch(rect_patch)
            if save_res_png :
                fig.savefig(os.path.splitext(fls[j])[0]+&#39;_evaluation_box.png&#39;, dpi=300)


    def estimate_SNRs(self, **kwargs):
        &#39;&#39;&#39;
        Estimate SNRs in Image A and Image B based on single-image SNR calculation.

        kwargs
        ---------
        frame_inds : list of int
            List oif frame indecis to use to display the evaluation box.
            Default are [nfrs//10, nfrs//2, nfrs//10*9]
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        sliding_evaluation_box : boolean
            if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
        start_evaluation_box : list of 4 int
            see above
        stop_evaluation_box : list of 4 int
            see above
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        data_dir : str
            data directory (path)
        Sample_ID : str
            Sample ID
        ImgB_fraction : float
            Optional fractional weight of Image B to use for constructing the fused image: FusedImage = ImageA*(1.0-ImgB_fraction) + ImageB*ImgB_fraction
            If not provided, the value determined from rSNR ratios will be used.
        invert_data : boolean
            If True - the data is inverted
        perfrom_transformation : boolean
            If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.
        extrapolate_signal : boolean
            extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.
        flipY : boolean
            If True, the data will be flipped along Y-axis. Default is False.

        &#39;&#39;&#39;
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
        start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
        stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        int_order = kwargs.get(&#34;int_order&#34;, self.int_order)
        invert_data =  kwargs.get(&#34;invert_data&#34;, False)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, False )
        ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, 0.00 )
        flipY = kwargs.get(&#34;flipY&#34;, False)
        pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)
        perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True) and hasattr(self, &#39;tr_matr_cum_residual&#39;)
        extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)

        fls = self.fls
        nfrs = len(fls)
        default_indecis = [nfrs//10, nfrs//2, nfrs//10*9]
        frame_inds = kwargs.get(&#34;frame_inds&#34;, default_indecis)

        test_frame = FIBSEM_frame(fls[0], ftype=ftype)

        xi = 0
        yi = 0
        xsz = test_frame.XResolution
        xa = xi + xsz
        ysz = test_frame.YResolution
        ya = yi + ysz

        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        frame_img = np.zeros((ysz, xsz))
        xSNRAs=[]
        ySNRAs=[]
        rSNRAs=[]
        xSNRBs=[]
        ySNRBs=[]
        rSNRBs=[]

        for j in tqdm(frame_inds, desc=&#39;Analyzing Auto-Correlation SNRs &#39;):

            frame = FIBSEM_frame(fls[j], ftype=ftype)
            if pad_edges and perfrom_transformation:
                shape = [frame.YResolution, frame.XResolution]
                xmn, xmx, ymn, ymx = determine_pad_offsets(shape, self.tr_matr_cum_residual)
                padx = np.int16(xmx - xmn)
                pady = np.int16(ymx - ymn)
                xi = np.int16(np.max([xmx, 0]))
                yi = np.int16(np.max([ymx, 0]))
                # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
                # so that the transformed images are not clipped.
                # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
                # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
                # those are calculated below base on the amount of padding calculated above
                shift_matrix = np.array([[1.0, 0.0, xi],
                                         [0.0, 1.0, yi],
                                         [0.0, 0.0, 1.0]])
                inv_shift_matrix = np.linalg.inv(shift_matrix)
            else:
                padx = 0
                pady = 0
                xi = 0
                yi = 0
                shift_matrix = np.eye(3,3)
                inv_shift_matrix = np.eye(3,3)

            xsz = frame.XResolution + padx
            xa = xi + frame.XResolution
            ysz = frame.YResolution + pady
            ya = yi + frame.YResolution

            xi_eval = xi + evaluation_box[2]
            if evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + evaluation_box[3]
            else:
                xa_eval = xa
            yi_eval = yi + evaluation_box[0]
            if evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + evaluation_box[1]
            else:
                ya_eval = ya

            if sliding_evaluation_box:
                dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
                dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
            else:
                dx_eval = 0
                dy_eval = 0

            frame_imgA = np.zeros((ysz, xsz))
            if self.DetB != &#39;None&#39;:
                frame_imgB = np.zeros((ysz, xsz))

            if invert_data:
                frame_imgA[yi:ya, xi:xa] = np.negative(frame.RawImageA.astype(float))
                if self.DetB != &#39;None&#39;:
                    frame_imgB[yi:ya, xi:xa] = np.negative(frame.RawImageB.astype(float))
            else:
                frame_imgA[yi:ya, xi:xa]  = frame.RawImageA.astype(float)
                if self.DetB != &#39;None&#39;:
                    frame_imgB[yi:ya, xi:xa]  = frame.RawImageB.astype(float)

            if perfrom_transformation:
                transf = ProjectiveTransform(matrix = shift_matrix @ (self.tr_matr_cum_residual[j] @ inv_shift_matrix))
                frame_imgA_reg = warp(frame_imgA, transf, order = int_order, preserve_range=True)
                if self.DetB != &#39;None&#39;:
                    frame_imgB_reg = warp(frame_imgB, transf, order = int_order, preserve_range=True)
            else:
                frame_imgA_reg = frame_imgA.copy()
                if self.DetB != &#39;None&#39;:
                    frame_imgB_reg = frame_imgB.copy()

            if flipY:
                frame_imgA_reg = np.flip(frame_imgA_reg, axis=0)
                if self.DetB != &#39;None&#39;:
                    frame_imgB_reg = np.flip(frame_imgB_reg, axis=0)

            if sliding_evaluation_box:
                xi_eval = start_evaluation_box[2] + dx_eval*j//nfrs
                yi_eval = start_evaluation_box[0] + dy_eval*j//nfrs
                if start_evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + start_evaluation_box[3]
                else:
                    xa_eval = xsz
                if start_evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + start_evaluation_box[1]
                else:
                    ya_eval = ysz

            &#39;&#39;&#39;
            if invert_data:
                if test_frame.EightBit==0:
                    frame_imgA = np.negative(FIBSEM_frame(fls[j], ftype=ftype).RawImageA)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB = np.negative(FIBSEM_frame(fls[j], ftype=ftype).RawImageB)
                else:
                    frame_imgA  =  uint8(255) - FIBSEM_frame(fls[j], ftype=ftype).RawImageA
                    if self.DetB != &#39;None&#39;:
                        frame_imgB  =  uint8(255) - FIBSEM_frame(fls[j], ftype=ftype).RawImageB

            else:
                frame_imgA  = FIBSEM_frame(fls[j], ftype=ftype).RawImageA
                if self.DetB != &#39;None&#39;:
                    frame_imgB  = FIBSEM_frame(fls[j], ftype=ftype).RawImageB

            if flipY:
                frame_imgA = np.flip(frame_imgA, axis=0)
                frame_imgB = np.flip(frame_imgB, axis=0)
            &#39;&#39;&#39;
            frame_imgA_eval = frame_imgA_reg[yi_eval:ya_eval, xi_eval:xa_eval]
            SNR_png = os.path.splitext(os.path.split(fls[j])[1])[0] + &#39;.png&#39;
            SNR_png_fname = os.path.join(data_dir, SNR_png)
            ImageA_xSNR, ImageA_ySNR, ImageA_rSNR= Single_Image_SNR(frame_imgA_eval, extrapolate_signal=extrapolate_signal, save_res_png=save_res_png,
                                                                        res_fname = SNR_png_fname.replace(&#39;.png&#39;, &#39;_ImgA_SNR.png&#39;),
                                                                        img_label=&#39;Image A, frame={:d}&#39;.format(j))
            xSNRAs.append(ImageA_xSNR)
            ySNRAs.append(ImageA_ySNR)
            rSNRAs.append(ImageA_rSNR)
            if self.DetB != &#39;None&#39;:
                frame_imgB_eval = frame_imgB_reg[yi_eval:ya_eval, xi_eval:xa_eval]
                ImageB_xSNR, ImageB_ySNR, ImageB_rSNR = Single_Image_SNR(frame_imgB_eval, extrapolate_signal=extrapolate_signal, save_res_png=save_res_png,
                                                                            res_fname = SNR_png_fname.replace(&#39;.png&#39;, &#39;_ImgB_SNR.png&#39;),
                                                                            img_label=&#39;Image B, frame={:d}&#39;.format(j))
                xSNRBs.append(ImageB_xSNR)
                ySNRBs.append(ImageB_ySNR)
                rSNRBs.append(ImageB_rSNR)

        fig, ax = subplots(1,1, figsize = (6,4))
        ax.plot(frame_inds, xSNRAs, &#39;r+&#39;, label=&#39;Image A x-SNR&#39;)
        ax.plot(frame_inds, ySNRAs, &#39;b+&#39;, label=&#39;Image A y-SNR&#39;)
        ax.plot(frame_inds, rSNRAs, &#39;g+&#39;, label=&#39;Image A r-SNR&#39;)
        if self.DetB != &#39;None&#39;:
            ax.plot(frame_inds, xSNRBs, &#39;rx&#39;, linestyle=&#39;dotted&#39;, label=&#39;Image B x-SNR&#39;)
            ax.plot(frame_inds, ySNRBs, &#39;bx&#39;, linestyle=&#39;dotted&#39;, label=&#39;Image B y-SNR&#39;)
            ax.plot(frame_inds, rSNRBs, &#39;gx&#39;, linestyle=&#39;dotted&#39;, label=&#39;Image B r-SNR&#39;)
            ImgB_fraction_xSNR = np.mean(np.array(xSNRBs)/(np.array(xSNRAs) + np.array(xSNRBs)))
            ImgB_fraction_ySNR = np.mean(np.array(ySNRBs)/(np.array(ySNRAs) + np.array(ySNRBs)))
            ImgB_fraction_rSNR = np.mean(np.array(rSNRBs)/(np.array(rSNRAs) + np.array(rSNRBs)))
            if ImgB_fraction &lt; 1e-9:
                ImgB_fraction = ImgB_fraction_rSNR
            ax.text(0.1, 0.5, &#39;ImgB fraction (x-SNR) = {:.4f}&#39;.format(ImgB_fraction_xSNR), color=&#39;r&#39;, transform=ax.transAxes)
            ax.text(0.1, 0.42, &#39;ImgB fraction (y-SNR) = {:.4f}&#39;.format(ImgB_fraction_ySNR), color=&#39;b&#39;, transform=ax.transAxes)
            ax.text(0.1, 0.34, &#39;ImgB fraction (r-SNR) = {:.4f}&#39;.format(ImgB_fraction_rSNR), color=&#39;g&#39;, transform=ax.transAxes)

            xSNRFs=[]
            ySNRFs=[]
            rSNRFs=[]
            for j in tqdm(frame_inds, desc=&#39;Re-analyzing Auto-Correlation SNRs for fused image&#39;):
                frame = FIBSEM_frame(fls[j], ftype=ftype)
                if pad_edges and perfrom_transformation:
                    shape = [frame.YResolution, frame.XResolution]
                    xmn, xmx, ymn, ymx = determine_pad_offsets(shape, self.tr_matr_cum_residual)
                    padx = np.int16(xmx - xmn)
                    pady = np.int16(ymx - ymn)
                    xi = np.int16(np.max([xmx, 0]))
                    yi = np.int16(np.max([ymx, 0]))
                    # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
                    # so that the transformed images are not clipped.
                    # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
                    # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
                    # those are calculated below base on the amount of padding calculated above
                    shift_matrix = np.array([[1.0, 0.0, xi],
                                             [0.0, 1.0, yi],
                                             [0.0, 0.0, 1.0]])
                    inv_shift_matrix = np.linalg.inv(shift_matrix)
                else:
                    padx = 0
                    pady = 0
                    xi = 0
                    yi = 0
                    shift_matrix = np.eye(3,3)
                    inv_shift_matrix = np.eye(3,3)

                xsz = frame.XResolution + padx
                xa = xi + frame.XResolution
                ysz = frame.YResolution + pady
                ya = yi + frame.YResolution

                xi_eval = xi + evaluation_box[2]
                if evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + evaluation_box[3]
                else:
                    xa_eval = xa
                yi_eval = yi + evaluation_box[0]
                if evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + evaluation_box[1]
                else:
                    ya_eval = ya

                if sliding_evaluation_box:
                    dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
                    dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
                else:
                    dx_eval = 0
                    dy_eval = 0

                frame_imgA = np.zeros((ysz, xsz))
                if self.DetB != &#39;None&#39;:
                    frame_imgB = np.zeros((ysz, xsz))

                if invert_data:
                    frame_imgA[yi:ya, xi:xa] = np.negative(frame.RawImageA.astype(float))
                    if self.DetB != &#39;None&#39;:
                        frame_imgB[yi:ya, xi:xa] = np.negative(frame.RawImageB.astype(float))
                else:
                    frame_imgA[yi:ya, xi:xa]  = frame.RawImageA.astype(float)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB[yi:ya, xi:xa]  = frame.RawImageB.astype(float)

                if perfrom_transformation:
                    transf = ProjectiveTransform(matrix = shift_matrix @ (self.tr_matr_cum_residual[j] @ inv_shift_matrix))
                    frame_imgA_reg = warp(frame_imgA, transf, order = int_order, preserve_range=True)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB_reg = warp(frame_imgB, transf, order = int_order, preserve_range=True)
                else:
                    frame_imgA_reg = frame_imgA.copy()
                    if self.DetB != &#39;None&#39;:
                        frame_imgB_reg = frame_imgB.copy()

                if flipY:
                    frame_imgA_reg = np.flip(frame_imgA_reg, axis=0)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB_reg = np.flip(frame_imgB_reg, axis=0)

                if sliding_evaluation_box:
                    xi_eval = start_evaluation_box[2] + dx_eval*j//nfrs
                    yi_eval = start_evaluation_box[0] + dy_eval*j//nfrs
                    if start_evaluation_box[3] &gt; 0:
                        xa_eval = xi_eval + start_evaluation_box[3]
                    else:
                        xa_eval = xsz
                    if start_evaluation_box[1] &gt; 0:
                        ya_eval = yi_eval + start_evaluation_box[1]
                    else:
                        ya_eval = ysz

                frame_imgA_eval = frame_imgA_reg[yi_eval:ya_eval, xi_eval:xa_eval]
                frame_imgB_eval = frame_imgB_reg[yi_eval:ya_eval, xi_eval:xa_eval]

                frame_imgF_eval = frame_imgA_eval * (1.0 - ImgB_fraction) + frame_imgB_eval * ImgB_fraction
                ImageF_xSNR, ImageF_ySNR, ImageF_rSNR = Single_Image_SNR(frame_imgF_eval, extrapolate_signal=extrapolate_signal, save_res_png=save_res_png,
                                                                        res_fname = SNR_png_fname.replace(&#39;.png&#39;, &#39;_ImgB_fr{:.3f}_SNR.png&#39;.format(ImgB_fraction)),
                                                                        img_label=&#39;Fused, ImB_fr={:.4f}, frame={:d}&#39;.format(ImgB_fraction, j))
                xSNRFs.append(ImageF_xSNR)
                ySNRFs.append(ImageF_ySNR)
                rSNRFs.append(ImageF_rSNR)

            ax.plot(frame_inds, xSNRFs, &#39;rd&#39;, linestyle=&#39;dashed&#39;, label=&#39;Fused Image x-SNR&#39;)
            ax.plot(frame_inds, ySNRFs, &#39;bd&#39;, linestyle=&#39;dashed&#39;, label=&#39;Fused Image y-SNR&#39;)
            ax.plot(frame_inds, rSNRFs, &#39;gd&#39;, linestyle=&#39;dashed&#39;, label=&#39;Fused Image r-SNR&#39;)

        else:
            ImgB_fraction_xSNR = 0.0
            ImgB_fraction_ySNR = 0.0
            ImgB_fraction_rSNR = 0.0
        ax.grid(True)
        ax.legend()
        ax.set_title(Sample_ID + &#39;  &#39; + data_dir, fontsize=8)
        ax.set_xlabel(&#39;Frame&#39;)
        ax.set_ylabel(&#39;SNR&#39;)
        if save_res_png :
            fig_filename = os.path.join(data_dir, os.path.splitext(fnm_reg)[0]+&#39;SNR_evaluation_mult_frame.png&#39;)
            fig.savefig(fig_filename, dpi=300)

        return ImgB_fraction_xSNR, ImgB_fraction_ySNR, ImgB_fraction_rSNR


    def evaluate_ImgB_fractions(self, ImgB_fractions, frame_inds, **kwargs):
        &#39;&#39;&#39;
        Calculate NCC and SNR vs Image B fraction over a set of frames.

        ImgB_fractions : list
            List of fractions to estimate the NCC and SNR
        frame_inds : int array
            array of frame indices to perform NCC / SNR evaluation

        kwargs
        ---------
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        extrapolate_signal : boolean
            extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
        ftype : int
            file type (0 - Shan Xu&#39;s .dat, 1 - tif)
        data_dir : str
            data directory (path)
        Sample_ID : str
            Sample ID

        invert_data : boolean
            If True - the data is inverted
        save_res_png  : boolean
            Save PNG images of the intermediate processing statistics and final registration quality check
        pad_edges : boolean
            If True, the data will be padded before transformation to avoid clipping.


        Returns
        SNRimpr_max_position, SNRimpr_max, ImgB_fractions, SNRs
        &#39;&#39;&#39;
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        if hasattr(self, &#39;invert_data&#39;):
            invert_data = kwargs.get(&#34;invert_data&#34;, self.invert_data)
        else:
            invert_data = kwargs.get(&#34;invert_data&#34;, False)
        if hasattr(self, &#39;flipY&#39;):
            flipY = kwargs.get(&#34;flipY&#34;, self.flipY)
        else:
            flipY = kwargs.get(&#34;flipY&#34;, flipY)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, False )
        extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)

        nbr = len(ImgB_fractions)
        kwargs[&#39;zbin_factor&#39;] = 1

        test_frame = FIBSEM_frame(self.fls[frame_inds[0]])

        xi = 0
        yi = 0
        xsz = test_frame.XResolution
        xa = xi + xsz
        ysz = test_frame.YResolution
        ya = yi + ysz

        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        br_results = []
        xSNRFs=[]
        ySNRFs=[]
        rSNRFs=[]

        for ImgB_fraction in tqdm(ImgB_fractions, desc=&#39;Evaluating Img B fractions&#39;):
            kwargs[&#39;ImgB_fraction&#39;] = ImgB_fraction
            kwargs[&#39;disp_res&#39;] = False
            kwargs[&#39;evaluation_box&#39;] = evaluation_box
            kwargs[&#39;flipY&#39;] = flipY
            kwargs[&#39;invert_data&#39;] = invert_data
            DASK_client = &#39;&#39;
            kwargs[&#39;disp_res&#39;] = False
            br_res, br_res_xlsx = self.transform_and_save(DASK_client,
                                                                save_transformed_dataset=False,
                                                                save_registration_summary=False,
                                                                frame_inds=frame_inds,
                                                                use_DASK=False,
                                                                save_sample_frames_png=False,
                                                                eval_metrics = [&#39;NCC&#39;],
                                                                **kwargs)
            br_results.append(br_res)

            if invert_data:
                if test_frame.EightBit==0:
                    frame_imgA = np.negative(test_frame.RawImageA)
                    if self.DetB != &#39;None&#39;:
                        frame_imgB = np.negative(test_frame.RawImageB)
                else:
                    frame_imgA  =  uint8(255) - test_frame.RawImageA
                    if self.DetB != &#39;None&#39;:
                        frame_imgB  =  uint8(255) - test_frame.RawImageB
            else:
                frame_imgA  = test_frame.RawImageA
                if self.DetB != &#39;None&#39;:
                    frame_imgB  = test_frame.RawImageB
            if flipY:
                frame_imgA = np.flip(frame_imgA, axis=0)
                frame_imgB = np.flip(frame_imgB, axis=0)

            frame_imgA_eval = frame_imgA[yi_eval:ya_eval, xi_eval:xa_eval]
            frame_imgB_eval = frame_imgB[yi_eval:ya_eval, xi_eval:xa_eval]

            frame_imgF_eval = frame_imgA_eval * (1.0 - ImgB_fraction) + frame_imgB_eval * ImgB_fraction
            ImageF_xSNR, ImageF_ySNR, ImageF_rSNR = Single_Image_SNR(frame_imgF_eval,
                                                                    extrapolate_signal=extrapolate_signal,
                                                                    disp_res=False,
                                                                    save_res_png=False,
                                                                    res_fname = &#39;&#39;,
                                                                    img_label=&#39;&#39;)
            xSNRFs.append(ImageF_xSNR)
            ySNRFs.append(ImageF_ySNR)
            rSNRFs.append(ImageF_rSNR)

        fig, axs = subplots(4,1, figsize=(6,11))
        fig.subplots_adjust(left=0.12, bottom=0.06, right=0.99, top=0.96, wspace=0.25, hspace=0.24)
        try:
            ncc0 = (br_results[0])[&#39;NCC&#39;]
        except:
            ncc0 = (br_results[0])[&#39;Image NCC&#39;]
        SNR0 = ncc0 / (1-ncc0)
        SNRimpr_cc = []
        SNRs = []

        for j, (ImgB_fraction, br_result) in enumerate(zip(ImgB_fractions, br_results)):
            my_col = get_cmap(&#34;gist_rainbow_r&#34;)((nbr-j)/(nbr-1))
            try:
                ncc = br_result[&#39;NCC&#39;]
            except:
                ncc = br_result[&#39;Image NCC&#39;]
            SNR = ncc / (1.0-ncc)
            frames_local = br_result[&#39;Frame&#39;]
            axs[0].plot(frames_local, SNR, color=my_col, label = &#39;ImgB fraction = {:.2f}&#39;.format(ImgB_fraction))
            axs[1].plot(frames_local, SNR/SNR0, color=my_col, label = &#39;ImgB fraction = {:.2f}&#39;.format(ImgB_fraction))
            SNRimpr_cc.append(np.mean(SNR/SNR0))
            SNRs.append(np.mean(SNR))

        SNRimpr_ac = np.array(rSNRFs) / rSNRFs[0]

        SNRimpr_cc_max = np.max(SNRimpr_cc)
        SNRimpr_cc_max_ind = np.argmax(SNRimpr_cc)
        ImgB_fraction_max = ImgB_fractions[SNRimpr_cc_max_ind]
        xi = max(0, (SNRimpr_cc_max_ind-3))
        xa = min((SNRimpr_cc_max_ind+3), len(ImgB_fractions))
        ImgB_fr_range = ImgB_fractions[xi : xa]
        SNRimpr_cc_range = SNRimpr_cc[xi : xa]
        popt = np.polyfit(ImgB_fr_range, SNRimpr_cc_range, 2)
        SNRimpr_cc_fit_max_pos = -0.5 * popt[1] / popt[0]
        ImgB_fr_fit_cc = np.linspace(ImgB_fr_range[0], ImgB_fr_range[-1], 21)
        SNRimpr_cc_fit = np.polyval(popt, ImgB_fr_fit_cc)
        if popt[0] &lt; 0 and SNRimpr_cc_fit_max_pos &gt; ImgB_fractions[0] and SNRimpr_cc_fit_max_pos&lt;ImgB_fractions[-1]:
            SNRimpr_cc_max_position = SNRimpr_cc_fit_max_pos
            SNRimpr_cc_max = np.polyval(popt, SNRimpr_cc_max_position)
        else:
            SNRimpr_cc_max_position = ImgB_fraction_max

        SNRimpr_ac_max = np.max(SNRimpr_ac)
        SNRimpr_ac_max_ind = np.argmax(SNRimpr_ac)
        ImgB_fraction_max = ImgB_fractions[SNRimpr_ac_max_ind]
        xi = max(0, (SNRimpr_ac_max_ind-3))
        xa = min((SNRimpr_ac_max_ind+3), len(ImgB_fractions))
        ImgB_fr_range = ImgB_fractions[xi : xa]
        SNRimpr_ac_range = SNRimpr_ac[xi : xa]
        popt = np.polyfit(ImgB_fr_range, SNRimpr_ac_range, 2)
        SNRimpr_ac_fit_max_pos = -0.5 * popt[1] / popt[0]
        ImgB_fr_fit_ac = np.linspace(ImgB_fr_range[0], ImgB_fr_range[-1], 21)
        SNRimpr_ac_fit = np.polyval(popt, ImgB_fr_fit_ac)
        if popt[0] &lt; 0 and SNRimpr_ac_fit_max_pos &gt; ImgB_fractions[0] and SNRimpr_ac_fit_max_pos&lt;ImgB_fractions[-1]:
            SNRimpr_ac_max_position = SNRimpr_ac_fit_max_pos
            SNRimpr_ac_max = np.polyval(popt, SNRimpr_ac_max_position)
        else:
            SNRimpr_ac_max_position = ImgB_fraction_max

        fs=10
        axs[0].grid(True)
        axs[0].set_ylabel(&#39;Frame-to-Frame SNR&#39;, fontsize=fs)
        axs[0].set_xlabel(&#39;Frame&#39;, fontsize=fs)
        axs[0].legend(fontsize=fs-1)
        axs[0].set_title(Sample_ID + &#39;  &#39; + data_dir, fontsize=fs)
        axs[1].grid(True)
        axs[1].set_ylabel(&#39;Frame-to-Frame SNR Improvement&#39;, fontsize=fs)
        axs[1].set_xlabel(&#39;Frame&#39;, fontsize=fs)

        axs[2].plot(ImgB_fractions, rSNRFs, &#39;rd&#39;, label=&#39;Data (auto-correlation)&#39;)
        axs[2].grid(True)
        axs[2].set_ylabel(&#39;Auto-Corr SNR&#39;, fontsize=fs)

        axs[3].plot(ImgB_fractions, SNRimpr_cc, &#39;cs&#39;, label=&#39;Data (cross-corr.)&#39;)
        axs[3].plot(ImgB_fr_fit_cc, SNRimpr_cc_fit, &#39;b&#39;, label=&#39;Fit (cross-corr.)&#39;)
        axs[3].plot(SNRimpr_cc_max_position, SNRimpr_cc_max, &#39;bx&#39;, markersize=10, label=&#39;Max SNR Impr. (cc)&#39;)
        axs[3].text(0.4, 0.35, &#39;Max CC SNR Improvement={:.3f}&#39;.format(SNRimpr_cc_max), transform=axs[3].transAxes, fontsize=fs)
        axs[3].text(0.4, 0.25, &#39;@ Img B Fraction ={:.3f}&#39;.format(SNRimpr_cc_max_position), transform=axs[3].transAxes, fontsize=fs)
        axs[3].plot(ImgB_fractions, SNRimpr_ac, &#39;rd&#39;, label=&#39;Data (auto-corr.)&#39;)
        axs[3].plot(ImgB_fr_fit_ac, SNRimpr_ac_fit, &#39;magenta&#39;, label=&#39;Fit (auto-corr.)&#39;)
        axs[3].plot(SNRimpr_ac_max_position, SNRimpr_ac_max, &#39;mx&#39;, markersize=10, label=&#39;Max SNR Impr. (ac)&#39;)
        axs[3].text(0.4, 0.15, &#39;Max AC SNR Improvement={:.3f}&#39;.format(SNRimpr_ac_max), transform=axs[3].transAxes, fontsize=fs)
        axs[3].text(0.4, 0.05, &#39;@ Img B Fraction ={:.3f}&#39;.format(SNRimpr_ac_max_position), transform=axs[3].transAxes, fontsize=fs)

        axs[3].legend(fontsize=fs-2, loc=&#39;upper left&#39;)
        axs[3].grid(True)
        axs[3].set_ylabel(&#39;Mean SNR improvement&#39;, fontsize=fs)
        axs[3].set_xlabel(&#39;Image B fraction&#39;, fontsize=fs)

        if save_res_png :
            fname_image = os.path.join(data_dir, os.path.splitext(fnm_reg)[0]+&#39;_SNR_vs_ImgB_ratio_evaluation.png&#39;)
            fig.savefig(fname_image, dpi=300)

        return SNRimpr_cc_max_position, SNRimpr_cc_max, ImgB_fractions, SNRs, rSNRFs</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.SIFT_evaluation"><code class="name flex">
<span>def <span class="ident">SIFT_evaluation</span></span>(<span>self, eval_fls=[], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate SIFT settings and perfromance of few test frames (eval_fls). ©G.Shtengel 10/2021 gleb.shtengel@gmail.com</p>
<p>Parameters:
eval_fls : array of str
filenames for the data frames to be used for SIFT evaluation</p>
<h2 id="kwargs">Kwargs</h2>
<p>data_dir : str
data directory (path)
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
fnm_reg : str
filename for the final registed dataset
threshold_min : float
CDF threshold for determining the minimum data value
threshold_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF
evaluation_box : list of 4 int
evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
TransformType : object reference
Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
Choose from the following options:
ShiftTransform - only x-shift and y-shift
XScaleShiftTransform
-
x-scale, x-shift, y-shift
ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
AffineTransform -
full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
l2_matrix : 2D float array
matrix of regularization (shrinkage) parameters
targ_vector = 1D float array
target vector for regularization
solver : str
Solver used for SIFT ('RANSAC' or 'LinReg')
drmax : float
In the case of 'RANSAC' - Maximum distance for a data point to be classified as an inlier.
In the case of 'LinReg' - outlier threshold for iterative regression
max_iter : int
Max number of iterations in the iterative procedure above (RANSAC or LinReg)
BFMatcher : boolean
If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
save_matches : boolean
If True, matches will be saved into individual files
kp_max_num : int
Max number of key-points to be matched.
Key-points in every frame are indexed (in descending order) by the strength of the response.
Only kp_max_num is kept for further processing.
Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
SIFT_nfeatures : int
SIFT libary default is 0. The number of best features to retain.
The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
SIFT_nOctaveLayers : int
SIFT libary default
is 3. The number of layers in each octave.
3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
SIFT_contrastThreshold : double
SIFT libary default
is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
The larger the threshold, the less features are produced by the detector.
The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
When nOctaveLayers is set to default and if you want to use the value used in
D. Lowe paper (0.03), set this argument to 0.09.
SIFT_edgeThreshold : double
SIFT libary default
is 10. The threshold used to filter out edge-like features.
Note that the its meaning is different from the contrastThreshold,
i.e. the larger the edgeThreshold, the less features are filtered out
(more features are retained).
SIFT_sigma : double
SIFT library default is 1.6.
The sigma of the Gaussian applied to the input image at the octave #0.
If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check</p>
<p>Returns:
dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def SIFT_evaluation(self, eval_fls = [], **kwargs):
    &#39;&#39;&#39;
    Evaluate SIFT settings and perfromance of few test frames (eval_fls). ©G.Shtengel 10/2021 gleb.shtengel@gmail.com

    Parameters:
    eval_fls : array of str
        filenames for the data frames to be used for SIFT evaluation

    kwargs
    ---------
    data_dir : str
        data directory (path)
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    fnm_reg : str
        filename for the final registed dataset
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for key-point extraction
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    TransformType : object reference
        Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
        Choose from the following options:
            ShiftTransform - only x-shift and y-shift
            XScaleShiftTransform  -  x-scale, x-shift, y-shift
            ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
            AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
            RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
    l2_matrix : 2D float array
        matrix of regularization (shrinkage) parameters
    targ_vector = 1D float array
        target vector for regularization
    solver : str
        Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
    drmax : float
        In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
        In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
    max_iter : int
        Max number of iterations in the iterative procedure above (RANSAC or LinReg)
    BFMatcher : boolean
        If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches : boolean
        If True, matches will be saved into individual files
    kp_max_num : int
        Max number of key-points to be matched.
        Key-points in every frame are indexed (in descending order) by the strength of the response.
        Only kp_max_num is kept for further processing.
        Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)
    SIFT_nfeatures : int
        SIFT libary default is 0. The number of best features to retain.
        The features are ranked by their scores (measured in SIFT algorithm as the local contrast)
    SIFT_nOctaveLayers : int
        SIFT libary default  is 3. The number of layers in each octave.
        3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.
    SIFT_contrastThreshold : double
        SIFT libary default  is 0.04. The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions.
        The larger the threshold, the less features are produced by the detector.
        The contrast threshold will be divided by nOctaveLayers when the filtering is applied.
        When nOctaveLayers is set to default and if you want to use the value used in
        D. Lowe paper (0.03), set this argument to 0.09.
    SIFT_edgeThreshold : double
        SIFT libary default  is 10. The threshold used to filter out edge-like features.
        Note that the its meaning is different from the contrastThreshold,
        i.e. the larger the edgeThreshold, the less features are filtered out
        (more features are retained).
    SIFT_sigma : double
        SIFT library default is 1.6.  The sigma of the Gaussian applied to the input image at the octave #0.
        If your image is captured with a weak camera with soft lenses, you might want to reduce the number.
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check

    Returns:
    dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts
    &#39;&#39;&#39;
    if len(eval_fls) == 0:
        eval_fls = [self.fls[self.nfrs//2], self.fls[self.nfrs//2+1]]
    data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
    ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
    threshold_min = kwargs.get(&#34;threshold_min&#34;, self.threshold_min)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, self.threshold_max)
    nbins = kwargs.get(&#34;nbins&#34;, self.nbins)
    TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
    solver = kwargs.get(&#34;solver&#34;, self.solver)
    drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
    max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
    SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, self.SIFT_nfeatures)
    SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, self.SIFT_nOctaveLayers)
    SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, self.SIFT_contrastThreshold)
    SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, self.SIFT_edgeThreshold)
    SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, self.SIFT_sigma)
    Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
    save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png)
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])

    SIFT_evaluation_kwargs = {&#39;ftype&#39; : ftype,
                            &#39;Sample_ID&#39; : Sample_ID,
                            &#39;data_dir&#39; : data_dir,
                            &#39;fnm_reg&#39; : fnm_reg,
                            &#39;threshold_min&#39; : threshold_min,
                            &#39;threshold_max&#39; : threshold_max,
                            &#39;nbins&#39; : nbins,
                            &#39;evaluation_box&#39; : evaluation_box,
                            &#39;TransformType&#39; : TransformType,
                            &#39;l2_matrix&#39; : l2_matrix,
                            &#39;targ_vector&#39; : targ_vector,
                            &#39;solver&#39; : solver,
                            &#39;drmax&#39; : drmax,
                            &#39;max_iter&#39; : max_iter,
                            &#39;kp_max_num&#39; : kp_max_num,
                            &#39;SIFT_Transform&#39; : TransformType,
                            &#39;SIFT_nfeatures&#39; : SIFT_nfeatures,
                            &#39;SIFT_nOctaveLayers&#39; : SIFT_nOctaveLayers,
                            &#39;SIFT_contrastThreshold&#39; : SIFT_contrastThreshold,
                            &#39;SIFT_edgeThreshold&#39; : SIFT_edgeThreshold,
                            &#39;SIFT_sigma&#39; : SIFT_sigma,
                            &#39;Lowe_Ratio_Threshold&#39; : Lowe_Ratio_Threshold,
                            &#39;BFMatcher&#39; : BFMatcher,
                            &#39;save_matches&#39; : save_matches,
                            &#39;save_res_png&#39;  : save_res_png}

    dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts = SIFT_evaluation_dataset(eval_fls, **SIFT_evaluation_kwargs)
    src_pts_filtered, dst_pts_filtered = kpts
    print(&#39;Transformation Matrix determined using &#39;+ TransformType.__name__ +&#39; using &#39; + solver + &#39; solver&#39;)
    print(transform_matrix)
    print(&#39;{:d} keypoint matches were detected with {:.1f} pixel outlier threshold&#39;.format(n_matches, drmax))
    print(&#39;Number of iterations: {:d}&#39;.format(iteration))
    return dmin, dmax, comp_time, transform_matrix, n_matches, iteration, kpts</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.check_for_nomatch_frames"><code class="name flex">
<span>def <span class="ident">check_for_nomatch_frames</span></span>(<span>self, thr_npt, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate cumulative transformation matrix</p>
<h2 id="parameters">Parameters:</h2>
<p>thr_npt : int
minimum number of matches. If the pair has less than this - it is reported as "suspicious" and is excluded.</p>
<h2 id="kwargs">Kwargs</h2>
<p>data_dir : str
data directory (path)
fnm_reg : str
filename for the final registed dataset
Sample_ID : str
Sample ID
TransformType : object reference
Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
Choose from the following options:
ShiftTransform - only x-shift and y-shift
XScaleShiftTransform
-
x-scale, x-shift, y-shift
ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
AffineTransform -
full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
l2_matrix : 2D float array
matrix of regularization (shrinkage) parameters
targ_vector = 1D float array
target vector for regularization
solver : str
Solver used for SIFT ('RANSAC' or 'LinReg')
drmax : float
In the case of 'RANSAC' - Maximum distance for a data point to be classified as an inlier.
In the case of 'LinReg' - outlier threshold for iterative regression
max_iter : int
Max number of iterations in the iterative procedure above (RANSAC or LinReg)
BFMatcher : boolean
If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
save_matches : boolean
If True, matches will be saved into individual files
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check
preserve_scales : boolean
If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
fit_params : list
Example: ['SG', 501, 3]
- perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
Other options are:
['LF'] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
['PF', 2]
- use polynomial fit (in this case of order 2)
subtract_linear_fit : [boolean, boolean]
List of two Boolean values for two directions: X- and Y-.
If True, the linear slopes along X- and Y- directions (respectively)
will be subtracted from the cumulative shifts.
This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
pad_edges : boolean
If True, the data will be padded before transformation to avoid clipping.</p>
<p>Returns:
tr_matr_cum_residual : list of 2D arrays of float
Cumulative transformation matrices</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_for_nomatch_frames(self, thr_npt, **kwargs):
    &#39;&#39;&#39;
    Calculate cumulative transformation matrix

    Parameters:
    -----------
    thr_npt : int
        minimum number of matches. If the pair has less than this - it is reported as &#34;suspicious&#34; and is excluded.

    kwargs
    ---------
    data_dir : str
        data directory (path)
    fnm_reg : str
        filename for the final registed dataset
    Sample_ID : str
        Sample ID
    TransformType : object reference
            Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
            Choose from the following options:
                ShiftTransform - only x-shift and y-shift
                XScaleShiftTransform  -  x-scale, x-shift, y-shift
                ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
    l2_matrix : 2D float array
        matrix of regularization (shrinkage) parameters
    targ_vector = 1D float array
        target vector for regularization
    solver : str
        Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
    drmax : float
        In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
        In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
    max_iter : int
        Max number of iterations in the iterative procedure above (RANSAC or LinReg)
    BFMatcher : boolean
        If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches : boolean
        If True, matches will be saved into individual files
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    preserve_scales : boolean
        If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
    fit_params : list
        Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
        Other options are:
            [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
            [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
    subtract_linear_fit : [boolean, boolean]
        List of two Boolean values for two directions: X- and Y-.
        If True, the linear slopes along X- and Y- directions (respectively)
        will be subtracted from the cumulative shifts.
        This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.

    Returns:
    tr_matr_cum_residual : list of 2D arrays of float
        Cumulative transformation matrices
    &#39;&#39;&#39;
    self.thr_npt = thr_npt
    data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
    TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
    l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
    targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
    solver = kwargs.get(&#34;solver&#34;, self.solver)
    drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
    max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
    BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
    save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
    kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
    preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, self.preserve_scales)
    fit_params =  kwargs.get(&#34;fit_params&#34;, self.fit_params)
    subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, self.subtract_linear_fit)
    subtract_FOVtrend_from_fit =  kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, self.subtract_FOVtrend_from_fit)
    pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)

    res_nomatch_check = check_for_nomatch_frames_dataset(self.fls, self.fnms, self.fnms_matches,
                                 self.transformation_matrix, self.error_abs_mean, self.npts,
                                 thr_npt,
                                 data_dir = self.data_dir, fnm_reg = self.fnm_reg)
    frames_to_remove, self.fls, self.fnms, self.fnms_matches, self.error_abs_mean, self.npts, self.transformation_matrix = res_nomatch_check

    if len(frames_to_remove) &gt; 0:
        TM_kwargs = {&#39;fnm_reg&#39; : fnm_reg,
                        &#39;data_dir&#39; : data_dir,
                        &#39;TransformType&#39; : TransformType,
                        &#39;Sample_ID&#39; : Sample_ID,
                        &#39;l2_matrix&#39; : l2_matrix,
                        &#39;targ_vector&#39;: targ_vector,
                        &#39;solver&#39; : solver,
                        &#39;drmax&#39; : drmax,
                        &#39;max_iter&#39; : max_iter,
                        &#39;BFMatcher&#39; : BFMatcher,
                        &#39;save_matches&#39; : save_matches,
                        &#39;kp_max_num&#39; : kp_max_num,
                        &#39;save_res_png &#39; : save_res_png ,
                        &#39;preserve_scales&#39; : preserve_scales,
                        &#39;fit_params&#39; : fit_params,
                        &#39;subtract_linear_fit&#39; : subtract_linear_fit,
                        &#39;subtract_FOVtrend_from_fit&#39; : subtract_FOVtrend_from_fit,
                        &#39;pad_edges&#39; : pad_edges}
        self.tr_matr_cum_residual, self.transf_matrix_xlsx_file = process_transf_matrix(self.transformation_matrix,
                                         self.FOVtrend_x,
                                         self.FOVtrend_y,
                                         self.fnms_matches,
                                         self.npts,
                                         self.error_abs_mean,
                                         **TM_kwargs)
    return self.tr_matr_cum_residual, self.transf_matrix_xlsx_file</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.convert_raw_data_to_tif_files"><code class="name flex">
<span>def <span class="ident">convert_raw_data_to_tif_files</span></span>(<span>self, DASK_client='', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert binary ".dat" files into ".tif" files.</p>
<p>Parameters:
DASK_client : instance of the DASK client object</p>
<h2 id="kwargs">Kwargs</h2>
<p>use_DASK : boolean
use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_raw_data_to_tif_files(self, DASK_client = &#39;&#39;, **kwargs):
    &#39;&#39;&#39;
    Convert binary &#34;.dat&#34; files into &#34;.tif&#34; files.

    Parameters:
    DASK_client : instance of the DASK client object

    kwargs
    ---------
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    &#39;&#39;&#39;
    if hasattr(self, &#34;use_DASK&#34;):
        use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
    else:
        use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    if hasattr(self, &#34;DASK_client_retries&#34;):
        DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
    else:
        DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    if self.ftype ==0 :
        print(&#39;Step 2a: Creating &#34;*InLens.tif&#34; files using DASK distributed&#39;)
        t00 = time.time()
        if use_DASK:
            try:
                futures = DASK_client.map(save_inlens_data, self.fls, retries = DASK_client_retries)
                fls_new = np.array(DASK_client.gather(futures))
            except:
                fls_new = []
                for fl in tqdm(self.fls, desc = &#39;Converting .dat data files into .tif format&#39;):
                        fls_new.append(save_inlens_data(fl))
        else:
            fls_new = []
            for fl in tqdm(self.fls, desc = &#39;Converting .dat data files into .tif format&#39;):
                fls_new.append(save_inlens_data(fl))

        t01 = time.time()
        print(&#39;Step 2a: Elapsed time: {:.2f} seconds&#39;.format(t01 - t00))
        print(&#39;Step 2a: Quick check if all files were converted: &#39;, np.array_equal(self.fls, fls_new))
    else:
        print(&#39;Step 2a: data is already in TIF format&#39;)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.determine_transformations"><code class="name flex">
<span>def <span class="ident">determine_transformations</span></span>(<span>self, DASK_client, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine transformation matrices for sequential frame pairs</p>
<p>Parameters:
DASK_client : instance of the DASK client object</p>
<h2 id="kwargs">Kwargs</h2>
<p>use_DASK : boolean
use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
TransformType : object reference
Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
Choose from the following options:
ShiftTransform - only x-shift and y-shift
XScaleShiftTransform
-
x-scale, x-shift, y-shift
ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
AffineTransform -
full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
l2_matrix : 2D float array
matrix of regularization (shrinkage) parameters
targ_vector = 1D float array
target vector for regularization
solver : str
Solver used for SIFT ('RANSAC' or 'LinReg')
drmax : float
In the case of 'RANSAC' - Maximum distance for a data point to be classified as an inlier.
In the case of 'LinReg' - outlier threshold for iterative regression
max_iter : int
Max number of iterations in the iterative procedure above (RANSAC or LinReg)
Lowe_Ratio_Threshold : float
threshold for Lowe's Ratio Test
BFMatcher : boolean
If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
save_matches : boolean
If True, matches will be saved into individual files
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check</p>
<p>Returns:
results_s4 : array of lists containing the reults:
results_s4 = [transformation_matrix, fnm_matches, npt, error_abs_mean]
transformation_matrix : 2D float array
transformation matrix for each sequential frame pair
fnm_matches : str
filename containing the matches used to determin the transformation for the par of frames
npts : int
number of matches
error_abs_mean : float
mean abs error of registration for all matched Key-Points</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_transformations(self, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Determine transformation matrices for sequential frame pairs

    Parameters:
    DASK_client : instance of the DASK client object

    kwargs
    ---------
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    TransformType : object reference
            Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
            Choose from the following options:
                ShiftTransform - only x-shift and y-shift
                XScaleShiftTransform  -  x-scale, x-shift, y-shift
                ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
    l2_matrix : 2D float array
        matrix of regularization (shrinkage) parameters
    targ_vector = 1D float array
        target vector for regularization
    solver : str
        Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
    drmax : float
        In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
        In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
    max_iter : int
        Max number of iterations in the iterative procedure above (RANSAC or LinReg)
    Lowe_Ratio_Threshold : float
        threshold for Lowe&#39;s Ratio Test
    BFMatcher : boolean
        If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches : boolean
        If True, matches will be saved into individual files
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check


    Returns:
    results_s4 : array of lists containing the reults:
        results_s4 = [transformation_matrix, fnm_matches, npt, error_abs_mean]
        transformation_matrix : 2D float array
            transformation matrix for each sequential frame pair
        fnm_matches : str
            filename containing the matches used to determin the transformation for the par of frames
        npts : int
            number of matches
        error_abs_mean : float
            mean abs error of registration for all matched Key-Points
    &#39;&#39;&#39;
    if len(self.fnms) == 0:
        print(&#39;No data on individual key-point data files, peform key-point search&#39;)
        results_s4 = []
    else:
        if hasattr(self, &#34;use_DASK&#34;):
            use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
        else:
            use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
        if hasattr(self, &#34;DASK_client_retries&#34;):
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
        else:
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
        l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
        targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
        solver = kwargs.get(&#34;solver&#34;, self.solver)
        drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
        max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
        kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
        Lowe_Ratio_Threshold = kwargs.get(&#34;Lowe_Ratio_Threshold&#34;, 0.7)   # threshold for Lowe&#39;s Ratio Test
        BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
        save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        dt_kwargs = {&#39;ftype&#39; : ftype,
                        &#39;TransformType&#39; : TransformType,
                        &#39;l2_matrix&#39; : l2_matrix,
                        &#39;targ_vector&#39;: targ_vector,
                        &#39;solver&#39; : solver,
                        &#39;drmax&#39; : drmax,
                        &#39;max_iter&#39; : max_iter,
                        &#39;BFMatcher&#39; : BFMatcher,
                        &#39;save_matches&#39; : save_matches,
                        &#39;kp_max_num&#39; : kp_max_num,
                        &#39;Lowe_Ratio_Threshold&#39; : Lowe_Ratio_Threshold}

        params_s4 = []
        for j, fnm in enumerate(self.fnms[:-1]):
            fname1 = self.fnms[j]
            fname2 = self.fnms[j+1]
            params_s4.append([fname1, fname2, dt_kwargs])
        if use_DASK:
            print(&#39;Using DASK distributed&#39;)
            futures4 = DASK_client.map(determine_transformations_files, params_s4, retries = DASK_client_retries)
            #determine_transformations_files returns (transform_matrix, fnm_matches, kpts, iteration)
            results_s4 = DASK_client.gather(futures4)
        else:
            print(&#39;Using Local Computation&#39;)
            results_s4 = []
            for param_s4 in tqdm(params_s4, desc = &#39;Extracting Transformation Parameters: &#39;):
                results_s4.append(determine_transformations_files(param_s4))
        #determine_transformations_files returns (transform_matrix, fnm_matches, kpts, errors, iteration)
        self.transformation_matrix = np.nan_to_num(np.array([result[0] for result in results_s4]))
        self.fnms_matches = [result[1] for result in results_s4]
        self.error_abs_mean = np.nan_to_num(np.array([result[3] for result in results_s4]))
        self.npts = np.nan_to_num(np.array([len(result[2][0])  for result in results_s4]))
        print(&#39;Mean Number of Keypoints :&#39;, np.mean(self.npts).astype(np.int16))
    return results_s4</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.estimate_SNRs"><code class="name flex">
<span>def <span class="ident">estimate_SNRs</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate SNRs in Image A and Image B based on single-image SNR calculation.</p>
<h2 id="kwargs">Kwargs</h2>
<p>frame_inds : list of int
List oif frame indecis to use to display the evaluation box.
Default are [nfrs//10, nfrs//2, nfrs//10<em>9]
evaluation_box : list of 4 int
evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
sliding_evaluation_box : boolean
if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
start_evaluation_box : list of 4 int
see above
stop_evaluation_box : list of 4 int
see above
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
data_dir : str
data directory (path)
Sample_ID : str
Sample ID
ImgB_fraction : float
Optional fractional weight of Image B to use for constructing the fused image: FusedImage = ImageA</em>(1.0-ImgB_fraction) + ImageB*ImgB_fraction
If not provided, the value determined from rSNR ratios will be used.
invert_data : boolean
If True - the data is inverted
perfrom_transformation : boolean
If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed
pad_edges : boolean
If True, the data will be padded before transformation to avoid clipping.
extrapolate_signal : boolean
extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check
pad_edges : boolean
If True, the data will be padded before transformation to avoid clipping.
flipY : boolean
If True, the data will be flipped along Y-axis. Default is False.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate_SNRs(self, **kwargs):
    &#39;&#39;&#39;
    Estimate SNRs in Image A and Image B based on single-image SNR calculation.

    kwargs
    ---------
    frame_inds : list of int
        List oif frame indecis to use to display the evaluation box.
        Default are [nfrs//10, nfrs//2, nfrs//10*9]
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    data_dir : str
        data directory (path)
    Sample_ID : str
        Sample ID
    ImgB_fraction : float
        Optional fractional weight of Image B to use for constructing the fused image: FusedImage = ImageA*(1.0-ImgB_fraction) + ImageB*ImgB_fraction
        If not provided, the value determined from rSNR ratios will be used.
    invert_data : boolean
        If True - the data is inverted
    perfrom_transformation : boolean
        If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.
    extrapolate_signal : boolean
        extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.
    flipY : boolean
        If True, the data will be flipped along Y-axis. Default is False.

    &#39;&#39;&#39;
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
    data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
    int_order = kwargs.get(&#34;int_order&#34;, self.int_order)
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, False )
    ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, 0.00 )
    flipY = kwargs.get(&#34;flipY&#34;, False)
    pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)
    perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True) and hasattr(self, &#39;tr_matr_cum_residual&#39;)
    extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)

    fls = self.fls
    nfrs = len(fls)
    default_indecis = [nfrs//10, nfrs//2, nfrs//10*9]
    frame_inds = kwargs.get(&#34;frame_inds&#34;, default_indecis)

    test_frame = FIBSEM_frame(fls[0], ftype=ftype)

    xi = 0
    yi = 0
    xsz = test_frame.XResolution
    xa = xi + xsz
    ysz = test_frame.YResolution
    ya = yi + ysz

    xi_eval = xi + evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = xa
    yi_eval = yi + evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ya

    frame_img = np.zeros((ysz, xsz))
    xSNRAs=[]
    ySNRAs=[]
    rSNRAs=[]
    xSNRBs=[]
    ySNRBs=[]
    rSNRBs=[]

    for j in tqdm(frame_inds, desc=&#39;Analyzing Auto-Correlation SNRs &#39;):

        frame = FIBSEM_frame(fls[j], ftype=ftype)
        if pad_edges and perfrom_transformation:
            shape = [frame.YResolution, frame.XResolution]
            xmn, xmx, ymn, ymx = determine_pad_offsets(shape, self.tr_matr_cum_residual)
            padx = np.int16(xmx - xmn)
            pady = np.int16(ymx - ymn)
            xi = np.int16(np.max([xmx, 0]))
            yi = np.int16(np.max([ymx, 0]))
            # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
            # so that the transformed images are not clipped.
            # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
            # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
            # those are calculated below base on the amount of padding calculated above
            shift_matrix = np.array([[1.0, 0.0, xi],
                                     [0.0, 1.0, yi],
                                     [0.0, 0.0, 1.0]])
            inv_shift_matrix = np.linalg.inv(shift_matrix)
        else:
            padx = 0
            pady = 0
            xi = 0
            yi = 0
            shift_matrix = np.eye(3,3)
            inv_shift_matrix = np.eye(3,3)

        xsz = frame.XResolution + padx
        xa = xi + frame.XResolution
        ysz = frame.YResolution + pady
        ya = yi + frame.YResolution

        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        if sliding_evaluation_box:
            dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
            dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
        else:
            dx_eval = 0
            dy_eval = 0

        frame_imgA = np.zeros((ysz, xsz))
        if self.DetB != &#39;None&#39;:
            frame_imgB = np.zeros((ysz, xsz))

        if invert_data:
            frame_imgA[yi:ya, xi:xa] = np.negative(frame.RawImageA.astype(float))
            if self.DetB != &#39;None&#39;:
                frame_imgB[yi:ya, xi:xa] = np.negative(frame.RawImageB.astype(float))
        else:
            frame_imgA[yi:ya, xi:xa]  = frame.RawImageA.astype(float)
            if self.DetB != &#39;None&#39;:
                frame_imgB[yi:ya, xi:xa]  = frame.RawImageB.astype(float)

        if perfrom_transformation:
            transf = ProjectiveTransform(matrix = shift_matrix @ (self.tr_matr_cum_residual[j] @ inv_shift_matrix))
            frame_imgA_reg = warp(frame_imgA, transf, order = int_order, preserve_range=True)
            if self.DetB != &#39;None&#39;:
                frame_imgB_reg = warp(frame_imgB, transf, order = int_order, preserve_range=True)
        else:
            frame_imgA_reg = frame_imgA.copy()
            if self.DetB != &#39;None&#39;:
                frame_imgB_reg = frame_imgB.copy()

        if flipY:
            frame_imgA_reg = np.flip(frame_imgA_reg, axis=0)
            if self.DetB != &#39;None&#39;:
                frame_imgB_reg = np.flip(frame_imgB_reg, axis=0)

        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*j//nfrs
            yi_eval = start_evaluation_box[0] + dy_eval*j//nfrs
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = xsz
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ysz

        &#39;&#39;&#39;
        if invert_data:
            if test_frame.EightBit==0:
                frame_imgA = np.negative(FIBSEM_frame(fls[j], ftype=ftype).RawImageA)
                if self.DetB != &#39;None&#39;:
                    frame_imgB = np.negative(FIBSEM_frame(fls[j], ftype=ftype).RawImageB)
            else:
                frame_imgA  =  uint8(255) - FIBSEM_frame(fls[j], ftype=ftype).RawImageA
                if self.DetB != &#39;None&#39;:
                    frame_imgB  =  uint8(255) - FIBSEM_frame(fls[j], ftype=ftype).RawImageB

        else:
            frame_imgA  = FIBSEM_frame(fls[j], ftype=ftype).RawImageA
            if self.DetB != &#39;None&#39;:
                frame_imgB  = FIBSEM_frame(fls[j], ftype=ftype).RawImageB

        if flipY:
            frame_imgA = np.flip(frame_imgA, axis=0)
            frame_imgB = np.flip(frame_imgB, axis=0)
        &#39;&#39;&#39;
        frame_imgA_eval = frame_imgA_reg[yi_eval:ya_eval, xi_eval:xa_eval]
        SNR_png = os.path.splitext(os.path.split(fls[j])[1])[0] + &#39;.png&#39;
        SNR_png_fname = os.path.join(data_dir, SNR_png)
        ImageA_xSNR, ImageA_ySNR, ImageA_rSNR= Single_Image_SNR(frame_imgA_eval, extrapolate_signal=extrapolate_signal, save_res_png=save_res_png,
                                                                    res_fname = SNR_png_fname.replace(&#39;.png&#39;, &#39;_ImgA_SNR.png&#39;),
                                                                    img_label=&#39;Image A, frame={:d}&#39;.format(j))
        xSNRAs.append(ImageA_xSNR)
        ySNRAs.append(ImageA_ySNR)
        rSNRAs.append(ImageA_rSNR)
        if self.DetB != &#39;None&#39;:
            frame_imgB_eval = frame_imgB_reg[yi_eval:ya_eval, xi_eval:xa_eval]
            ImageB_xSNR, ImageB_ySNR, ImageB_rSNR = Single_Image_SNR(frame_imgB_eval, extrapolate_signal=extrapolate_signal, save_res_png=save_res_png,
                                                                        res_fname = SNR_png_fname.replace(&#39;.png&#39;, &#39;_ImgB_SNR.png&#39;),
                                                                        img_label=&#39;Image B, frame={:d}&#39;.format(j))
            xSNRBs.append(ImageB_xSNR)
            ySNRBs.append(ImageB_ySNR)
            rSNRBs.append(ImageB_rSNR)

    fig, ax = subplots(1,1, figsize = (6,4))
    ax.plot(frame_inds, xSNRAs, &#39;r+&#39;, label=&#39;Image A x-SNR&#39;)
    ax.plot(frame_inds, ySNRAs, &#39;b+&#39;, label=&#39;Image A y-SNR&#39;)
    ax.plot(frame_inds, rSNRAs, &#39;g+&#39;, label=&#39;Image A r-SNR&#39;)
    if self.DetB != &#39;None&#39;:
        ax.plot(frame_inds, xSNRBs, &#39;rx&#39;, linestyle=&#39;dotted&#39;, label=&#39;Image B x-SNR&#39;)
        ax.plot(frame_inds, ySNRBs, &#39;bx&#39;, linestyle=&#39;dotted&#39;, label=&#39;Image B y-SNR&#39;)
        ax.plot(frame_inds, rSNRBs, &#39;gx&#39;, linestyle=&#39;dotted&#39;, label=&#39;Image B r-SNR&#39;)
        ImgB_fraction_xSNR = np.mean(np.array(xSNRBs)/(np.array(xSNRAs) + np.array(xSNRBs)))
        ImgB_fraction_ySNR = np.mean(np.array(ySNRBs)/(np.array(ySNRAs) + np.array(ySNRBs)))
        ImgB_fraction_rSNR = np.mean(np.array(rSNRBs)/(np.array(rSNRAs) + np.array(rSNRBs)))
        if ImgB_fraction &lt; 1e-9:
            ImgB_fraction = ImgB_fraction_rSNR
        ax.text(0.1, 0.5, &#39;ImgB fraction (x-SNR) = {:.4f}&#39;.format(ImgB_fraction_xSNR), color=&#39;r&#39;, transform=ax.transAxes)
        ax.text(0.1, 0.42, &#39;ImgB fraction (y-SNR) = {:.4f}&#39;.format(ImgB_fraction_ySNR), color=&#39;b&#39;, transform=ax.transAxes)
        ax.text(0.1, 0.34, &#39;ImgB fraction (r-SNR) = {:.4f}&#39;.format(ImgB_fraction_rSNR), color=&#39;g&#39;, transform=ax.transAxes)

        xSNRFs=[]
        ySNRFs=[]
        rSNRFs=[]
        for j in tqdm(frame_inds, desc=&#39;Re-analyzing Auto-Correlation SNRs for fused image&#39;):
            frame = FIBSEM_frame(fls[j], ftype=ftype)
            if pad_edges and perfrom_transformation:
                shape = [frame.YResolution, frame.XResolution]
                xmn, xmx, ymn, ymx = determine_pad_offsets(shape, self.tr_matr_cum_residual)
                padx = np.int16(xmx - xmn)
                pady = np.int16(ymx - ymn)
                xi = np.int16(np.max([xmx, 0]))
                yi = np.int16(np.max([ymx, 0]))
                # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
                # so that the transformed images are not clipped.
                # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
                # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
                # those are calculated below base on the amount of padding calculated above
                shift_matrix = np.array([[1.0, 0.0, xi],
                                         [0.0, 1.0, yi],
                                         [0.0, 0.0, 1.0]])
                inv_shift_matrix = np.linalg.inv(shift_matrix)
            else:
                padx = 0
                pady = 0
                xi = 0
                yi = 0
                shift_matrix = np.eye(3,3)
                inv_shift_matrix = np.eye(3,3)

            xsz = frame.XResolution + padx
            xa = xi + frame.XResolution
            ysz = frame.YResolution + pady
            ya = yi + frame.YResolution

            xi_eval = xi + evaluation_box[2]
            if evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + evaluation_box[3]
            else:
                xa_eval = xa
            yi_eval = yi + evaluation_box[0]
            if evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + evaluation_box[1]
            else:
                ya_eval = ya

            if sliding_evaluation_box:
                dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
                dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
            else:
                dx_eval = 0
                dy_eval = 0

            frame_imgA = np.zeros((ysz, xsz))
            if self.DetB != &#39;None&#39;:
                frame_imgB = np.zeros((ysz, xsz))

            if invert_data:
                frame_imgA[yi:ya, xi:xa] = np.negative(frame.RawImageA.astype(float))
                if self.DetB != &#39;None&#39;:
                    frame_imgB[yi:ya, xi:xa] = np.negative(frame.RawImageB.astype(float))
            else:
                frame_imgA[yi:ya, xi:xa]  = frame.RawImageA.astype(float)
                if self.DetB != &#39;None&#39;:
                    frame_imgB[yi:ya, xi:xa]  = frame.RawImageB.astype(float)

            if perfrom_transformation:
                transf = ProjectiveTransform(matrix = shift_matrix @ (self.tr_matr_cum_residual[j] @ inv_shift_matrix))
                frame_imgA_reg = warp(frame_imgA, transf, order = int_order, preserve_range=True)
                if self.DetB != &#39;None&#39;:
                    frame_imgB_reg = warp(frame_imgB, transf, order = int_order, preserve_range=True)
            else:
                frame_imgA_reg = frame_imgA.copy()
                if self.DetB != &#39;None&#39;:
                    frame_imgB_reg = frame_imgB.copy()

            if flipY:
                frame_imgA_reg = np.flip(frame_imgA_reg, axis=0)
                if self.DetB != &#39;None&#39;:
                    frame_imgB_reg = np.flip(frame_imgB_reg, axis=0)

            if sliding_evaluation_box:
                xi_eval = start_evaluation_box[2] + dx_eval*j//nfrs
                yi_eval = start_evaluation_box[0] + dy_eval*j//nfrs
                if start_evaluation_box[3] &gt; 0:
                    xa_eval = xi_eval + start_evaluation_box[3]
                else:
                    xa_eval = xsz
                if start_evaluation_box[1] &gt; 0:
                    ya_eval = yi_eval + start_evaluation_box[1]
                else:
                    ya_eval = ysz

            frame_imgA_eval = frame_imgA_reg[yi_eval:ya_eval, xi_eval:xa_eval]
            frame_imgB_eval = frame_imgB_reg[yi_eval:ya_eval, xi_eval:xa_eval]

            frame_imgF_eval = frame_imgA_eval * (1.0 - ImgB_fraction) + frame_imgB_eval * ImgB_fraction
            ImageF_xSNR, ImageF_ySNR, ImageF_rSNR = Single_Image_SNR(frame_imgF_eval, extrapolate_signal=extrapolate_signal, save_res_png=save_res_png,
                                                                    res_fname = SNR_png_fname.replace(&#39;.png&#39;, &#39;_ImgB_fr{:.3f}_SNR.png&#39;.format(ImgB_fraction)),
                                                                    img_label=&#39;Fused, ImB_fr={:.4f}, frame={:d}&#39;.format(ImgB_fraction, j))
            xSNRFs.append(ImageF_xSNR)
            ySNRFs.append(ImageF_ySNR)
            rSNRFs.append(ImageF_rSNR)

        ax.plot(frame_inds, xSNRFs, &#39;rd&#39;, linestyle=&#39;dashed&#39;, label=&#39;Fused Image x-SNR&#39;)
        ax.plot(frame_inds, ySNRFs, &#39;bd&#39;, linestyle=&#39;dashed&#39;, label=&#39;Fused Image y-SNR&#39;)
        ax.plot(frame_inds, rSNRFs, &#39;gd&#39;, linestyle=&#39;dashed&#39;, label=&#39;Fused Image r-SNR&#39;)

    else:
        ImgB_fraction_xSNR = 0.0
        ImgB_fraction_ySNR = 0.0
        ImgB_fraction_rSNR = 0.0
    ax.grid(True)
    ax.legend()
    ax.set_title(Sample_ID + &#39;  &#39; + data_dir, fontsize=8)
    ax.set_xlabel(&#39;Frame&#39;)
    ax.set_ylabel(&#39;SNR&#39;)
    if save_res_png :
        fig_filename = os.path.join(data_dir, os.path.splitext(fnm_reg)[0]+&#39;SNR_evaluation_mult_frame.png&#39;)
        fig.savefig(fig_filename, dpi=300)

    return ImgB_fraction_xSNR, ImgB_fraction_ySNR, ImgB_fraction_rSNR</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.evaluate_FIBSEM_statistics"><code class="name flex">
<span>def <span class="ident">evaluate_FIBSEM_statistics</span></span>(<span>self, DASK_client, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates parameters of FIBSEM data set (Min/Max, Working Distance (WD), Milling Y Voltage (MV), FOV center positions).</p>
<p>Parameters:
use_DASK : boolean
perform remote DASK computations
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails</p>
<p>kwargs:
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
frame_inds : array
Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
data_dir : str
data directory (path)
for saving the data
threshold_min : float
CDF threshold for determining the minimum data value
threshold_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF
sliding_minmax : boolean
if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
if False - same data_min_glob and data_max_glob will be used for all files
fit_params : list
Example: ['SG', 501, 3]
- perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
Other options are:
['LF'] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
['PF', 2]
- use polynomial fit (in this case of order 2)
Mill_Volt_Rate_um_per_V : float
Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.
FIBSEM_Data_xlsx : str
Filepath of the Excell file for the FIBSEM data set data to be saved (Data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)
disp_res : bolean
If True (default), intermediate messages and results will be displayed.</p>
<p>Returns:
list of 9 parameters: FIBSEM_Data_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding, mill_rate_WD, mill_rate_MV, center_x, center_y
FIBSEM_Data_xlsx : str
path to Excel file with the FIBSEM data
data_min_glob : float
min data value for I8 conversion (open CV SIFT requires I8)
data_man_glob : float
max data value for I8 conversion (open CV SIFT requires I8)
data_min_sliding : float array
min data values (one per file) for I8 conversion
data_max_sliding : float array
max data values (one per file) for I8 conversion</p>
<pre><code>mill_rate_WD : float array
    Milling rate calculated based on Working Distance (WD)
mill_rate_MV : float array
    Milling rate calculated based on Milling Y Voltage (MV)
center_x : float array
    FOV Center X-coordinate extrated from the header data
center_y : float array
    FOV Center Y-coordinate extrated from the header data
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_FIBSEM_statistics(self, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Evaluates parameters of FIBSEM data set (Min/Max, Working Distance (WD), Milling Y Voltage (MV), FOV center positions).

    Parameters:
    use_DASK : boolean
        perform remote DASK computations
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails

    kwargs:
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    frame_inds : array
        Array of frames to be used for evaluation. If not provided, evaluzation will be performed on all frames
    data_dir : str
        data directory (path)  for saving the data
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    sliding_minmax : boolean
        if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
        if False - same data_min_glob and data_max_glob will be used for all files
    fit_params : list
        Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
        Other options are:
            [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
            [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
    Mill_Volt_Rate_um_per_V : float
        Milling Voltage to Z conversion (µm/V). Defaul is 31.235258870176065.
    FIBSEM_Data_xlsx : str
        Filepath of the Excell file for the FIBSEM data set data to be saved (Data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)
    disp_res : bolean
        If True (default), intermediate messages and results will be displayed.

    Returns:
    list of 9 parameters: FIBSEM_Data_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding, mill_rate_WD, mill_rate_MV, center_x, center_y
        FIBSEM_Data_xlsx : str
            path to Excel file with the FIBSEM data
        data_min_glob : float
            min data value for I8 conversion (open CV SIFT requires I8)
        data_man_glob : float
            max data value for I8 conversion (open CV SIFT requires I8)
        data_min_sliding : float array
            min data values (one per file) for I8 conversion
        data_max_sliding : float array
            max data values (one per file) for I8 conversion

        mill_rate_WD : float array
            Milling rate calculated based on Working Distance (WD)
        mill_rate_MV : float array
            Milling rate calculated based on Milling Y Voltage (MV)
        center_x : float array
            FOV Center X-coordinate extrated from the header data
        center_y : float array
            FOV Center Y-coordinate extrated from the header data
    &#39;&#39;&#39;
    if hasattr(self, &#34;use_DASK&#34;):
        use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
    else:
        use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    if hasattr(self, &#34;DASK_client_retries&#34;):
        DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
    else:
        DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
    frame_inds = kwargs.get(&#34;frame_inds&#34;, np.arange(len(self.fls)))
    data_dir = self.data_dir
    threshold_min = kwargs.get(&#34;threshold_min&#34;, self.threshold_min)
    threshold_max = kwargs.get(&#34;threshold_max&#34;, self.threshold_max)
    nbins = kwargs.get(&#34;nbins&#34;, self.nbins)
    sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, self.sliding_minmax)
    fit_params = kwargs.get(&#34;fit_params&#34;, self.fit_params)

    if hasattr(self, &#39;Mill_Volt_Rate_um_per_V&#39;):
        Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, self.Mill_Volt_Rate_um_per_V)
    else:
        Mill_Volt_Rate_um_per_V = kwargs.get(&#34;Mill_Volt_Rate_um_per_V&#34;, 31.235258870176065)
    FIBSEM_Data_xlsx = kwargs.get(&#39;FIBSEM_Data_xlsx&#39;, &#39;FIBSEM_Data_xlsx.xlsx&#39;)
    disp_res = kwargs.get(&#39;disp_res&#39;, True)

    local_kwargs = {&#39;use_DASK&#39; : use_DASK,
                    &#39;DASK_client_retries&#39; : DASK_client_retries,
                    &#39;ftype&#39; : ftype,
                    &#39;frame_inds&#39; : frame_inds,
                    &#39;data_dir&#39; : data_dir,
                    &#39;threshold_min&#39; : threshold_min,
                    &#39;threshold_max&#39; : threshold_max,
                    &#39;nbins&#39; : nbins,
                    &#39;sliding_minmax&#39; : sliding_minmax,
                    &#39;fit_params&#39; : fit_params,
                    &#39;Mill_Volt_Rate_um_per_V&#39; : Mill_Volt_Rate_um_per_V,
                    &#39;FIBSEM_Data_xlsx&#39; : FIBSEM_Data_xlsx,
                    &#39;disp_res&#39; : disp_res}

    if disp_res:
        print(&#39;Evaluating the parameters of FIBSEM data set (data Min/Max, Working Distance, Milling Y Voltage, FOV center positions)&#39;)
    self.FIBSEM_Data = evaluate_FIBSEM_frames_dataset(self.fls, DASK_client, **local_kwargs)
    self.data_minmax = self.FIBSEM_Data[0:5]
    WD = self.FIBSEM_Data[5]
    MillingYVoltage = self.FIBSEM_Data[6]

    apert = np.min((51, len(self.FIBSEM_Data[7])-1))
    self.FOVtrend_x = savgol_filter(self.FIBSEM_Data[7]*1.0, apert, 1) - self.FIBSEM_Data[7][0]
    self.FOVtrend_y = savgol_filter(self.FIBSEM_Data[8]*1.0, apert, 1) - self.FIBSEM_Data[8][0]

    WD_fit_coef = np.polyfit(frame_inds, WD, 1)
    rate_WD = WD_fit_coef[0]*1.0e6

    MV_fit_coef = np.polyfit(frame_inds, MillingYVoltage, 1)
    rate_MV = MV_fit_coef[0]*Mill_Volt_Rate_um_per_V*-1.0e3

    Z_pixel_size_WD = rate_WD
    Z_pixel_size_MV = rate_MV

    if ftype == 0:
        if disp_res:
            if self.zbin_factor &gt; 1:
                print(&#39;Z pixel (after {:d}-x Z-binning) = {:.2f} nm - based on WD data&#39;.format(self.zbin_factor, Z_pixel_size_WD*self.zbin_factor))
                print(&#39;Z pixel (after {:d}-x Z-binning) = {:.2f} nm - based on Milling Voltage data&#39;.format(self.zbin_factor, Z_pixel_size_MV*self.zbin_factor))
            else:
                print(&#39;Z pixel = {:.2f} nm  - based on WD data&#39;.format(Z_pixel_size_WD))
                print(&#39;Z pixel = {:.2f} nm  - based on Milling Voltage data&#39;.format(Z_pixel_size_MV))

        self.voxel_size = np.rec.array((self.PixelSize,  self.PixelSize,  Z_pixel_size_WD), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
    else:
        if disp_res:
            print(&#39;No milling rate data is available, isotropic voxel size is set to {:.2f} nm&#39;.format(self.PixelSize))
        self.voxel_size = np.rec.array((self.PixelSize,  self.PixelSize,  Z_pixel_size_WD), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])

    return self.FIBSEM_Data</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.evaluate_ImgB_fractions"><code class="name flex">
<span>def <span class="ident">evaluate_ImgB_fractions</span></span>(<span>self, ImgB_fractions, frame_inds, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate NCC and SNR vs Image B fraction over a set of frames.</p>
<p>ImgB_fractions : list
List of fractions to estimate the NCC and SNR
frame_inds : int array
array of frame indices to perform NCC / SNR evaluation</p>
<h2 id="kwargs">Kwargs</h2>
<p>evaluation_box : list of 4 int
evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
extrapolate_signal : boolean
extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
data_dir : str
data directory (path)
Sample_ID : str
Sample ID</p>
<p>invert_data : boolean
If True - the data is inverted
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check
pad_edges : boolean
If True, the data will be padded before transformation to avoid clipping.</p>
<p>Returns
SNRimpr_max_position, SNRimpr_max, ImgB_fractions, SNRs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_ImgB_fractions(self, ImgB_fractions, frame_inds, **kwargs):
    &#39;&#39;&#39;
    Calculate NCC and SNR vs Image B fraction over a set of frames.

    ImgB_fractions : list
        List of fractions to estimate the NCC and SNR
    frame_inds : int array
        array of frame indices to perform NCC / SNR evaluation

    kwargs
    ---------
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    extrapolate_signal : boolean
        extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    data_dir : str
        data directory (path)
    Sample_ID : str
        Sample ID

    invert_data : boolean
        If True - the data is inverted
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.


    Returns
    SNRimpr_max_position, SNRimpr_max, ImgB_fractions, SNRs
    &#39;&#39;&#39;
    data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
    if hasattr(self, &#39;invert_data&#39;):
        invert_data = kwargs.get(&#34;invert_data&#34;, self.invert_data)
    else:
        invert_data = kwargs.get(&#34;invert_data&#34;, False)
    if hasattr(self, &#39;flipY&#39;):
        flipY = kwargs.get(&#34;flipY&#34;, self.flipY)
    else:
        flipY = kwargs.get(&#34;flipY&#34;, flipY)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, False )
    extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)

    nbr = len(ImgB_fractions)
    kwargs[&#39;zbin_factor&#39;] = 1

    test_frame = FIBSEM_frame(self.fls[frame_inds[0]])

    xi = 0
    yi = 0
    xsz = test_frame.XResolution
    xa = xi + xsz
    ysz = test_frame.YResolution
    ya = yi + ysz

    xi_eval = xi + evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = xa
    yi_eval = yi + evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ya

    br_results = []
    xSNRFs=[]
    ySNRFs=[]
    rSNRFs=[]

    for ImgB_fraction in tqdm(ImgB_fractions, desc=&#39;Evaluating Img B fractions&#39;):
        kwargs[&#39;ImgB_fraction&#39;] = ImgB_fraction
        kwargs[&#39;disp_res&#39;] = False
        kwargs[&#39;evaluation_box&#39;] = evaluation_box
        kwargs[&#39;flipY&#39;] = flipY
        kwargs[&#39;invert_data&#39;] = invert_data
        DASK_client = &#39;&#39;
        kwargs[&#39;disp_res&#39;] = False
        br_res, br_res_xlsx = self.transform_and_save(DASK_client,
                                                            save_transformed_dataset=False,
                                                            save_registration_summary=False,
                                                            frame_inds=frame_inds,
                                                            use_DASK=False,
                                                            save_sample_frames_png=False,
                                                            eval_metrics = [&#39;NCC&#39;],
                                                            **kwargs)
        br_results.append(br_res)

        if invert_data:
            if test_frame.EightBit==0:
                frame_imgA = np.negative(test_frame.RawImageA)
                if self.DetB != &#39;None&#39;:
                    frame_imgB = np.negative(test_frame.RawImageB)
            else:
                frame_imgA  =  uint8(255) - test_frame.RawImageA
                if self.DetB != &#39;None&#39;:
                    frame_imgB  =  uint8(255) - test_frame.RawImageB
        else:
            frame_imgA  = test_frame.RawImageA
            if self.DetB != &#39;None&#39;:
                frame_imgB  = test_frame.RawImageB
        if flipY:
            frame_imgA = np.flip(frame_imgA, axis=0)
            frame_imgB = np.flip(frame_imgB, axis=0)

        frame_imgA_eval = frame_imgA[yi_eval:ya_eval, xi_eval:xa_eval]
        frame_imgB_eval = frame_imgB[yi_eval:ya_eval, xi_eval:xa_eval]

        frame_imgF_eval = frame_imgA_eval * (1.0 - ImgB_fraction) + frame_imgB_eval * ImgB_fraction
        ImageF_xSNR, ImageF_ySNR, ImageF_rSNR = Single_Image_SNR(frame_imgF_eval,
                                                                extrapolate_signal=extrapolate_signal,
                                                                disp_res=False,
                                                                save_res_png=False,
                                                                res_fname = &#39;&#39;,
                                                                img_label=&#39;&#39;)
        xSNRFs.append(ImageF_xSNR)
        ySNRFs.append(ImageF_ySNR)
        rSNRFs.append(ImageF_rSNR)

    fig, axs = subplots(4,1, figsize=(6,11))
    fig.subplots_adjust(left=0.12, bottom=0.06, right=0.99, top=0.96, wspace=0.25, hspace=0.24)
    try:
        ncc0 = (br_results[0])[&#39;NCC&#39;]
    except:
        ncc0 = (br_results[0])[&#39;Image NCC&#39;]
    SNR0 = ncc0 / (1-ncc0)
    SNRimpr_cc = []
    SNRs = []

    for j, (ImgB_fraction, br_result) in enumerate(zip(ImgB_fractions, br_results)):
        my_col = get_cmap(&#34;gist_rainbow_r&#34;)((nbr-j)/(nbr-1))
        try:
            ncc = br_result[&#39;NCC&#39;]
        except:
            ncc = br_result[&#39;Image NCC&#39;]
        SNR = ncc / (1.0-ncc)
        frames_local = br_result[&#39;Frame&#39;]
        axs[0].plot(frames_local, SNR, color=my_col, label = &#39;ImgB fraction = {:.2f}&#39;.format(ImgB_fraction))
        axs[1].plot(frames_local, SNR/SNR0, color=my_col, label = &#39;ImgB fraction = {:.2f}&#39;.format(ImgB_fraction))
        SNRimpr_cc.append(np.mean(SNR/SNR0))
        SNRs.append(np.mean(SNR))

    SNRimpr_ac = np.array(rSNRFs) / rSNRFs[0]

    SNRimpr_cc_max = np.max(SNRimpr_cc)
    SNRimpr_cc_max_ind = np.argmax(SNRimpr_cc)
    ImgB_fraction_max = ImgB_fractions[SNRimpr_cc_max_ind]
    xi = max(0, (SNRimpr_cc_max_ind-3))
    xa = min((SNRimpr_cc_max_ind+3), len(ImgB_fractions))
    ImgB_fr_range = ImgB_fractions[xi : xa]
    SNRimpr_cc_range = SNRimpr_cc[xi : xa]
    popt = np.polyfit(ImgB_fr_range, SNRimpr_cc_range, 2)
    SNRimpr_cc_fit_max_pos = -0.5 * popt[1] / popt[0]
    ImgB_fr_fit_cc = np.linspace(ImgB_fr_range[0], ImgB_fr_range[-1], 21)
    SNRimpr_cc_fit = np.polyval(popt, ImgB_fr_fit_cc)
    if popt[0] &lt; 0 and SNRimpr_cc_fit_max_pos &gt; ImgB_fractions[0] and SNRimpr_cc_fit_max_pos&lt;ImgB_fractions[-1]:
        SNRimpr_cc_max_position = SNRimpr_cc_fit_max_pos
        SNRimpr_cc_max = np.polyval(popt, SNRimpr_cc_max_position)
    else:
        SNRimpr_cc_max_position = ImgB_fraction_max

    SNRimpr_ac_max = np.max(SNRimpr_ac)
    SNRimpr_ac_max_ind = np.argmax(SNRimpr_ac)
    ImgB_fraction_max = ImgB_fractions[SNRimpr_ac_max_ind]
    xi = max(0, (SNRimpr_ac_max_ind-3))
    xa = min((SNRimpr_ac_max_ind+3), len(ImgB_fractions))
    ImgB_fr_range = ImgB_fractions[xi : xa]
    SNRimpr_ac_range = SNRimpr_ac[xi : xa]
    popt = np.polyfit(ImgB_fr_range, SNRimpr_ac_range, 2)
    SNRimpr_ac_fit_max_pos = -0.5 * popt[1] / popt[0]
    ImgB_fr_fit_ac = np.linspace(ImgB_fr_range[0], ImgB_fr_range[-1], 21)
    SNRimpr_ac_fit = np.polyval(popt, ImgB_fr_fit_ac)
    if popt[0] &lt; 0 and SNRimpr_ac_fit_max_pos &gt; ImgB_fractions[0] and SNRimpr_ac_fit_max_pos&lt;ImgB_fractions[-1]:
        SNRimpr_ac_max_position = SNRimpr_ac_fit_max_pos
        SNRimpr_ac_max = np.polyval(popt, SNRimpr_ac_max_position)
    else:
        SNRimpr_ac_max_position = ImgB_fraction_max

    fs=10
    axs[0].grid(True)
    axs[0].set_ylabel(&#39;Frame-to-Frame SNR&#39;, fontsize=fs)
    axs[0].set_xlabel(&#39;Frame&#39;, fontsize=fs)
    axs[0].legend(fontsize=fs-1)
    axs[0].set_title(Sample_ID + &#39;  &#39; + data_dir, fontsize=fs)
    axs[1].grid(True)
    axs[1].set_ylabel(&#39;Frame-to-Frame SNR Improvement&#39;, fontsize=fs)
    axs[1].set_xlabel(&#39;Frame&#39;, fontsize=fs)

    axs[2].plot(ImgB_fractions, rSNRFs, &#39;rd&#39;, label=&#39;Data (auto-correlation)&#39;)
    axs[2].grid(True)
    axs[2].set_ylabel(&#39;Auto-Corr SNR&#39;, fontsize=fs)

    axs[3].plot(ImgB_fractions, SNRimpr_cc, &#39;cs&#39;, label=&#39;Data (cross-corr.)&#39;)
    axs[3].plot(ImgB_fr_fit_cc, SNRimpr_cc_fit, &#39;b&#39;, label=&#39;Fit (cross-corr.)&#39;)
    axs[3].plot(SNRimpr_cc_max_position, SNRimpr_cc_max, &#39;bx&#39;, markersize=10, label=&#39;Max SNR Impr. (cc)&#39;)
    axs[3].text(0.4, 0.35, &#39;Max CC SNR Improvement={:.3f}&#39;.format(SNRimpr_cc_max), transform=axs[3].transAxes, fontsize=fs)
    axs[3].text(0.4, 0.25, &#39;@ Img B Fraction ={:.3f}&#39;.format(SNRimpr_cc_max_position), transform=axs[3].transAxes, fontsize=fs)
    axs[3].plot(ImgB_fractions, SNRimpr_ac, &#39;rd&#39;, label=&#39;Data (auto-corr.)&#39;)
    axs[3].plot(ImgB_fr_fit_ac, SNRimpr_ac_fit, &#39;magenta&#39;, label=&#39;Fit (auto-corr.)&#39;)
    axs[3].plot(SNRimpr_ac_max_position, SNRimpr_ac_max, &#39;mx&#39;, markersize=10, label=&#39;Max SNR Impr. (ac)&#39;)
    axs[3].text(0.4, 0.15, &#39;Max AC SNR Improvement={:.3f}&#39;.format(SNRimpr_ac_max), transform=axs[3].transAxes, fontsize=fs)
    axs[3].text(0.4, 0.05, &#39;@ Img B Fraction ={:.3f}&#39;.format(SNRimpr_ac_max_position), transform=axs[3].transAxes, fontsize=fs)

    axs[3].legend(fontsize=fs-2, loc=&#39;upper left&#39;)
    axs[3].grid(True)
    axs[3].set_ylabel(&#39;Mean SNR improvement&#39;, fontsize=fs)
    axs[3].set_xlabel(&#39;Image B fraction&#39;, fontsize=fs)

    if save_res_png :
        fname_image = os.path.join(data_dir, os.path.splitext(fnm_reg)[0]+&#39;_SNR_vs_ImgB_ratio_evaluation.png&#39;)
        fig.savefig(fname_image, dpi=300)

    return SNRimpr_cc_max_position, SNRimpr_cc_max, ImgB_fractions, SNRs, rSNRFs</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.extract_keypoints"><code class="name flex">
<span>def <span class="ident">extract_keypoints</span></span>(<span>self, DASK_client, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract Key-Points and Descriptors</p>
<p>Parameters:
DASK_client : instance of the DASK client object</p>
<h2 id="kwargs">Kwargs</h2>
<p>use_DASK : boolean
use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
EightBit : int
0 - 16-bit data, 1: 8-bit data
fnm_reg : str
filename for the final registed dataset
threshold_min : float
CDF threshold for determining the minimum data value
threshold_max : float
CDF threshold for determining the maximum data value
nbins : int
number of histogram bins for building the PDF and CDF
sliding_minmax : boolean
if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
if False - same data_min_glob and data_max_glob will be used for all files
data_minmax : list of 5 parameters
minmax_xlsx : str
path to Excel file with Min/Max data
data_min_glob : float
min data value for I8 conversion (open CV SIFT requires I8)
data_min_sliding : float array
min data values (one per file) for I8 conversion
data_max_sliding : float array
max data values (one per file) for I8 conversion
data_minmax_glob : 2D float array
min and max data values without sliding averaging
kp_max_num : int
Max number of key-points to be matched.
Key-points in every frame are indexed (in descending order) by the strength of the response.
Only kp_max_num is kept for further processing.
Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)</p>
<p>Returns:
fnms : array of str
filenames for binary files kontaining Key-Point and Descriptors for each frame</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_keypoints(self, DASK_client, **kwargs):
    &#39;&#39;&#39;
    Extract Key-Points and Descriptors

    Parameters:
    DASK_client : instance of the DASK client object

    kwargs
    ---------
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    EightBit : int
        0 - 16-bit data, 1: 8-bit data
    fnm_reg : str
        filename for the final registed dataset
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    sliding_minmax : boolean
        if True - data min and max will be taken from data_min_sliding and data_max_sliding arrays
        if False - same data_min_glob and data_max_glob will be used for all files
    data_minmax : list of 5 parameters
        minmax_xlsx : str
            path to Excel file with Min/Max data
        data_min_glob : float
            min data value for I8 conversion (open CV SIFT requires I8)
        data_min_sliding : float array
            min data values (one per file) for I8 conversion
        data_max_sliding : float array
            max data values (one per file) for I8 conversion
        data_minmax_glob : 2D float array
            min and max data values without sliding averaging
    kp_max_num : int
        Max number of key-points to be matched.
        Key-points in every frame are indexed (in descending order) by the strength of the response.
        Only kp_max_num is kept for further processing.
        Set this value to -1 if you want to keep ALL keypoints (may take forever to process!)

    Returns:
    fnms : array of str
        filenames for binary files kontaining Key-Point and Descriptors for each frame
    &#39;&#39;&#39;
    if len(self.fls) == 0:
        print(&#39;Data set not defined, perform initialization first&#39;)
        fnms = []
    else:
        if hasattr(self, &#34;use_DASK&#34;):
            use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
        else:
            use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
        if hasattr(self, &#34;DASK_client_retries&#34;):
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
        else:
            DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        data_dir = self.data_dir
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        threshold_min = kwargs.get(&#34;threshold_min&#34;, self.threshold_min)
        threshold_max = kwargs.get(&#34;threshold_max&#34;, self.threshold_max)
        nbins = kwargs.get(&#34;nbins&#34;, self.nbins)
        sliding_minmax = kwargs.get(&#34;sliding_minmax&#34;, self.sliding_minmax)
        data_minmax = kwargs.get(&#34;data_minmax&#34;, self.data_minmax)
        kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)

        SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, self.SIFT_nfeatures)
        SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, self.SIFT_nOctaveLayers)
        SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, self.SIFT_contrastThreshold)
        SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, self.SIFT_edgeThreshold)
        SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, self.SIFT_sigma)

        minmax_xlsx, data_min_glob, data_max_glob, data_min_sliding, data_max_sliding = data_minmax
        kpt_kwargs = {&#39;ftype&#39; : ftype,
                    &#39;threshold_min&#39; : threshold_min,
                    &#39;threshold_max&#39; : threshold_max,
                    &#39;nbins&#39; : nbins,
                    &#39;kp_max_num&#39; : kp_max_num,
                    &#39;SIFT_nfeatures&#39; : SIFT_nfeatures,
                    &#39;SIFT_nOctaveLayers&#39; : SIFT_nOctaveLayers,
                    &#39;SIFT_contrastThreshold&#39; : SIFT_contrastThreshold,
                    &#39;SIFT_edgeThreshold&#39; : SIFT_edgeThreshold,
                    &#39;SIFT_sigma&#39; : SIFT_sigma}

        if sliding_minmax:
            params_s3 = [[dts3[0], dts3[1], dts3[2], kpt_kwargs] for dts3 in zip(self.fls, data_min_sliding, data_max_sliding)]
        else:
            params_s3 = [[fl, data_min_glob, data_max_glob, kpt_kwargs] for fl in self.fls]
        if use_DASK:
            print(&#39;Using DASK distributed&#39;)
            futures_s3 = DASK_client.map(extract_keypoints_descr_files, params_s3, retries = DASK_client_retries)
            fnms = DASK_client.gather(futures_s3)
        else:
            print(&#39;Using Local Computation&#39;)
            fnms = []
            for j, param_s3 in enumerate(tqdm(params_s3, desc=&#39;Extracting Key Points and Descriptors: &#39;)):
                fnms.append(extract_keypoints_descr_files(param_s3))

        self.fnms = fnms
    return fnms</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.process_transformation_matrix"><code class="name flex">
<span>def <span class="ident">process_transformation_matrix</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate cumulative transformation matrix</p>
<h2 id="kwargs">Kwargs</h2>
<p>data_dir : str
data directory (path)
fnm_reg : str
filename for the final registed dataset
Sample_ID : str
Sample ID
TransformType : object reference
Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
Choose from the following options:
ShiftTransform - only x-shift and y-shift
XScaleShiftTransform
-
x-scale, x-shift, y-shift
ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
AffineTransform -
full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
l2_matrix : 2D float array
matrix of regularization (shrinkage) parameters
targ_vector = 1D float array
target vector for regularization
solver : str
Solver used for SIFT ('RANSAC' or 'LinReg')
drmax : float
In the case of 'RANSAC' - Maximum distance for a data point to be classified as an inlier.
In the case of 'LinReg' - outlier threshold for iterative regression
max_iter : int
Max number of iterations in the iterative procedure above (RANSAC or LinReg)
BFMatcher : boolean
If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
save_matches : boolean
If True, matches will be saved into individual files
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check
preserve_scales : boolean
If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
fit_params : list
Example: ['SG', 501, 3]
- perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
Other options are:
['LF'] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
['PF', 2]
- use polynomial fit (in this case of order 2)
subtract_linear_fit : [boolean, boolean]
List of two Boolean values for two directions: X- and Y-.
If True, the linear slopes along X- and Y- directions (respectively)
will be subtracted from the cumulative shifts.
This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
pad_edges : boolean
If True, the data will be padded before transformation to avoid clipping.</p>
<p>Returns:
tr_matr_cum_residual, tr_matr_cum_xlsx_file : list of 2D arrays of float and the filename of the XLSX file with the transf matrix results
Cumulative transformation matrices</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_transformation_matrix(self, **kwargs):
    &#39;&#39;&#39;
    Calculate cumulative transformation matrix

    kwargs
    ---------
    data_dir : str
        data directory (path)
    fnm_reg : str
        filename for the final registed dataset
    Sample_ID : str
        Sample ID
    TransformType : object reference
            Transformation model used by SIFT for determining the transformation matrix from Key-Point pairs.
            Choose from the following options:
                ShiftTransform - only x-shift and y-shift
                XScaleShiftTransform  -  x-scale, x-shift, y-shift
                ScaleShiftTransform - x-scale, y-scale, x-shift, y-shift
                AffineTransform -  full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift)
                RegularizedAffineTransform - full Affine (x-scale, y-scale, rotation, shear, x-shift, y-shift) with regularization on deviation from ShiftTransform
    l2_matrix : 2D float array
        matrix of regularization (shrinkage) parameters
    targ_vector = 1D float array
        target vector for regularization
    solver : str
        Solver used for SIFT (&#39;RANSAC&#39; or &#39;LinReg&#39;)
    drmax : float
        In the case of &#39;RANSAC&#39; - Maximum distance for a data point to be classified as an inlier.
        In the case of &#39;LinReg&#39; - outlier threshold for iterative regression
    max_iter : int
        Max number of iterations in the iterative procedure above (RANSAC or LinReg)
    BFMatcher : boolean
        If True, the BF Matcher is used for keypont matching, otherwise FLANN will be used
    save_matches : boolean
        If True, matches will be saved into individual files
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    preserve_scales : boolean
        If True, the cumulative transformation matrix will be adjusted using the settings defined by fit_params below.
    fit_params : list
        Example: [&#39;SG&#39;, 501, 3]  - perform the above adjustment using Savitzky-Golay (SG) filter with parameters - window size 501, polynomial order 3.
        Other options are:
            [&#39;LF&#39;] - use linear fit with forces start points Sxx and Syy = 1 and Sxy and Syx = 0
            [&#39;PF&#39;, 2]  - use polynomial fit (in this case of order 2)
    subtract_linear_fit : [boolean, boolean]
        List of two Boolean values for two directions: X- and Y-.
        If True, the linear slopes along X- and Y- directions (respectively)
        will be subtracted from the cumulative shifts.
        This is performed after the optimal frame-to-frame shifts are recalculated for preserve_scales = True.
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.

    Returns:
    tr_matr_cum_residual, tr_matr_cum_xlsx_file : list of 2D arrays of float and the filename of the XLSX file with the transf matrix results
        Cumulative transformation matrices
    &#39;&#39;&#39;
    if len(self.transformation_matrix) == 0:
        print(&#39;No data on individual key-point matches, peform key-point search / matching first&#39;)
        self.tr_matr_cum_residual = []
    else:
        data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
        fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
        TransformType = kwargs.get(&#34;TransformType&#34;, self.TransformType)
        SIFT_nfeatures = kwargs.get(&#34;SIFT_nfeatures&#34;, self.SIFT_nfeatures)
        SIFT_nOctaveLayers = kwargs.get(&#34;SIFT_nOctaveLayers&#34;, self.SIFT_nOctaveLayers)
        SIFT_contrastThreshold = kwargs.get(&#34;SIFT_contrastThreshold&#34;, self.SIFT_contrastThreshold)
        SIFT_edgeThreshold = kwargs.get(&#34;SIFT_edgeThreshold&#34;, self.SIFT_edgeThreshold)
        SIFT_sigma = kwargs.get(&#34;SIFT_sigma&#34;, self.SIFT_sigma)
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        l2_matrix = kwargs.get(&#34;l2_matrix&#34;, self.l2_matrix)
        targ_vector = kwargs.get(&#34;targ_vector&#34;, self.targ_vector)
        solver = kwargs.get(&#34;solver&#34;, self.solver)
        drmax = kwargs.get(&#34;drmax&#34;, self.drmax)
        max_iter = kwargs.get(&#34;max_iter&#34;, self.max_iter)
        BFMatcher = kwargs.get(&#34;BFMatcher&#34;, self.BFMatcher)
        save_matches = kwargs.get(&#34;save_matches&#34;, self.save_matches)
        kp_max_num = kwargs.get(&#34;kp_max_num&#34;, self.kp_max_num)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
        preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, self.preserve_scales)
        fit_params =  kwargs.get(&#34;fit_params&#34;, self.fit_params)
        subtract_linear_fit =  kwargs.get(&#34;subtract_linear_fit&#34;, self.subtract_linear_fit)
        subtract_FOVtrend_from_fit =  kwargs.get(&#34;subtract_FOVtrend_from_fit&#34;, self.subtract_FOVtrend_from_fit)
        pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)

        TM_kwargs = {&#39;fnm_reg&#39; : fnm_reg,
                        &#39;data_dir&#39; : data_dir,
                        &#39;TransformType&#39; : TransformType,
                        &#39;SIFT_nfeatures&#39; : SIFT_nfeatures,
                        &#39;SIFT_nOctaveLayers&#39; : SIFT_nOctaveLayers,
                        &#39;SIFT_contrastThreshold&#39; : SIFT_contrastThreshold,
                        &#39;SIFT_edgeThreshold&#39; : SIFT_edgeThreshold,
                        &#39;SIFT_sigma&#39; : SIFT_sigma,
                        &#39;Sample_ID&#39; : Sample_ID,
                        &#39;l2_matrix&#39; : l2_matrix,
                        &#39;targ_vector&#39;: targ_vector,
                        &#39;solver&#39; : solver,
                        &#39;drmax&#39; : drmax,
                        &#39;max_iter&#39; : max_iter,
                        &#39;BFMatcher&#39; : BFMatcher,
                        &#39;save_matches&#39; : save_matches,
                        &#39;kp_max_num&#39; : kp_max_num,
                        &#39;save_res_png &#39; : save_res_png ,
                        &#39;preserve_scales&#39; : preserve_scales,
                        &#39;fit_params&#39; : fit_params,
                        &#39;subtract_linear_fit&#39; : subtract_linear_fit,
                        &#39;subtract_FOVtrend_from_fit&#39; : subtract_FOVtrend_from_fit,
                        &#39;pad_edges&#39; : pad_edges}
        self.tr_matr_cum_residual, self.transf_matrix_xlsx_file = process_transf_matrix(self.transformation_matrix,
                                         self.FOVtrend_x,
                                         self.FOVtrend_y,
                                         self.fnms_matches,
                                         self.npts,
                                         self.error_abs_mean,
                                         **TM_kwargs)
    return self.tr_matr_cum_residual, self.transf_matrix_xlsx_file</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.save_parameters"><code class="name flex">
<span>def <span class="ident">save_parameters</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Save transformation attributes and parameters (including transformation matrices).</p>
<h2 id="kwargs">kwargs:</h2>
<p>dump_filename : string
String containing the name of the binary dump for saving all attributes of the current istance of the FIBSEM_dataset object.</p>
<p>Returns:
dump_filename : string</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_parameters(self, **kwargs):
    &#39;&#39;&#39;
    Save transformation attributes and parameters (including transformation matrices).

    kwargs:
    -------
    dump_filename : string
        String containing the name of the binary dump for saving all attributes of the current istance of the FIBSEM_dataset object.


    Returns:
    dump_filename : string
    &#39;&#39;&#39;
    default_dump_filename = os.path.join(self.data_dir, self.fnm_reg.replace(&#39;.mrc&#39;, &#39;_params.bin&#39;))
    dump_filename = kwargs.get(&#34;dump_filename&#34;, default_dump_filename)

    pickle.dump(self.__dict__, open(dump_filename, &#39;wb&#39;))

    npts_fnm = dump_filename.replace(&#39;_params.bin&#39;, &#39;_Npts_Errs_data.csv&#39;)
    Tr_matrix_xls_fnm = dump_filename.replace(&#39;_params.bin&#39;, &#39;_Transform_Matrix_data.csv&#39;)

    # Save the keypoint statistics into a CSV file
    columns=[&#39;Npts&#39;, &#39;Mean Abs Error&#39;]
    npdt = pd.DataFrame(np.vstack((self.npts, self.error_abs_mean)).T, columns = columns, index = None)
    npdt.to_csv(npts_fnm, index = None)

    # Save the X-Y shift data and keypoint statistics into a CSV file
    columns=[&#39;T00 (Sxx)&#39;, &#39;T01 (Sxy)&#39;, &#39;T02 (Tx)&#39;,
             &#39;T10 (Syx)&#39;, &#39;T11 (Syy)&#39;, &#39;T12 (Ty)&#39;,
             &#39;T20 (0.0)&#39;, &#39;T21 (0.0)&#39;, &#39;T22 (1.0)&#39;]
    tr_mx_dt = pd.DataFrame(self.transformation_matrix.reshape((len(self.transformation_matrix), 9)), columns = columns, index = None)
    tr_mx_dt.to_csv(Tr_matrix_xls_fnm, index = None)
    return dump_filename</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.show_eval_box"><code class="name flex">
<span>def <span class="ident">show_eval_box</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Show the box used for evaluating the registration quality</p>
<h2 id="kwargs">Kwargs</h2>
<p>evaluation_box : list of 4 int
evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
sliding_evaluation_box : boolean
if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
start_evaluation_box : list of 4 int
see above
stop_evaluation_box : list of 4 int
see above
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
data_dir : str
data directory (path)
fnm_reg : str
filename for the final registed dataset
Sample_ID : str
Sample ID
int_order : int
The order of interpolation (when transforming the data).
The order has to be in the range 0-5:
0: Nearest-neighbor
1: Bi-linear (default)
2: Bi-quadratic
3: Bi-cubic
4: Bi-quartic
5: Bi-quintic
perfrom_transformation : boolean
If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed
invert_data : boolean
If True - the data is inverted
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check
pad_edges : boolean
If True, the data will be padded before transformation to avoid clipping.
flipY : boolean
If True, the data will be flipped along Y-axis. Default is False.
frame_inds : array or list of int
Array or list oif frame indecis to use to display the evaluation box.
Default are [nfrs//10, nfrs//2, nfrs//10*9]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_eval_box(self, **kwargs):
    &#39;&#39;&#39;
    Show the box used for evaluating the registration quality

    kwargs
    ---------
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    data_dir : str
        data directory (path)
    fnm_reg : str
        filename for the final registed dataset
    Sample_ID : str
        Sample ID
    int_order : int
        The order of interpolation (when transforming the data).
            The order has to be in the range 0-5:
                0: Nearest-neighbor
                1: Bi-linear (default)
                2: Bi-quadratic
                3: Bi-cubic
                4: Bi-quartic
                5: Bi-quintic
    perfrom_transformation : boolean
        If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed
    invert_data : boolean
        If True - the data is inverted
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    pad_edges : boolean
        If True, the data will be padded before transformation to avoid clipping.
    flipY : boolean
        If True, the data will be flipped along Y-axis. Default is False.
    frame_inds : array or list of int
        Array or list oif frame indecis to use to display the evaluation box.
        Default are [nfrs//10, nfrs//2, nfrs//10*9]
    &#39;&#39;&#39;
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
    data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
    int_order = kwargs.get(&#34;int_order&#34;, self.int_order)
    perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True) and hasattr(self, &#39;tr_matr_cum_residual&#39;)
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    flipY = kwargs.get(&#34;flipY&#34;, False)
    pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
    fls = self.fls
    nfrs = len(fls)
    default_indecis = [nfrs//10, nfrs//2, nfrs//10*9]
    frame_inds = kwargs.get(&#34;frame_inds&#34;, default_indecis)

    for j in frame_inds:
        frame = FIBSEM_frame(fls[j], ftype=ftype)
        if pad_edges and perfrom_transformation:
            shape = [frame.YResolution, frame.XResolution]
            xmn, xmx, ymn, ymx = determine_pad_offsets(shape, self.tr_matr_cum_residual)
            padx = np.int16(xmx - xmn)
            pady = np.int16(ymx - ymn)
            xi = np.int16(np.max([xmx, 0]))
            yi = np.int16(np.max([ymx, 0]))
            # The initial transformation matrices are calculated with no padding.Padding is done prior to transformation
            # so that the transformed images are not clipped.
            # Such padding means shift (by xi and yi values). Therefore the new transformation matrix
            # for padded frames will be (Shift Matrix)x(Transformation Matrix)x(Inverse Shift Matrix)
            # those are calculated below base on the amount of padding calculated above
            shift_matrix = np.array([[1.0, 0.0, xi],
                                     [0.0, 1.0, yi],
                                     [0.0, 0.0, 1.0]])
            inv_shift_matrix = np.linalg.inv(shift_matrix)
        else:
            padx = 0
            pady = 0
            xi = 0
            yi = 0
            shift_matrix = np.eye(3,3)
            inv_shift_matrix = np.eye(3,3)

        xsz = frame.XResolution + padx
        xa = xi + frame.XResolution
        ysz = frame.YResolution + pady
        ya = yi + frame.YResolution

        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        if sliding_evaluation_box:
            dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
            dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
        else:
            dx_eval = 0
            dy_eval = 0

        frame_img = np.zeros((ysz, xsz))

        if invert_data:
            frame_img[yi:ya, xi:xa] = np.negative(frame.RawImageA.astype(float))
            &#39;&#39;&#39;
            if frame.EightBit==0:
                frame_img[yi:ya, xi:xa] = np.negative(frame.RawImageA)
            else:
                frame_img[yi:ya, xi:xa]  =  uint8(255) - frame.RawImageA
            &#39;&#39;&#39;
        else:
            frame_img[yi:ya, xi:xa]  = frame.RawImageA.astype(float)

        if perfrom_transformation:
            transf = ProjectiveTransform(matrix = shift_matrix @ (self.tr_matr_cum_residual[j] @ inv_shift_matrix))
            frame_img_reg = warp(frame_img, transf, order = int_order, preserve_range=True)
        else:
            frame_img_reg = frame_img.copy()

        if flipY:
            frame_img_reg = np.flip(frame_img_reg, axis=0)

        if sliding_evaluation_box:
            xi_eval = start_evaluation_box[2] + dx_eval*j//nfrs
            yi_eval = start_evaluation_box[0] + dy_eval*j//nfrs
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = xsz
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ysz

        vmin, vmax = get_min_max_thresholds(frame_img_reg[yi_eval:ya_eval, xi_eval:xa_eval], disp_res=False)
        fig, ax = subplots(1,1, figsize=(10.0, 11.0*ysz/xsz))
        ax.imshow(frame_img_reg, cmap=&#39;Greys&#39;, vmin=vmin, vmax=vmax)
        ax.grid(True, color = &#34;cyan&#34;)
        ax.set_title(fls[j])
        rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=2.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        if save_res_png :
            fig.savefig(os.path.splitext(fls[j])[0]+&#39;_evaluation_box.png&#39;, dpi=300)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.transform_and_save"><code class="name flex">
<span>def <span class="ident">transform_and_save</span></span>(<span>self, DASK_client, save_transformed_dataset=True, save_registration_summary=True, frame_inds=array(-1), **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform the frames using the cumulative transformation matrix and save the data set into .mrc and/or .h5 file</p>
<p>Parameters
DASK_client : instance of the DASK client object
save_transformed_dataset : boolean
If true, the transformed data set will be saved into MRC file
save_registration_summary : bolean
If True, the registration analysis data will be saved into XLSX file
frame_inds : int array (or list)
Array of frame indecis. If not set or set to np.array((-1)), all frames will be transformed</p>
<h2 id="kwargs">Kwargs</h2>
<p>use_DASK : boolean
use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
DASK_client_retries : int (default to 0)
Number of allowed automatic retries if a task fails
ftype : int
file type (0 - Shan Xu's .dat, 1 - tif)
data_dir : str
data directory (path)
fnm_types : list of strings
File type(s) for output data. Options are: ['h5', 'mrc'].
Defauls is 'mrc'. 'h5' is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
fnm_reg : str
filename for the final registed dataset
ImgB_fraction : float
fractional ratio of Image B to be used for constructing the fuksed image:
ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction
add_offset : boolean
If True - the Dark Oount offset will be added before saving to make values positive (set True if saving into BigDataViewer HDF5 - it uses UI16 data format)
save_res_png
: boolean
Save PNG images of the intermediate processing statistics and final registration quality check
perfrom_transformation : boolean
If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed.
invert_data : boolean
If True - the data is inverted.
flatten_image : bolean
perform image flattening
image_correction_file : str
full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
flipY : boolean
If True, the data will be flipped along Y-axis. Default is False.
zbin_factor : int
binning factor along Z-axis
eval_metrics : list of str
list of evaluation metrics to use. default is ['NSAD', 'NCC', 'NMI', 'FSC']
fnm_types : list of strings
File type(s) for output data. Options are: ['h5', 'mrc'].
Defauls is 'mrc'. 'h5' is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
evaluation_box : list of 4 int
evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
sliding_evaluation_box : boolean
if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
start_evaluation_box : list of 4 int
see above
stop_evaluation_box : list of 4 int
see above
save_sample_frames_png : bolean
If True, sample frames with superimposed eval box and registration analysis data will be saved into png files
dtp
: dtype
Python data type for saving. Deafult is int16, the other option currently is uint8.
disp_res : bolean
If True (default), intermediate messages and results will be displayed.</p>
<p>Returns:
reg_summary, reg_summary_xlsx
reg_summary : pandas DataFrame
reg_summary = pd.DataFrame(np.vstack((npts, error_abs_mean, image_nsad, image_ncc, image_mi)
reg_summary_xlsx : name of the XLSX workbook containing the data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_and_save(self, DASK_client, save_transformed_dataset=True, save_registration_summary=True, frame_inds=np.array((-1)), **kwargs):
    &#39;&#39;&#39;
    Transform the frames using the cumulative transformation matrix and save the data set into .mrc and/or .h5 file

    Parameters
    DASK_client : instance of the DASK client object
    save_transformed_dataset : boolean
        If true, the transformed data set will be saved into MRC file
    save_registration_summary : bolean
        If True, the registration analysis data will be saved into XLSX file
    frame_inds : int array (or list)
        Array of frame indecis. If not set or set to np.array((-1)), all frames will be transformed

    kwargs
    ---------
    use_DASK : boolean
        use python DASK package to parallelize the computation or not (False is used mostly for debug purposes).
    DASK_client_retries : int (default to 0)
        Number of allowed automatic retries if a task fails
    ftype : int
        file type (0 - Shan Xu&#39;s .dat, 1 - tif)
    data_dir : str
        data directory (path)
    fnm_types : list of strings
        File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
        Defauls is &#39;mrc&#39;. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
    fnm_reg : str
        filename for the final registed dataset
    ImgB_fraction : float
        fractional ratio of Image B to be used for constructing the fuksed image:
        ImageFused = ImageA * (1.0-ImgB_fraction) + ImageB * ImgB_fraction
    add_offset : boolean
        If True - the Dark Oount offset will be added before saving to make values positive (set True if saving into BigDataViewer HDF5 - it uses UI16 data format)
    save_res_png  : boolean
        Save PNG images of the intermediate processing statistics and final registration quality check
    perfrom_transformation : boolean
        If True - the data is transformed using existing cumulative transformation matrix. If False - the data is not transformed.
    invert_data : boolean
        If True - the data is inverted.
    flatten_image : bolean
        perform image flattening
    image_correction_file : str
        full path to a binary filename that contains source name (image_correction_source) and correction array (img_correction_array)
    flipY : boolean
        If True, the data will be flipped along Y-axis. Default is False.
    zbin_factor : int
        binning factor along Z-axis
    eval_metrics : list of str
        list of evaluation metrics to use. default is [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;]
    fnm_types : list of strings
        File type(s) for output data. Options are: [&#39;h5&#39;, &#39;mrc&#39;].
        Defauls is &#39;mrc&#39;. &#39;h5&#39; is BigDataViewer HDF5 format, uses npy2bdv package. Use empty list if do not want to save the data.
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    sliding_evaluation_box : boolean
        if True, then the evaluation box will be linearly interpolated between sliding_evaluation_box and stop_evaluation_box
    start_evaluation_box : list of 4 int
        see above
    stop_evaluation_box : list of 4 int
        see above
    save_sample_frames_png : bolean
        If True, sample frames with superimposed eval box and registration analysis data will be saved into png files
    dtp  : dtype
        Python data type for saving. Deafult is int16, the other option currently is uint8.
    disp_res : bolean
        If True (default), intermediate messages and results will be displayed.

    Returns:
    reg_summary, reg_summary_xlsx
        reg_summary : pandas DataFrame
        reg_summary = pd.DataFrame(np.vstack((npts, error_abs_mean, image_nsad, image_ncc, image_mi)
        reg_summary_xlsx : name of the XLSX workbook containing the data
    &#39;&#39;&#39;
    if (frame_inds == np.array((-1))).all():
        frame_inds = np.arange(len(self.fls))

    if hasattr(self, &#34;use_DASK&#34;):
        use_DASK = kwargs.get(&#34;use_DASK&#34;, self.use_DASK)
    else:
        use_DASK = kwargs.get(&#34;use_DASK&#34;, False)
    if hasattr(self, &#34;DASK_client_retries&#34;):
        DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, self.DASK_client_retries)
    else:
        DASK_client_retries = kwargs.get(&#34;DASK_client_retries&#34;, 0)
    ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
    data_dir = kwargs.get(&#34;data_dir&#34;, self.data_dir)
    if hasattr(self, &#39;XResolution&#39;):
        XResolution_default = self.XResolution
    else:
        XResolution_default = FIBSEM_frame(self.fls[len(self.fls)//2]).XResolution
    XResolution = kwargs.get(&#34;XResolution&#34;, XResolution_default)
    if hasattr(self, &#39;YResolution&#39;):
        YResolution_default = self.YResolution
    else:
        YResolution_default = FIBSEM_frame(self.fls[len(self.fls)//2]).YResolution
    YResolution = kwargs.get(&#34;YResolution&#34;, YResolution_default)

    fnm_reg = kwargs.get(&#34;fnm_reg&#34;, self.fnm_reg)
    if hasattr(self, &#39;fnm_types&#39;):
        fnm_types = kwargs.get(&#34;fnm_types&#34;, self.fnm_types)
    else:
        fnm_types = kwargs.get(&#34;fnm_types&#34;, [&#39;mrc&#39;])
    ImgB_fraction = kwargs.get(&#34;ImgB_fraction&#34;, self.ImgB_fraction)
    if self.DetB == &#39;None&#39;:
        ImgB_fraction = 0.0
    if hasattr(self, &#39;add_offset&#39;):
        add_offset = kwargs.get(&#34;add_offset&#34;, self.add_offset)
    else:
        add_offset = kwargs.get(&#34;add_offset&#34;, False)
    save_sample_frames_png = kwargs.get(&#34;save_sample_frames_png&#34;, True)
    pad_edges =  kwargs.get(&#34;pad_edges&#34;, self.pad_edges)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, self.save_res_png )
    if hasattr(self, &#39;eval_metrics&#39;):
        eval_metrics =  kwargs.get(&#34;eval_metrics&#34;, self.eval_metrics)
    else:
        eval_metrics = kwargs.get(&#39;eval_metrics&#39;, [&#39;NSAD&#39;, &#39;NCC&#39;, &#39;NMI&#39;, &#39;FSC&#39;])
    if hasattr(self, &#39;zbin_factor&#39;):
        zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, self.zbin_factor)         # binning factor in z-direction (milling direction). Data will be binned when saving the final result. Default is 1.
    else:
        zbin_factor =  kwargs.get(&#34;zbin_factor&#34;, 1)
    if hasattr(self, &#39;voxel_size&#39;):
        voxel_size = kwargs.get(&#34;voxel_size&#34;, self.voxel_size)
    else:
        voxel_size_default = np.rec.array((self.PixelSize,  self.PixelSize,  self.PixelSize), dtype=[(&#39;x&#39;, &#39;&lt;f4&#39;), (&#39;y&#39;, &#39;&lt;f4&#39;), (&#39;z&#39;, &#39;&lt;f4&#39;)])
        voxel_size = kwargs.get(&#34;voxel_size&#34;, voxel_size_default)
    voxel_size_zbinned = voxel_size.copy()
    voxel_size_zbinned.z = voxel_size.z * zbin_factor
    if hasattr(self, &#39;flipY&#39;):
        flipY = kwargs.get(&#34;flipY&#34;, self.flipY)
    else:
        flipY = kwargs.get(&#34;flipY&#34;, False)
    if hasattr(self, &#39;dump_filename&#39;):
        dump_filename = kwargs.get(&#34;dump_filename&#34;, self.dump_filename)
    else:
        dump_filename = kwargs.get(&#34;dump_filename&#34;, &#39;&#39;)
    int_order = kwargs.get(&#34;int_order&#34;, self.int_order)
    preserve_scales =  kwargs.get(&#34;preserve_scales&#34;, self.preserve_scales)
    if hasattr(self, &#39;flatten_image&#39;):
        flatten_image = kwargs.get(&#34;flatten_image&#34;, self.flatten_image)
    else:
        flatten_image = kwargs.get(&#34;flatten_image&#34;, False)
    if hasattr(self, &#39;image_correction_file&#39;):
        image_correction_file = kwargs.get(&#34;image_correction_file&#34;, self.image_correction_file)
    else:
        image_correction_file = kwargs.get(&#34;image_correction_file&#34;, &#39;&#39;)
    perfrom_transformation =  kwargs.get(&#34;perfrom_transformation&#34;, True)  and hasattr(self, &#39;tr_matr_cum_residual&#39;)
    if hasattr(self, &#39;invert_data&#39;):
        invert_data = kwargs.get(&#34;invert_data&#34;, self.invert_data)
    else:
        invert_data = kwargs.get(&#34;invert_data&#34;, False)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    sliding_evaluation_box = kwargs.get(&#34;sliding_evaluation_box&#34;, False)
    start_evaluation_box = kwargs.get(&#34;start_evaluation_box&#34;, [0, 0, 0, 0])
    stop_evaluation_box = kwargs.get(&#34;stop_evaluation_box&#34;, [0, 0, 0, 0])
    disp_res  = kwargs.get(&#34;disp_res&#34;, True )
    dtp = kwargs.get(&#34;dtp&#34;, int16)  # Python data type for saving. Deafult is int16, the other option currently is uint8.

    save_kwargs = {&#39;fnm_types&#39; : fnm_types,
                    &#39;fnm_reg&#39; : fnm_reg,
                    &#39;use_DASK&#39; : use_DASK,
                    &#39;DASK_client_retries&#39; : DASK_client_retries,
                    &#39;ftype&#39; : ftype,
                    &#39;XResolution&#39; : XResolution,
                    &#39;YResolution&#39; : YResolution,
                    &#39;data_dir&#39; : data_dir,
                    &#39;voxel_size&#39; : voxel_size_zbinned,
                    &#39;pad_edges&#39; : pad_edges,
                    &#39;ImgB_fraction&#39; : ImgB_fraction,
                    &#39;save_res_png &#39; : save_res_png ,
                    &#39;dump_filename&#39; : dump_filename,
                    &#39;dtp&#39; : dtp,
                    &#39;zbin_factor&#39; : zbin_factor,
                    &#39;eval_metrics&#39; : eval_metrics,
                    &#39;flipY&#39; : flipY,
                    &#39;int_order&#39; : int_order,
                    &#39;preserve_scales&#39; : preserve_scales,
                    &#39;flatten_image&#39; : flatten_image,
                    &#39;image_correction_file&#39; : image_correction_file,
                    &#39;perfrom_transformation&#39; : perfrom_transformation,
                    &#39;invert_data&#39; : invert_data,
                    &#39;evaluation_box&#39; : evaluation_box,
                    &#39;sliding_evaluation_box&#39; : sliding_evaluation_box,
                    &#39;start_evaluation_box&#39; : start_evaluation_box,
                    &#39;stop_evaluation_box&#39; : stop_evaluation_box,
                    &#39;save_sample_frames_png&#39; : save_sample_frames_png,
                    &#39;save_registration_summary&#39; : save_registration_summary,
                    &#39;disp_res&#39; : disp_res}

    # first, transform, bin and save frame chunks into individual tif files
    if disp_res:
        print(&#39;Transforming and Saving Intermediate Registered Frames&#39;)
    registered_filenames = transform_and_save_frames(DASK_client, frame_inds, self.fls, self.tr_matr_cum_residual, **save_kwargs)

    frame0 = tiff.imread(registered_filenames[0])
    ny, nx = np.shape(frame0)
    if disp_res:
        print(&#39;Analyzing Registration Quality&#39;)
    if pad_edges and perfrom_transformation:
        xmn, xmx, ymn, ymx = determine_pad_offsets([ny, nx], self.tr_matr_cum_residual)
        padx = int(xmx - xmn)
        pady = int(ymx - ymn)
        xi = int(np.max([xmx, 0]))
        yi = int(np.max([ymx, 0]))
    else:
        padx = 0
        pady = 0
        xi = 0
        yi = 0
    if sliding_evaluation_box:
        dx_eval = stop_evaluation_box[2]-start_evaluation_box[2]
        dy_eval = stop_evaluation_box[0]-start_evaluation_box[0]
    else:
        dx_eval = 0
        dy_eval = 0

    eval_bounds = []
    for j, registered_filename in enumerate(tqdm(registered_filenames, desc=&#39;Setting up evaluation bounds&#39;, display = disp_res)):
        if sliding_evaluation_box:
            xi_eval = xi + start_evaluation_box[2] + dx_eval*j//nz
            yi_eval = yi + start_evaluation_box[0] + dy_eval*j//nz
            if start_evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + start_evaluation_box[3]
            else:
                xa_eval = nx
            if start_evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + start_evaluation_box[1]
            else:
                ya_eval = ny
        else:
            xi_eval = xi + evaluation_box[2]
            if evaluation_box[3] &gt; 0:
                xa_eval = xi_eval + evaluation_box[3]
            else:
                xa_eval = nx
            yi_eval = yi + evaluation_box[0]
            if evaluation_box[1] &gt; 0:
                ya_eval = yi_eval + evaluation_box[1]
            else:
                ya_eval = ny
        eval_bounds.append([xi_eval, xa_eval, yi_eval, ya_eval])

    save_kwargs[&#39;eval_bounds&#39;] = eval_bounds

    reg_summary, reg_summary_xlsx = analyze_registration_frames(DASK_client, registered_filenames, **save_kwargs)

    if save_transformed_dataset:
        if add_offset:
            offset = self.Scaling[1, 0] * (1.0-ImgB_fraction) + self.Scaling[1, 1] * ImgB_fraction
        if disp_res:
            print(&#34;Creating Dask Array Stack&#34;)
        # now build dask array of the transformed dataset
        # read the first file to get the shape and dtype (ASSUMING THAT ALL FILES SHARE THE SAME SHAPE/TYPE)
        lazy_imread = dask.delayed(tiff.imread)  # lazy reader
        lazy_arrays = [lazy_imread(fn) for fn in registered_filenames]
        dask_arrays = [ da.from_delayed(delayed_reader, shape=frame0.shape, dtype=frame0.dtype)   for delayed_reader in lazy_arrays]
        # Stack infividual frames into one large dask.array
        if add_offset:
            FIBSEMstack = da.stack(dask_arrays, axis=0) - offset
        else:
            FIBSEMstack = da.stack(dask_arrays, axis=0)
        #nz, ny, nx = FIBSEMstack.shape
        fnms_saved = save_data_stack(FIBSEMstack, **save_kwargs)
    else:
        if disp_res:
            print(&#39;Registered data set is NOT saved into a file&#39;)

    # Remove Intermediate Registered Frame Files
    for registered_filename in tqdm(registered_filenames, desc=&#39;Removing Intermediate Registered Frame Files: &#39;, display = disp_res):
        try:
            os.remove(registered_filename)
        except:
            pass

    return reg_summary, reg_summary_xlsx</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame"><code class="flex name class">
<span>class <span class="ident">FIBSEM_frame</span></span>
<span>(</span><span>fname, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A class representing single FIB-SEM data frame.
©G.Shtengel 10/2021 gleb.shtengel@gmail.com
Contains the info/settings on a single FIB-SEM data frame and the procedures that can be performed on it.</p>
<h2 id="attributes-only-some-more-important-are-listed-here">Attributes (only some more important are listed here)</h2>
<p>fname : str
filename of the individual data frame
header : str
1024 bytes - header
FileVersion : int
file version number
ChanNum : int
Number of channels
EightBit : int
8-bit data switch: 0 for 16-bit data, 1 for 8-bit data
Scaling : 2D array of floats
scaling parameters allowing to convert I16 data into actual electron counts
Sample_ID : str
Sample_ID
Notes : str
Experiment notes
DetA : str
Detector A name
DetB : str
Detector B name ('None' if there is no Detector B)
XResolution : int
number of pixels - frame size in horizontal direction
YResolution : int
number of pixels - frame size in vertical direction
PixelSize : float
pixel size in nm. Default is 8.0</p>
<h2 id="methods">Methods</h2>
<p>print_header()
Prints a formatted content of the file header</p>
<p>display_images()
Display auto-scaled detector images without saving the figure into the file.</p>
<p>save_images_jpeg(**kwargs)
Display auto-scaled detector images and save the figure into JPEG file (s).</p>
<p>save_images_tif(images_to_save = 'Both')
Save the detector images into TIF file (s).</p>
<p>get_image_min_max(image_name = 'ImageA', thr_min = 1.0e-4, thr_max = 1.0e-3, nbins=256, disp_res = False)
Calculates the data range of the EM data.</p>
<p>RawImageA_8bit_thresholds(thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
Convert the Image A into 8-bit array</p>
<p>RawImageB_8bit_thresholds(thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
Convert the Image B into 8-bit array</p>
<p>save_snapshot(display = True, dpi=300, thr_min = 1.0e-3, thr_max = 1.0e-3, nbins=256, **kwargs):
Builds an image that contains both the Detector A and Detector B (if present) images as well as a table with important FIB-SEM parameters.</p>
<p>analyze_noise_ROIs(<strong>kwargs):
Analyses the noise statistics in the selected ROI's of the EM data. (Calls Single_Image_Noise_ROIs(img, Noise_ROIs, Hist_ROI, </strong>kwargs):)</p>
<p>analyze_noise_statistics(<strong>kwargs):
Analyses the noise statistics of the EM data image. (Calls Single_Image_Noise_Statistics(img, </strong>kwargs):)</p>
<p>analyze_SNR_autocorr(<strong>kwargs):
Estimates SNR using auto-correlation analysis of a single image. (Calls Single_Image_SNR(img, </strong>kwargs):)</p>
<p>show_eval_box(**kwargs):
Show the box used for evaluating the noise</p>
<p>determine_field_fattening_parameters(<strong>kwargs):
Perfrom 2D parabolic fit (calls Perform_2D_fit(Img, estimator, </strong>kwargs)) and determine the field-flattening parameters</p>
<p>flatten_image(**kwargs):
Flatten the image</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FIBSEM_frame:
    &#34;&#34;&#34;
    A class representing single FIB-SEM data frame.
    ©G.Shtengel 10/2021 gleb.shtengel@gmail.com
    Contains the info/settings on a single FIB-SEM data frame and the procedures that can be performed on it.

    Attributes (only some more important are listed here)
    ----------
    fname : str
        filename of the individual data frame
    header : str
        1024 bytes - header
    FileVersion : int
        file version number
    ChanNum : int
        Number of channels
    EightBit : int
        8-bit data switch: 0 for 16-bit data, 1 for 8-bit data
    Scaling : 2D array of floats
        scaling parameters allowing to convert I16 data into actual electron counts
    Sample_ID : str
        Sample_ID
    Notes : str
        Experiment notes
    DetA : str
        Detector A name
    DetB : str
        Detector B name (&#39;None&#39; if there is no Detector B)
    XResolution : int
        number of pixels - frame size in horizontal direction
    YResolution : int
        number of pixels - frame size in vertical direction
    PixelSize : float
        pixel size in nm. Default is 8.0

    Methods
    -------
    print_header()
        Prints a formatted content of the file header

    display_images()
        Display auto-scaled detector images without saving the figure into the file.

    save_images_jpeg(**kwargs)
        Display auto-scaled detector images and save the figure into JPEG file (s).

    save_images_tif(images_to_save = &#39;Both&#39;)
        Save the detector images into TIF file (s).

    get_image_min_max(image_name = &#39;ImageA&#39;, thr_min = 1.0e-4, thr_max = 1.0e-3, nbins=256, disp_res = False)
        Calculates the data range of the EM data.

    RawImageA_8bit_thresholds(thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
        Convert the Image A into 8-bit array

    RawImageB_8bit_thresholds(thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
            Convert the Image B into 8-bit array

    save_snapshot(display = True, dpi=300, thr_min = 1.0e-3, thr_max = 1.0e-3, nbins=256, **kwargs):
        Builds an image that contains both the Detector A and Detector B (if present) images as well as a table with important FIB-SEM parameters.

    analyze_noise_ROIs(**kwargs):
        Analyses the noise statistics in the selected ROI&#39;s of the EM data. (Calls Single_Image_Noise_ROIs(img, Noise_ROIs, Hist_ROI, **kwargs):)

    analyze_noise_statistics(**kwargs):
        Analyses the noise statistics of the EM data image. (Calls Single_Image_Noise_Statistics(img, **kwargs):)

    analyze_SNR_autocorr(**kwargs):
        Estimates SNR using auto-correlation analysis of a single image. (Calls Single_Image_SNR(img, **kwargs):)

    show_eval_box(**kwargs):
        Show the box used for evaluating the noise

    determine_field_fattening_parameters(**kwargs):
        Perfrom 2D parabolic fit (calls Perform_2D_fit(Img, estimator, **kwargs)) and determine the field-flattening parameters

    flatten_image(**kwargs):
        Flatten the image
    &#34;&#34;&#34;

    def __init__(self, fname, **kwargs):
        self.fname = fname
        self.ftype = kwargs.get(&#34;ftype&#34;, 0) # ftype=0 - Shan Xu&#39;s binary format  ftype=1 - tif files
        self.use_dask_arrays = kwargs.get(&#34;use_dask_arrays&#34;, False)
        if self.ftype == 1:
            self.RawImageA = tiff.imread(fname)

    # for tif files
        if self.ftype == 1:
            self.FileVersion = -1
            self.DetA = &#39;Detector A&#39;     # Name of detector A
            self.DetB = &#39;None&#39;     # Name of detector B
            try:
                with PILImage.open(self.fname) as img:
                    tif_header = {TAGS[key] : img.tag[key] for key in img.tag_v2}
                    self.header = tif_header
                try:
                    if tif_header[&#39;BitsPerSample&#39;][0]==8:
                        self.EightBit = 1
                    else:
                        self.EightBit = 0
                except:
                    self.EightBit = int(type(self.RawImageA[0,0])==np.uint8)
            except:
                self.header = &#39;&#39;
                self.EightBit = int(type(self.RawImageA[0,0])==np.uint8)

            self.PixelSize = kwargs.get(&#34;PixelSize&#34;, 8.0)
            self.Sample_ID = kwargs.get(&#34;Sample_ID&#34;, &#39;&#39;)
            self.YResolution, self.XResolution = self.RawImageA.shape
            self.Scaling = np.array([[1.0, 0.0, 1.0, 1.0], [1.0, 0.0, 1.0, 1.0]]).T

    # for Shan Xu&#39;s data files
        if self.ftype == 0:
            fid = open(self.fname, &#34;rb&#34;)
            fid.seek(0, 0)
            self.header = fid.read(1024) # Read in self.header
            self.FileMagicNum = unpack(&#39;&gt;L&#39;,self.header[0:4])[0]                       # Read in magic number, should be 3555587570
            self.FileVersion = unpack(&#39;&gt;h&#39;,self.header[4:6])[0]                        # Read in file version number
            self.FileType = unpack(&#39;&gt;h&#39;,self.header[6:8])[0]                           # Read in file type, 1 is Zeiss Neon detectors
            self.SWdate = (unpack(&#39;10s&#39;,self.header[8:18])[0]).decode(&#34;utf-8&#34;)         # Read in SW date
            self.TimeStep = unpack(&#39;&gt;d&#39;,self.header[24:32])[0]                         # Read in AI sampling time (including oversampling) in seconds
            self.ChanNum = unpack(&#39;b&#39;,self.header[32:33])[0]                           # Read in number of channels
            self.EightBit = unpack(&#39;b&#39;,self.header[33:34])[0]                          # Read in 8-bit data switch

            if self.FileVersion == 1:
                # Read in self.AI channel self.Scaling factors, (col#: self.AI#), (row#: offset, gain, 2nd order, 3rd order)
                self.ScalingS = unpack(&#39;&gt;&#39;+str(4*self.ChanNum)+&#39;d&#39;,self.header[36:(36+self.ChanNum*32)])
            elif self.FileVersion == 2 or self.FileVersion == 3 or self.FileVersion == 4 or self.FileVersion == 5 or self.FileVersion == 6:
                self.ScalingS = unpack(&#39;&gt;&#39;+str(4*self.ChanNum)+&#39;f&#39;,self.header[36:(36+self.ChanNum*16)])
            else:
                self.ScalingS = unpack(&#39;&gt;8f&#39;,self.header[36:68])
                self.Scaling = transpose(np.asarray(self.ScalingS).reshape(2,4))

            if self.FileVersion &gt; 8 :
                self.RestartFlag = unpack(&#39;b&#39;,self.header[68:69])[0]              # Read in restart flag
                self.StageMove = unpack(&#39;b&#39;,self.header[69:70])[0]                # Read in stage move flag
                self.FirstPixelX = unpack(&#39;&gt;l&#39;,self.header[70:74])[0]              # Read in first pixel X coordinate (center = 0)
                self.FirstPixelY = unpack(&#39;&gt;l&#39;,self.header[74:78])[0]              # Read in first pixel Y coordinate (center = 0)

            self.XResolution = unpack(&#39;&gt;L&#39;,self.header[100:104])[0]                # Read X resolution
            self.YResolution = unpack(&#39;&gt;L&#39;,self.header[104:108])[0]                # Read Y resolution

            if self.FileVersion == 1 or self.FileVersion == 2 or self.FileVersion == 3:
                self.Oversampling = unpack(&#39;&gt;B&#39;,self.header[108:109])[0]               # self.AI oversampling
                self.AIDelay = unpack(&#39;&gt;h&#39;,self.header[109:111])[0]                    # self.AI delay (# of samples)
            else:
                self.Oversampling = unpack(&#39;&gt;H&#39;,self.header[108:110])[0]

            self.ZeissScanSpeed = unpack(&#39;b&#39;,self.header[111:112])[0] # Scan speed (Zeiss #)

            if self.FileVersion == 1 or self.FileVersion == 2 or self.FileVersion == 3:
                self.ScanRate = unpack(&#39;&gt;d&#39;,self.header[112:120])[0]                   # Actual AO (scanning) rate
                self.FramelineRampdownRatio = unpack(&#39;&gt;d&#39;,self.header[120:128])[0]     # Frameline rampdown ratio
                self.Xmin = unpack(&#39;&gt;d&#39;,self.header[128:136])[0]                       # X coil minimum voltage
                self.Xmax = unpack(&#39;&gt;d&#39;,self.header[136:144])[0]                       # X coil maximum voltage
                self.Detmin = -10                                                      # Detector minimum voltage
                self.Detmax = 10                                                       # Detector maximum voltage
            else:
                self.ScanRate = unpack(&#39;&gt;f&#39;,self.header[112:116])[0]                   # Actual AO (scanning) rate
                self.FramelineRampdownRatio = unpack(&#39;&gt;f&#39;,self.header[116:120])[0]     # Frameline rampdown ratio
                self.Xmin = unpack(&#39;&gt;f&#39;,self.header[120:124])[0]                       # X coil minimum voltage
                self.Xmax = unpack(&#39;&gt;f&#39;,self.header[124:128])[0]                       # X coil maximum voltage
                self.Detmin = unpack(&#39;&gt;f&#39;,self.header[128:132])[0]                     # Detector minimum voltage
                self.Detmax = unpack(&#39;&gt;f&#39;,self.header[132:136])[0]                     # Detector maximum voltage
                self.DecimatingFactor = unpack(&#39;&gt;H&#39;,self.header[136:138])[0]           # Decimating factor

            self.AI1 = unpack(&#39;b&#39;,self.header[151:152])[0]                              # self.AI Ch1
            self.AI2 = unpack(&#39;b&#39;,self.header[152:153])[0]                              # self.AI Ch2
            self.AI3 = unpack(&#39;b&#39;,self.header[153:154])[0]                              # self.AI Ch3
            self.AI4 = unpack(&#39;b&#39;,self.header[154:155])[0]                              # self.AI Ch4

            self.Notes = (unpack(&#39;200s&#39;,self.header[180:380])[0]).decode(&#34;utf-8&#34;)       # Read in notes

            if self.FileVersion &gt; 8 :
                self.Sample_ID = (unpack(&#39;25s&#39;,self.header[155:180])[0]).decode(&#34;utf-8&#34;).strip(&#39;\x00&#39;) # Read in Sample ID
            else:
                self.Sample_ID = self.Notes.split(&#39;,&#39;)[0].strip(&#39;\x00&#39;)

            if self.FileVersion == 1 or self.FileVersion == 2:
                self.DetA = (unpack(&#39;10s&#39;,self.header[380:390])[0]).decode(&#34;utf-8&#34;)     # Name of detector A
                self.DetB = (unpack(&#39;18s&#39;,self.header[390:408])[0]).decode(&#34;utf-8&#34;)     # Name of detector B
                self.DetC = (unpack(&#39;20s&#39;,self.header[700:720])[0]).decode(&#34;utf-8&#34;)     # Name of detector C
                self.DetD = (unpack(&#39;20s&#39;,self.header[720:740])[0]).decode(&#34;utf-8&#34;)     # Name of detector D
                self.Mag = unpack(&#39;&gt;d&#39;,self.header[408:416])[0]                         # Magnification
                self.PixelSize = unpack(&#39;&gt;d&#39;,self.header[416:424])[0]                   # Pixel size in nm
                self.WD = unpack(&#39;&gt;d&#39;,self.header[424:432])[0]                          # Working distance in mm
                self.EHT = unpack(&#39;&gt;d&#39;,self.header[432:440])[0]                         # EHT in kV
                self.SEMApr = unpack(&#39;b&#39;,self.header[440:441])[0]                       # SEM aperture number
                self.HighCurrent = unpack(&#39;b&#39;,self.header[441:442])[0]                  # high current mode (1=on, 0=off)
                self.SEMCurr = unpack(&#39;&gt;d&#39;,self.header[448:456])[0]                     # SEM probe current in A
                self.SEMRot = unpack(&#39;&gt;d&#39;,self.header[456:464])[0]                      # SEM scan roation in degree
                self.ChamVac = unpack(&#39;&gt;d&#39;,self.header[464:472])[0]                     # Chamber vacuum
                self.GunVac = unpack(&#39;&gt;d&#39;,self.header[472:480])[0]                      # E-gun vacuum
                self.SEMStiX = unpack(&#39;&gt;d&#39;,self.header[480:488])[0]                     # SEM stigmation X
                self.SEMStiY = unpack(&#39;&gt;d&#39;,self.header[488:496])[0]                     # SEM stigmation Y
                self.SEMAlnX = unpack(&#39;&gt;d&#39;,self.header[496:504])[0]                     # SEM aperture alignment X
                self.SEMAlnY = unpack(&#39;&gt;d&#39;,self.header[504:512])[0]                     # SEM aperture alignment Y
                self.StageX = unpack(&#39;&gt;d&#39;,self.header[512:520])[0]                      # Stage position X in mm
                self.StageY = unpack(&#39;&gt;d&#39;,self.header[520:528])[0]                      # Stage position Y in mm
                self.StageZ = unpack(&#39;&gt;d&#39;,self.header[528:536])[0]                      # Stage position Z in mm
                self.StageT = unpack(&#39;&gt;d&#39;,self.header[536:544])[0]                      # Stage position T in degree
                self.StageR = unpack(&#39;&gt;d&#39;,self.header[544:552])[0]                      # Stage position R in degree
                self.StageM = unpack(&#39;&gt;d&#39;,self.header[552:560])[0]                      # Stage position M in mm
                self.BrightnessA = unpack(&#39;&gt;d&#39;,self.header[560:568])[0]                 # Detector A brightness (%)
                self.ContrastA = unpack(&#39;&gt;d&#39;,self.header[568:576])[0]                   # Detector A contrast (%)
                self.BrightnessB = unpack(&#39;&gt;d&#39;,self.header[576:584])[0]                 # Detector B brightness (%)
                self.ContrastB = unpack(&#39;&gt;d&#39;,self.header[584:592])[0]                   # Detector B contrast (%)
                self.Mode = unpack(&#39;b&#39;,self.header[600:601])[0]                         # FIB mode: 0=SEM, 1=FIB, 2=Milling, 3=SEM+FIB, 4=Mill+SEM, 5=SEM Drift Correction, 6=FIB Drift Correction, 7=No Beam, 8=External, 9=External+SEM
                self.FIBFocus = unpack(&#39;&gt;d&#39;,self.header[608:616])[0]                    # FIB focus in kV
                self.FIBProb = unpack(&#39;b&#39;,self.header[616:617])[0]                      # FIB probe number
                self.FIBCurr = unpack(&#39;&gt;d&#39;,self.header[624:632])[0]                     # FIB emission current
                self.FIBRot = unpack(&#39;&gt;d&#39;,self.header[632:640])[0]                      # FIB scan rotation
                self.FIBAlnX = unpack(&#39;&gt;d&#39;,self.header[640:648])[0]                     # FIB aperture alignment X
                self.FIBAlnY = unpack(&#39;&gt;d&#39;,self.header[648:656])[0]                     # FIB aperture alignment Y
                self.FIBStiX = unpack(&#39;&gt;d&#39;,self.header[656:664])[0]                     # FIB stigmation X
                self.FIBStiY = unpack(&#39;&gt;d&#39;,self.header[664:672])[0]                     # FIB stigmation Y
                self.FIBShiftX = unpack(&#39;&gt;d&#39;,self.header[672:680])[0]                   # FIB beam shift X in micron
                self.FIBShiftY = unpack(&#39;&gt;d&#39;,self.header[680:688])[0]                   # FIB beam shift Y in micron
            else:
                self.DetA = (unpack(&#39;10s&#39;,self.header[380:390])[0]).decode(&#34;utf-8&#34;)     # Name of detector A
                self.DetB = (unpack(&#39;18s&#39;,self.header[390:408])[0]).decode(&#34;utf-8&#34;)     # Name of detector B
                self.DetC = (unpack(&#39;20s&#39;,self.header[410:430])[0]).decode(&#34;utf-8&#34;)     # Name of detector C
                self.DetD = (unpack(&#39;20s&#39;,self.header[430:450])[0]).decode(&#34;utf-8&#34;)     # Name of detector D
                self.Mag = unpack(&#39;&gt;f&#39;,self.header[460:464])[0]                         # Magnification
                self.PixelSize = unpack(&#39;&gt;f&#39;,self.header[464:468])[0]                   # Pixel size in nm
                self.WD = unpack(&#39;&gt;f&#39;,self.header[468:472])[0]                          # Working distance in mm
                self.EHT = unpack(&#39;&gt;f&#39;,self.header[472:476])[0]                         # EHT in kV
                self.SEMApr = unpack(&#39;b&#39;,self.header[480:481])[0]                       # SEM aperture number
                self.HighCurrent = unpack(&#39;b&#39;,self.header[481:482])[0]                  # high current mode (1=on, 0=off)
                self.SEMCurr = unpack(&#39;&gt;f&#39;,self.header[490:494])[0]                     # SEM probe current in A
                self.SEMRot = unpack(&#39;&gt;f&#39;,self.header[494:498])[0]                      # SEM scan roation in degree
                self.ChamVac = unpack(&#39;&gt;f&#39;,self.header[498:502])[0]                     # Chamber vacuum
                self.GunVac = unpack(&#39;&gt;f&#39;,self.header[502:506])[0]                      # E-gun vacuum
                self.SEMShiftX = unpack(&#39;&gt;f&#39;,self.header[510:514])[0]                   # SEM beam shift X
                self.SEMShiftY = unpack(&#39;&gt;f&#39;,self.header[514:518])[0]                   # SEM beam shift Y
                self.SEMStiX = unpack(&#39;&gt;f&#39;,self.header[518:522])[0]                     # SEM stigmation X
                self.SEMStiY = unpack(&#39;&gt;f&#39;,self.header[522:526])[0]                     # SEM stigmation Y
                self.SEMAlnX = unpack(&#39;&gt;f&#39;,self.header[526:530])[0]                     # SEM aperture alignment X
                self.SEMAlnY = unpack(&#39;&gt;f&#39;,self.header[530:534])[0]                     # SEM aperture alignment Y
                self.StageX = unpack(&#39;&gt;f&#39;,self.header[534:538])[0]                      # Stage position X in mm
                self.StageY = unpack(&#39;&gt;f&#39;,self.header[538:542])[0]                      # Stage position Y in mm
                self.StageZ = unpack(&#39;&gt;f&#39;,self.header[542:546])[0]                      # Stage position Z in mm
                self.StageT = unpack(&#39;&gt;f&#39;,self.header[546:550])[0]                      # Stage position T in degree
                self.StageR = unpack(&#39;&gt;f&#39;,self.header[550:554])[0]                      # Stage position R in degree
                self.StageM = unpack(&#39;&gt;f&#39;,self.header[554:558])[0]                      # Stage position M in mm
                self.BrightnessA = unpack(&#39;&gt;f&#39;,self.header[560:564])[0]                 # Detector A brightness (%)
                self.ContrastA = unpack(&#39;&gt;f&#39;,self.header[564:568])[0]                   # Detector A contrast (%)
                self.BrightnessB = unpack(&#39;&gt;f&#39;,self.header[568:572])[0]                 # Detector B brightness (%)
                self.ContrastB = unpack(&#39;&gt;f&#39;,self.header[572:576])[0]                   # Detector B contrast (%)
                self.Mode = unpack(&#39;b&#39;,self.header[600:601])[0]                         # FIB mode: 0=SEM, 1=FIB, 2=Milling, 3=SEM+FIB, 4=Mill+SEM, 5=SEM Drift Correction, 6=FIB Drift Correction, 7=No Beam, 8=External, 9=External+SEM
                self.FIBFocus = unpack(&#39;&gt;f&#39;,self.header[604:608])[0]                    # FIB focus in kV
                self.FIBProb = unpack(&#39;b&#39;,self.header[608:609])[0]                      # FIB probe number
                self.FIBCurr = unpack(&#39;&gt;f&#39;,self.header[620:624])[0]                     # FIB emission current
                self.FIBRot = unpack(&#39;&gt;f&#39;,self.header[624:628])[0]                      # FIB scan rotation
                self.FIBAlnX = unpack(&#39;&gt;f&#39;,self.header[628:632])[0]                     # FIB aperture alignment X
                self.FIBAlnY = unpack(&#39;&gt;f&#39;,self.header[632:636])[0]                     # FIB aperture alignment Y
                self.FIBStiX = unpack(&#39;&gt;f&#39;,self.header[636:640])[0]                     # FIB stigmation X
                self.FIBStiY = unpack(&#39;&gt;f&#39;,self.header[640:644])[0]                     # FIB stigmation Y
                self.FIBShiftX = unpack(&#39;&gt;f&#39;,self.header[644:648])[0]                   # FIB beam shift X in micron
                self.FIBShiftY = unpack(&#39;&gt;f&#39;,self.header[648:652])[0]                   # FIB beam shift Y in micron

            if self.FileVersion &gt; 4:
                self.MillingXResolution = unpack(&#39;&gt;L&#39;,self.header[652:656])[0]                       # FIB milling X resolution
                self.MillingYResolution = unpack(&#39;&gt;L&#39;,self.header[656:660])[0]                       # FIB milling Y resolution
                self.MillingXSize = unpack(&#39;&gt;f&#39;,self.header[660:664])[0]                             # FIB milling X size (um)
                self.MillingYSize = unpack(&#39;&gt;f&#39;,self.header[664:668])[0]                             # FIB milling Y size (um)
                self.MillingULAng = unpack(&#39;&gt;f&#39;,self.header[668:672])[0]                             # FIB milling upper left inner angle (deg)
                self.MillingURAng = unpack(&#39;&gt;f&#39;,self.header[672:676])[0]                             # FIB milling upper right inner angle (deg)
                self.MillingLineTime = unpack(&#39;&gt;f&#39;,self.header[676:680])[0]                          # FIB line milling time (s)
                self.FIBFOV = unpack(&#39;&gt;f&#39;,self.header[680:684])[0]                                   # FIB FOV (um)
                self.MillingLinesPerImage = unpack(&#39;&gt;H&#39;,self.header[684:686])[0]                     # FIB milling lines per image
                self.MillingPIDOn = unpack(&#39;&gt;b&#39;,self.header[686:687])[0]                             # FIB milling PID on
                self.MillingPIDMeasured = unpack(&#39;&gt;b&#39;,self.header[689:690])[0]                       # FIB milling PID measured (0:specimen, 1:beamdump)
                self.MillingPIDTarget = unpack(&#39;&gt;f&#39;,self.header[690:694])[0]                         # FIB milling PID target
                self.MillingPIDTargetSlope = unpack(&#39;&gt;f&#39;,self.header[694:698])[0]                    # FIB milling PID target slope
                self.MillingPIDP = unpack(&#39;&gt;f&#39;,self.header[698:702])[0]                              # FIB milling PID P
                self.MillingPIDI = unpack(&#39;&gt;f&#39;,self.header[702:706])[0]                              # FIB milling PID I
                self.MillingPIDD = unpack(&#39;&gt;f&#39;,self.header[706:710])[0]                              # FIB milling PID D
                self.MachineID = (unpack(&#39;30s&#39;,self.header[800:830])[0]).decode(&#34;utf-8&#34;)             # Machine ID
                self.SEMSpecimenI = unpack(&#39;&gt;f&#39;,self.header[672:676])[0]                             # SEM specimen current (nA)

            if self.FileVersion &gt; 5 :
                self.Temperature = unpack(&#39;&gt;f&#39;,self.header[850:854])[0]                              # Temperature (F)
                self.FaradayCupI = unpack(&#39;&gt;f&#39;,self.header[854:858])[0]                              # Faraday cup current (nA)
                self.FIBSpecimenI = unpack(&#39;&gt;f&#39;,self.header[858:862])[0]                             # FIB specimen current (nA)
                self.BeamDump1I = unpack(&#39;&gt;f&#39;,self.header[862:866])[0]                               # Beam dump 1 current (nA)
                self.SEMSpecimenI = unpack(&#39;&gt;f&#39;,self.header[866:870])[0]                             # SEM specimen current (nA)
                self.MillingYVoltage = unpack(&#39;&gt;f&#39;,self.header[870:874])[0]                          # Milling Y voltage (V)
                self.FocusIndex = unpack(&#39;&gt;f&#39;,self.header[874:878])[0]                               # Focus index
                self.FIBSliceNum = unpack(&#39;&gt;L&#39;,self.header[878:882])[0]                              # FIB slice #

            if self.FileVersion &gt; 7:
                self.BeamDump2I = unpack(&#39;&gt;f&#39;,self.header[882:886])[0]                              # Beam dump 2 current (nA)
                self.MillingI = unpack(&#39;&gt;f&#39;,self.header[886:890])[0]                                # Milling current (nA)

            self.FileLength = unpack(&#39;&gt;q&#39;,self.header[1000:1008])[0]                                # Read in file length in bytes

#                Finish self.header read
#
#                Read raw data
#                fid.seek(1024, 0)
#                n_elements = self.ChanNum * self.XResolution * self.YResolution
#                print(n_elements, self.ChanNum, self.XResolution, self.YResolution)
#                if self.EightBit==1:
#                    raw_data = fid.read(n_elements) # Read in data
#                    Raw = unpack(&#39;&gt;&#39;+str(n_elements)+&#39;B&#39;,raw_data)
#                else:
#                    #raw_data = fid.read(2*n_elements) # Read in data
#                    #Raw = unpack(&#39;&gt;&#39;+str(n_elements)+&#39;h&#39;,raw_data)
#                fid.close
#                finish reading raw data

            n_elements = self.ChanNum * self.XResolution * self.YResolution
            fid.seek(1024, 0)
            if self.EightBit==1:
                dt = np.dtype(np.uint8)
                dt = dt.newbyteorder(&#39;&gt;&#39;)
                if self.use_dask_arrays:
                    Raw = da.from_array(np.frombuffer(fid.read(n_elements), dtype=dt))
                else:
                    Raw = np.frombuffer(fid.read(n_elements), dtype=dt)
            else:
                dt = np.dtype(np.int16)
                dt = dt.newbyteorder(&#39;&gt;&#39;)
                if self.use_dask_arrays:
                    Raw = da.from_array(np.frombuffer(fid.read(2*n_elements),dtype=dt))
                else:
                    Raw = np.frombuffer(fid.read(2*n_elements),dtype=dt)
            fid.close
            # finish reading raw data

            Raw = np.array(Raw).reshape(self.YResolution, self.XResolution, self.ChanNum)
            #print(shape(Raw), type(Raw), type(Raw[0,0]))

            #data = np.asarray(datab).reshape(self.YResolution,self.XResolution,ChanNum)
            if self.EightBit == 1:
                if self.AI1 == 1:
                    self.RawImageA = Raw[:,:,0]
                    self.ImageA = (Raw[:,:,0].astype(float32)*self.ScanRate/self.Scaling[0,0]/self.Scaling[2,0]/self.Scaling[3,0]+self.Scaling[1,0]).astype(int32)
                    if self.AI2 == 1:
                        self.RawImageB = Raw[:,:,1]
                        self.ImageB = (Raw[:,:,1].astype(float32)*self.ScanRate/self.Scaling[0,1]/self.Scaling[2,1]/self.Scaling[3,1]+self.Scaling[1,1]).astype(int32)
                elif self.AI2 == 1:
                    self.RawImageB = Raw[:,:,0]
                    self.ImageB = (Raw[:,:,0].astype(float32)*self.ScanRate/self.Scaling[0,0]/self.Scaling[2,0]/self.Scaling[3,0]+self.Scaling[1,0]).astype(int32)
            else:
                if self.FileVersion == 1 or self.FileVersion == 2 or self.FileVersion == 3 or self.FileVersion == 4 or self.FileVersion == 5 or self.FileVersion == 6:
                    if self.AI1 == 1:
                        self.RawImageA = Raw[:,:,0]
                        self.ImageA = self.Scaling[0,0] + self.RawImageA * self.Scaling[1,0]  # Converts raw I16 data to voltage based on self.Scaling factors
                        if self.AI2 == 1:
                            self.RawImageB = Raw[:,:,1]
                            self.ImageB = self.Scaling[0,1] + self.RawImageB * self.Scaling[1,1]
                            if self.AI3 == 1:
                                self.RawImageC = (Raw[:,:,2]).reshape(self.YResolution,self.XResolution)
                                self.ImageC = self.Scaling[0,2] + self.RawImageC * self.Scaling[1,2]
                                if self.AI4 == 1:
                                    self.RawImageD = (Raw[:,:,3]).reshape(self.YResolution,self.XResolution)
                                    self.ImageD = self.Scaling[0,3] + self.RawImageD * self.Scaling[1,3]
                            elif self.AI4 == 1:
                                self.RawImageD = (Raw[:,:,2]).reshape(self.YResolution,self.XResolution)
                                self.ImageD = self.Scaling[0,2] + self.RawImageD * self.Scaling[1,2]
                        elif self.AI3 == 1:
                            self.RawImageC = Raw[:,:,1]
                            self.ImageC = self.Scaling[0,1] + self.RawImageC * self.Scaling[1,1]
                            if self.AI4 == 1:
                                self.RawImageD = (Raw[:,:,2]).reshape(self.YResolution,self.XResolution)
                                self.ImageD = self.Scaling[0,2] + self.RawImageD * self.Scaling[1,2]
                        elif self.AI4 == 1:
                            self.RawImageD = Raw[:,:,1]
                            self.ImageD = self.Scaling[0,1] + self.RawImageD * self.Scaling[1,1]
                    elif self.AI2 == 1:
                        self.RawImageB = Raw[:,:,0]
                        self.ImageB = self.Scaling[0,0] + self.RawImageB * self.Scaling[1,0]
                        if self.AI3 == 1:
                            self.RawImageC = Raw[:,:,1]
                            self.ImageC = self.Scaling[0,1] + self.RawImageC * self.Scaling[1,1]
                            if self.AI4 == 1:
                                self.RawImageD = (Raw[:,:,2]).reshape(self.YResolution,self.XResolution)
                                self.ImageD = self.Scaling[0,2] + self.RawImageD * self.Scaling[1,2]
                        elif self.AI4 == 1:
                            self.RawImageD = Raw[:,:,1]
                            self.ImageD = self.Scaling[0,1] + self.RawImageD * self.Scaling[1,1]
                    elif self.AI3 == 1:
                        self.RawImageC = Raw[:,:,0]
                        self.ImageC = self.Scaling[0,0] + self.RawImageC * self.Scaling[1,0]
                        if self.AI4 == 1:
                            self.RawImageD = Raw[:,:,1]
                            self.ImageD = self.Scaling[0,1] + self.RawImageD * self.Scaling[1,1]
                    elif self.AI4 == 1:
                        self.RawImageD = Raw[:,:,0]
                        self.ImageD = self.Scaling[0,0] + self.RawImageD * self.Scaling[1,0]

                elif self.FileVersion == 7:
                    if self.AI1 == 1:
                        self.RawImageA = Raw[:,:,0]
                        self.ImageA = (self.RawImageA - self.Scaling[1,0])*self.Scaling[2,0]
                        if self.AI2 == 1:
                            self.RawImageB = Raw[:,:,1]
                            self.ImageB = (self.RawImageB - self.Scaling[1,1])*self.Scaling[2,1]
                    elif self.AI2 == 1:
                        self.RawImageB = Raw[:,:,0]
                        self.ImageB = (self.RawImageB - self.Scaling[1,1])*self.Scaling[2,1]

                elif  self.FileVersion == 8 or self.FileVersion == 9:
                    self.ElectronFactor1 = 0.1;             # 16-bit intensity is 10x electron counts
                    self.Scaling[3,0] = self.ElectronFactor1
                    self.ElectronFactor2 = 0.1;             # 16-bit intensity is 10x electron counts
                    self.Scaling[3,1] = self.ElectronFactor2
                    if self.AI1 == 1:
                        self.RawImageA = Raw[:,:,0]
                        self.ImageA = (self.RawImageA - self.Scaling[1,0]) * self.Scaling[2,0] / self.ScanRate * self.Scaling[0,0] / self.ElectronFactor1
                        # Converts raw I16 data to voltage based on self.Scaling factors
                        if self.AI2 == 1:
                            self.RawImageB = Raw[:,:,1]
                            self.ImageB = (self.RawImageB - self.Scaling[1,1]) * self.Scaling[2,1] / self.ScanRate * self.Scaling[0,1] / self.ElectronFactor2
                    elif self.AI2 == 1:
                        self.RawImageB = Raw[:,:,0]
                        self.ImageB = (self.RawImageB - self.Scaling[1,1]) * self.Scaling[2,1] / self.ScanRate * self.Scaling[0,1] / self.ElectronFactor2

    def print_header(self):
        &#39;&#39;&#39;
        Prints a formatted content of the file header

        &#39;&#39;&#39;
        if self.FileVersion == -1 :
            print(&#39;Sample_ID=&#39;, self.Sample_ID)
            print(&#39;DetA=&#39;, self.DetA)
            print(&#39;DetB=&#39;, self.DetB)
            print(&#39;EightBit=&#39;, self.EightBit)
            print(&#39;XResolution=&#39;, self.XResolution)
            print(&#39;YResolution=&#39;, self.YResolution)
            print(&#39;PixelSize=&#39;, self.PixelSize)
        else:
            print(&#39;FileMagicNum=&#39;, self.FileMagicNum)
            print(&#39;FileVersion=&#39;, self.FileVersion)
            print(&#39;FileType=&#39;, self.FileType)
            print(&#39;SWdate=&#39;, self.SWdate)
            print(&#39;TimeStep=&#39;, self.TimeStep)
            print(&#39;ChanNum=&#39;, self.ChanNum)
            print(&#39;EightBit=&#39;, self.EightBit)
            print(&#39;Scaling=&#39;, self.Scaling)
            if self.FileVersion &gt; 8 :
                print(&#39;RestartFlag=&#39;, self.RestartFlag)
                print(&#39;StageMove=&#39;, self.StageMove)
                print(&#39;FirstPixelX=&#39;, self.FirstPixelX)
                print(&#39;FirstPixelY=&#39;, self.FirstPixelY)
            print(&#39;XResolution=&#39;, self.XResolution)
            print(&#39;YResolution=&#39;, self.YResolution)
            if self.FileVersion == 1 or self.FileVersion == 2 or self.FileVersion == 3:
                print(&#39;AIDelay=&#39;, self.AIDelay)
            print(&#39;Oversampling=&#39;, self.Oversampling)
            print(&#39;ZeissScanSpeed=&#39;, self.ZeissScanSpeed)
            print(&#39;DecimatingFactor=&#39;, self.DecimatingFactor)
            print(&#39;ScanRate=&#39;, self.ScanRate)
            print(&#39;FramelineRampdownRatio=&#39;, self.FramelineRampdownRatio)
            print(&#39;Xmin=&#39;, self.Xmin)
            print(&#39;Xmax=&#39;, self.Xmax)
            print(&#39;Detmin=&#39;, self.Detmin)
            print(&#39;Detmax=&#39;, self.Detmax)
            print(&#39;AI1=&#39;, self.AI1)
            print(&#39;AI2=&#39;, self.AI2)
            print(&#39;AI3=&#39;, self.AI3)
            print(&#39;AI4=&#39;, self.AI4)
            if self.FileVersion &gt; 8 :
                 print(&#39;Sample_ID=&#39;, self.Sample_ID)
            print(&#39;Notes=&#39;, self.Notes)
            print(&#39;SEMShiftX=&#39;, self.SEMShiftX)
            print(&#39;SEMShiftY=&#39;, self.SEMShiftY)
            print(&#39;DetA=&#39;, self.DetA)
            print(&#39;DetB=&#39;, self.DetB)
            print(&#39;DetC=&#39;, self.DetC)
            print(&#39;DetD=&#39;, self.DetD)
            print(&#39;Mag=&#39;, self.Mag)
            print(&#39;PixelSize=&#39;, self.PixelSize)
            print(&#39;WD=&#39;, self.WD)
            print(&#39;EHT=&#39;, self.EHT)
            print(&#39;SEMApr=&#39;, self.SEMApr)
            print(&#39;HighCurrent=&#39;, self.HighCurrent)
            print(&#39;SEMCurr=&#39;, self.SEMCurr)
            print(&#39;SEMRot=&#39;, self.SEMRot)
            print(&#39;ChamVac=&#39;, self.ChamVac)
            print(&#39;GunVac=&#39;, self.GunVac)
            print(&#39;SEMStiX=&#39;, self.SEMStiX)
            print(&#39;SEMStiY=&#39;, self.SEMStiY)
            print(&#39;SEMAlnX=&#39;, self.SEMAlnX)
            print(&#39;SEMAlnY=&#39;, self.SEMAlnY)
            print(&#39;StageX=&#39;, self.StageX)
            print(&#39;StageY=&#39;, self.StageY)
            print(&#39;StageZ=&#39;, self.StageZ)
            print(&#39;StageT=&#39;, self.StageT)
            print(&#39;StageR=&#39;, self.StageR)
            print(&#39;StageM=&#39;, self.StageM)
            print(&#39;BrightnessA=&#39;, self.BrightnessA)
            print(&#39;ContrastA=&#39;, self.ContrastA)
            print(&#39;BrightnessB=&#39;, self.BrightnessB)
            print(&#39;ContrastB=&#39;, self.ContrastB)
            print(&#39;Mode=&#39;, self.Mode)
            print(&#39;FIBFocus=&#39;, self.FIBFocus)
            print(&#39;FIBProb=&#39;, self.FIBProb)
            print(&#39;FIBCurr=&#39;, self.FIBCurr)
            print(&#39;FIBRot=&#39;, self.FIBRot)
            print(&#39;FIBAlnX=&#39;, self.FIBAlnX)
            print(&#39;FIBAlnY=&#39;, self.FIBAlnY)
            print(&#39;FIBStiX=&#39;, self.FIBStiX)
            print(&#39;FIBStiY=&#39;, self.FIBStiY)
            print(&#39;FIBShiftX=&#39;, self.FIBShiftX)
            print(&#39;FIBShiftY=&#39;, self.FIBShiftY)
            if self.FileVersion &gt; 4:
                print(&#39;MillingXResolution=&#39;, self.MillingXResolution)
                print(&#39;MillingYResolution=&#39;, self.MillingYResolution)
                print(&#39;MillingXSize=&#39;, self.MillingXSize)
                print(&#39;MillingYSize=&#39;, self.MillingYSize)
                print(&#39;MillingULAng=&#39;, self.MillingULAng)
                print(&#39;MillingURAng=&#39;, self.MillingURAng)
                print(&#39;MillingLineTime=&#39;, self.MillingLineTime)
                print(&#39;FIBFOV (um)=&#39;, self.FIBFOV)
                print(&#39;MillingPIDOn=&#39;, self.MillingPIDOn)
                print(&#39;MillingPIDMeasured=&#39;, self.MillingPIDMeasured)
                print(&#39;MillingPIDTarget=&#39;, self.MillingPIDTarget)
                print(&#39;MillingPIDTargetSlope=&#39;, self.MillingPIDTargetSlope)
                print(&#39;MillingPIDP=&#39;, self.MillingPIDP)
                print(&#39;MillingPIDI=&#39;, self.MillingPIDI)
                print(&#39;MillingPIDD=&#39;, self.MillingPIDD)
                print(&#39;MachineID=&#39;, self.MachineID)
                print(&#39;SEMSpecimenI=&#39;, self.SEMSpecimenI)
            if self.FileVersion &gt; 5:
                print(&#39;Temperature=&#39;, self.Temperature)
                print(&#39;FaradayCupI=&#39;, self.FaradayCupI)
                print(&#39;FIBSpecimenI=&#39;, self.FIBSpecimenI)
                print(&#39;BeamDump1I=&#39;, self.BeamDump1I)
                print(&#39;MillingYVoltage=&#39;, self.MillingYVoltage)
                print(&#39;FocusIndex=&#39;, self.FocusIndex)
                print(&#39;FIBSliceNum=&#39;, self.FIBSliceNum)
            if self.FileVersion &gt; 7:
                print(&#39;BeamDump2I=&#39;, self.BeamDump2I)
                print(&#39;MillingI=&#39;, self.MillingI)
            print(&#39;SEMSpecimenI=&#39;, self.SEMSpecimenI)
            print(&#39;FileLength=&#39;, self.FileLength)

    def display_images(self):
        &#39;&#39;&#39;
        Display auto-scaled detector images without saving the figure into the file.

        &#39;&#39;&#39;
        fig, axs = subplots(2, 1, figsize=(10,5))
        axs[0].imshow(self.RawImageA, cmap=&#39;Greys&#39;)
        axs[1].imshow(self.RawImageB, cmap=&#39;Greys&#39;)
        ttls = [&#39;Detector A: &#39;+self.DetA.strip(&#39;\x00&#39;), &#39;Detector B: &#39;+self.DetB.strip(&#39;\x00&#39;)]
        for ax, ttl in zip(axs, ttls):
            ax.axis(False)
            ax.set_title(ttl, fontsize=10)
        fig.suptitle(self.fname)

    def save_images_jpeg(self, **kwargs):
        &#39;&#39;&#39;
        Display auto-scaled detector images and save the figure into JPEG file (s).

        Parameters
        ----------
        kwargs:
        images_to_save : str
            Images to save. options are: &#39;A&#39;, &#39;B&#39;, or &#39;Both&#39; (default).
        invert : boolean
            If True, the image will be inverted.

        &#39;&#39;&#39;
        images_to_save = kwargs.get(&#34;images_to_save&#34;, &#39;Both&#39;)
        invert = kwargs.get(&#34;invert&#34;, False)

        if images_to_save == &#39;Both&#39; or images_to_save == &#39;A&#39;:
            if self.ftype == 0:
                fname_jpg = os.path.splitext(self.fname)[0] + &#39;_&#39; + self.DetA.strip(&#39;\x00&#39;) + &#39;.jpg&#39;
            else:
                fname_jpg = os.path.splitext(self.fname)[0] + &#39;DetA.jpg&#39;
            Img = self.RawImageA_8bit_thresholds()[0]
            if invert:
                Img =  uint8(255) - Img
            PILImage.fromarray(Img).save(fname_jpg)

        try:
            if images_to_save == &#39;Both&#39; or images_to_save == &#39;B&#39;:
                if self.ftype == 0:
                    fname_jpg = os.path.splitext(self.fname)[0] +  &#39;_&#39; + self.DetB.strip(&#39;\x00&#39;) + &#39;.jpg&#39;
                else:
                    fname_jpg = os.path.splitext(self.fname)[0] + &#39;DetB.jpg&#39;
                Img = self.RawImageB_8bit_thresholds()[0]
                if invert:
                    Img =  uint8(255) - Img
                PILImage.fromarray(Img).save(fname_jpg)
        except:
            print(&#39;No Detector B image to save&#39;)

    def save_images_tif(self, images_to_save = &#39;Both&#39;):
        &#39;&#39;&#39;
        Save the detector images into TIF file (s).

        Parameters
        ----------
        images_to_save : str
            Images to save. options are: &#39;A&#39;, &#39;B&#39;, or &#39;Both&#39; (default).

        &#39;&#39;&#39;
        if self.ftype == 0:
            if images_to_save == &#39;Both&#39; or images_to_save == &#39;A&#39;:
                fnameA = os.path.splitext(self.fname)[0] + &#39;_&#39; + self.DetA.strip(&#39;\x00&#39;) + &#39;.tif&#39;
                tiff.imsave(fnameA, self.RawImageA)
            if self.DetB != &#39;None&#39;:
                if images_to_save == &#39;Both&#39; or images_to_save == &#39;B&#39;:
                    fnameB = os.path.splitext(self.fname)[0] + &#39;_&#39; + self.DetB.strip(&#39;\x00&#39;) + &#39;.tif&#39;
                    tiff.imsave(fnameB, self.RawImageB)
        else:
            print(&#39;original File is already in TIF format&#39;)

    def get_image_min_max(self, image_name = &#39;ImageA&#39;, thr_min = 1.0e-4, thr_max = 1.0e-3, nbins=256, disp_res = False):
        &#39;&#39;&#39;
        Calculates the data range of the EM data. ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

        Calculates histogram of pixel intensities of of the loaded image
        with number of bins determined by parameter nbins (default = 256)
        and normalizes it to get the probability distribution function (PDF),
        from which a cumulative distribution function (CDF) is calculated.
        Then given the threshold_min, threshold_max parameters,
        the minimum and maximum values for the image are found by finding
        the intensities at which CDF= threshold_min and (1- threshold_max), respectively.

        Parameters
        ----------
        image_name : string
            the name of the image to perform this operations (defaulut is &#39;RawImageA&#39;)
        threshold_min : float
            CDF threshold for determining the minimum data value
        threshold_max : float
            CDF threshold for determining the maximum data value
        nbins : int
            number of histogram bins for building the PDF and CDF
        disp_res : boolean
            (default is False) - to plot/ display the results

        Returns:
            dmin, dmax: (float) minimum and maximum values of the data range.
        &#39;&#39;&#39;
        if image_name == &#39;ImageA&#39;:
            im = self.ImageA
        if image_name == &#39;ImageB&#39;:
            im = self.ImageB
        if image_name == &#39;RawImageA&#39;:
            im = self.RawImageA
        if image_name == &#39;RawImageB&#39;:
            im = self.RawImageB
        return get_min_max_thresholds(im, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=disp_res)

    def RawImageA_8bit_thresholds(self, thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
        &#39;&#39;&#39;
        Convert the Image A into 8-bit array

        Parameters
        ----------
        thr_min : float
            lower CDF threshold for determining the minimum data value
        thr_max : float
            upper CDF threshold for determining the maximum data value
        data_min : float
            If different from data_max, this value will be used as low bound for I8 data conversion
        data_max : float
            If different from data_min, this value will be used as high bound for I8 data conversion
        nbins : int
            number of histogram bins for building the PDF and CDF

        Returns
        dt, data_min, data_max
            dt : 2D uint8 array
                Converted data
            data_min : float
                value used as low bound for I8 data conversion
            data_max : float
                value used as high bound for I8 data conversion
        &#39;&#39;&#39;
        if self.EightBit==1:
            #print(&#39;8-bit image already - no need to convert&#39;)
            dt = self.RawImageA
        else:
            if data_min == data_max:
                data_min, data_max = self.get_image_min_max(image_name =&#39;RawImageA&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
            dt = ((np.clip(self.RawImageA, data_min, data_max) - data_min)/(data_max-data_min)*255.0).astype(np.uint8)
        return dt, data_min, data_max

    def RawImageB_8bit_thresholds(self, thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
        &#39;&#39;&#39;
        Convert the Image B into 8-bit array

        Parameters
        ----------
        thr_min : float
            lower CDF threshold for determining the minimum data value
        thr_max : float
            upper CDF threshold for determining the maximum data value
        data_min : float
            If different from data_max, this value will be used as low bound for I8 data conversion
        data_max : float
            If different from data_min, this value will be used as high bound for I8 data conversion
        nbins : int
            number of histogram bins for building the PDF and CDF

        Returns
        dt, data_min, data_max
            dt : 2D uint8 array
                Converted data
            data_min : float
                value used as low bound for I8 data conversion
            data_max : float
                value used as high bound for I8 data conversion
        &#39;&#39;&#39;
        if self.EightBit==1:
            #print(&#39;8-bit image already - no need to convert&#39;)
            dt = self.RawImageB
        else:
            if data_min == data_max:
                data_min, data_max = self.get_image_min_max(image_name =&#39;RawImageB&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
            dt = ((np.clip(self.RawImageB, data_min, data_max) - data_min)/(data_max-data_min)*255.0).astype(np.uint8)
        return dt, data_min, data_max

    def save_snapshot(self, **kwargs):
        &#39;&#39;&#39;
        Builds an image that contains both the Detector A and Detector B (if present) images as well as a table with important FIB-SEM parameters.

        kwargs:
         ----------
        thr_min : float
            lower CDF threshold for determining the minimum data value. Default is 1.0e-3
        thr_max : float
            upper CDF threshold for determining the maximum data value. Default is 1.0e-3
        data_min : float
            If different from data_max, this value will be used as low bound for I8 data conversion
        data_max : float
            If different from data_min, this value will be used as high bound for I8 data conversion
        nbins : int
            number of histogram bins for building the PDF and CDF
        disp_res : True
            If True display the results
        dpi : int
            Default is 300
        snapshot_name : string
            the name of the image to perform this operations (defaulut is frame_name + &#39;_snapshot.png&#39;).



        Returns
        dt, data_min, data_max
            dt : 2D uint8 array
                Converted data
            data_min : float
                value used as low bound for I8 data conversion
            data_max : float
                value used as high bound for I8 data conversion
        &#39;&#39;&#39;
        thr_min = kwargs.get(&#39;thr_min&#39;, 1.0e-3)
        thr_max = kwargs.get(&#39;thr_max&#39;, 1.0e-3)
        nbins = kwargs.get(&#39;nbins&#39;, 256)
        disp_res = kwargs.get(&#39;disp_res&#39;, True)
        dpi = kwargs.get(&#39;dpi&#39;, 300)
        snapshot_name = kwargs.get(&#39;snapshot_name&#39;, os.path.splitext(self.fname)[0] + &#39;_snapshot.png&#39;)

        ifDetB = (self.DetB != &#39;None&#39;)
        if ifDetB:
            try:
                dminB, dmaxB = self.get_image_min_max(image_name =&#39;RawImageB&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
                fig, axs = subplots(3, 1, figsize=(11,8))
            except:
                ifDetB = False
                pass
        if not ifDetB:
            fig, axs = subplots(2, 1, figsize=(7,8))
        fig.subplots_adjust(left=0.01, bottom=0.01, right=0.99, top=0.90, wspace=0.15, hspace=0.1)
        dminA, dmaxA = self.get_image_min_max(image_name =&#39;RawImageA&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
        axs[1].imshow(self.RawImageA, cmap=&#39;Greys&#39;, vmin=dminA, vmax=dmaxA)
        if ifDetB:
            axs[2].imshow(self.RawImageB, cmap=&#39;Greys&#39;, vmin=dminB, vmax=dmaxB)
        try:
            ttls = [self.Notes.strip(&#39;\x00&#39;),
                &#39;Detector A:  &#39;+ self.DetA.strip(&#39;\x00&#39;) + &#39;,  Data Range:  {:.1f} ÷ {:.1f} with thr_min={:.1e}, thr_max={:.1e}&#39;.format(dminA, dmaxA, thr_min, thr_max) + &#39;    (Brightness: {:.1f}, Contrast: {:.1f})&#39;.format(self.BrightnessA, self.ContrastA),
                &#39;Detector B:  &#39;+ self.DetB.strip(&#39;\x00&#39;) + &#39;,  Data Range:  {:.1f} ÷ {:.1f} with thr_min={:.1e}, thr_max={:.1e}&#39;.format(dminB, dmaxB, thr_min, thr_max) + &#39;    (Brightness: {:.1f}, Contrast: {:.1f})&#39;.format(self.BrightnessB, self.ContrastB)]
        except:
            ttls = [&#39;&#39;, &#39;Detector A&#39;, &#39;&#39;]
        for j, ax in enumerate(axs):
            ax.axis(False)
            ax.set_title(ttls[j], fontsize=10)
        fig.suptitle(self.fname)

        if self.FileVersion &gt; 8:
            cell_text = [[&#39;Sample ID&#39;, &#39;{:s}&#39;.format(self.Sample_ID.strip(&#39;\x00&#39;)), &#39;&#39;,
                          &#39;Frame Size&#39;, &#39;{:d} x {:d}&#39;.format(self.XResolution, self.YResolution), &#39;&#39;,
                          &#39;Scan Rate&#39;, &#39;{:.3f} MHz&#39;.format(self.ScanRate/1.0e6)],
                        [&#39;Machine ID&#39;, &#39;{:s}&#39;.format(self.MachineID.strip(&#39;\x00&#39;)), &#39;&#39;,
                          &#39;Pixel Size&#39;, &#39;{:.1f} nm&#39;.format(self.PixelSize), &#39;&#39;,
                          &#39;Oversampling&#39;, &#39;{:d}&#39;.format(self.Oversampling)],
                         [&#39;FileVersion&#39;, &#39;{:d}&#39;.format(self.FileVersion), &#39;&#39;,
                          &#39;Working Dist.&#39;, &#39;{:.3f} mm&#39;.format(self.WD), &#39;&#39;,
                          &#39;FIB Focus&#39;, &#39;{:.1f}  V&#39;.format(self.FIBFocus)],
                         [&#39;Bit Depth&#39;, &#39;{:d}&#39;.format(8 *(2 - self.EightBit)), &#39;&#39;,
                         &#39;EHT Voltage\n\nSEM Current&#39;, &#39;{:.3f} kV \n\n{:.3f} nA&#39;.format(self.EHT, self.SEMCurr*1.0e9), &#39;&#39;,
                         &#39;FIB Probe&#39;, &#39;{:d}&#39;.format(self.FIBProb)]]
        else:
            if self.FileVersion &gt; 0:
                cell_text = [[&#39;&#39;, &#39;&#39;, &#39;&#39;,
                              &#39;Frame Size&#39;, &#39;{:d} x {:d}&#39;.format(self.XResolution, self.YResolution), &#39;&#39;,
                              &#39;Scan Rate&#39;, &#39;{:.3f} MHz&#39;.format(self.ScanRate/1.0e6)],
                            [&#39;Machine ID&#39;, &#39;{:s}&#39;.format(self.MachineID.strip(&#39;\x00&#39;)), &#39;&#39;,
                              &#39;Pixel Size&#39;, &#39;{:.1f} nm&#39;.format(self.PixelSize), &#39;&#39;,
                              &#39;Oversampling&#39;, &#39;{:d}&#39;.format(self.Oversampling)],
                             [&#39;FileVersion&#39;, &#39;{:d}&#39;.format(self.FileVersion), &#39;&#39;,
                              &#39;Working Dist.&#39;, &#39;{:.3f} mm&#39;.format(self.WD), &#39;&#39;,
                              &#39;FIB Focus&#39;, &#39;{:.1f}  V&#39;.format(self.FIBFocus)],
                             [&#39;Bit Depth&#39;, &#39;{:d}&#39;.format(8 *(2 - self.EightBit)), &#39;&#39;,
                             &#39;EHT Voltage&#39;, &#39;{:.3f} kV&#39;.format(self.EHT), &#39;&#39;,
                             &#39;FIB Probe&#39;, &#39;{:d}&#39;.format(self.FIBProb)]]
            else:
                cell_text = [[&#39;&#39;, &#39;&#39;, &#39;&#39;,
                              &#39;Frame Size&#39;, &#39;{:d} x {:d}&#39;.format(self.XResolution, self.YResolution), &#39;&#39;,
                              &#39;Scan Rate&#39;, &#39;&#39;],
                            [&#39;Machine ID&#39;, &#39;&#39;, &#39;&#39;,
                              &#39;Pixel Size&#39;, &#39;{:.1f} nm&#39;.format(self.PixelSize), &#39;&#39;,
                              &#39;Oversampling&#39;, &#39;&#39;],
                             [&#39;FileVersion&#39;, &#39;{:d}&#39;.format(self.FileVersion), &#39;&#39;,
                              &#39;Working Dist.&#39;, &#39; &#39;, &#39;&#39;,
                              &#39;FIB Focus&#39;, &#39;&#39;],
                             [&#39;Bit Depth&#39;, &#39;{:d}&#39;.format(8 *(2 - self.EightBit)), &#39;&#39;,
                             &#39;EHT Voltage&#39;, &#39;&#39;, &#39;&#39;,
                             &#39;FIB Probe&#39;, &#39;&#39;]]
        llw0=0.3
        llw1=0.18
        llw2=0.02
        clw = [llw1, llw0, llw2, llw1, llw1, llw2, llw1, llw1]
        tbl = axs[0].table(cellText=cell_text,
                           colWidths=clw,
                           cellLoc=&#39;center&#39;,
                           colLoc=&#39;center&#39;,
                           bbox = [0.02, 0, 0.96, 1.0],
                           #bbox = [0.45, 1.02, 2.8, 0.55],
                           zorder=10)

        fig.savefig(snapshot_name, dpi=dpi)
        if disp_res == False:
            plt.close(fig)

    def analyze_noise_ROIs(self, Noise_ROIs, Hist_ROI, **kwargs):
        &#39;&#39;&#39;
        Analyses the noise statistics in the selected ROI&#39;s of the EM data.
        ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

        Calls Single_Image_Noise_ROIs(img, Noise_ROIs, Hist_ROI, **kwargs)
        Performs following:
        1. For each of the selected ROI&#39;s, this method will perfrom the following:
            1a. Smooth the data by 2D convolution with a given kernel.
            1b. Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
            1c. Calculate the mean intensity value of the data and variance of the above &#34;Noise&#34;
        2. Plot the dependence of the noise variance vs. image intensity.
        3. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
            it will be determined from the header data:
                for RawImageA it is self.Scaling[1,0]
                for RawImageB it is self.Scaling[1,1]
        4. The equation is determined for a line that passes through the point:
                Intensity=DarkCount and Noise Variance = 0
                and is a best fit for the [Mean Intensity, Noise Variance] points
                determined for each ROI (Step 1 above).
        5. Another ROI (defined by Hist_ROI parameter) is used to built an
            intensity histogram of the actual data. Peak of that histogram is determined.
        6. The data is plotted. Two values of SNR are defined from the slope of the line in Step 4:
            PSNR (Peak SNR) = Mean Intensity/sqrt(Noise Variance) at the intensity
                at the histogram peak determined in the Step 5.
            DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
                where Max and Min Intensity are determined by corresponding cummulative
                threshold parameters, and Noise Variance is taken at the intensity
                in the middle of the range (Min Intensity + Max Intensity)/2.0

        Parameters
        ----------
        Noise_ROIs : list of lists: [[left, right, top, bottom]]
            list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the noise.
        Hist_ROI : list [left, right, top, bottom]
            coordinates (indices) of the boundaries of the image subset to evaluate the real data histogram.

        kwargs:
        image_name : string
            the name of the image to perform this operations (defaulut is &#39;RawImageA&#39;).
        DarkCount : float
            the value of the Intensity Data at 0.
        kernel : 2D float array
            a kernel to perfrom 2D smoothing convolution.
        filename : str
            filename - used for plotting the data. If not explicitly defined will use the instance attribute self.fname
        nbins_disp : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
        thresholds_disp : list [thr_min_disp, thr_max_disp]
            (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
        nbins_analysis : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
        thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
            (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
        nbins_analysis : int
             (default 256) number of histogram bins for building the data histogram in Step 5.
        disp_res : boolean
            (default is False) - to plot/ display the results

        Returns:
        mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR
            mean_vals and var_vals are the Mean Intensity and Noise Variance values for the Noise_ROIs (Step 1)
            NF_slope is the slope of the linear fit curve (Step 4)
            PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 6)
        &#39;&#39;&#39;
        image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)

        if image_name == &#39;RawImageA&#39;:
            ImgEM = self.RawImageA.astype(float)
            DarkCount = self.Scaling[1,0]
        if image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;:
            ImgEM = self.RawImageB.astype(float)
            DarkCount = self.Scaling[1,1]

        if (image_name == &#39;RawImageA&#39;) or (image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;):
            st = 1.0/np.sqrt(2.0)
            def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
            def_kernel = def_kernel/def_kernel.sum()
            kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
            DarkCount = kwargs.get(&#34;DarkCount&#34;, DarkCount)
            nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
            thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
            nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
            thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
            Notes = kwargs.get(&#34;Notes&#34;, self.Notes.strip(&#39;\x00&#39;))
            kwargs[&#39;kernel&#39;] = kernel
            kwargs[&#39;DarkCount&#39;] = DarkCount
            kwargs[&#39;img_label&#39;] = image_name
            kwargs[&#39;res_fname&#39;] = os.path.splitext(self.fname)[0] + &#39;_&#39; + image_name + &#39;_Noise_Analysis_ROIs.png&#39;
            kwargs[&#39;Notes&#39;] = Notes
            mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR = Single_Image_Noise_ROIs(ImgEM, Noise_ROIs, Hist_ROI, **kwargs)

        else:
            print(&#39;No valid image name selected&#39;)
            mean_vals = 0.0
            var_vals = 0.0
            NF_slope = 0.0
            PSNR = 0.0
            MSNR = 0.0
            DSNR = 0.0

        return mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR

    def analyze_noise_statistics(self, **kwargs):
        &#39;&#39;&#39;
        Analyses the noise statistics of the EM data image.
        ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

        Calls Single_Image_Noise_Statistics(img, **kwargs)
        Performs following:
        1. Smooth the image by 2D convolution with a given kernel.
        2. Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
        3. Build a histogram of Smoothed Image.
        4. For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
        5. Plot the dependence of the noise variance vs. image intensity.
        6. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
            it will be set to 0
        7. The equation is determined for a line that passes through the point:
                Intensity=DarkCount and Noise Variance = 0
                and is a best fit for the [Mean Intensity, Noise Variance] points
                determined for each ROI (Step 1 above).
        8. The data is plotted. Two values of SNR are defined from the slope of the line in Step 7:
            PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity
                at the histogram peak determined in the Step 3.
            MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
            DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
                where Max and Min Intensity are determined by corresponding cummulative
                threshold parameters, and Noise Variance is taken at the intensity
                in the middle of the range (Min Intensity + Max Intensity)/2.0

        Parameters
        ----------
            kwargs:
            image_name : str
                Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
            evaluation_box : list of 4 int
                evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
                if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
            DarkCount : float
                the value of the Intensity Data at 0.
            kernel : 2D float array
                a kernel to perfrom 2D smoothing convolution.
            nbins_disp : int
                (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
            thresholds_disp : list [thr_min_disp, thr_max_disp]
                (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
            nbins_analysis : int
                (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
            thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
                (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
            nbins_analysis : int
                 (default 256) number of histogram bins for building the data histogram in Step 5.
            disp_res : boolean
                (default is False) - to plot/ display the results
            save_res_png : boolean
                save the analysis output into a PNG file (default is True)
            res_fname : string
                filename for the result image (&#39;Noise_Analysis.png&#39;)
            img_label : string
                optional image label
            Notes : string
                optional additional notes
            dpi : int

        Returns:
        mean_vals, var_vals, I0, PSNR, DSNR, popt, result
            mean_vals and var_vals are the Mean Intensity and Noise Variance values for Step 5
            I0 is zero intercept (should be close to DarkCount)
            PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 8)
        &#39;&#39;&#39;
        image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)

        if image_name == &#39;RawImageA&#39;:
            ImgEM = self.RawImageA.astype(float)
            DarkCount = self.Scaling[1,0]
        if image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;:
            ImgEM = self.RawImageB.astype(float)
            DarkCount = self.Scaling[1,1]

        if (image_name == &#39;RawImageA&#39;) or (image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;):
            st = 1.0/np.sqrt(2.0)
            evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
            def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
            def_kernel = def_kernel/def_kernel.sum()
            kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
            DarkCount = kwargs.get(&#34;DarkCount&#34;, DarkCount)
            nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
            thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
            nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
            thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
            disp_res = kwargs.get(&#34;disp_res&#34;, True)
            save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
            default_res_name = os.path.splitext(self.fname)[0] + &#39;_Noise_Analysis_&#39; + image_name + &#39;.png&#39;
            res_fname = kwargs.get(&#34;res_fname&#34;, default_res_name)
            img_label = kwargs.get(&#34;img_label&#34;, self.Sample_ID)
            Notes = kwargs.get(&#34;Notes&#34;, self.Notes.strip(&#39;\x00&#39;))
            dpi = kwargs.get(&#34;dpi&#34;, 300)

            noise_kwargs = {&#39;image_name&#39; : image_name,
                            &#39;evaluation_box&#39; : evaluation_box,
                            &#39;kernel&#39; : kernel,
                            &#39;DarkCount&#39; : DarkCount,
                            &#39;nbins_disp&#39; : nbins_disp,
                            &#39;thresholds_disp&#39; : thresholds_disp,
                            &#39;nbins_analysis&#39; : nbins_analysis,
                            &#39;thresholds_analysis&#39; : thresholds_analysis,
                            &#39;disp_res&#39; : disp_res,
                            &#39;save_res_png&#39; : save_res_png,
                            &#39;res_fname&#39; : res_fname,
                            &#39;Notes&#39; : Notes,
                            &#39;dpi&#39; : dpi}

            mean_vals, var_vals, I0, PSNR, DSNR, popt, result =  Single_Image_Noise_Statistics(ImgEM, **noise_kwargs)
        else:
            mean_vals, var_vals, I0, PSNR, DSNR, popt, result = [], [], 0.0, 0.0, np.array((0.0, 0.0)), []
        return mean_vals, var_vals, I0, PSNR, DSNR, popt, result


    def analyze_SNR_autocorr(self, **kwargs):
        &#39;&#39;&#39;
        Estimates SNR using auto-correlation analysis of a single image.
        ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

        Calculates SNR of a single image base on auto-correlation analysis of a single image, after [1].
        Calls function Single_Image_SNR(img, **kwargs)

        Parameters
        ---------
        kwargs:
        image_name : str
            Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
        edge_fraction : float
            fraction of the full autocetrrelation range used to calculate the &#34;mean value&#34; (default is 0.10)
        extrapolate_signal : boolean
            extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
        disp_res : boolean
            display results (plots) (default is True)
        save_res_png : boolean
            save the analysis output into a PNG file (default is True)
        res_fname : string
            filename for the sesult image (&#39;SNR_result.png&#39;)
        img_label : string
            optional image label
        dpi : int
            dots-per-inch resolution for the output image

        Returns:
            xSNR, ySNR : float, float
                SNR determind using the method in [1] along X- and Y- directions.
                If there is a direction with slow varying data - that direction provides more accurate SNR estimate
                Y-streaks in typical FIB-SEM data provide slow varying Y-component becuase streaks
                usually get increasingly worse with increasing Y.
                So for typical FIB-SEM data use ySNR

        [1] J. T. L. Thong et al, Single-image signal-tonoise ratio estimation. Scanning, 328–336 (2001).
        &#39;&#39;&#39;
        image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        edge_fraction = kwargs.get(&#34;edge_fraction&#34;, 0.10)
        extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)
        disp_res = kwargs.get(&#34;disp_res&#34;, True)
        save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
        default_res_name = os.path.splitext(self.fname)[0] + &#39;_AutoCorr_Noise_Analysis_&#39; + image_name + &#39;.png&#39;
        res_fname = kwargs.get(&#34;res_fname&#34;, default_res_name)
        dpi = kwargs.get(&#34;dpi&#34;, 300)

        SNR_kwargs = {&#39;edge_fraction&#39; : edge_fraction,
                        &#39;extrapolate_signal&#39; : extrapolate_signal,
                        &#39;disp_res&#39; : disp_res,
                        &#39;save_res_png&#39; : save_res_png,
                        &#39;res_fname&#39; : res_fname,
                        &#39;img_label&#39; : image_name,
                        &#39;dpi&#39; : dpi}

        if image_name == &#39;RawImageA&#39;:
            img = self.RawImageA
        if image_name == &#39;RawImageB&#39;:
            img = self.RawImageB
        if image_name == &#39;ImageA&#39;:
            img = self.ImageA
        if image_name == &#39;ImageB&#39;:
            img = self.ImageB

        xi = 0
        yi = 0
        ysz, xsz = img.shape
        xa = xi + xsz
        ya = yi + ysz
        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        xSNR, ySNR, rSNR= Single_Image_SNR(img[yi_eval:ya_eval, xi_eval:xa_eval], **SNR_kwargs)

        return xSNR, ySNR, rSNR


    def show_eval_box(self, **kwargs):
        &#39;&#39;&#39;
        Show the box used for noise analysis.
        ©G.Shtengel, 04/2021. gleb.shtengel@gmail.com

        kwargs
        ---------
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        image_name : str
            Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
        data_dir : str
            data directory (path)
        Sample_ID : str
            Sample ID
        invert_data : boolean
            If True - the data is inverted
        save_res_png  : boolean
            Save PNG image of the frame overlaid with with evaluation box
        &#39;&#39;&#39;
        image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
        data_dir = kwargs.get(&#34;data_dir&#34;, os.path.dirname(self.fname))
        Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
        nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
        thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
        invert_data =  kwargs.get(&#34;invert_data&#34;, False)
        save_res_png  = kwargs.get(&#34;save_res_png&#34;, False )

        if image_name == &#39;RawImageA&#39;:
            img = self.RawImageA
        if image_name == &#39;RawImageB&#39;:
            img = self.RawImageB
        if image_name == &#39;ImageA&#39;:
            img = self.ImageA
        if image_name == &#39;ImageB&#39;:
            img = self.ImageB

        xi = 0
        yi = 0
        ysz, xsz = img.shape
        xa = xi + xsz
        ya = yi + ysz
        xi_eval = xi + evaluation_box[2]
        if evaluation_box[3] &gt; 0:
            xa_eval = xi_eval + evaluation_box[3]
        else:
            xa_eval = xa
        yi_eval = yi + evaluation_box[0]
        if evaluation_box[1] &gt; 0:
            ya_eval = yi_eval + evaluation_box[1]
        else:
            ya_eval = ya

        range_disp = get_min_max_thresholds(img[yi_eval:ya_eval, xi_eval:xa_eval], thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res=False)

        fig, ax = subplots(1,1, figsize = (10.0, 11.0*ysz/xsz))
        ax.imshow(img, cmap=&#39;Greys&#39;, vmin = range_disp[0], vmax = range_disp[1])
        ax.grid(True, color = &#34;cyan&#34;)
        ax.set_title(self.fname)
        rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=2.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
        ax.add_patch(rect_patch)
        if save_res_png :
            fig.savefig(os.path.splitext(self.fname+&#39;_evaluation_box.png&#39;, dpi=300))


    def determine_field_fattening_parameters(self, **kwargs):
        &#39;&#39;&#39;
        Perfrom 2D parabolic fit (calls Perform_2D_fit(Img, estimator, **kwargs)) and determine the field-flattening parameters

        Parameters
        ----------
        kwargs:
        image_names : list of str
            Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
        estimator : RANSACRegressor(),
                    LinearRegression(),
                    TheilSenRegressor(),
                    HuberRegressor()
        bins : int
            binsize for image binning. If not provided, bins=10
        Analysis_ROIs : list of lists: [[left, right, top, bottom]]
            list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the parabolic fit.
        calc_corr : boolean
            If True - the full image correction is calculated
        ignore_Y  : boolean
            If True - the parabolic fit to only X is perfromed
        Xsect : int
            X - coordinate for Y-crossection
        Ysect : int
            Y - coordinate for X-crossection
        disp_res : boolean
            (default is False) - to plot/ display the results
        save_res_png : boolean
            save the analysis output into a PNG file (default is False)
        save_correction_binary = boolean
            save the mage)name and img_correction_array data into a binary file
        res_fname : string
            filename for the result image (&#39;**_Image_Flattening.png&#39;). The binary image is derived from the same root, e.g. &#39;**_Image_Flattening.bin&#39;
        label : string
            optional image label
        dpi : int

        Returns:
        img_correction_coeffs, img_correction_arrays
        &#39;&#39;&#39;
        image_names = kwargs.get(&#34;image_names&#34;, [&#39;RawImageA&#39;])
        estimator = kwargs.get(&#34;estimator&#34;, LinearRegression())
        if &#34;estimator&#34; in kwargs:
            del kwargs[&#34;estimator&#34;]
        calc_corr = kwargs.get(&#34;calc_corr&#34;, False)
        ignore_Y = kwargs.get(&#34;ignore_Y&#34;, False)
        lbl = kwargs.get(&#34;label&#34;, &#39;&#39;)
        disp_res = kwargs.get(&#34;disp_res&#34;, True)
        bins = kwargs.get(&#34;bins&#34;, 10) #bins = 10
        Analysis_ROIs = kwargs.get(&#34;Analysis_ROIs&#34;, [])
        save_res_png = kwargs.get(&#34;save_res_png&#34;, False)
        res_fname = kwargs.get(&#34;res_fname&#34;, os.path.splitext(self.fname)[0]+&#39;_Image_Flattening.png&#39;)
        save_correction_binary = kwargs.get(&#34;save_correction_binary&#34;, False)
        dpi = kwargs.get(&#34;dpi&#34;, 300)

        img_correction_arrays = []
        img_correction_coeffs = []
        for image_name in image_names:
            if image_name == &#39;RawImageA&#39;:
                img = self.RawImageA - self.Scaling[1,0]
            if image_name == &#39;RawImageB&#39;:
                img = self.RawImageB - self.Scaling[1,1]
            if image_name == &#39;ImageA&#39;:
                img = self.ImageA
            if image_name == &#39;ImageB&#39;:
                img = self.ImageB

            ysz, xsz = img.shape
            Xsect = kwargs.get(&#34;Xsect&#34;, xsz//2)
            Ysect = kwargs.get(&#34;Ysect&#34;, ysz//2)

            intercept, coefs, mse, img_correction_array = Perform_2D_fit(img, estimator, image_name=image_name, **kwargs)
            img_correction_arrays.append(img_correction_array)
            img_correction_coeffs.append(coefs)

        if calc_corr:
            self.image_correction_sources = image_names
            self.img_correction_arrays = img_correction_arrays
            if save_correction_binary:
                bin_fname = res_fname.replace(&#39;png&#39;, &#39;bin&#39;)
                pickle.dump([image_names, img_correction_arrays], open(bin_fname, &#39;wb&#39;)) # saves source name and correction array into the binary file
                self.image_correction_file = res_fname.replace(&#39;png&#39;, &#39;bin&#39;)
                print(&#39;Image Flattening Info saved into the binary file: &#39;, self.image_correction_file)
        #self.intercept = intercept
        self.img_correction_coeffs = img_correction_coeffs
        return intercept, img_correction_coeffs, img_correction_arrays


    def flatten_image(self, **kwargs):
        &#39;&#39;&#39;
        Flatten the image(s). Image flattening parameters must be determined (determine_field_fattening_parameters)

        Parameters
        ----------
        kwargs:
        image_correction_file : str
            full path to a binary filename that contains source names (image_correction_sources) and correction arrays (img_correction_arrays)
            if image_correction_file exists, the data is loaded from it.
        image_correction_sources : list of str
            Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
        img_correction_arrays : list of 2D arrays
            arrays containing field flatteting info

        Returns:
        flattened_images : list of 2D arrays
        &#39;&#39;&#39;

        if hasattr(self, &#39;image_correction_file&#39;):
            image_correction_file = kwargs.get(&#34;image_correction_file&#34;, self.image_correction_file)
        else:
            image_correction_file = kwargs.get(&#34;image_correction_file&#34;, &#39;&#39;)

        try:
            # try loading the image correction data from the binary file
            with open(image_correction_file, &#34;rb&#34;) as f:
                [image_correction_sources,  img_correction_arrays] = pickle.load(f)
        except:
            #  if that did not work, see if the correction data was provided directly
            if hasattr(self, &#39;image_correction_source&#39;):
                image_correction_sources = kwargs.get(&#34;image_correction_sources&#34;, self.image_correction_sources)
            else:
                image_correction_sources = kwargs.get(&#34;image_correction_sources&#34;, [False])

            if hasattr(self, &#39;img_correction_arrays&#39;):
                img_correction_arrays = kwargs.get(&#34;img_correction_arrays&#34;, self.img_correction_arrays)
            else:
                img_correction_arrays = kwargs.get(&#34;img_correction_arrays&#34;, [False])

        flattened_images = []
        for image_correction_source, img_correction_array in zip(image_correction_sources, img_correction_arrays):
            if (image_correction_source is not False) and (img_correction_array is not False):
                if image_correction_source == &#39;RawImageA&#39;:
                    flattened_image = (self.RawImageA - self.Scaling[1,0])*img_correction_array + self.Scaling[1,0]
                if image_correction_source == &#39;RawImageB&#39;:
                    flattened_image = (self.RawImageB - self.Scaling[1,1])*img_correction_array + self.Scaling[1,1]
                if image_correction_source == &#39;ImageA&#39;:
                    flattened_image = self.ImageA*img_correction_array
                if image_correction_source == &#39;ImageB&#39;:
                    flattened_image = self.ImageB*img_correction_array
            else:
                if image_correction_source == &#39;RawImageA&#39;:
                    flattened_image = self.RawImageA
                if image_correction_source == &#39;RawImageB&#39;:
                    flattened_image = self.RawImageB
                if image_correction_source == &#39;ImageA&#39;:
                    flattened_image = self.ImageA
                if image_correction_source == &#39;ImageB&#39;:
                    flattened_image = self.ImageB
            flattened_images.append(flattened_image)

        return flattened_images</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.RawImageA_8bit_thresholds"><code class="name flex">
<span>def <span class="ident">RawImageA_8bit_thresholds</span></span>(<span>self, thr_min=0.001, thr_max=0.001, data_min=-1, data_max=-1, nbins=256)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert the Image A into 8-bit array</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>thr_min</code></strong> :&ensp;<code>float</code></dt>
<dd>lower CDF threshold for determining the minimum data value</dd>
<dt><strong><code>thr_max</code></strong> :&ensp;<code>float</code></dt>
<dd>upper CDF threshold for determining the maximum data value</dd>
<dt><strong><code>data_min</code></strong> :&ensp;<code>float</code></dt>
<dd>If different from data_max, this value will be used as low bound for I8 data conversion</dd>
<dt><strong><code>data_max</code></strong> :&ensp;<code>float</code></dt>
<dd>If different from data_min, this value will be used as high bound for I8 data conversion</dd>
<dt><strong><code>nbins</code></strong> :&ensp;<code>int</code></dt>
<dd>number of histogram bins for building the PDF and CDF</dd>
<dt><strong><code>Returns</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>dt</code></strong>, <strong><code>data_min</code></strong>, <strong><code>data_max</code></strong></dt>
<dd>dt : 2D uint8 array
Converted data
data_min : float
value used as low bound for I8 data conversion
data_max : float
value used as high bound for I8 data conversion</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def RawImageA_8bit_thresholds(self, thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
    &#39;&#39;&#39;
    Convert the Image A into 8-bit array

    Parameters
    ----------
    thr_min : float
        lower CDF threshold for determining the minimum data value
    thr_max : float
        upper CDF threshold for determining the maximum data value
    data_min : float
        If different from data_max, this value will be used as low bound for I8 data conversion
    data_max : float
        If different from data_min, this value will be used as high bound for I8 data conversion
    nbins : int
        number of histogram bins for building the PDF and CDF

    Returns
    dt, data_min, data_max
        dt : 2D uint8 array
            Converted data
        data_min : float
            value used as low bound for I8 data conversion
        data_max : float
            value used as high bound for I8 data conversion
    &#39;&#39;&#39;
    if self.EightBit==1:
        #print(&#39;8-bit image already - no need to convert&#39;)
        dt = self.RawImageA
    else:
        if data_min == data_max:
            data_min, data_max = self.get_image_min_max(image_name =&#39;RawImageA&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
        dt = ((np.clip(self.RawImageA, data_min, data_max) - data_min)/(data_max-data_min)*255.0).astype(np.uint8)
    return dt, data_min, data_max</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.RawImageB_8bit_thresholds"><code class="name flex">
<span>def <span class="ident">RawImageB_8bit_thresholds</span></span>(<span>self, thr_min=0.001, thr_max=0.001, data_min=-1, data_max=-1, nbins=256)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert the Image B into 8-bit array</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>thr_min</code></strong> :&ensp;<code>float</code></dt>
<dd>lower CDF threshold for determining the minimum data value</dd>
<dt><strong><code>thr_max</code></strong> :&ensp;<code>float</code></dt>
<dd>upper CDF threshold for determining the maximum data value</dd>
<dt><strong><code>data_min</code></strong> :&ensp;<code>float</code></dt>
<dd>If different from data_max, this value will be used as low bound for I8 data conversion</dd>
<dt><strong><code>data_max</code></strong> :&ensp;<code>float</code></dt>
<dd>If different from data_min, this value will be used as high bound for I8 data conversion</dd>
<dt><strong><code>nbins</code></strong> :&ensp;<code>int</code></dt>
<dd>number of histogram bins for building the PDF and CDF</dd>
<dt><strong><code>Returns</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>dt</code></strong>, <strong><code>data_min</code></strong>, <strong><code>data_max</code></strong></dt>
<dd>dt : 2D uint8 array
Converted data
data_min : float
value used as low bound for I8 data conversion
data_max : float
value used as high bound for I8 data conversion</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def RawImageB_8bit_thresholds(self, thr_min = 1.0e-3, thr_max = 1.0e-3, data_min = -1, data_max = -1, nbins=256):
    &#39;&#39;&#39;
    Convert the Image B into 8-bit array

    Parameters
    ----------
    thr_min : float
        lower CDF threshold for determining the minimum data value
    thr_max : float
        upper CDF threshold for determining the maximum data value
    data_min : float
        If different from data_max, this value will be used as low bound for I8 data conversion
    data_max : float
        If different from data_min, this value will be used as high bound for I8 data conversion
    nbins : int
        number of histogram bins for building the PDF and CDF

    Returns
    dt, data_min, data_max
        dt : 2D uint8 array
            Converted data
        data_min : float
            value used as low bound for I8 data conversion
        data_max : float
            value used as high bound for I8 data conversion
    &#39;&#39;&#39;
    if self.EightBit==1:
        #print(&#39;8-bit image already - no need to convert&#39;)
        dt = self.RawImageB
    else:
        if data_min == data_max:
            data_min, data_max = self.get_image_min_max(image_name =&#39;RawImageB&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
        dt = ((np.clip(self.RawImageB, data_min, data_max) - data_min)/(data_max-data_min)*255.0).astype(np.uint8)
    return dt, data_min, data_max</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.analyze_SNR_autocorr"><code class="name flex">
<span>def <span class="ident">analyze_SNR_autocorr</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimates SNR using auto-correlation analysis of a single image.
©G.Shtengel 04/2022 gleb.shtengel@gmail.com</p>
<p>Calculates SNR of a single image base on auto-correlation analysis of a single image, after [1].
Calls function Single_Image_SNR(img, **kwargs)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>kwargs:</dt>
<dt><strong><code>image_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Options are: 'RawImageA' (default), 'RawImageB', 'ImageA', 'ImageB'</dd>
<dt><strong><code>edge_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>fraction of the full autocetrrelation range used to calculate the "mean value" (default is 0.10)</dd>
<dt><strong><code>extrapolate_signal</code></strong> :&ensp;<code>boolean</code></dt>
<dd>extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True</dd>
<dt><strong><code>evaluation_box</code></strong> :&ensp;<code>list</code> of <code>4 int</code></dt>
<dd>evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.</dd>
<dt><strong><code>disp_res</code></strong> :&ensp;<code>boolean</code></dt>
<dd>display results (plots) (default is True)</dd>
<dt><strong><code>save_res_png</code></strong> :&ensp;<code>boolean</code></dt>
<dd>save the analysis output into a PNG file (default is True)</dd>
<dt><strong><code>res_fname</code></strong> :&ensp;<code>string</code></dt>
<dd>filename for the sesult image ('SNR_result.png')</dd>
<dt><strong><code>img_label</code></strong> :&ensp;<code>string</code></dt>
<dd>optional image label</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code></dt>
<dd>dots-per-inch resolution for the output image</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xSNR, ySNR </code></dt>
<dd>float, float
SNR determind using the method in [1] along X- and Y- directions.
If there is a direction with slow varying data - that direction provides more accurate SNR estimate
Y-streaks in typical FIB-SEM data provide slow varying Y-component becuase streaks
usually get increasingly worse with increasing Y.
So for typical FIB-SEM data use ySNR</dd>
</dl>
<p>[1] J. T. L. Thong et al, Single-image signal-tonoise ratio estimation. Scanning, 328–336 (2001).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def analyze_SNR_autocorr(self, **kwargs):
    &#39;&#39;&#39;
    Estimates SNR using auto-correlation analysis of a single image.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Calculates SNR of a single image base on auto-correlation analysis of a single image, after [1].
    Calls function Single_Image_SNR(img, **kwargs)

    Parameters
    ---------
    kwargs:
    image_name : str
        Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
    edge_fraction : float
        fraction of the full autocetrrelation range used to calculate the &#34;mean value&#34; (default is 0.10)
    extrapolate_signal : boolean
        extrapolate to find signal autocorrelationb at 0-point (without noise). Default is True
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration.
    disp_res : boolean
        display results (plots) (default is True)
    save_res_png : boolean
        save the analysis output into a PNG file (default is True)
    res_fname : string
        filename for the sesult image (&#39;SNR_result.png&#39;)
    img_label : string
        optional image label
    dpi : int
        dots-per-inch resolution for the output image

    Returns:
        xSNR, ySNR : float, float
            SNR determind using the method in [1] along X- and Y- directions.
            If there is a direction with slow varying data - that direction provides more accurate SNR estimate
            Y-streaks in typical FIB-SEM data provide slow varying Y-component becuase streaks
            usually get increasingly worse with increasing Y.
            So for typical FIB-SEM data use ySNR

    [1] J. T. L. Thong et al, Single-image signal-tonoise ratio estimation. Scanning, 328–336 (2001).
    &#39;&#39;&#39;
    image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    edge_fraction = kwargs.get(&#34;edge_fraction&#34;, 0.10)
    extrapolate_signal = kwargs.get(&#39;extrapolate_signal&#39;, True)
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
    default_res_name = os.path.splitext(self.fname)[0] + &#39;_AutoCorr_Noise_Analysis_&#39; + image_name + &#39;.png&#39;
    res_fname = kwargs.get(&#34;res_fname&#34;, default_res_name)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    SNR_kwargs = {&#39;edge_fraction&#39; : edge_fraction,
                    &#39;extrapolate_signal&#39; : extrapolate_signal,
                    &#39;disp_res&#39; : disp_res,
                    &#39;save_res_png&#39; : save_res_png,
                    &#39;res_fname&#39; : res_fname,
                    &#39;img_label&#39; : image_name,
                    &#39;dpi&#39; : dpi}

    if image_name == &#39;RawImageA&#39;:
        img = self.RawImageA
    if image_name == &#39;RawImageB&#39;:
        img = self.RawImageB
    if image_name == &#39;ImageA&#39;:
        img = self.ImageA
    if image_name == &#39;ImageB&#39;:
        img = self.ImageB

    xi = 0
    yi = 0
    ysz, xsz = img.shape
    xa = xi + xsz
    ya = yi + ysz
    xi_eval = xi + evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = xa
    yi_eval = yi + evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ya

    xSNR, ySNR, rSNR= Single_Image_SNR(img[yi_eval:ya_eval, xi_eval:xa_eval], **SNR_kwargs)

    return xSNR, ySNR, rSNR</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.analyze_noise_ROIs"><code class="name flex">
<span>def <span class="ident">analyze_noise_ROIs</span></span>(<span>self, Noise_ROIs, Hist_ROI, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyses the noise statistics in the selected ROI's of the EM data.
©G.Shtengel 04/2022 gleb.shtengel@gmail.com</p>
<p>Calls Single_Image_Noise_ROIs(img, Noise_ROIs, Hist_ROI, **kwargs)
Performs following:
1. For each of the selected ROI's, this method will perfrom the following:
1a. Smooth the data by 2D convolution with a given kernel.
1b. Determine "Noise" as difference between the original raw and smoothed data.
1c. Calculate the mean intensity value of the data and variance of the above "Noise"
2. Plot the dependence of the noise variance vs. image intensity.
3. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
it will be determined from the header data:
for RawImageA it is self.Scaling[1,0]
for RawImageB it is self.Scaling[1,1]
4. The equation is determined for a line that passes through the point:
Intensity=DarkCount and Noise Variance = 0
and is a best fit for the [Mean Intensity, Noise Variance] points
determined for each ROI (Step 1 above).
5. Another ROI (defined by Hist_ROI parameter) is used to built an
intensity histogram of the actual data. Peak of that histogram is determined.
6. The data is plotted. Two values of SNR are defined from the slope of the line in Step 4:
PSNR (Peak SNR) = Mean Intensity/sqrt(Noise Variance) at the intensity
at the histogram peak determined in the Step 5.
DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
where Max and Min Intensity are determined by corresponding cummulative
threshold parameters, and Noise Variance is taken at the intensity
in the middle of the range (Min Intensity + Max Intensity)/2.0</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>Noise_ROIs</code></strong> :&ensp;<code>list</code> of <code>lists: [[left, right, top, bottom]]</code></dt>
<dd>list of coordinates (indices) for each of the ROI's - the boundaries of the image subset to evaluate the noise.</dd>
<dt><strong><code>Hist_ROI</code></strong> :&ensp;<code>list [left, right, top, bottom]</code></dt>
<dd>coordinates (indices) of the boundaries of the image subset to evaluate the real data histogram.</dd>
<dt>kwargs:</dt>
<dt><strong><code>image_name</code></strong> :&ensp;<code>string</code></dt>
<dd>the name of the image to perform this operations (defaulut is 'RawImageA').</dd>
<dt><strong><code>DarkCount</code></strong> :&ensp;<code>float</code></dt>
<dd>the value of the Intensity Data at 0.</dd>
<dt><strong><code>kernel</code></strong> :&ensp;<code>2D float array</code></dt>
<dd>a kernel to perfrom 2D smoothing convolution.</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>filename - used for plotting the data. If not explicitly defined will use the instance attribute self.fname</dd>
<dt><strong><code>nbins_disp</code></strong> :&ensp;<code>int</code></dt>
<dd>(default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.</dd>
<dt><strong><code>thresholds_disp</code></strong> :&ensp;<code>list [thr_min_disp, thr_max_disp]</code></dt>
<dd>(default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.</dd>
<dt><strong><code>nbins_analysis</code></strong> :&ensp;<code>int</code></dt>
<dd>(default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.</dd>
<dt><strong><code>thresholds_analysis</code></strong> :&ensp;<code>list [thr_min_analysis, thr_max_analysis]</code></dt>
<dd>(default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.</dd>
<dt><strong><code>nbins_analysis</code></strong> :&ensp;<code>int</code></dt>
<dd>(default 256) number of histogram bins for building the data histogram in Step 5.</dd>
<dt><strong><code>disp_res</code></strong> :&ensp;<code>boolean</code></dt>
<dd>(default is False) - to plot/ display the results</dd>
<dt>Returns:</dt>
<dt><strong><code>mean_vals</code></strong>, <strong><code>var_vals</code></strong>, <strong><code>NF_slope</code></strong>, <strong><code>PSNR</code></strong>, <strong><code>MSNR</code></strong>, <strong><code>DSNR</code></strong></dt>
<dd>mean_vals and var_vals are the Mean Intensity and Noise Variance values for the Noise_ROIs (Step 1)
NF_slope is the slope of the linear fit curve (Step 4)
PSNR and DSNR are Peak and Dynamic SNR's (Step 6)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def analyze_noise_ROIs(self, Noise_ROIs, Hist_ROI, **kwargs):
    &#39;&#39;&#39;
    Analyses the noise statistics in the selected ROI&#39;s of the EM data.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Calls Single_Image_Noise_ROIs(img, Noise_ROIs, Hist_ROI, **kwargs)
    Performs following:
    1. For each of the selected ROI&#39;s, this method will perfrom the following:
        1a. Smooth the data by 2D convolution with a given kernel.
        1b. Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
        1c. Calculate the mean intensity value of the data and variance of the above &#34;Noise&#34;
    2. Plot the dependence of the noise variance vs. image intensity.
    3. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
        it will be determined from the header data:
            for RawImageA it is self.Scaling[1,0]
            for RawImageB it is self.Scaling[1,1]
    4. The equation is determined for a line that passes through the point:
            Intensity=DarkCount and Noise Variance = 0
            and is a best fit for the [Mean Intensity, Noise Variance] points
            determined for each ROI (Step 1 above).
    5. Another ROI (defined by Hist_ROI parameter) is used to built an
        intensity histogram of the actual data. Peak of that histogram is determined.
    6. The data is plotted. Two values of SNR are defined from the slope of the line in Step 4:
        PSNR (Peak SNR) = Mean Intensity/sqrt(Noise Variance) at the intensity
            at the histogram peak determined in the Step 5.
        DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
            where Max and Min Intensity are determined by corresponding cummulative
            threshold parameters, and Noise Variance is taken at the intensity
            in the middle of the range (Min Intensity + Max Intensity)/2.0

    Parameters
    ----------
    Noise_ROIs : list of lists: [[left, right, top, bottom]]
        list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the noise.
    Hist_ROI : list [left, right, top, bottom]
        coordinates (indices) of the boundaries of the image subset to evaluate the real data histogram.

    kwargs:
    image_name : string
        the name of the image to perform this operations (defaulut is &#39;RawImageA&#39;).
    DarkCount : float
        the value of the Intensity Data at 0.
    kernel : 2D float array
        a kernel to perfrom 2D smoothing convolution.
    filename : str
        filename - used for plotting the data. If not explicitly defined will use the instance attribute self.fname
    nbins_disp : int
        (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
    thresholds_disp : list [thr_min_disp, thr_max_disp]
        (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
    nbins_analysis : int
        (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
    thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
        (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
    nbins_analysis : int
         (default 256) number of histogram bins for building the data histogram in Step 5.
    disp_res : boolean
        (default is False) - to plot/ display the results

    Returns:
    mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR
        mean_vals and var_vals are the Mean Intensity and Noise Variance values for the Noise_ROIs (Step 1)
        NF_slope is the slope of the linear fit curve (Step 4)
        PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 6)
    &#39;&#39;&#39;
    image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)

    if image_name == &#39;RawImageA&#39;:
        ImgEM = self.RawImageA.astype(float)
        DarkCount = self.Scaling[1,0]
    if image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;:
        ImgEM = self.RawImageB.astype(float)
        DarkCount = self.Scaling[1,1]

    if (image_name == &#39;RawImageA&#39;) or (image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;):
        st = 1.0/np.sqrt(2.0)
        def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
        def_kernel = def_kernel/def_kernel.sum()
        kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
        DarkCount = kwargs.get(&#34;DarkCount&#34;, DarkCount)
        nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
        thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
        nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
        thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
        Notes = kwargs.get(&#34;Notes&#34;, self.Notes.strip(&#39;\x00&#39;))
        kwargs[&#39;kernel&#39;] = kernel
        kwargs[&#39;DarkCount&#39;] = DarkCount
        kwargs[&#39;img_label&#39;] = image_name
        kwargs[&#39;res_fname&#39;] = os.path.splitext(self.fname)[0] + &#39;_&#39; + image_name + &#39;_Noise_Analysis_ROIs.png&#39;
        kwargs[&#39;Notes&#39;] = Notes
        mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR = Single_Image_Noise_ROIs(ImgEM, Noise_ROIs, Hist_ROI, **kwargs)

    else:
        print(&#39;No valid image name selected&#39;)
        mean_vals = 0.0
        var_vals = 0.0
        NF_slope = 0.0
        PSNR = 0.0
        MSNR = 0.0
        DSNR = 0.0

    return mean_vals, var_vals, NF_slope, PSNR, MSNR, DSNR</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.analyze_noise_statistics"><code class="name flex">
<span>def <span class="ident">analyze_noise_statistics</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyses the noise statistics of the EM data image.
©G.Shtengel 04/2022 gleb.shtengel@gmail.com</p>
<p>Calls Single_Image_Noise_Statistics(img, **kwargs)
Performs following:
1. Smooth the image by 2D convolution with a given kernel.
2. Determine "Noise" as difference between the original raw and smoothed data.
3. Build a histogram of Smoothed Image.
4. For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
5. Plot the dependence of the noise variance vs. image intensity.
6. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
it will be set to 0
7. The equation is determined for a line that passes through the point:
Intensity=DarkCount and Noise Variance = 0
and is a best fit for the [Mean Intensity, Noise Variance] points
determined for each ROI (Step 1 above).
8. The data is plotted. Two values of SNR are defined from the slope of the line in Step 7:
PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity
at the histogram peak determined in the Step 3.
MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
where Max and Min Intensity are determined by corresponding cummulative
threshold parameters, and Noise Variance is taken at the intensity
in the middle of the range (Min Intensity + Max Intensity)/2.0</p>
<h2 id="parameters">Parameters</h2>
<pre><code>kwargs:
image_name : str
    Options are: 'RawImageA' (default), 'RawImageB', 'ImageA', 'ImageB'
evaluation_box : list of 4 int
    evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
    if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
DarkCount : float
    the value of the Intensity Data at 0.
kernel : 2D float array
    a kernel to perfrom 2D smoothing convolution.
nbins_disp : int
    (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
thresholds_disp : list [thr_min_disp, thr_max_disp]
    (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
nbins_analysis : int
    (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
    (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
nbins_analysis : int
     (default 256) number of histogram bins for building the data histogram in Step 5.
disp_res : boolean
    (default is False) - to plot/ display the results
save_res_png : boolean
    save the analysis output into a PNG file (default is True)
res_fname : string
    filename for the result image ('Noise_Analysis.png')
img_label : string
    optional image label
Notes : string
    optional additional notes
dpi : int
</code></pre>
<dl>
<dt>Returns:</dt>
<dt><strong><code>mean_vals</code></strong>, <strong><code>var_vals</code></strong>, <strong><code>I0</code></strong>, <strong><code>PSNR</code></strong>, <strong><code>DSNR</code></strong>, <strong><code>popt</code></strong>, <strong><code>result</code></strong></dt>
<dd>mean_vals and var_vals are the Mean Intensity and Noise Variance values for Step 5
I0 is zero intercept (should be close to DarkCount)
PSNR and DSNR are Peak and Dynamic SNR's (Step 8)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def analyze_noise_statistics(self, **kwargs):
    &#39;&#39;&#39;
    Analyses the noise statistics of the EM data image.
    ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Calls Single_Image_Noise_Statistics(img, **kwargs)
    Performs following:
    1. Smooth the image by 2D convolution with a given kernel.
    2. Determine &#34;Noise&#34; as difference between the original raw and smoothed data.
    3. Build a histogram of Smoothed Image.
    4. For each histogram bin of the Smoothed Image (Step 3), calculate the mean value and variance for the same pixels in the original image.
    5. Plot the dependence of the noise variance vs. image intensity.
    6. One of the parameters is a DarkCount. If it is not explicitly defined as input parameter,
        it will be set to 0
    7. The equation is determined for a line that passes through the point:
            Intensity=DarkCount and Noise Variance = 0
            and is a best fit for the [Mean Intensity, Noise Variance] points
            determined for each ROI (Step 1 above).
    8. The data is plotted. Two values of SNR are defined from the slope of the line in Step 7:
        PSNR (Peak SNR) = Intensity /sqrt(Noise Variance) at the intensity
            at the histogram peak determined in the Step 3.
        MSNR (Mean SNR) = Mean Intensity /sqrt(Noise Variance)
        DSNR (Dynamic SNR) = (Max Intensity - Min Intensity) / sqrt(Noise Variance),
            where Max and Min Intensity are determined by corresponding cummulative
            threshold parameters, and Noise Variance is taken at the intensity
            in the middle of the range (Min Intensity + Max Intensity)/2.0

    Parameters
    ----------
        kwargs:
        image_name : str
            Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
        evaluation_box : list of 4 int
            evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
            if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
        DarkCount : float
            the value of the Intensity Data at 0.
        kernel : 2D float array
            a kernel to perfrom 2D smoothing convolution.
        nbins_disp : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for data display.
        thresholds_disp : list [thr_min_disp, thr_max_disp]
            (default [1e-3, 1e-3]) CDF threshold for determining the min and max data values for display.
        nbins_analysis : int
            (default 256) number of histogram bins for building the PDF and CDF to determine the data range for building the data histogram in Step 5.
        thresholds_analysis: list [thr_min_analysis, thr_max_analysis]
            (default [2e-2, 2e-2]) CDF threshold for building the data histogram in Step 5.
        nbins_analysis : int
             (default 256) number of histogram bins for building the data histogram in Step 5.
        disp_res : boolean
            (default is False) - to plot/ display the results
        save_res_png : boolean
            save the analysis output into a PNG file (default is True)
        res_fname : string
            filename for the result image (&#39;Noise_Analysis.png&#39;)
        img_label : string
            optional image label
        Notes : string
            optional additional notes
        dpi : int

    Returns:
    mean_vals, var_vals, I0, PSNR, DSNR, popt, result
        mean_vals and var_vals are the Mean Intensity and Noise Variance values for Step 5
        I0 is zero intercept (should be close to DarkCount)
        PSNR and DSNR are Peak and Dynamic SNR&#39;s (Step 8)
    &#39;&#39;&#39;
    image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)

    if image_name == &#39;RawImageA&#39;:
        ImgEM = self.RawImageA.astype(float)
        DarkCount = self.Scaling[1,0]
    if image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;:
        ImgEM = self.RawImageB.astype(float)
        DarkCount = self.Scaling[1,1]

    if (image_name == &#39;RawImageA&#39;) or (image_name == &#39;RawImageB&#39; and self.DetB != &#39;None&#39;):
        st = 1.0/np.sqrt(2.0)
        evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
        def_kernel = np.array([[st, 1.0, st],[1.0,1.0,1.0], [st, 1.0, st]]).astype(float)
        def_kernel = def_kernel/def_kernel.sum()
        kernel = kwargs.get(&#34;kernel&#34;, def_kernel)
        DarkCount = kwargs.get(&#34;DarkCount&#34;, DarkCount)
        nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
        thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
        nbins_analysis = kwargs.get(&#34;nbins_analysis&#34;, 100)
        thresholds_analysis = kwargs.get(&#34;thresholds_analysis&#34;, [2e-2, 1e-2])
        disp_res = kwargs.get(&#34;disp_res&#34;, True)
        save_res_png = kwargs.get(&#34;save_res_png&#34;, True)
        default_res_name = os.path.splitext(self.fname)[0] + &#39;_Noise_Analysis_&#39; + image_name + &#39;.png&#39;
        res_fname = kwargs.get(&#34;res_fname&#34;, default_res_name)
        img_label = kwargs.get(&#34;img_label&#34;, self.Sample_ID)
        Notes = kwargs.get(&#34;Notes&#34;, self.Notes.strip(&#39;\x00&#39;))
        dpi = kwargs.get(&#34;dpi&#34;, 300)

        noise_kwargs = {&#39;image_name&#39; : image_name,
                        &#39;evaluation_box&#39; : evaluation_box,
                        &#39;kernel&#39; : kernel,
                        &#39;DarkCount&#39; : DarkCount,
                        &#39;nbins_disp&#39; : nbins_disp,
                        &#39;thresholds_disp&#39; : thresholds_disp,
                        &#39;nbins_analysis&#39; : nbins_analysis,
                        &#39;thresholds_analysis&#39; : thresholds_analysis,
                        &#39;disp_res&#39; : disp_res,
                        &#39;save_res_png&#39; : save_res_png,
                        &#39;res_fname&#39; : res_fname,
                        &#39;Notes&#39; : Notes,
                        &#39;dpi&#39; : dpi}

        mean_vals, var_vals, I0, PSNR, DSNR, popt, result =  Single_Image_Noise_Statistics(ImgEM, **noise_kwargs)
    else:
        mean_vals, var_vals, I0, PSNR, DSNR, popt, result = [], [], 0.0, 0.0, np.array((0.0, 0.0)), []
    return mean_vals, var_vals, I0, PSNR, DSNR, popt, result</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.determine_field_fattening_parameters"><code class="name flex">
<span>def <span class="ident">determine_field_fattening_parameters</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Perfrom 2D parabolic fit (calls Perform_2D_fit(Img, estimator, **kwargs)) and determine the field-flattening parameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>kwargs:</dt>
<dt><strong><code>image_names</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Options are: 'RawImageA' (default), 'RawImageB', 'ImageA', 'ImageB'</dd>
<dt><strong><code>estimator</code></strong> :&ensp;<code>RANSACRegressor(),</code></dt>
<dd>LinearRegression(),
TheilSenRegressor(),
HuberRegressor()</dd>
<dt><strong><code>bins</code></strong> :&ensp;<code>int</code></dt>
<dd>binsize for image binning. If not provided, bins=10</dd>
<dt><strong><code>Analysis_ROIs</code></strong> :&ensp;<code>list</code> of <code>lists: [[left, right, top, bottom]]</code></dt>
<dd>list of coordinates (indices) for each of the ROI's - the boundaries of the image subset to evaluate the parabolic fit.</dd>
<dt><strong><code>calc_corr</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True - the full image correction is calculated</dd>
<dt>ignore_Y
: boolean</dt>
<dt>If True - the parabolic fit to only X is perfromed</dt>
<dt><strong><code>Xsect</code></strong> :&ensp;<code>int</code></dt>
<dd>X - coordinate for Y-crossection</dd>
<dt><strong><code>Ysect</code></strong> :&ensp;<code>int</code></dt>
<dd>Y - coordinate for X-crossection</dd>
<dt><strong><code>disp_res</code></strong> :&ensp;<code>boolean</code></dt>
<dd>(default is False) - to plot/ display the results</dd>
<dt><strong><code>save_res_png</code></strong> :&ensp;<code>boolean</code></dt>
<dd>save the analysis output into a PNG file (default is False)</dd>
<dt>save_correction_binary = boolean</dt>
<dt>save the mage)name and img_correction_array data into a binary file</dt>
<dt><strong><code>res_fname</code></strong> :&ensp;<code>string</code></dt>
<dd>filename for the result image ('<strong>_Image_Flattening.png'). The binary image is derived from the same root, e.g. '</strong>_Image_Flattening.bin'</dd>
<dt><strong><code>label</code></strong> :&ensp;<code>string</code></dt>
<dd>optional image label</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt>Returns:</dt>
<dt><strong><code>img_correction_coeffs</code></strong>, <strong><code>img_correction_arrays</code></strong></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_field_fattening_parameters(self, **kwargs):
    &#39;&#39;&#39;
    Perfrom 2D parabolic fit (calls Perform_2D_fit(Img, estimator, **kwargs)) and determine the field-flattening parameters

    Parameters
    ----------
    kwargs:
    image_names : list of str
        Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
    estimator : RANSACRegressor(),
                LinearRegression(),
                TheilSenRegressor(),
                HuberRegressor()
    bins : int
        binsize for image binning. If not provided, bins=10
    Analysis_ROIs : list of lists: [[left, right, top, bottom]]
        list of coordinates (indices) for each of the ROI&#39;s - the boundaries of the image subset to evaluate the parabolic fit.
    calc_corr : boolean
        If True - the full image correction is calculated
    ignore_Y  : boolean
        If True - the parabolic fit to only X is perfromed
    Xsect : int
        X - coordinate for Y-crossection
    Ysect : int
        Y - coordinate for X-crossection
    disp_res : boolean
        (default is False) - to plot/ display the results
    save_res_png : boolean
        save the analysis output into a PNG file (default is False)
    save_correction_binary = boolean
        save the mage)name and img_correction_array data into a binary file
    res_fname : string
        filename for the result image (&#39;**_Image_Flattening.png&#39;). The binary image is derived from the same root, e.g. &#39;**_Image_Flattening.bin&#39;
    label : string
        optional image label
    dpi : int

    Returns:
    img_correction_coeffs, img_correction_arrays
    &#39;&#39;&#39;
    image_names = kwargs.get(&#34;image_names&#34;, [&#39;RawImageA&#39;])
    estimator = kwargs.get(&#34;estimator&#34;, LinearRegression())
    if &#34;estimator&#34; in kwargs:
        del kwargs[&#34;estimator&#34;]
    calc_corr = kwargs.get(&#34;calc_corr&#34;, False)
    ignore_Y = kwargs.get(&#34;ignore_Y&#34;, False)
    lbl = kwargs.get(&#34;label&#34;, &#39;&#39;)
    disp_res = kwargs.get(&#34;disp_res&#34;, True)
    bins = kwargs.get(&#34;bins&#34;, 10) #bins = 10
    Analysis_ROIs = kwargs.get(&#34;Analysis_ROIs&#34;, [])
    save_res_png = kwargs.get(&#34;save_res_png&#34;, False)
    res_fname = kwargs.get(&#34;res_fname&#34;, os.path.splitext(self.fname)[0]+&#39;_Image_Flattening.png&#39;)
    save_correction_binary = kwargs.get(&#34;save_correction_binary&#34;, False)
    dpi = kwargs.get(&#34;dpi&#34;, 300)

    img_correction_arrays = []
    img_correction_coeffs = []
    for image_name in image_names:
        if image_name == &#39;RawImageA&#39;:
            img = self.RawImageA - self.Scaling[1,0]
        if image_name == &#39;RawImageB&#39;:
            img = self.RawImageB - self.Scaling[1,1]
        if image_name == &#39;ImageA&#39;:
            img = self.ImageA
        if image_name == &#39;ImageB&#39;:
            img = self.ImageB

        ysz, xsz = img.shape
        Xsect = kwargs.get(&#34;Xsect&#34;, xsz//2)
        Ysect = kwargs.get(&#34;Ysect&#34;, ysz//2)

        intercept, coefs, mse, img_correction_array = Perform_2D_fit(img, estimator, image_name=image_name, **kwargs)
        img_correction_arrays.append(img_correction_array)
        img_correction_coeffs.append(coefs)

    if calc_corr:
        self.image_correction_sources = image_names
        self.img_correction_arrays = img_correction_arrays
        if save_correction_binary:
            bin_fname = res_fname.replace(&#39;png&#39;, &#39;bin&#39;)
            pickle.dump([image_names, img_correction_arrays], open(bin_fname, &#39;wb&#39;)) # saves source name and correction array into the binary file
            self.image_correction_file = res_fname.replace(&#39;png&#39;, &#39;bin&#39;)
            print(&#39;Image Flattening Info saved into the binary file: &#39;, self.image_correction_file)
    #self.intercept = intercept
    self.img_correction_coeffs = img_correction_coeffs
    return intercept, img_correction_coeffs, img_correction_arrays</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.display_images"><code class="name flex">
<span>def <span class="ident">display_images</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Display auto-scaled detector images without saving the figure into the file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def display_images(self):
    &#39;&#39;&#39;
    Display auto-scaled detector images without saving the figure into the file.

    &#39;&#39;&#39;
    fig, axs = subplots(2, 1, figsize=(10,5))
    axs[0].imshow(self.RawImageA, cmap=&#39;Greys&#39;)
    axs[1].imshow(self.RawImageB, cmap=&#39;Greys&#39;)
    ttls = [&#39;Detector A: &#39;+self.DetA.strip(&#39;\x00&#39;), &#39;Detector B: &#39;+self.DetB.strip(&#39;\x00&#39;)]
    for ax, ttl in zip(axs, ttls):
        ax.axis(False)
        ax.set_title(ttl, fontsize=10)
    fig.suptitle(self.fname)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.flatten_image"><code class="name flex">
<span>def <span class="ident">flatten_image</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Flatten the image(s). Image flattening parameters must be determined (determine_field_fattening_parameters)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>kwargs:</dt>
<dt><strong><code>image_correction_file</code></strong> :&ensp;<code>str</code></dt>
<dd>full path to a binary filename that contains source names (image_correction_sources) and correction arrays (img_correction_arrays)
if image_correction_file exists, the data is loaded from it.</dd>
<dt><strong><code>image_correction_sources</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Options are: 'RawImageA' (default), 'RawImageB', 'ImageA', 'ImageB'</dd>
<dt><strong><code>img_correction_arrays</code></strong> :&ensp;<code>list</code> of <code>2D arrays</code></dt>
<dd>arrays containing field flatteting info</dd>
<dt>Returns:</dt>
<dt><strong><code>flattened_images</code></strong> :&ensp;<code>list</code> of <code>2D arrays</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten_image(self, **kwargs):
    &#39;&#39;&#39;
    Flatten the image(s). Image flattening parameters must be determined (determine_field_fattening_parameters)

    Parameters
    ----------
    kwargs:
    image_correction_file : str
        full path to a binary filename that contains source names (image_correction_sources) and correction arrays (img_correction_arrays)
        if image_correction_file exists, the data is loaded from it.
    image_correction_sources : list of str
        Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
    img_correction_arrays : list of 2D arrays
        arrays containing field flatteting info

    Returns:
    flattened_images : list of 2D arrays
    &#39;&#39;&#39;

    if hasattr(self, &#39;image_correction_file&#39;):
        image_correction_file = kwargs.get(&#34;image_correction_file&#34;, self.image_correction_file)
    else:
        image_correction_file = kwargs.get(&#34;image_correction_file&#34;, &#39;&#39;)

    try:
        # try loading the image correction data from the binary file
        with open(image_correction_file, &#34;rb&#34;) as f:
            [image_correction_sources,  img_correction_arrays] = pickle.load(f)
    except:
        #  if that did not work, see if the correction data was provided directly
        if hasattr(self, &#39;image_correction_source&#39;):
            image_correction_sources = kwargs.get(&#34;image_correction_sources&#34;, self.image_correction_sources)
        else:
            image_correction_sources = kwargs.get(&#34;image_correction_sources&#34;, [False])

        if hasattr(self, &#39;img_correction_arrays&#39;):
            img_correction_arrays = kwargs.get(&#34;img_correction_arrays&#34;, self.img_correction_arrays)
        else:
            img_correction_arrays = kwargs.get(&#34;img_correction_arrays&#34;, [False])

    flattened_images = []
    for image_correction_source, img_correction_array in zip(image_correction_sources, img_correction_arrays):
        if (image_correction_source is not False) and (img_correction_array is not False):
            if image_correction_source == &#39;RawImageA&#39;:
                flattened_image = (self.RawImageA - self.Scaling[1,0])*img_correction_array + self.Scaling[1,0]
            if image_correction_source == &#39;RawImageB&#39;:
                flattened_image = (self.RawImageB - self.Scaling[1,1])*img_correction_array + self.Scaling[1,1]
            if image_correction_source == &#39;ImageA&#39;:
                flattened_image = self.ImageA*img_correction_array
            if image_correction_source == &#39;ImageB&#39;:
                flattened_image = self.ImageB*img_correction_array
        else:
            if image_correction_source == &#39;RawImageA&#39;:
                flattened_image = self.RawImageA
            if image_correction_source == &#39;RawImageB&#39;:
                flattened_image = self.RawImageB
            if image_correction_source == &#39;ImageA&#39;:
                flattened_image = self.ImageA
            if image_correction_source == &#39;ImageB&#39;:
                flattened_image = self.ImageB
        flattened_images.append(flattened_image)

    return flattened_images</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.get_image_min_max"><code class="name flex">
<span>def <span class="ident">get_image_min_max</span></span>(<span>self, image_name='ImageA', thr_min=0.0001, thr_max=0.001, nbins=256, disp_res=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the data range of the EM data. ©G.Shtengel 04/2022 gleb.shtengel@gmail.com</p>
<p>Calculates histogram of pixel intensities of of the loaded image
with number of bins determined by parameter nbins (default = 256)
and normalizes it to get the probability distribution function (PDF),
from which a cumulative distribution function (CDF) is calculated.
Then given the threshold_min, threshold_max parameters,
the minimum and maximum values for the image are found by finding
the intensities at which CDF= threshold_min and (1- threshold_max), respectively.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>image_name</code></strong> :&ensp;<code>string</code></dt>
<dd>the name of the image to perform this operations (defaulut is 'RawImageA')</dd>
<dt><strong><code>threshold_min</code></strong> :&ensp;<code>float</code></dt>
<dd>CDF threshold for determining the minimum data value</dd>
<dt><strong><code>threshold_max</code></strong> :&ensp;<code>float</code></dt>
<dd>CDF threshold for determining the maximum data value</dd>
<dt><strong><code>nbins</code></strong> :&ensp;<code>int</code></dt>
<dd>number of histogram bins for building the PDF and CDF</dd>
<dt><strong><code>disp_res</code></strong> :&ensp;<code>boolean</code></dt>
<dd>(default is False) - to plot/ display the results</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dmin, dmax</code></dt>
<dd>(float) minimum and maximum values of the data range.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_image_min_max(self, image_name = &#39;ImageA&#39;, thr_min = 1.0e-4, thr_max = 1.0e-3, nbins=256, disp_res = False):
    &#39;&#39;&#39;
    Calculates the data range of the EM data. ©G.Shtengel 04/2022 gleb.shtengel@gmail.com

    Calculates histogram of pixel intensities of of the loaded image
    with number of bins determined by parameter nbins (default = 256)
    and normalizes it to get the probability distribution function (PDF),
    from which a cumulative distribution function (CDF) is calculated.
    Then given the threshold_min, threshold_max parameters,
    the minimum and maximum values for the image are found by finding
    the intensities at which CDF= threshold_min and (1- threshold_max), respectively.

    Parameters
    ----------
    image_name : string
        the name of the image to perform this operations (defaulut is &#39;RawImageA&#39;)
    threshold_min : float
        CDF threshold for determining the minimum data value
    threshold_max : float
        CDF threshold for determining the maximum data value
    nbins : int
        number of histogram bins for building the PDF and CDF
    disp_res : boolean
        (default is False) - to plot/ display the results

    Returns:
        dmin, dmax: (float) minimum and maximum values of the data range.
    &#39;&#39;&#39;
    if image_name == &#39;ImageA&#39;:
        im = self.ImageA
    if image_name == &#39;ImageB&#39;:
        im = self.ImageB
    if image_name == &#39;RawImageA&#39;:
        im = self.RawImageA
    if image_name == &#39;RawImageB&#39;:
        im = self.RawImageB
    return get_min_max_thresholds(im, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=disp_res)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.print_header"><code class="name flex">
<span>def <span class="ident">print_header</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Prints a formatted content of the file header</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_header(self):
    &#39;&#39;&#39;
    Prints a formatted content of the file header

    &#39;&#39;&#39;
    if self.FileVersion == -1 :
        print(&#39;Sample_ID=&#39;, self.Sample_ID)
        print(&#39;DetA=&#39;, self.DetA)
        print(&#39;DetB=&#39;, self.DetB)
        print(&#39;EightBit=&#39;, self.EightBit)
        print(&#39;XResolution=&#39;, self.XResolution)
        print(&#39;YResolution=&#39;, self.YResolution)
        print(&#39;PixelSize=&#39;, self.PixelSize)
    else:
        print(&#39;FileMagicNum=&#39;, self.FileMagicNum)
        print(&#39;FileVersion=&#39;, self.FileVersion)
        print(&#39;FileType=&#39;, self.FileType)
        print(&#39;SWdate=&#39;, self.SWdate)
        print(&#39;TimeStep=&#39;, self.TimeStep)
        print(&#39;ChanNum=&#39;, self.ChanNum)
        print(&#39;EightBit=&#39;, self.EightBit)
        print(&#39;Scaling=&#39;, self.Scaling)
        if self.FileVersion &gt; 8 :
            print(&#39;RestartFlag=&#39;, self.RestartFlag)
            print(&#39;StageMove=&#39;, self.StageMove)
            print(&#39;FirstPixelX=&#39;, self.FirstPixelX)
            print(&#39;FirstPixelY=&#39;, self.FirstPixelY)
        print(&#39;XResolution=&#39;, self.XResolution)
        print(&#39;YResolution=&#39;, self.YResolution)
        if self.FileVersion == 1 or self.FileVersion == 2 or self.FileVersion == 3:
            print(&#39;AIDelay=&#39;, self.AIDelay)
        print(&#39;Oversampling=&#39;, self.Oversampling)
        print(&#39;ZeissScanSpeed=&#39;, self.ZeissScanSpeed)
        print(&#39;DecimatingFactor=&#39;, self.DecimatingFactor)
        print(&#39;ScanRate=&#39;, self.ScanRate)
        print(&#39;FramelineRampdownRatio=&#39;, self.FramelineRampdownRatio)
        print(&#39;Xmin=&#39;, self.Xmin)
        print(&#39;Xmax=&#39;, self.Xmax)
        print(&#39;Detmin=&#39;, self.Detmin)
        print(&#39;Detmax=&#39;, self.Detmax)
        print(&#39;AI1=&#39;, self.AI1)
        print(&#39;AI2=&#39;, self.AI2)
        print(&#39;AI3=&#39;, self.AI3)
        print(&#39;AI4=&#39;, self.AI4)
        if self.FileVersion &gt; 8 :
             print(&#39;Sample_ID=&#39;, self.Sample_ID)
        print(&#39;Notes=&#39;, self.Notes)
        print(&#39;SEMShiftX=&#39;, self.SEMShiftX)
        print(&#39;SEMShiftY=&#39;, self.SEMShiftY)
        print(&#39;DetA=&#39;, self.DetA)
        print(&#39;DetB=&#39;, self.DetB)
        print(&#39;DetC=&#39;, self.DetC)
        print(&#39;DetD=&#39;, self.DetD)
        print(&#39;Mag=&#39;, self.Mag)
        print(&#39;PixelSize=&#39;, self.PixelSize)
        print(&#39;WD=&#39;, self.WD)
        print(&#39;EHT=&#39;, self.EHT)
        print(&#39;SEMApr=&#39;, self.SEMApr)
        print(&#39;HighCurrent=&#39;, self.HighCurrent)
        print(&#39;SEMCurr=&#39;, self.SEMCurr)
        print(&#39;SEMRot=&#39;, self.SEMRot)
        print(&#39;ChamVac=&#39;, self.ChamVac)
        print(&#39;GunVac=&#39;, self.GunVac)
        print(&#39;SEMStiX=&#39;, self.SEMStiX)
        print(&#39;SEMStiY=&#39;, self.SEMStiY)
        print(&#39;SEMAlnX=&#39;, self.SEMAlnX)
        print(&#39;SEMAlnY=&#39;, self.SEMAlnY)
        print(&#39;StageX=&#39;, self.StageX)
        print(&#39;StageY=&#39;, self.StageY)
        print(&#39;StageZ=&#39;, self.StageZ)
        print(&#39;StageT=&#39;, self.StageT)
        print(&#39;StageR=&#39;, self.StageR)
        print(&#39;StageM=&#39;, self.StageM)
        print(&#39;BrightnessA=&#39;, self.BrightnessA)
        print(&#39;ContrastA=&#39;, self.ContrastA)
        print(&#39;BrightnessB=&#39;, self.BrightnessB)
        print(&#39;ContrastB=&#39;, self.ContrastB)
        print(&#39;Mode=&#39;, self.Mode)
        print(&#39;FIBFocus=&#39;, self.FIBFocus)
        print(&#39;FIBProb=&#39;, self.FIBProb)
        print(&#39;FIBCurr=&#39;, self.FIBCurr)
        print(&#39;FIBRot=&#39;, self.FIBRot)
        print(&#39;FIBAlnX=&#39;, self.FIBAlnX)
        print(&#39;FIBAlnY=&#39;, self.FIBAlnY)
        print(&#39;FIBStiX=&#39;, self.FIBStiX)
        print(&#39;FIBStiY=&#39;, self.FIBStiY)
        print(&#39;FIBShiftX=&#39;, self.FIBShiftX)
        print(&#39;FIBShiftY=&#39;, self.FIBShiftY)
        if self.FileVersion &gt; 4:
            print(&#39;MillingXResolution=&#39;, self.MillingXResolution)
            print(&#39;MillingYResolution=&#39;, self.MillingYResolution)
            print(&#39;MillingXSize=&#39;, self.MillingXSize)
            print(&#39;MillingYSize=&#39;, self.MillingYSize)
            print(&#39;MillingULAng=&#39;, self.MillingULAng)
            print(&#39;MillingURAng=&#39;, self.MillingURAng)
            print(&#39;MillingLineTime=&#39;, self.MillingLineTime)
            print(&#39;FIBFOV (um)=&#39;, self.FIBFOV)
            print(&#39;MillingPIDOn=&#39;, self.MillingPIDOn)
            print(&#39;MillingPIDMeasured=&#39;, self.MillingPIDMeasured)
            print(&#39;MillingPIDTarget=&#39;, self.MillingPIDTarget)
            print(&#39;MillingPIDTargetSlope=&#39;, self.MillingPIDTargetSlope)
            print(&#39;MillingPIDP=&#39;, self.MillingPIDP)
            print(&#39;MillingPIDI=&#39;, self.MillingPIDI)
            print(&#39;MillingPIDD=&#39;, self.MillingPIDD)
            print(&#39;MachineID=&#39;, self.MachineID)
            print(&#39;SEMSpecimenI=&#39;, self.SEMSpecimenI)
        if self.FileVersion &gt; 5:
            print(&#39;Temperature=&#39;, self.Temperature)
            print(&#39;FaradayCupI=&#39;, self.FaradayCupI)
            print(&#39;FIBSpecimenI=&#39;, self.FIBSpecimenI)
            print(&#39;BeamDump1I=&#39;, self.BeamDump1I)
            print(&#39;MillingYVoltage=&#39;, self.MillingYVoltage)
            print(&#39;FocusIndex=&#39;, self.FocusIndex)
            print(&#39;FIBSliceNum=&#39;, self.FIBSliceNum)
        if self.FileVersion &gt; 7:
            print(&#39;BeamDump2I=&#39;, self.BeamDump2I)
            print(&#39;MillingI=&#39;, self.MillingI)
        print(&#39;SEMSpecimenI=&#39;, self.SEMSpecimenI)
        print(&#39;FileLength=&#39;, self.FileLength)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.save_images_jpeg"><code class="name flex">
<span>def <span class="ident">save_images_jpeg</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Display auto-scaled detector images and save the figure into JPEG file (s).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>kwargs:</dt>
<dt><strong><code>images_to_save</code></strong> :&ensp;<code>str</code></dt>
<dd>Images to save. options are: 'A', 'B', or 'Both' (default).</dd>
<dt><strong><code>invert</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, the image will be inverted.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_images_jpeg(self, **kwargs):
    &#39;&#39;&#39;
    Display auto-scaled detector images and save the figure into JPEG file (s).

    Parameters
    ----------
    kwargs:
    images_to_save : str
        Images to save. options are: &#39;A&#39;, &#39;B&#39;, or &#39;Both&#39; (default).
    invert : boolean
        If True, the image will be inverted.

    &#39;&#39;&#39;
    images_to_save = kwargs.get(&#34;images_to_save&#34;, &#39;Both&#39;)
    invert = kwargs.get(&#34;invert&#34;, False)

    if images_to_save == &#39;Both&#39; or images_to_save == &#39;A&#39;:
        if self.ftype == 0:
            fname_jpg = os.path.splitext(self.fname)[0] + &#39;_&#39; + self.DetA.strip(&#39;\x00&#39;) + &#39;.jpg&#39;
        else:
            fname_jpg = os.path.splitext(self.fname)[0] + &#39;DetA.jpg&#39;
        Img = self.RawImageA_8bit_thresholds()[0]
        if invert:
            Img =  uint8(255) - Img
        PILImage.fromarray(Img).save(fname_jpg)

    try:
        if images_to_save == &#39;Both&#39; or images_to_save == &#39;B&#39;:
            if self.ftype == 0:
                fname_jpg = os.path.splitext(self.fname)[0] +  &#39;_&#39; + self.DetB.strip(&#39;\x00&#39;) + &#39;.jpg&#39;
            else:
                fname_jpg = os.path.splitext(self.fname)[0] + &#39;DetB.jpg&#39;
            Img = self.RawImageB_8bit_thresholds()[0]
            if invert:
                Img =  uint8(255) - Img
            PILImage.fromarray(Img).save(fname_jpg)
    except:
        print(&#39;No Detector B image to save&#39;)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.save_images_tif"><code class="name flex">
<span>def <span class="ident">save_images_tif</span></span>(<span>self, images_to_save='Both')</span>
</code></dt>
<dd>
<div class="desc"><p>Save the detector images into TIF file (s).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>images_to_save</code></strong> :&ensp;<code>str</code></dt>
<dd>Images to save. options are: 'A', 'B', or 'Both' (default).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_images_tif(self, images_to_save = &#39;Both&#39;):
    &#39;&#39;&#39;
    Save the detector images into TIF file (s).

    Parameters
    ----------
    images_to_save : str
        Images to save. options are: &#39;A&#39;, &#39;B&#39;, or &#39;Both&#39; (default).

    &#39;&#39;&#39;
    if self.ftype == 0:
        if images_to_save == &#39;Both&#39; or images_to_save == &#39;A&#39;:
            fnameA = os.path.splitext(self.fname)[0] + &#39;_&#39; + self.DetA.strip(&#39;\x00&#39;) + &#39;.tif&#39;
            tiff.imsave(fnameA, self.RawImageA)
        if self.DetB != &#39;None&#39;:
            if images_to_save == &#39;Both&#39; or images_to_save == &#39;B&#39;:
                fnameB = os.path.splitext(self.fname)[0] + &#39;_&#39; + self.DetB.strip(&#39;\x00&#39;) + &#39;.tif&#39;
                tiff.imsave(fnameB, self.RawImageB)
    else:
        print(&#39;original File is already in TIF format&#39;)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.save_snapshot"><code class="name flex">
<span>def <span class="ident">save_snapshot</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds an image that contains both the Detector A and Detector B (if present) images as well as a table with important FIB-SEM parameters.</p>
<p>kwargs:</p>
<hr>
<p>thr_min : float
lower CDF threshold for determining the minimum data value. Default is 1.0e-3
thr_max : float
upper CDF threshold for determining the maximum data value. Default is 1.0e-3
data_min : float
If different from data_max, this value will be used as low bound for I8 data conversion
data_max : float
If different from data_min, this value will be used as high bound for I8 data conversion
nbins : int
number of histogram bins for building the PDF and CDF
disp_res : True
If True display the results
dpi : int
Default is 300
snapshot_name : string
the name of the image to perform this operations (defaulut is frame_name + '_snapshot.png').</p>
<p>Returns
dt, data_min, data_max
dt : 2D uint8 array
Converted data
data_min : float
value used as low bound for I8 data conversion
data_max : float
value used as high bound for I8 data conversion</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_snapshot(self, **kwargs):
    &#39;&#39;&#39;
    Builds an image that contains both the Detector A and Detector B (if present) images as well as a table with important FIB-SEM parameters.

    kwargs:
     ----------
    thr_min : float
        lower CDF threshold for determining the minimum data value. Default is 1.0e-3
    thr_max : float
        upper CDF threshold for determining the maximum data value. Default is 1.0e-3
    data_min : float
        If different from data_max, this value will be used as low bound for I8 data conversion
    data_max : float
        If different from data_min, this value will be used as high bound for I8 data conversion
    nbins : int
        number of histogram bins for building the PDF and CDF
    disp_res : True
        If True display the results
    dpi : int
        Default is 300
    snapshot_name : string
        the name of the image to perform this operations (defaulut is frame_name + &#39;_snapshot.png&#39;).



    Returns
    dt, data_min, data_max
        dt : 2D uint8 array
            Converted data
        data_min : float
            value used as low bound for I8 data conversion
        data_max : float
            value used as high bound for I8 data conversion
    &#39;&#39;&#39;
    thr_min = kwargs.get(&#39;thr_min&#39;, 1.0e-3)
    thr_max = kwargs.get(&#39;thr_max&#39;, 1.0e-3)
    nbins = kwargs.get(&#39;nbins&#39;, 256)
    disp_res = kwargs.get(&#39;disp_res&#39;, True)
    dpi = kwargs.get(&#39;dpi&#39;, 300)
    snapshot_name = kwargs.get(&#39;snapshot_name&#39;, os.path.splitext(self.fname)[0] + &#39;_snapshot.png&#39;)

    ifDetB = (self.DetB != &#39;None&#39;)
    if ifDetB:
        try:
            dminB, dmaxB = self.get_image_min_max(image_name =&#39;RawImageB&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
            fig, axs = subplots(3, 1, figsize=(11,8))
        except:
            ifDetB = False
            pass
    if not ifDetB:
        fig, axs = subplots(2, 1, figsize=(7,8))
    fig.subplots_adjust(left=0.01, bottom=0.01, right=0.99, top=0.90, wspace=0.15, hspace=0.1)
    dminA, dmaxA = self.get_image_min_max(image_name =&#39;RawImageA&#39;, thr_min=thr_min, thr_max=thr_max, nbins=nbins, disp_res=False)
    axs[1].imshow(self.RawImageA, cmap=&#39;Greys&#39;, vmin=dminA, vmax=dmaxA)
    if ifDetB:
        axs[2].imshow(self.RawImageB, cmap=&#39;Greys&#39;, vmin=dminB, vmax=dmaxB)
    try:
        ttls = [self.Notes.strip(&#39;\x00&#39;),
            &#39;Detector A:  &#39;+ self.DetA.strip(&#39;\x00&#39;) + &#39;,  Data Range:  {:.1f} ÷ {:.1f} with thr_min={:.1e}, thr_max={:.1e}&#39;.format(dminA, dmaxA, thr_min, thr_max) + &#39;    (Brightness: {:.1f}, Contrast: {:.1f})&#39;.format(self.BrightnessA, self.ContrastA),
            &#39;Detector B:  &#39;+ self.DetB.strip(&#39;\x00&#39;) + &#39;,  Data Range:  {:.1f} ÷ {:.1f} with thr_min={:.1e}, thr_max={:.1e}&#39;.format(dminB, dmaxB, thr_min, thr_max) + &#39;    (Brightness: {:.1f}, Contrast: {:.1f})&#39;.format(self.BrightnessB, self.ContrastB)]
    except:
        ttls = [&#39;&#39;, &#39;Detector A&#39;, &#39;&#39;]
    for j, ax in enumerate(axs):
        ax.axis(False)
        ax.set_title(ttls[j], fontsize=10)
    fig.suptitle(self.fname)

    if self.FileVersion &gt; 8:
        cell_text = [[&#39;Sample ID&#39;, &#39;{:s}&#39;.format(self.Sample_ID.strip(&#39;\x00&#39;)), &#39;&#39;,
                      &#39;Frame Size&#39;, &#39;{:d} x {:d}&#39;.format(self.XResolution, self.YResolution), &#39;&#39;,
                      &#39;Scan Rate&#39;, &#39;{:.3f} MHz&#39;.format(self.ScanRate/1.0e6)],
                    [&#39;Machine ID&#39;, &#39;{:s}&#39;.format(self.MachineID.strip(&#39;\x00&#39;)), &#39;&#39;,
                      &#39;Pixel Size&#39;, &#39;{:.1f} nm&#39;.format(self.PixelSize), &#39;&#39;,
                      &#39;Oversampling&#39;, &#39;{:d}&#39;.format(self.Oversampling)],
                     [&#39;FileVersion&#39;, &#39;{:d}&#39;.format(self.FileVersion), &#39;&#39;,
                      &#39;Working Dist.&#39;, &#39;{:.3f} mm&#39;.format(self.WD), &#39;&#39;,
                      &#39;FIB Focus&#39;, &#39;{:.1f}  V&#39;.format(self.FIBFocus)],
                     [&#39;Bit Depth&#39;, &#39;{:d}&#39;.format(8 *(2 - self.EightBit)), &#39;&#39;,
                     &#39;EHT Voltage\n\nSEM Current&#39;, &#39;{:.3f} kV \n\n{:.3f} nA&#39;.format(self.EHT, self.SEMCurr*1.0e9), &#39;&#39;,
                     &#39;FIB Probe&#39;, &#39;{:d}&#39;.format(self.FIBProb)]]
    else:
        if self.FileVersion &gt; 0:
            cell_text = [[&#39;&#39;, &#39;&#39;, &#39;&#39;,
                          &#39;Frame Size&#39;, &#39;{:d} x {:d}&#39;.format(self.XResolution, self.YResolution), &#39;&#39;,
                          &#39;Scan Rate&#39;, &#39;{:.3f} MHz&#39;.format(self.ScanRate/1.0e6)],
                        [&#39;Machine ID&#39;, &#39;{:s}&#39;.format(self.MachineID.strip(&#39;\x00&#39;)), &#39;&#39;,
                          &#39;Pixel Size&#39;, &#39;{:.1f} nm&#39;.format(self.PixelSize), &#39;&#39;,
                          &#39;Oversampling&#39;, &#39;{:d}&#39;.format(self.Oversampling)],
                         [&#39;FileVersion&#39;, &#39;{:d}&#39;.format(self.FileVersion), &#39;&#39;,
                          &#39;Working Dist.&#39;, &#39;{:.3f} mm&#39;.format(self.WD), &#39;&#39;,
                          &#39;FIB Focus&#39;, &#39;{:.1f}  V&#39;.format(self.FIBFocus)],
                         [&#39;Bit Depth&#39;, &#39;{:d}&#39;.format(8 *(2 - self.EightBit)), &#39;&#39;,
                         &#39;EHT Voltage&#39;, &#39;{:.3f} kV&#39;.format(self.EHT), &#39;&#39;,
                         &#39;FIB Probe&#39;, &#39;{:d}&#39;.format(self.FIBProb)]]
        else:
            cell_text = [[&#39;&#39;, &#39;&#39;, &#39;&#39;,
                          &#39;Frame Size&#39;, &#39;{:d} x {:d}&#39;.format(self.XResolution, self.YResolution), &#39;&#39;,
                          &#39;Scan Rate&#39;, &#39;&#39;],
                        [&#39;Machine ID&#39;, &#39;&#39;, &#39;&#39;,
                          &#39;Pixel Size&#39;, &#39;{:.1f} nm&#39;.format(self.PixelSize), &#39;&#39;,
                          &#39;Oversampling&#39;, &#39;&#39;],
                         [&#39;FileVersion&#39;, &#39;{:d}&#39;.format(self.FileVersion), &#39;&#39;,
                          &#39;Working Dist.&#39;, &#39; &#39;, &#39;&#39;,
                          &#39;FIB Focus&#39;, &#39;&#39;],
                         [&#39;Bit Depth&#39;, &#39;{:d}&#39;.format(8 *(2 - self.EightBit)), &#39;&#39;,
                         &#39;EHT Voltage&#39;, &#39;&#39;, &#39;&#39;,
                         &#39;FIB Probe&#39;, &#39;&#39;]]
    llw0=0.3
    llw1=0.18
    llw2=0.02
    clw = [llw1, llw0, llw2, llw1, llw1, llw2, llw1, llw1]
    tbl = axs[0].table(cellText=cell_text,
                       colWidths=clw,
                       cellLoc=&#39;center&#39;,
                       colLoc=&#39;center&#39;,
                       bbox = [0.02, 0, 0.96, 1.0],
                       #bbox = [0.45, 1.02, 2.8, 0.55],
                       zorder=10)

    fig.savefig(snapshot_name, dpi=dpi)
    if disp_res == False:
        plt.close(fig)</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.show_eval_box"><code class="name flex">
<span>def <span class="ident">show_eval_box</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Show the box used for noise analysis.
©G.Shtengel, 04/2021. gleb.shtengel@gmail.com</p>
<h2 id="kwargs">Kwargs</h2>
<p>evaluation_box : list of 4 int
evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
image_name : str
Options are: 'RawImageA' (default), 'RawImageB', 'ImageA', 'ImageB'
data_dir : str
data directory (path)
Sample_ID : str
Sample ID
invert_data : boolean
If True - the data is inverted
save_res_png
: boolean
Save PNG image of the frame overlaid with with evaluation box</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_eval_box(self, **kwargs):
    &#39;&#39;&#39;
    Show the box used for noise analysis.
    ©G.Shtengel, 04/2021. gleb.shtengel@gmail.com

    kwargs
    ---------
    evaluation_box : list of 4 int
        evaluation_box = [top, height, left, width] boundaries of the box used for evaluating the image registration
        if evaluation_box is not set or evaluation_box = [0, 0, 0, 0], the entire image is used.
    image_name : str
        Options are: &#39;RawImageA&#39; (default), &#39;RawImageB&#39;, &#39;ImageA&#39;, &#39;ImageB&#39;
    data_dir : str
        data directory (path)
    Sample_ID : str
        Sample ID
    invert_data : boolean
        If True - the data is inverted
    save_res_png  : boolean
        Save PNG image of the frame overlaid with with evaluation box
    &#39;&#39;&#39;
    image_name = kwargs.get(&#34;image_name&#34;, &#39;RawImageA&#39;)
    evaluation_box = kwargs.get(&#34;evaluation_box&#34;, [0, 0, 0, 0])
    ftype = kwargs.get(&#34;ftype&#34;, self.ftype)
    data_dir = kwargs.get(&#34;data_dir&#34;, os.path.dirname(self.fname))
    Sample_ID = kwargs.get(&#34;Sample_ID&#34;, self.Sample_ID)
    nbins_disp = kwargs.get(&#34;nbins_disp&#34;, 256)
    thresholds_disp = kwargs.get(&#34;thresholds_disp&#34;, [1e-3, 1e-3])
    invert_data =  kwargs.get(&#34;invert_data&#34;, False)
    save_res_png  = kwargs.get(&#34;save_res_png&#34;, False )

    if image_name == &#39;RawImageA&#39;:
        img = self.RawImageA
    if image_name == &#39;RawImageB&#39;:
        img = self.RawImageB
    if image_name == &#39;ImageA&#39;:
        img = self.ImageA
    if image_name == &#39;ImageB&#39;:
        img = self.ImageB

    xi = 0
    yi = 0
    ysz, xsz = img.shape
    xa = xi + xsz
    ya = yi + ysz
    xi_eval = xi + evaluation_box[2]
    if evaluation_box[3] &gt; 0:
        xa_eval = xi_eval + evaluation_box[3]
    else:
        xa_eval = xa
    yi_eval = yi + evaluation_box[0]
    if evaluation_box[1] &gt; 0:
        ya_eval = yi_eval + evaluation_box[1]
    else:
        ya_eval = ya

    range_disp = get_min_max_thresholds(img[yi_eval:ya_eval, xi_eval:xa_eval], thr_min = thresholds_disp[0], thr_max = thresholds_disp[1], nbins = nbins_disp, disp_res=False)

    fig, ax = subplots(1,1, figsize = (10.0, 11.0*ysz/xsz))
    ax.imshow(img, cmap=&#39;Greys&#39;, vmin = range_disp[0], vmax = range_disp[1])
    ax.grid(True, color = &#34;cyan&#34;)
    ax.set_title(self.fname)
    rect_patch = patches.Rectangle((xi_eval,yi_eval),abs(xa_eval-xi_eval)-2,abs(ya_eval-yi_eval)-2, linewidth=2.0, edgecolor=&#39;yellow&#39;,facecolor=&#39;none&#39;)
    ax.add_patch(rect_patch)
    if save_res_png :
        fig.savefig(os.path.splitext(self.fname+&#39;_evaluation_box.png&#39;, dpi=300))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform"><code class="flex name class">
<span>class <span class="ident">RegularizedAffineTransform</span></span>
<span>(</span><span>matrix=None, scale=None, rotation=None, shear=None, translation=None, l2_matrix=None, targ_vector=None, *, dimensionality=2)</span>
</code></dt>
<dd>
<div class="desc"><p>Regularized Affine transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com</p>
<p>Has the following form::
X = a0<em>x + a1</em>y + a2 =
= sx<em>x</em>cos(rotation) - sy<em>y</em>sin(rotation + shear) + a2
Y = b0<em>x + b1</em>y + b2 =
= sx<em>x</em>sin(rotation) + sy<em>y</em>cos(rotation + shear) + b2
where <code>sx</code> and <code>sy</code> are scale factors in the x and y directions,
and the homogeneous transformation matrix is::
[[a0
a1
a2]
[b0
b1
b2]
[0
0
1]]
In 2D, the transformation parameters can be given as the homogeneous
transformation matrix, above, or as the implicit parameters, scale,
rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
only the matrix form is allowed.
In narrower transforms, such as the Euclidean (only rotation and
translation) or Similarity (rotation, translation, and a global scale
factor) transforms, it is possible to specify 3D transforms using implicit
parameters also.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>matrix</code></strong> :&ensp;<code>(D+1, D+1) array</code>, optional</dt>
<dd>Homogeneous transformation matrix. If this matrix is provided, it is an
error to provide any of scale, rotation, shear, or translation.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>{s as float</code> or <code>(sx, sy) as array, list</code> or <code>tuple}</code>, optional</dt>
<dd>Scale factor(s). If a single value, it will be assigned to both
sx and sy. Only available for 2D.<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.17</p>
Added support for supplying a single scalar value.</div>
</dd>
<dt><strong><code>rotation</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Rotation angle in counter-clockwise direction as radians. Only
available for 2D.</dd>
<dt><strong><code>shear</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Shear angle in counter-clockwise direction as radians. Only available
for 2D.</dd>
<dt><strong><code>translation</code></strong> :&ensp;<code>(tx, ty) as array, list</code> or <code>tuple</code>, optional</dt>
<dd>Translation parameters. Only available for 2D.</dd>
<dt><strong><code>dimensionality</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The dimensionality of the transform. This is not used if any other
parameters are provided.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>(D+1, D+1) array</code></dt>
<dd>Homogeneous transformation matrix.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If both <code>matrix</code> and any of the other parameters are provided.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RegularizedAffineTransform(ProjectiveTransform):
    &#34;&#34;&#34;
    Regularized Affine transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

    Has the following form::
        X = a0*x + a1*y + a2 =
          = sx*x*cos(rotation) - sy*y*sin(rotation + shear) + a2
        Y = b0*x + b1*y + b2 =
          = sx*x*sin(rotation) + sy*y*cos(rotation + shear) + b2
    where ``sx`` and ``sy`` are scale factors in the x and y directions,
    and the homogeneous transformation matrix is::
        [[a0  a1  a2]
         [b0  b1  b2]
         [0   0    1]]
    In 2D, the transformation parameters can be given as the homogeneous
    transformation matrix, above, or as the implicit parameters, scale,
    rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
    only the matrix form is allowed.
    In narrower transforms, such as the Euclidean (only rotation and
    translation) or Similarity (rotation, translation, and a global scale
    factor) transforms, it is possible to specify 3D transforms using implicit
    parameters also.

    Parameters
    ----------
    matrix : (D+1, D+1) array, optional
        Homogeneous transformation matrix. If this matrix is provided, it is an
        error to provide any of scale, rotation, shear, or translation.
    scale : {s as float or (sx, sy) as array, list or tuple}, optional
        Scale factor(s). If a single value, it will be assigned to both
        sx and sy. Only available for 2D.
        .. versionadded:: 0.17
           Added support for supplying a single scalar value.
    rotation : float, optional
        Rotation angle in counter-clockwise direction as radians. Only
        available for 2D.
    shear : float, optional
        Shear angle in counter-clockwise direction as radians. Only available
        for 2D.
    translation : (tx, ty) as array, list or tuple, optional
        Translation parameters. Only available for 2D.
    dimensionality : int, optional
        The dimensionality of the transform. This is not used if any other
        parameters are provided.
    Attributes
    ----------
    params : (D+1, D+1) array
        Homogeneous transformation matrix.
    Raises
    ------
    ValueError
        If both ``matrix`` and any of the other parameters are provided.
    &#34;&#34;&#34;

    def __init__(self, matrix=None, scale=None, rotation=None, shear=None,
                 translation=None, l2_matrix =None, targ_vector=None, *, dimensionality=2):

        self.l2_matrix = l2_matrix      # regularization vector
        self.targ_vector = targ_vector  # target
        params = any(param is not None
                     for param in (scale, rotation, shear, translation))

        # these parameters get overwritten if a higher-D matrix is given
        self._coeffs = range(dimensionality * (dimensionality + 1))

        if params and matrix is not None:
            raise ValueError(&#34;You cannot specify the transformation matrix and&#34;
                             &#34; the implicit parameters at the same time.&#34;)
        if params and dimensionality &gt; 2:
            raise ValueError(&#39;Parameter input is only supported in 2D.&#39;)
        elif matrix is not None:
            if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:
                raise ValueError(&#34;Invalid shape of transformation matrix.&#34;)
            else:
                dimensionality = matrix.shape[0] - 1
                nparam = dimensionality * (dimensionality + 1)
            self._coeffs = range(nparam)
            self.params = matrix
        elif params:  # note: 2D only
            if scale is None:
                scale = (1, 1)
            if rotation is None:
                rotation = 0
            if shear is None:
                shear = 0
            if translation is None:
                translation = (0, 0)

            if np.isscalar(scale):
                sx = sy = scale
            else:
                sx, sy = scale

            self.params = np.array([
                [sx * math.cos(rotation), -sy * math.sin(rotation + shear), 0],
                [sx * math.sin(rotation),  sy * math.cos(rotation + shear), 0],
                [                      0,                                0, 1]
            ])
            self.params[0:2, 2] = translation
        else:
            # default to an identity transform
            self.params = np.eye(dimensionality + 1)


    @property
    def scale(self):
        return np.sqrt(np.sum(self.params ** 2, axis=0))[:self.dimensionality]

    @property
    def rotation(self):
        if self.dimensionality != 2:
            raise NotImplementedError(
                &#39;The rotation property is only implemented for 2D transforms.&#39;
            )
        return math.atan2(self.params[1, 0], self.params[0, 0])

    @property
    def shear(self):
        if self.dimensionality != 2:
            raise NotImplementedError(
                &#39;The shear property is only implemented for 2D transforms.&#39;
            )
        beta = math.atan2(- self.params[0, 1], self.params[1, 1])
        return beta - self.rotation

    @property
    def translation(self):
        return self.params[0:self.dimensionality, self.dimensionality]



        # Thise are functions used for different steps of image analysis and registration</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>skimage.transform._geometric.ProjectiveTransform</li>
<li>skimage.transform._geometric.GeometricTransform</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.rotation"><code class="name">var <span class="ident">rotation</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rotation(self):
    if self.dimensionality != 2:
        raise NotImplementedError(
            &#39;The rotation property is only implemented for 2D transforms.&#39;
        )
    return math.atan2(self.params[1, 0], self.params[0, 0])</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.scale"><code class="name">var <span class="ident">scale</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def scale(self):
    return np.sqrt(np.sum(self.params ** 2, axis=0))[:self.dimensionality]</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.shear"><code class="name">var <span class="ident">shear</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shear(self):
    if self.dimensionality != 2:
        raise NotImplementedError(
            &#39;The shear property is only implemented for 2D transforms.&#39;
        )
    beta = math.atan2(- self.params[0, 1], self.params[1, 1])
    return beta - self.rotation</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.translation"><code class="name">var <span class="ident">translation</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def translation(self):
    return self.params[0:self.dimensionality, self.dimensionality]



    # Thise are functions used for different steps of image analysis and registration</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform"><code class="flex name class">
<span>class <span class="ident">ScaleShiftTransform</span></span>
<span>(</span><span>matrix=None, scale=None, translation=None, *, dimensionality=2)</span>
</code></dt>
<dd>
<div class="desc"><p>ScaleShift transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com</p>
<p>Has the following form::
X = a0<em>x +
a2 = sx</em>x + a2
Y = b1<em>y + b2 = sy</em>y + b2
where <code>sx</code> and <code>sy</code> are scale factors in the x and y directions,
and the homogeneous transformation matrix is::
[[a0
0
a2]
[0
b1
b2]
[0
0
1]]
In 2D, the transformation parameters can be given as the homogeneous
transformation matrix, above, or as the implicit parameters, scale,
rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
only the matrix form is allowed.
In narrower transforms, such as the Euclidean (only rotation and
translation) or Similarity (rotation, translation, and a global scale
factor) transforms, it is possible to specify 3D transforms using implicit
parameters also.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>matrix</code></strong> :&ensp;<code>(D+1, D+1) array</code>, optional</dt>
<dd>Homogeneous transformation matrix. If this matrix is provided, it is an
error to provide any of scale, rotation, shear, or translation.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>{s as float</code> or <code>(sx, sy) as array, list</code> or <code>tuple}</code>, optional</dt>
<dd>Scale factor(s). If a single value, it will be assigned to both
sx and sy. Only available for 2D.<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.17</p>
Added support for supplying a single scalar value.</div>
</dd>
<dt><strong><code>translation</code></strong> :&ensp;<code>(tx, ty) as array, list</code> or <code>tuple</code>, optional</dt>
<dd>Translation parameters. Only available for 2D.</dd>
<dt><strong><code>dimensionality</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The dimensionality of the transform. This is not used if any other
parameters are provided.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>(D+1, D+1) array</code></dt>
<dd>Homogeneous transformation matrix.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If both <code>matrix</code> and any of the other parameters are provided.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScaleShiftTransform(ProjectiveTransform):
    &#39;&#39;&#39;
    ScaleShift transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

    Has the following form::
        X = a0*x +  a2 = sx*x + a2
        Y = b1*y + b2 = sy*y + b2
    where ``sx`` and ``sy`` are scale factors in the x and y directions,
    and the homogeneous transformation matrix is::
        [[a0  0   a2]
         [0   b1  b2]
         [0   0    1]]
    In 2D, the transformation parameters can be given as the homogeneous
    transformation matrix, above, or as the implicit parameters, scale,
    rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
    only the matrix form is allowed.
    In narrower transforms, such as the Euclidean (only rotation and
    translation) or Similarity (rotation, translation, and a global scale
    factor) transforms, it is possible to specify 3D transforms using implicit
    parameters also.

    Parameters
    ----------
    matrix : (D+1, D+1) array, optional
        Homogeneous transformation matrix. If this matrix is provided, it is an
        error to provide any of scale, rotation, shear, or translation.
    scale : {s as float or (sx, sy) as array, list or tuple}, optional
        Scale factor(s). If a single value, it will be assigned to both
        sx and sy. Only available for 2D.
        .. versionadded:: 0.17
           Added support for supplying a single scalar value.
    translation : (tx, ty) as array, list or tuple, optional
        Translation parameters. Only available for 2D.
    dimensionality : int, optional
        The dimensionality of the transform. This is not used if any other
        parameters are provided.
    Attributes
    ----------
    params : (D+1, D+1) array
        Homogeneous transformation matrix.
    Raises
    ------
    ValueError
        If both ``matrix`` and any of the other parameters are provided.
    &#39;&#39;&#39;

    def __init__(self, matrix=None, scale=None, translation=None, *, dimensionality=2):
        params = (scale is not None) or (translation is not None)
        # these parameters get overwritten if a higher-D matrix is given
        self._coeffs = range(dimensionality * (dimensionality + 1))

        if params and matrix is not None:
            raise ValueError(&#34;You cannot specify the transformation matrix and&#34;
                             &#34; the implicit parameters at the same time.&#34;)
        if params and dimensionality &gt; 2:
            raise ValueError(&#39;Parameter input is only supported in 2D.&#39;)
        elif matrix is not None:
            if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:
                raise ValueError(&#34;Invalid shape of transformation matrix.&#34;)
            else:
                dimensionality = matrix.shape[0] - 1
                nparam = dimensionality * (dimensionality + 1)
            self._coeffs = range(nparam)
            self.params = matrix
            self.params[0,1] = 0
            self.params[1,0] = 0
        elif params:  # note: 2D only
            if scale is None:
                scale = (1, 1)

            if translation is None:
                translation = (0, 0)

            if np.isscalar(scale):
                sx = sy = scale
            else:
                sx, sy = scale

            self.params = np.array([[sx, 0,  0],
                                    [0,  sy, 0],
                                    [0,  0,  1]])
            self.params[0:2, 2] = translation
        else:
            # default to an identity transform
            self.params = np.eye(dimensionality + 1)
    def estimate(self, src, dst):
        &#34;&#34;&#34;
                Parameters
        ----------
        src : (N, 2) array
            Source coordinates.
        dst : (N, 2) array
            Destination coordinates.
        Returns
        -------
        success : bool
            True, if model estimation succeeds.
        &#34;&#34;&#34;

        n, d = src.shape
        xsrc = np.array(src)[:,0].astype(float)
        ysrc = np.array(src)[:,1].astype(float)
        xdst = np.array(dst)[:,0].astype(float)
        ydst = np.array(dst)[:,1].astype(float)
        s00 = np.sum(xsrc)
        s01 = np.sum(xdst)
        sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
        s10 = np.sum(ysrc)
        s11 = np.sum(ydst)
        sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)

        tx = np.mean(xdst) - sx * np.mean(xsrc)
        ty = np.mean(ydst) - sy * np.mean(ysrc)

        self.params = np.array([[sx, 0,  tx],
                                [0,  sy, ty],
                                [0,  0,  1]])
        return True

    @property
    def scale(self):
        return np.sqrt(np.sum(self.params ** 2, axis=0))[:self.dimensionality]

    @property
    def translation(self):
        return self.params[0:self.dimensionality, self.dimensionality]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>skimage.transform._geometric.ProjectiveTransform</li>
<li>skimage.transform._geometric.GeometricTransform</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform.scale"><code class="name">var <span class="ident">scale</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def scale(self):
    return np.sqrt(np.sum(self.params ** 2, axis=0))[:self.dimensionality]</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform.translation"><code class="name">var <span class="ident">translation</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def translation(self):
    return self.params[0:self.dimensionality, self.dimensionality]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform.estimate"><code class="name flex">
<span>def <span class="ident">estimate</span></span>(<span>self, src, dst)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>src</code></strong> :&ensp;<code>(N, 2) array</code></dt>
<dd>Source coordinates.</dd>
<dt><strong><code>dst</code></strong> :&ensp;<code>(N, 2) array</code></dt>
<dd>Destination coordinates.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>success</code></strong> :&ensp;<code>bool</code></dt>
<dd>True, if model estimation succeeds.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate(self, src, dst):
    &#34;&#34;&#34;
            Parameters
    ----------
    src : (N, 2) array
        Source coordinates.
    dst : (N, 2) array
        Destination coordinates.
    Returns
    -------
    success : bool
        True, if model estimation succeeds.
    &#34;&#34;&#34;

    n, d = src.shape
    xsrc = np.array(src)[:,0].astype(float)
    ysrc = np.array(src)[:,1].astype(float)
    xdst = np.array(dst)[:,0].astype(float)
    ydst = np.array(dst)[:,1].astype(float)
    s00 = np.sum(xsrc)
    s01 = np.sum(xdst)
    sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
    s10 = np.sum(ysrc)
    s11 = np.sum(ydst)
    sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)

    tx = np.mean(xdst) - sx * np.mean(xsrc)
    ty = np.mean(ydst) - sy * np.mean(ysrc)

    self.params = np.array([[sx, 0,  tx],
                            [0,  sy, ty],
                            [0,  0,  1]])
    return True</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform"><code class="flex name class">
<span>class <span class="ident">ShiftTransform</span></span>
<span>(</span><span>matrix=None, translation=None, *, dimensionality=2)</span>
</code></dt>
<dd>
<div class="desc"><p>ScaleShift transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com</p>
<p>Has the following form:
X = x +
a2
Y = y + b2
and the homogeneous transformation matrix is::
[[1
0
a2]
[0
1
b2]
[0
0
1]]
In 2D, the transformation parameters can be given as the homogeneous
transformation matrix, above, or as the implicit parameters, scale,
rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
only the matrix form is allowed.
In narrower transforms, such as the Euclidean (only rotation and
translation) or Similarity (rotation, translation, and a global scale
factor) transforms, it is possible to specify 3D transforms using implicit
parameters also.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>matrix</code></strong> :&ensp;<code>(D+1, D+1) array</code>, optional</dt>
<dd>Homogeneous transformation matrix. If this matrix is provided, it is an
error to provide any of scale, rotation, shear, or translation.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>{s as float</code> or <code>(sx, sy) as array, list</code> or <code>tuple}</code>, optional</dt>
<dd>Scale factor(s). If a single value, it will be assigned to both
sx and sy. Only available for 2D.<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.17</p>
Added support for supplying a single scalar value.</div>
</dd>
<dt><strong><code>translation</code></strong> :&ensp;<code>(tx, ty) as array, list</code> or <code>tuple</code>, optional</dt>
<dd>Translation parameters. Only available for 2D.</dd>
<dt><strong><code>dimensionality</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The dimensionality of the transform. This is not used if any other
parameters are provided.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>(D+1, D+1) array</code></dt>
<dd>Homogeneous transformation matrix.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If both <code>matrix</code> and any of the other parameters are provided.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ShiftTransform(ProjectiveTransform):
    &#34;&#34;&#34;
    ScaleShift transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

    Has the following form:
        X = x +  a2
        Y = y + b2
    and the homogeneous transformation matrix is::
        [[1  0   a2]
         [0   1  b2]
         [0   0    1]]
    In 2D, the transformation parameters can be given as the homogeneous
    transformation matrix, above, or as the implicit parameters, scale,
    rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
    only the matrix form is allowed.
    In narrower transforms, such as the Euclidean (only rotation and
    translation) or Similarity (rotation, translation, and a global scale
    factor) transforms, it is possible to specify 3D transforms using implicit
    parameters also.

    Parameters
    ----------
    matrix : (D+1, D+1) array, optional
        Homogeneous transformation matrix. If this matrix is provided, it is an
        error to provide any of scale, rotation, shear, or translation.
    scale : {s as float or (sx, sy) as array, list or tuple}, optional
        Scale factor(s). If a single value, it will be assigned to both
        sx and sy. Only available for 2D.
        .. versionadded:: 0.17
           Added support for supplying a single scalar value.
    translation : (tx, ty) as array, list or tuple, optional
        Translation parameters. Only available for 2D.
    dimensionality : int, optional
        The dimensionality of the transform. This is not used if any other
        parameters are provided.
    Attributes
    ----------
    params : (D+1, D+1) array
        Homogeneous transformation matrix.
    Raises
    ------
    ValueError
        If both ``matrix`` and any of the other parameters are provided.
    &#34;&#34;&#34;

    def __init__(self, matrix=None, translation=None, *, dimensionality=2):

        # these parameters get overwritten if a higher-D matrix is given
        self._coeffs = range(dimensionality * (dimensionality + 1))

        if translation is not None and matrix is not None:
            raise ValueError(&#34;You cannot specify the transformation matrix and&#34;
                             &#34; the implicit parameters at the same time.&#34;)
        if translation is not None and dimensionality &gt; 2:
            raise ValueError(&#39;Parameter input is only supported in 2D.&#39;)
        elif matrix is not None:
            if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:
                raise ValueError(&#34;Invalid shape of transformation matrix.&#34;)
            else:
                dimensionality = matrix.shape[0] - 1
                nparam = dimensionality * (dimensionality + 1)
            self._coeffs = range(nparam)
            self.params = matrix
            self.params[0,1] = 0
            self.params[1,0] = 0
        elif translation is not None:  # note: 2D only
            self.params = np.array([[1.0, 0.0,  0.0],
                                    [0.0,  1.0, 0.0],
                                    [0.0,  0.0, 1.0]])
            self.params[0:2, 2] = translation
        else:
            # default to an identity transform
            self.params = np.eye(dimensionality + 1)
    def estimate(self, src, dst):
        &#39;&#39;&#39;
                Parameters
        ----------
        src : (N, 2) array
            Source coordinates.
        dst : (N, 2) array
            Destination coordinates.
        Returns
        -------
        success : bool
            True, if model estimation succeeds.
        &#39;&#39;&#39;
        translation = np.mean(np.array(dst.astype(float)-src.astype(float)), axis=0)
        self.params = np.array([[1.0, 0.0,  0.0],
                                [0.0,  1.0, 0.0],
                                [0.0,  0.0, 1.0]])
        self.params[0:2, 2] = translation
        return True

    @property
    def translation(self):
        return self.params[0:self.dimensionality, self.dimensionality]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>skimage.transform._geometric.ProjectiveTransform</li>
<li>skimage.transform._geometric.GeometricTransform</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform.translation"><code class="name">var <span class="ident">translation</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def translation(self):
    return self.params[0:self.dimensionality, self.dimensionality]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform.estimate"><code class="name flex">
<span>def <span class="ident">estimate</span></span>(<span>self, src, dst)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>src</code></strong> :&ensp;<code>(N, 2) array</code></dt>
<dd>Source coordinates.</dd>
<dt><strong><code>dst</code></strong> :&ensp;<code>(N, 2) array</code></dt>
<dd>Destination coordinates.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>success</code></strong> :&ensp;<code>bool</code></dt>
<dd>True, if model estimation succeeds.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate(self, src, dst):
    &#39;&#39;&#39;
            Parameters
    ----------
    src : (N, 2) array
        Source coordinates.
    dst : (N, 2) array
        Destination coordinates.
    Returns
    -------
    success : bool
        True, if model estimation succeeds.
    &#39;&#39;&#39;
    translation = np.mean(np.array(dst.astype(float)-src.astype(float)), axis=0)
    self.params = np.array([[1.0, 0.0,  0.0],
                            [0.0,  1.0, 0.0],
                            [0.0,  0.0, 1.0]])
    self.params[0:2, 2] = translation
    return True</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform"><code class="flex name class">
<span>class <span class="ident">XScaleShiftTransform</span></span>
<span>(</span><span>matrix=None, scale=None, translation=None, *, dimensionality=2)</span>
</code></dt>
<dd>
<div class="desc"><p>XScaleShift transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com</p>
<p>Has the following form::
X = a0<em>x +
a2 = sx</em>x + a2
Y = y + b2 = y + b2
where <code>sx</code> and <code>sy</code> are scale factors in the x and y directions,
and the homogeneous transformation matrix is::
[[a0
0
a2]
[0
1
b2]
[0
0
1]]
In 2D, the transformation parameters can be given as the homogeneous
transformation matrix, above, or as the implicit parameters, scale,
rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
only the matrix form is allowed.
In narrower transforms, such as the Euclidean (only rotation and
translation) or Similarity (rotation, translation, and a global scale
factor) transforms, it is possible to specify 3D transforms using implicit
parameters also.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>matrix</code></strong> :&ensp;<code>(D+1, D+1) array</code>, optional</dt>
<dd>Homogeneous transformation matrix. If this matrix is provided, it is an
error to provide any of scale, rotation, shear, or translation.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>{s as float</code> or <code>(sx, sy) as array, list</code> or <code>tuple}</code>, optional</dt>
<dd>Scale factor(s). If a single value, it will be assigned to both
sx and sy. Only available for 2D.<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.17</p>
Added support for supplying a single scalar value.</div>
</dd>
<dt><strong><code>translation</code></strong> :&ensp;<code>(tx, ty) as array, list</code> or <code>tuple</code>, optional</dt>
<dd>Translation parameters. Only available for 2D.</dd>
<dt><strong><code>dimensionality</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The dimensionality of the transform. This is not used if any other
parameters are provided.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>(D+1, D+1) array</code></dt>
<dd>Homogeneous transformation matrix.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If both <code>matrix</code> and any of the other parameters are provided.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class XScaleShiftTransform(ProjectiveTransform):
    &#39;&#39;&#39;
    XScaleShift transformation. ©G.Shtengel 11/2021 gleb.shtengel@gmail.com

    Has the following form::
        X = a0*x +  a2 = sx*x + a2
        Y = y + b2 = y + b2
    where ``sx`` and ``sy`` are scale factors in the x and y directions,
    and the homogeneous transformation matrix is::
        [[a0  0   a2]
         [0   1   b2]
         [0   0    1]]
    In 2D, the transformation parameters can be given as the homogeneous
    transformation matrix, above, or as the implicit parameters, scale,
    rotation, shear, and translation in x (a2) and y (b2). For 3D and higher,
    only the matrix form is allowed.
    In narrower transforms, such as the Euclidean (only rotation and
    translation) or Similarity (rotation, translation, and a global scale
    factor) transforms, it is possible to specify 3D transforms using implicit
    parameters also.

    Parameters
    ----------
    matrix : (D+1, D+1) array, optional
        Homogeneous transformation matrix. If this matrix is provided, it is an
        error to provide any of scale, rotation, shear, or translation.
    scale : {s as float or (sx, sy) as array, list or tuple}, optional
        Scale factor(s). If a single value, it will be assigned to both
        sx and sy. Only available for 2D.
        .. versionadded:: 0.17
           Added support for supplying a single scalar value.
    translation : (tx, ty) as array, list or tuple, optional
        Translation parameters. Only available for 2D.
    dimensionality : int, optional
        The dimensionality of the transform. This is not used if any other
        parameters are provided.
    Attributes
    ----------
    params : (D+1, D+1) array
        Homogeneous transformation matrix.
    Raises
    ------
    ValueError
        If both ``matrix`` and any of the other parameters are provided.
    &#39;&#39;&#39;

    def __init__(self, matrix=None, scale=None, translation=None, *, dimensionality=2):
        params = (scale is not None) or (translation is not None)
        # these parameters get overwritten if a higher-D matrix is given
        self._coeffs = range(dimensionality * (dimensionality + 1))

        if params and matrix is not None:
            raise ValueError(&#34;You cannot specify the transformation matrix and&#34;
                             &#34; the implicit parameters at the same time.&#34;)
        if params and dimensionality &gt; 2:
            raise ValueError(&#39;Parameter input is only supported in 2D.&#39;)
        elif matrix is not None:
            if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:
                raise ValueError(&#34;Invalid shape of transformation matrix.&#34;)
            else:
                dimensionality = matrix.shape[0] - 1
                nparam = dimensionality * (dimensionality + 1)
            self._coeffs = range(nparam)
            self.params = matrix
            self.params[0,1] = 0
            self.params[1,0] = 0
        elif params:  # note: 2D only
            if scale is None:
                scale = (1, 1)

            if translation is None:
                translation = (0, 0)

            if np.isscalar(scale):
                sx = scale
            else:
                sx = scale

            self.params = np.array([[sx, 0,  0],
                                    [0,  1, 0],
                                    [0,  0,  1]])
            self.params[0:2, 2] = translation
        else:
            # default to an identity transform
            self.params = np.eye(dimensionality + 1)
    def estimate(self, src, dst):
        &#34;&#34;&#34;
                Parameters
        ----------
        src : (N, 2) array
            Source coordinates.
        dst : (N, 2) array
            Destination coordinates.
        Returns
        -------
        success : bool
            True, if model estimation succeeds.
        &#34;&#34;&#34;

        n, d = src.shape
        xsrc = np.array(src)[:,0].astype(float)
        ysrc = np.array(src)[:,1].astype(float)
        xdst = np.array(dst)[:,0].astype(float)
        ydst = np.array(dst)[:,1].astype(float)
        s00 = np.sum(xsrc)
        s01 = np.sum(xdst)
        sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
        #s10 = np.sum(ysrc)
        #s11 = np.sum(ydst)
        #sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)
        sy = 1.0

        tx = np.mean(xdst) - sx * np.mean(xsrc)
        ty = np.mean(ydst) - sy * np.mean(ysrc)

        self.params = np.array([[sx, 0,  tx],
                                [0,  sy, ty],
                                [0,  0,  1]])
        return True

    def print_res(self):
        print(&#39;Printing from iside the class XScaleShiftTransform&#39;)

    @property
    def scale(self):
        return np.sqrt(np.sum(self.params ** 2, axis=0))[:self.dimensionality]

    @property
    def translation(self):
        return self.params[0:self.dimensionality, self.dimensionality]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>skimage.transform._geometric.ProjectiveTransform</li>
<li>skimage.transform._geometric.GeometricTransform</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.scale"><code class="name">var <span class="ident">scale</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def scale(self):
    return np.sqrt(np.sum(self.params ** 2, axis=0))[:self.dimensionality]</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.translation"><code class="name">var <span class="ident">translation</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def translation(self):
    return self.params[0:self.dimensionality, self.dimensionality]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.estimate"><code class="name flex">
<span>def <span class="ident">estimate</span></span>(<span>self, src, dst)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>src</code></strong> :&ensp;<code>(N, 2) array</code></dt>
<dd>Source coordinates.</dd>
<dt><strong><code>dst</code></strong> :&ensp;<code>(N, 2) array</code></dt>
<dd>Destination coordinates.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>success</code></strong> :&ensp;<code>bool</code></dt>
<dd>True, if model estimation succeeds.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate(self, src, dst):
    &#34;&#34;&#34;
            Parameters
    ----------
    src : (N, 2) array
        Source coordinates.
    dst : (N, 2) array
        Destination coordinates.
    Returns
    -------
    success : bool
        True, if model estimation succeeds.
    &#34;&#34;&#34;

    n, d = src.shape
    xsrc = np.array(src)[:,0].astype(float)
    ysrc = np.array(src)[:,1].astype(float)
    xdst = np.array(dst)[:,0].astype(float)
    ydst = np.array(dst)[:,1].astype(float)
    s00 = np.sum(xsrc)
    s01 = np.sum(xdst)
    sx = (n*np.dot(xsrc, xdst) - s00*s01)/(n*np.sum(xsrc*xsrc) - s00*s00)
    #s10 = np.sum(ysrc)
    #s11 = np.sum(ydst)
    #sy = (n*np.dot(ysrc, ydst) - s10*s11)/(n*np.sum(ysrc*ysrc) - s10*s10)
    sy = 1.0

    tx = np.mean(xdst) - sx * np.mean(xsrc)
    ty = np.mean(ydst) - sy * np.mean(ysrc)

    self.params = np.array([[sx, 0,  tx],
                            [0,  sy, ty],
                            [0,  0,  1]])
    return True</code></pre>
</details>
</dd>
<dt id="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.print_res"><code class="name flex">
<span>def <span class="ident">print_res</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_res(self):
    print(&#39;Printing from iside the class XScaleShiftTransform&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="SIFT_gs" href="index.html">SIFT_gs</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.Perform_2D_fit" href="#SIFT_gs.FIBSEM_SIFT_gs.Perform_2D_fit">Perform_2D_fit</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.SIFT_evaluation_dataset" href="#SIFT_gs.FIBSEM_SIFT_gs.SIFT_evaluation_dataset">SIFT_evaluation_dataset</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.SIFT_find_keypoints_dataset" href="#SIFT_gs.FIBSEM_SIFT_gs.SIFT_find_keypoints_dataset">SIFT_find_keypoints_dataset</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform0" href="#SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform0">ScaleShiftTransform0</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform0" href="#SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform0">ShiftTransform0</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.Single_Image_Noise_ROIs" href="#SIFT_gs.FIBSEM_SIFT_gs.Single_Image_Noise_ROIs">Single_Image_Noise_ROIs</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.Single_Image_Noise_Statistics" href="#SIFT_gs.FIBSEM_SIFT_gs.Single_Image_Noise_Statistics">Single_Image_Noise_Statistics</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.Single_Image_SNR" href="#SIFT_gs.FIBSEM_SIFT_gs.Single_Image_SNR">Single_Image_SNR</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.Two_Image_Analysis" href="#SIFT_gs.FIBSEM_SIFT_gs.Two_Image_Analysis">Two_Image_Analysis</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.Two_Image_FSC" href="#SIFT_gs.FIBSEM_SIFT_gs.Two_Image_FSC">Two_Image_FSC</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.Two_Image_NCC_SNR" href="#SIFT_gs.FIBSEM_SIFT_gs.Two_Image_NCC_SNR">Two_Image_NCC_SNR</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.analyze_mrc_stack_registration" href="#SIFT_gs.FIBSEM_SIFT_gs.analyze_mrc_stack_registration">analyze_mrc_stack_registration</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.analyze_registration_frames" href="#SIFT_gs.FIBSEM_SIFT_gs.analyze_registration_frames">analyze_registration_frames</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.analyze_tif_stack_registration" href="#SIFT_gs.FIBSEM_SIFT_gs.analyze_tif_stack_registration">analyze_tif_stack_registration</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.analyze_transformation_matrix" href="#SIFT_gs.FIBSEM_SIFT_gs.analyze_transformation_matrix">analyze_transformation_matrix</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.argmax2d" href="#SIFT_gs.FIBSEM_SIFT_gs.argmax2d">argmax2d</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.beta" href="#SIFT_gs.FIBSEM_SIFT_gs.beta">beta</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.bin_crop_frames" href="#SIFT_gs.FIBSEM_SIFT_gs.bin_crop_frames">bin_crop_frames</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.bin_crop_mrc_stack" href="#SIFT_gs.FIBSEM_SIFT_gs.bin_crop_mrc_stack">bin_crop_mrc_stack</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.bin_crop_mrc_stack_DASK" href="#SIFT_gs.FIBSEM_SIFT_gs.bin_crop_mrc_stack_DASK">bin_crop_mrc_stack_DASK</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.binomial" href="#SIFT_gs.FIBSEM_SIFT_gs.binomial">binomial</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.build_filename" href="#SIFT_gs.FIBSEM_SIFT_gs.build_filename">build_filename</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.check_for_nomatch_frames_dataset" href="#SIFT_gs.FIBSEM_SIFT_gs.check_for_nomatch_frames_dataset">check_for_nomatch_frames_dataset</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.chisquare" href="#SIFT_gs.FIBSEM_SIFT_gs.chisquare">chisquare</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.choice" href="#SIFT_gs.FIBSEM_SIFT_gs.choice">choice</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.determine_pad_offsets" href="#SIFT_gs.FIBSEM_SIFT_gs.determine_pad_offsets">determine_pad_offsets</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.determine_pad_offsets_old" href="#SIFT_gs.FIBSEM_SIFT_gs.determine_pad_offsets_old">determine_pad_offsets_old</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.determine_regularized_affine_transform" href="#SIFT_gs.FIBSEM_SIFT_gs.determine_regularized_affine_transform">determine_regularized_affine_transform</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.determine_transformation_matrix" href="#SIFT_gs.FIBSEM_SIFT_gs.determine_transformation_matrix">determine_transformation_matrix</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.determine_transformations_dataset" href="#SIFT_gs.FIBSEM_SIFT_gs.determine_transformations_dataset">determine_transformations_dataset</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.determine_transformations_files" href="#SIFT_gs.FIBSEM_SIFT_gs.determine_transformations_files">determine_transformations_files</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.dirichlet" href="#SIFT_gs.FIBSEM_SIFT_gs.dirichlet">dirichlet</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.estimate_kpts_transform_error" href="#SIFT_gs.FIBSEM_SIFT_gs.estimate_kpts_transform_error">estimate_kpts_transform_error</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.evaluate_FIBSEM_frame" href="#SIFT_gs.FIBSEM_SIFT_gs.evaluate_FIBSEM_frame">evaluate_FIBSEM_frame</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.evaluate_FIBSEM_frames_dataset" href="#SIFT_gs.FIBSEM_SIFT_gs.evaluate_FIBSEM_frames_dataset">evaluate_FIBSEM_frames_dataset</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.evaluate_registration_two_frames" href="#SIFT_gs.FIBSEM_SIFT_gs.evaluate_registration_two_frames">evaluate_registration_two_frames</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.evaluate_registration_two_frames_tif" href="#SIFT_gs.FIBSEM_SIFT_gs.evaluate_registration_two_frames_tif">evaluate_registration_two_frames_tif</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.exponential" href="#SIFT_gs.FIBSEM_SIFT_gs.exponential">exponential</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.extract_keypoints_dataset" href="#SIFT_gs.FIBSEM_SIFT_gs.extract_keypoints_dataset">extract_keypoints_dataset</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.extract_keypoints_descr_files" href="#SIFT_gs.FIBSEM_SIFT_gs.extract_keypoints_descr_files">extract_keypoints_descr_files</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.f" href="#SIFT_gs.FIBSEM_SIFT_gs.f">f</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.find_BW" href="#SIFT_gs.FIBSEM_SIFT_gs.find_BW">find_BW</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.find_fit" href="#SIFT_gs.FIBSEM_SIFT_gs.find_fit">find_fit</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.gamma" href="#SIFT_gs.FIBSEM_SIFT_gs.gamma">gamma</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.generate_report_FOV_center_shift_xlsx" href="#SIFT_gs.FIBSEM_SIFT_gs.generate_report_FOV_center_shift_xlsx">generate_report_FOV_center_shift_xlsx</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.generate_report_data_minmax_xlsx" href="#SIFT_gs.FIBSEM_SIFT_gs.generate_report_data_minmax_xlsx">generate_report_data_minmax_xlsx</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.generate_report_from_xls_registration_summary" href="#SIFT_gs.FIBSEM_SIFT_gs.generate_report_from_xls_registration_summary">generate_report_from_xls_registration_summary</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.generate_report_mill_rate_xlsx" href="#SIFT_gs.FIBSEM_SIFT_gs.generate_report_mill_rate_xlsx">generate_report_mill_rate_xlsx</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.generate_report_transf_matrix_details" href="#SIFT_gs.FIBSEM_SIFT_gs.generate_report_transf_matrix_details">generate_report_transf_matrix_details</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.generate_report_transf_matrix_from_xlsx" href="#SIFT_gs.FIBSEM_SIFT_gs.generate_report_transf_matrix_from_xlsx">generate_report_transf_matrix_from_xlsx</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.geometric" href="#SIFT_gs.FIBSEM_SIFT_gs.geometric">geometric</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.get_min_max_thresholds" href="#SIFT_gs.FIBSEM_SIFT_gs.get_min_max_thresholds">get_min_max_thresholds</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.get_spread" href="#SIFT_gs.FIBSEM_SIFT_gs.get_spread">get_spread</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.get_state" href="#SIFT_gs.FIBSEM_SIFT_gs.get_state">get_state</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.gumbel" href="#SIFT_gs.FIBSEM_SIFT_gs.gumbel">gumbel</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.hypergeometric" href="#SIFT_gs.FIBSEM_SIFT_gs.hypergeometric">hypergeometric</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.kp_to_list" href="#SIFT_gs.FIBSEM_SIFT_gs.kp_to_list">kp_to_list</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.laplace" href="#SIFT_gs.FIBSEM_SIFT_gs.laplace">laplace</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.list_to_kp" href="#SIFT_gs.FIBSEM_SIFT_gs.list_to_kp">list_to_kp</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.logistic" href="#SIFT_gs.FIBSEM_SIFT_gs.logistic">logistic</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.lognormal" href="#SIFT_gs.FIBSEM_SIFT_gs.lognormal">lognormal</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.logseries" href="#SIFT_gs.FIBSEM_SIFT_gs.logseries">logseries</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.multinomial" href="#SIFT_gs.FIBSEM_SIFT_gs.multinomial">multinomial</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.multivariate_normal" href="#SIFT_gs.FIBSEM_SIFT_gs.multivariate_normal">multivariate_normal</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.mutual_information_2d" href="#SIFT_gs.FIBSEM_SIFT_gs.mutual_information_2d">mutual_information_2d</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.mutual_information_2d_cp" href="#SIFT_gs.FIBSEM_SIFT_gs.mutual_information_2d_cp">mutual_information_2d_cp</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.negative_binomial" href="#SIFT_gs.FIBSEM_SIFT_gs.negative_binomial">negative_binomial</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.noncentral_chisquare" href="#SIFT_gs.FIBSEM_SIFT_gs.noncentral_chisquare">noncentral_chisquare</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.noncentral_f" href="#SIFT_gs.FIBSEM_SIFT_gs.noncentral_f">noncentral_f</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.normal" href="#SIFT_gs.FIBSEM_SIFT_gs.normal">normal</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.pareto" href="#SIFT_gs.FIBSEM_SIFT_gs.pareto">pareto</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.permutation" href="#SIFT_gs.FIBSEM_SIFT_gs.permutation">permutation</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.plot_registrtion_quality_xlsx" href="#SIFT_gs.FIBSEM_SIFT_gs.plot_registrtion_quality_xlsx">plot_registrtion_quality_xlsx</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.poisson" href="#SIFT_gs.FIBSEM_SIFT_gs.poisson">poisson</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.power" href="#SIFT_gs.FIBSEM_SIFT_gs.power">power</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.process_transf_matrix" href="#SIFT_gs.FIBSEM_SIFT_gs.process_transf_matrix">process_transf_matrix</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.radial_profile" href="#SIFT_gs.FIBSEM_SIFT_gs.radial_profile">radial_profile</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.radial_profile_select_angles" href="#SIFT_gs.FIBSEM_SIFT_gs.radial_profile_select_angles">radial_profile_select_angles</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.rand" href="#SIFT_gs.FIBSEM_SIFT_gs.rand">rand</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.randint" href="#SIFT_gs.FIBSEM_SIFT_gs.randint">randint</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.randn" href="#SIFT_gs.FIBSEM_SIFT_gs.randn">randn</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.random" href="#SIFT_gs.FIBSEM_SIFT_gs.random">random</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.random_integers" href="#SIFT_gs.FIBSEM_SIFT_gs.random_integers">random_integers</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.random_sample" href="#SIFT_gs.FIBSEM_SIFT_gs.random_sample">random_sample</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.rayleigh" href="#SIFT_gs.FIBSEM_SIFT_gs.rayleigh">rayleigh</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.read_kwargs_xlsx" href="#SIFT_gs.FIBSEM_SIFT_gs.read_kwargs_xlsx">read_kwargs_xlsx</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.read_transformation_matrix_from_xf_file" href="#SIFT_gs.FIBSEM_SIFT_gs.read_transformation_matrix_from_xf_file">read_transformation_matrix_from_xf_file</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.save_data_stack" href="#SIFT_gs.FIBSEM_SIFT_gs.save_data_stack">save_data_stack</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.save_inlens_data" href="#SIFT_gs.FIBSEM_SIFT_gs.save_inlens_data">save_inlens_data</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.set_state" href="#SIFT_gs.FIBSEM_SIFT_gs.set_state">set_state</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.show_eval_box_mrc_stack" href="#SIFT_gs.FIBSEM_SIFT_gs.show_eval_box_mrc_stack">show_eval_box_mrc_stack</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.show_eval_box_tif_stack" href="#SIFT_gs.FIBSEM_SIFT_gs.show_eval_box_tif_stack">show_eval_box_tif_stack</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.shuffle" href="#SIFT_gs.FIBSEM_SIFT_gs.shuffle">shuffle</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.smooth" href="#SIFT_gs.FIBSEM_SIFT_gs.smooth">smooth</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.standard_cauchy" href="#SIFT_gs.FIBSEM_SIFT_gs.standard_cauchy">standard_cauchy</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.standard_exponential" href="#SIFT_gs.FIBSEM_SIFT_gs.standard_exponential">standard_exponential</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.standard_gamma" href="#SIFT_gs.FIBSEM_SIFT_gs.standard_gamma">standard_gamma</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.standard_normal" href="#SIFT_gs.FIBSEM_SIFT_gs.standard_normal">standard_normal</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.standard_t" href="#SIFT_gs.FIBSEM_SIFT_gs.standard_t">standard_t</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.transform_and_save_chunk_of_frames" href="#SIFT_gs.FIBSEM_SIFT_gs.transform_and_save_chunk_of_frames">transform_and_save_chunk_of_frames</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.transform_and_save_frames" href="#SIFT_gs.FIBSEM_SIFT_gs.transform_and_save_frames">transform_and_save_frames</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.transform_chunk_of_frames" href="#SIFT_gs.FIBSEM_SIFT_gs.transform_chunk_of_frames">transform_chunk_of_frames</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.transform_two_chunks" href="#SIFT_gs.FIBSEM_SIFT_gs.transform_two_chunks">transform_two_chunks</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.triangular" href="#SIFT_gs.FIBSEM_SIFT_gs.triangular">triangular</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.uniform" href="#SIFT_gs.FIBSEM_SIFT_gs.uniform">uniform</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.vonmises" href="#SIFT_gs.FIBSEM_SIFT_gs.vonmises">vonmises</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.wald" href="#SIFT_gs.FIBSEM_SIFT_gs.wald">wald</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.weibull" href="#SIFT_gs.FIBSEM_SIFT_gs.weibull">weibull</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.zipf" href="#SIFT_gs.FIBSEM_SIFT_gs.zipf">zipf</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset">FIBSEM_dataset</a></code></h4>
<ul class="">
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.SIFT_evaluation" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.SIFT_evaluation">SIFT_evaluation</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.check_for_nomatch_frames" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.check_for_nomatch_frames">check_for_nomatch_frames</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.convert_raw_data_to_tif_files" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.convert_raw_data_to_tif_files">convert_raw_data_to_tif_files</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.determine_transformations" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.determine_transformations">determine_transformations</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.estimate_SNRs" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.estimate_SNRs">estimate_SNRs</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.evaluate_FIBSEM_statistics" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.evaluate_FIBSEM_statistics">evaluate_FIBSEM_statistics</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.evaluate_ImgB_fractions" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.evaluate_ImgB_fractions">evaluate_ImgB_fractions</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.extract_keypoints" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.extract_keypoints">extract_keypoints</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.process_transformation_matrix" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.process_transformation_matrix">process_transformation_matrix</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.save_parameters" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.save_parameters">save_parameters</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.show_eval_box" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.show_eval_box">show_eval_box</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.transform_and_save" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_dataset.transform_and_save">transform_and_save</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame">FIBSEM_frame</a></code></h4>
<ul class="">
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.RawImageA_8bit_thresholds" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.RawImageA_8bit_thresholds">RawImageA_8bit_thresholds</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.RawImageB_8bit_thresholds" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.RawImageB_8bit_thresholds">RawImageB_8bit_thresholds</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.analyze_SNR_autocorr" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.analyze_SNR_autocorr">analyze_SNR_autocorr</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.analyze_noise_ROIs" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.analyze_noise_ROIs">analyze_noise_ROIs</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.analyze_noise_statistics" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.analyze_noise_statistics">analyze_noise_statistics</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.determine_field_fattening_parameters" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.determine_field_fattening_parameters">determine_field_fattening_parameters</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.display_images" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.display_images">display_images</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.flatten_image" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.flatten_image">flatten_image</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.get_image_min_max" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.get_image_min_max">get_image_min_max</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.print_header" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.print_header">print_header</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.save_images_jpeg" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.save_images_jpeg">save_images_jpeg</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.save_images_tif" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.save_images_tif">save_images_tif</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.save_snapshot" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.save_snapshot">save_snapshot</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.show_eval_box" href="#SIFT_gs.FIBSEM_SIFT_gs.FIBSEM_frame.show_eval_box">show_eval_box</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform" href="#SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform">RegularizedAffineTransform</a></code></h4>
<ul class="">
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.rotation" href="#SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.rotation">rotation</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.scale" href="#SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.scale">scale</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.shear" href="#SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.shear">shear</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.translation" href="#SIFT_gs.FIBSEM_SIFT_gs.RegularizedAffineTransform.translation">translation</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform" href="#SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform">ScaleShiftTransform</a></code></h4>
<ul class="">
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform.estimate" href="#SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform.estimate">estimate</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform.scale" href="#SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform.scale">scale</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform.translation" href="#SIFT_gs.FIBSEM_SIFT_gs.ScaleShiftTransform.translation">translation</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform" href="#SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform">ShiftTransform</a></code></h4>
<ul class="">
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform.estimate" href="#SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform.estimate">estimate</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform.translation" href="#SIFT_gs.FIBSEM_SIFT_gs.ShiftTransform.translation">translation</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform" href="#SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform">XScaleShiftTransform</a></code></h4>
<ul class="">
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.estimate" href="#SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.estimate">estimate</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.print_res" href="#SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.print_res">print_res</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.scale" href="#SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.scale">scale</a></code></li>
<li><code><a title="SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.translation" href="#SIFT_gs.FIBSEM_SIFT_gs.XScaleShiftTransform.translation">translation</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>